请不吝点赞 订阅 转发 打赏支持明镜与点点栏目与人文和自然科学院院士香港理工大学研究生院院长曹建农先生普林斯顿大学教授欢迎您普林斯顿大学教授普林斯顿人工智能创新中心主任王梦迪女士上海市普陀区数据局副局长尹新先生欢迎伊利消费者运营数字化负责人范文竹女士欢迎京东探索研究院院长京东科技人工智能业务负责人HFE Fellow何晓东先生欢迎京东云副总裁京东云LINE4产品研发部负责人龚逸诚先生那说到生成是人工智能作为人工智能领域的一颗璀璨星星它能够模拟人类的创作过程生成前所未有的文本图像音频甚至是视频内容并且在数据分析 模式识别 创意生成等多个维度展现出惊人的能力技术的蓬勃发展也会让很多人感到忧虑大模型是否可控 生成式AI是否可控呢接下来的时间就让我们用热烈的掌声有请普林斯顿大学教授 普林斯顿人工智能创新中心主任王梦迪女士来为我们揭秘探讨大模型时代控制与智能掌声有请大家早上好非常非常荣幸今天应何晓东和院士的邀请来分享就是我们在学术界从大模型角度做了一些学术前沿的研究在大模型时代呢我们今天我想讨论的问题是如何让大模型可控今天正好我第一个讲所以可能我们稍微回顾一下历史首先就是什么是人工智能或者什么是智能这个概念其实是在上个世纪40年代 50年代有非常多的这方面的讨论一个公认的智能的一种定义是首先所有的机器和生物都是智能系统都是控制系统那么什么叫智能呢只要我有一个系统然后有一些不管是生物 是机械 是电子还是什么样的方式能够去了解到这个机器的状态并且反馈给这个机器或者说这个生物体让它维持在一定的状态轨迹上或者简单可以是简单如钟表或者像动物像人类这些都是智能系统本质智能是控制系统然后这个概念最早是来自于1948年出版的控制论然后这个诺贝尔·维纳他是一个美国的数学家他也公认是认为是控制论之父然而当我们讨论控制系统的时候其实越精密的仪器比如钟表或者等等其他的这些机械仪器当它们越精密的时候它们的智能其实越狭窄当我们现在讨论人工智能的时候控制已经不是传统意义上的我可以在任何一个复杂的系统或者复杂的环境里面通过大规模的神经网络去学习环境的信息去学习环境的世界模型并且不断地迭代对于环境的价值函数和在环境里面执行的这样的一个动态决策那么这就是强化学习然后关于强化学习我觉得就是第一次让全世界人都知道强化学习是什么应该是2014年的这个AlphaGo这是一个双智能体对抗的强化学习然后呢在今天强化学习已经到处都是了比如我们如果去玩一个游戏去玩这个王者荣耀那么我们对战的AI就是一个强化学习的算法这个强化学习算法他们也可以互相对战可能如果他们这个训练的好的话可能比人类还要厉害然后机器人巨声智能和自动驾驶这也是我们已经几乎在生活中快要接触到的强化学习然后另外一个我觉得很有意思的是可控和巨变如果我们看2022年人工智能技术最大的突破我想基本上每个人都会认为是2022年底的差GPT但是很有可能差GPT不是那一年最大的突破这是2022年DeepMind在Nature发表一篇论文它用强化学习能够非常精确地把核聚变reactor里面的等粒子场控制在一个非常高的精度而核聚变的控制这被公认为是自动控制最难的问题因为它本质是一个
这被公认为是自动控制能源最难的问题因为它本质是一个混沌的 chaotic system然后当核聚变的控制问题能解决的话它说明我们离清洁可控的核能源非常非常接近了对 然后所有的这些都是强化学习但我们今天要讨论的是大模型那么大模型 或者我们先来说大语言模型大模型首先它可控嘛然后这里我想跟大家推荐一本神书这是Kevin Kelly在1994年整整30年前出版的一本书叫《失控》这本书其实是从控制论开始介绍起并且这本30年前的书现在其实是不管是互联网的从业者还是人工智能学家很多人都读过这本书并且从它里得到启发这本书在30年前就预测了大规模神经网络并且它预测了当有很多很多简单的这些元件组合在一起的时候会有超出这个底层元件不能理解的智能在高等级涌现所以涌现emergence这个词最早来自于30年前的这本书这本书预测了就是后来的科技发展包括神经网络包括涌现等等但这本书最后留了一个开放式的结局它说当这种涌现的智能超越了人类所能控制的范围那时候该怎么办所以这本书预测的这个结局在现在30年以后已经是我们现在的处境了2023年OpenAI发布了 Intelligence Intelligence那么这也就是失控这本书预测的未来的结局那么在这个时间点如何能够保证这样一个AI系统比人类聪明的AI系统还能够为人们所用还能够被套上缰绳这其实就是这本书没有预测到的这也是现在大模型领域最重要的研究问题那么这个问题就是现在被称为super alignmentalignment意思是对齐如何让人工智能让大模型和人类想要的功能人类想要的偏好来对齐那么如何对齐然后现在最前沿的方法当然它远远不是最好的方法现在最前沿的对齐方法是通过人类的反馈去进一步的微调来对齐一个已经预训练好的大模型然后对齐的方式是强化学习它大概分成几步基本上我有一个预训练好的大模型之后我们后面会做这个supervised fine tuning就是进一步用人提供的数据来进行用用户的数据fine tuning然后呢我们需要收集真实用户的这个反馈数据这个反馈数据可以是一个点赞或者可以是一个踩或者这个反馈数据可以是就是我给他几段不同的这个生成的图片或者是语言让用户选择他更喜欢哪个就是这样非常非常简单的在互联网逻辑中并且在这个基础上最后一步是用强化学习算法来微调大模型把大模型的生成过程理解成是一个动态的策略并且在这上面进行微调来更好的向人类的喜好对齐所以这是一个基本的技术思路然后这个算法有非常非常多的变体那么说到就是这种对齐的强化学习算法强化学习算法虽然有很多年的研究但是还有非常多的不足就举一个例子我们先不说大型模型我们就说机器人或者说这种巨深的这种控制问题如果我只是单纯地收集人类数据用普通的方法去训练一个奖励函数然后直接用强化学习的话其实这里面会有好多的地方容易出错一个最常见出错的地方在于我收集的数据并不能真正覆盖我们想要的那个最优策略它对应的概率分布所以这里面会造成一个out of distribution的shift然后这件事情我们在一个机器人的一些场景里面去做了这个测试用最好的对齐强化学习算法会发现这里有一个很大的一个能达到的实际的奖励和真正的理论最优有一个非常大的gap然后为了解决这个问题呢我们之前有一系列工作然后最近的是去年的一个工作我们的解决方案是要做一个嵌套式的双层强化学习我不光想对齐最优策略我还想对齐数据的分布当我同时做这两件事的时候一个接约性的提高所以这个是对齐乔化学习在机器人然后进一步在大语言模型大语言模型的乔化学习和机器人的乔化学习有很多不同的地方有很多就是跟大模型跟transformer model本身一些具体的技术问题要解决然后这里有很多不同场景然后我自己的研究组我们最近在这里面希望能够做出很多不同的工作吧然后比如从对齐角度一个问题就是说我有一个大语言模型我可能要在不同的场景对不同的这个用户的对不同的user group要进行对齐如果我对每一个user group都重新微调这个大模型的话那么我就需要微调很多个模型但我们其实想要的是最好是一个模型它能够同时去解决不同的目标但是从强化学习角来讲这里面有很多技术上的难点所以我们专门设计了一个针对多目标对齐尤其是少数的用户不会被大部分用户就是少部分用户的一些小众的喜好不会被大部分用户直接淹没掉而是能够让强化学习算法同时顾及到不同人的不同的喜好和不同场景的不同目标函数然后就是在不同的一些数据的在不同对应的数据场景里面比如这个对习算法可以有很多很多变化比如说如果我们不是仅仅做一个单智能体的强化学习我们可以引入一个adversarial critic来做这种对抗式的学习
智能体的强化学习我们可以引入一个adversarial critic来做这种对抗式的学习这样也可以提高对齐的效率然后同时我们最近还刚发布了一个在线实时收集数据的在线强化学习对齐算法然后这里有些图吧应该比最好的这个迭代式的DPO效果也能提高是30%到70%之间然后这是对齐然后强化学习在大语言模型除了对齐其实还有其他其他的有很多可以其他的用法比如一个用法叫Speculative Decoding这个概念是2023年的Google Demand提出的这个用法是我不是为了对齐人类了我是为了能够加速大模型的推理因为大模型我每推理一次我基本上是要在整个的模型上做一个forward pass它是很贵的一个可能的解决方案是我们可以把一个大模型和一个小模型组合起来或者说一个大模型很多小模型组合起来那么当我想生成任务的时候我这个推理我让小模型先做然后大模型只要监督就好了如果小模型做的不对大模型就打过去说你重做就像是你要学生干活然后老师带着这个学生然后这样可以节省这个老师的时间那么这件事情呢那么怎么样去协调大模型和小模型什么时候小模型让大模型去监督希望这个整体的suitable的更好这个是叫special decoding然后我们刚刚发布的一个用强化学习来协调大模型和小模型之间的这个生成和间度的这个双向循环然后我们可以把推理速度提高超过两倍多对去解决更复杂的推理甚至是coding甚至是数学定理证明等等等等的问题就是希望能够找到复杂场景那个真正的最优策略它对应的奖励函数叫Q starQ star这个词最早也是就是open AI人讨论了然后漏出来的一个词所以大家现在就用Q-Star来指代这个理想中的有推理能力的最强大模型我们最近一个工作是说如果我从一个已经对齐或已经优化好的模型出发我们其实可以在这个de-coding的层面再去训练一个轻量级的强化学习的Head然后在这个基础上我们可以把一个原来已经对齐的模型对齐到一个新的更复杂的一个奖励函数去寻找一个新任务的这个所谓的Q-Star对所以我们这里说的是大模型大语言模型的可控当我们说控制的时候我们可以理解成我已经有了一个非常强大的一个预训练模型你想象就是我们来了一个一个外星人他非常非常的聪明但是我们需要让他学会怎么按照我们人类想要的方式来做事情解决我们想解决的问题在不同场景不同的离线再线一些各种各样不同的情况下怎么做这件事情这些都是怎么样控制大语言模型的生成那接下来换一个话题接下来就讨论另外一个模型刚才是大语言模型现在我们讨论生成式AI我们一般说生成式AI的时候叫做扩散算法或者扩散模型那么这个模型一般认为是和语言模型并驾齐驱的两个最重要的大模型它们可能结合在一起是能解决最多的是能够最大程度上推进人工智能的发展那么这种生成式AI是否是可控的然后先简单介绍一下这种生成生成式AI它的一个非常非常短的历史这个概念其实来自于2013年然后VE这个模型是用来把高维的数据压缩在低维然后再重新展开那么作为这种压缩模型的一个副产品然后人们很快发现说这个Decoder很有意思因为我可以完全不要这个Encoder我直接在Decoder上输入一个随机的一个高维的向量然后这个Decoder也能生成一个像模像样的图片这是最早它作为一个副产品然后另外一个当时曾经火过很长一段时间的模型叫做这个GANS对抗是GenerativeSero Networks然后对抗是这个GANS最早也是用来做预测做Classification但是同样它因为有一个对抗的结构在里面所以它带了一个这个Generator一个生成器然后同样就是这个生成器发现比Classification更有意思我发现我们可以生成一些现实中没有但虚虚如样所以现在我们说生成式AI的时候基本上是一系列算法能够从噪声变成数据然后可以做视频可以做音频可以做文字可以做很多很多其他应用等等所以这些叫生成式AI所以说我们虽然可以想象,即使我们可以去训练一个非常大的非常非常大的神经网络,直接从噪音变成数据,但你可以想象因为这是一个很复杂,并且它的landscape很崎岖的一个转化过程这个训练就几乎变成了不可能完成的那么扩散模型是完全不一样的扩散模型以及代表的新式的生成式AI完全改变了人工智能学者对生成的定义扩散式模型天然就是一个控制过程我们是需要找到一个控制的可控的状态轨迹从初始的噪音最终生成一个图片这个天然就是控制过程那么这时候我们的神经网络并不是说我要训练一个大神经网络从零直接到最后这是一个unet而是说我要训练一个神经网络它可以调控这个一步一步的从零一直到data的这样一个过程所以我们现在学习的是一个控制过程它的drift function所以换句话说生成式AI天然就是一个控制问题并且我们可以在这样一个基础上让它更加的可控对然后多说两句扩散模型Diffusion Model因为我自己是偏数学出身这个基本上是所有技术学习模型里面数学上最优美的一个因为它本身
这个是基本上是所有基础学习模型里面数学上最优美的一个因为它本质上就来自于我要对一个随机微分方程求逆因为从噪音生成数据是一个很难的问题但是从数据生成噪音是个特别简单的所以它的方法是我先找到一个非常非常简单的从数据变成纯白噪音的一个过程然后用它的这个数学原理去找到它的逆过程然后用训练的方式找到逆过程的这个draft function然后我们组也一直在做一些理论工作我们去年是基本上是证明了关于扩散模型的第一个统计理论并且我们证明了为什么扩散模型即使它从一个非常高维的白噪声出发能够生成在一个隐藏的低维流行上面的数据这也就解释了为什么我们可以从噪声生成图片从生成视频等等因为不管是图片还是视频它们其实都是有一个内在低质的一个隐藏的流行结构然后更进一步就是说我们可以让扩散模型更加的可控如果是从一个数学优化和求解器的角度来想这件事情就是说任何一个优化问题往往我们在想一个优化或者说是一个设计问题的时候我们可能是有一个目标函数然后这个目标函数有一个landscape然后我们要找一个算法可能是这个模拟退火可能是梯度可能是蒙特卡罗我们想说的是我能不能直接生成一个优化问题的最优解这件事情是可能的我们设计了一套通用的解决方案你可以给我任何一个已经训练好的扩散模型它已经是一个可控的过程我们可以在上面加一个额外的控制量它是某一种基于T2的规定guidance它可以同时保证我可以优化任何一个新的目标函数还能够保证我不会损失掉我已经学过的这个数据本身的低维流行结构也就是说我们可以利用扩散模型的泛化能力去从数据里面找到我们想生成的目标的内在的几何结构并且用非常轻量级的方法把任何一个扩散模型它生成我们想生成的新的问题的解那么这件事有什么用呢除了这个生成视频和美图除了这个手机上我们能看到我们经常接触的应用生成式AI其实是可以生成很多很多非常重要的我们想象不到的生物学上的设计工程上的设计等等比如我们普林斯顿我们刚刚成立的这个AI创新中心我们在积极地用扩散模型想去生成电路图的设计然后想去生成芯片的设计 Demand Paper然后因为晶体结构是需要有对称性需要满足很多托普上的逻辑的他们这些晶体结构都是可合成的然后这篇论文还估计如果他们没有用这种生成式模型而是用传统的方式去找的话可能人类科学家需要800年才能把这些所有的晶体结构都找到不是一个人类科学家是所有的人类科学家然后另外一个生成式AI应用式我们组今年年初在Nature Machine Technology上发表的我们发表了一个基于语言模型的MRA的生成模型那么为什么要生成MRA呢首先我们收集了大量的这种MRA的数据跨不同的物种这样我的语言模型就也能够学到这个MRA数据的内在逻辑然后我们在生成的时候我想生成翻译效率更高的MRA那么翻译效率为什么重要呢就任何一个MRA疫苗如果我们把它打一针打到身体里的时候它必须要翻译成蛋白质才能有用那么它多大程度上它是20%翻译成蛋白质还是90%翻译成蛋白质就直接决定了这个疫苗有没有用所以我们找到我们的MRA语言模型生成的MRA我们找了合作的试试验室进行了一个闭环的验证比现在临床上用的最好的MRA疫苗还要效率提高了30%多然后另外一个特别有趣的事情就是我们在讲强化学习强化学习本身可以用来微调扩散模型但是用扩散模型 equation而是直接从数据里面去找到一个机器人系统它内在的几何结构然后生成一个比数据里面更好的一个新的控制机器人的策略这种叫diffusion policydiffusion policy应该已经是所有的机器人领域 巨神智能领域都在用的一个技术对所以最后总结一下就是不管是扩散模型还是大语言模型如何让它可控一是让它安全二是让控制它满足我们具体的要求解决特定的优化问题我觉得这个是大模型通向真正能让我们能用通向AGI的一个必然的一个必经之路好 谢谢大家谢谢谢谢王茂林女士带来的精彩分享请您回席落座稍作休息那近年来随着人工智能和大模型技术的飞速发展边缘计算和纳模型技术呢正在深度融合引领着智联网进入到一个全新的发展阶段可以说这给我们的工作和生活带来了前所未有的变革那接下来的时间呢我们有请到的是欧洲人文和自然科学院院士香港理工大学研究生院院长曹建农先生带来边缘大模型赋能智联网的应用探索与实践的主题分享掌声有请大家早上好
谢谢大家早上好首先感谢京东论坛的邀请然后我今天给大家讲的是大模型如何在边缘可以提供给更多的比如说现在的人工智能物联网靠近我们很多大数据深层边缘的终端怎么能得到更好的应用那么刚刚那个王老师讲的大模型有很多各方面的应用包括可以在控制方面可以生成方面都有很多的应用所以现在我们是一个大模型的时代无论是国外国内我们看见大模型的研发和应用都如火如荼而且是热火朝天那么有各种各样的应用我刚刚已经王老师已经讲了很多了但是跟我们个人当个人或者我们家庭需要更多的应用是什么呢比如说是这种工厂的控制我们家庭里面的人的agent对话或者小朋友的辅导学习那么这个就需要很多人工智能或者是大模型深层模型在我们更靠近我们的这一端来进行研发那么现在为止基本上百分之百的大模型这样的一个训练它需要很多的计算资源那么云上的数据中心大量的GPU提供这样的一个设施来进行训练但是它也有一些局限性那局限性是什么呢大家可以想到就是说不是所有的这种企业这种公司包括我们个人都能access就是得到这样的一些资源因为毕竟资源是受限的第二就是说我们有很多我们自己的这样的数据我们也不愿意说把它上载到云上然后来使用大模型为什么因为大模型可能收集你的数据以后那你的适应性就会受到一些影响对吧那么第二就是你可能要上到云上的时候你需要网络然后你有很大的食言那对于一些实时性非常强的行用比如机器人的控制然后机床的这种实时的监测然后导航无人驾驶那这个食言可能就不能满足我们的要求所以有什么方法呢那实际上大家可以知道在过去差不多十年当中这个边缘计算实际上是一个非常非常热门的话题大概从2021年开始那它是一个网络那么靠近网络的边缘这一端就是靠近我们用户的这一端那么如果我们把云上的计算当时在云上需要的计算能够把它push到网络的边缘这一端靠近我们用户靠近我们产生数据和使用数据的地方那我们就会克服刚刚我讲的这些限制比如说你的数据不需要上载到云上不需要上载中心服务器上然后你的数据大量的数据特别是想一想我们现在工业互联网物联网它有很多很多的传感器产生很大量的数据如果你都通过这么一个网络上载到云端它占用很多网络带宽有很多实验所以如果在靠近网络的边缘这一端来使用它来计算它那你就不需要有这样的大量的实验那么还有一个就是说可能这个物联网因为我们做人工智能物联网的话你的现在是很多成千上万上百万的这样的传感器这样的设备如果你要都把它连到云上它的可扩展性也会受到影响所以边缘计算近年来就成为了一个非常热门的研究研究的一个领域那么过去讲边缘计算是讲计算就是很多大型的这种任务你怎么把它分解 4G包括我们的车包括我们路边的Uni包括现在越来越多的这种边缘的设备边缘服务器都是我们讲的这个边缘计算的一个结点那么因为人工智能的兴起所以这个边缘计算就把这个计算扩展到人工智能模型的训练和推演上所以边缘计算现在就变成一个叫边缘的AI所以过去叫边缘计算现在大家听见叫边缘AI那边缘AI就等于说你可以把一个比较复杂的人工智能模型在云上训练好然后你把它部署到边缘的设备上来那么推演这个推演推理比这个训练所使用的资源要少很多所以你可以做到把人工智能模型放在边缘设备上来推理那么实际上我们应用大模型的时候大多数都是利用实时产生的数据把它Fill到这大模型上然后通过它的推演然后你得到你想要的结果模型实际上已经训练好了那么边缘的AR在过去几年也有很多很多的这种研究很多这样的落地那么随着大模型的兴起那现在就在想那么大模型也是一种特殊的人工智能的模型但是大模型我刚刚也讲了它的数据量它的参数它需要的资源远远超过一般的人工智能模型能不能把大模型
它需要的资源远远超过一般的人工智能模型能不能把大模型也能部署在边缘设备上让我们来使用呢那这个就是我们现在在研究的这么一个话题所以如果刚刚我们在讨论的这种大模型这种基础模型能够部署到边缘设备上我们会有很多现在这种物联网想得到的应用的好处得不到的这样的一些优点都会得到为什么呢因为生成式或者是基础模型大模型它有很强的这种生成能力它有很强的这种理解能力那过去的AI模型它可能只是做一个决策或者做一个简单的任务针对一个特别的任务但是这个大模型它有一个泛化它有一个很朴实的这么一种理解能力所以一般过去的AI模型做不到的那么通过大模型它就能做到比如举个例子过去说大家都想来观察或者监察我们人的这种行为我们的健康那么你要专门做一个AI的模型你去做它可能是可以但是它泛化能力很少比如说我观察你的数据观察你的行为但是我把它布置到另外一个家庭另外一个人他可能就很难去学习到但是大模型为什么它是通过我们所有人很多人的行为来学习到的那么它因为又有很多大量的数据的分析的能力所以它对我们数据的这种观察一般复杂的这种模式的理解和推理能力也是一般的人工智能模型做不到的所以如果我们能够把大模型放到边缘上来做那我们就会对现在的物联网实时操作普通家庭需要的应用带来很多变革性的变化那么我们能不能做到呢因为很需要去做但我们能不能做到把大模型部署到边缘设备上来呢所以现在第一个大家想到的就是大模型如果太大你放到一个边缘设备边缘设备毕竟是它的计算资源很少的很受限的那么大家就会想到那我能不能把这个大模型把它变小现在也说就在我们的手机上现在手机上放上大模型放上这种生成模型已经也有了但是它是怎么做到了呢它把模型压缩第一个所以它技术就是做模型压缩那模型压缩也有几种方式它第一种方式就是说我就剪裁这种很大量大规模的神经元网络深度学习的网络里面的不需要等等的这种权重我就把它删掉尽量的 没用的我把它删掉这种叫第一种 叫剪裁第二种呢就是说知识的增留就是说大模型我把它删掉这个叫针流就是从一个我们所谓的老师型的模型把它变成一个学生型的模型这是第二种那么第三种呢就是说量化量化什么就是这种深度学习大模型里面它有很多这种参数参数的表示的精度比如说我可以用32位来表度我把它压缩到4位那这个精度把它压小压小以后整个的大模型的尺寸size就可以成倍成倍地减少那这样做完以后我现在我把一个大模型可以把它boost到一个中端或者边缘的一个设备上来做这是第一种方法第二种方法就是说那我不要大不需要这样的大模型但是我觉得小模型可以不可以小模型就是特别就是特定的一个任务然后也是通过大数据训练但是数据可能没有那么大量然后的参数也没有上千万亿上百万亿这样的十几亿或者就可以了那么这个小模型比如说出现了马克思的这个FiveStreet它的小模型大概只有3.8就是38亿这样的一个参数但是呢它取得的效果呢甚至好过一般的这样的一个大模型因为它是特地就是说在一个单个的靠近我们终端上比如我们手机上能不能部署一个大模型那么虽然大家可以去做我估计以后可能也会越来越多的这样的这种应用会出现但是它毕竟还是不太够的第一就是说大家可以看见就是说这张表里面显示的就是说NAMA2的这样的一个不同尺寸的不同参数的这样一个模型需要占到存储的需求和我们现有的这种边缘设备上的存储的这样的一个能力来比的话那么实际上边缘设备的存储的能力远远不够来存这样一个既是压缩过的或者训练好的小模型第二就是说如果你不是特别来开发这样的模型的话你就是把一般大模型压缩来做的话也是很难把它放到一个单个的一个这种边缘设备上来的那么还有一种什么方法呢那就是说那我们能不能用协作式的边缘计算就是说你一个节点不够但实际上我们平时的这种场景当中我们有各种各样的这种边缘设备你在家里面你可能你有这种你的家电设备每个家电设备可能就是一个边缘设备它也有这种计算能力比如说你还有你的车然后在工厂里你有机床有车然后你还特别自己来设计制造这么一个边缘终端接点那么如果这些边缘终端
如果这些边缘中端或者这些节点能够互相的协作把他们的资源共享出来然后共同的来完成这种大模型的推理任务那实际上效果会更好呢所以这个就是我们提出来的一个叫协作式边缘计算这也是我们去年在香港拿到一个很大的项目就几间大学一起合作来做这么一个系统这么一个系统就是说你已经有了不同的这样一个边缘的设备和边缘的系统然后你怎么把它能够通过网络连起来然后把它们上面的资源能够抽象起来建成一个可共享的资源池然后你把一个大型的模型把它分割分割成小的任务把它调度到不同的这样协作室的边缘设备上来能不能达到我们想要的结果所以通过实践实际上也有很多学术界的研究通过实践觉得这是一种很可行的方法那么具体你是怎么要去做呢就是我们就设计了这么一个专门针对于协作式边缘计算来做的这么一个大模型的部署调度的这么一个软件系统我们把它叫做Edge Shut这个边缘ShutShut实际上就是你把一个大的东西把它分成一个一个小的东西叫Shut它也可以考虑多个边缘节点它甚至可以考虑你如果有云的网络的存在我可以把云上的资源也能融入进来你如果有终端你有手机我也可以把终端融入进来就等于说你有一个易购的资源的各种各样的设备我都放在一起来进行调度然后它还有一个方面就是说根据我已有的这样易购的资源然后我来把一个给定的模型我把它来分割用最优的方法来分割那么就是存在两个优化算法一个就是说你怎么分割它第二分割好的这个任务怎么把它部署到你已有的这个有易购的资源的这种边缘甚至云的设备上去做它所以我们把它叫做EdgeShot然后这个平台还有一个好处就是说它保证在分割的时候分割的时候数据Embedding的这一层是一定要落在当地来做的为什么它不愿意把这个数据把它泄露出去就是我有数据隐私的这一部分我保证我不会把它分配到其他的就是不是我能控制的这种设备上所以这个你也可以根据你的适应化要求根据你实时化要求来动态的比如说我们做的还是一个Lama2的这么一个大模型Lama2它实际上有三种版本是根据它的参数它最小的那个就是说有70亿个这样的参数第二就是说130亿然后就是700亿这样一个参数这样一个full scale的这样一个大模型然后你根据你现有的已有的这种边缘设备或者终端设备或者有云设备的不同的配置你来决定你要用哪一个这个版本的因为你参数少的版本你自然的性能就不好或者你的精确度也不好所以我们可以根据它不同的配置来决定你用哪个版本然后同时呢我刚刚也讲了就是说你有什么样的这种设备如果只有一个单个节点的设备或者你是有多个边缘节点你有十个五十个一百个的我们把它第二种第三种就是说你不单有边缘节点你可以甚至有连到云上去的云服务器这个时候你能不能也能充分运营到云的这样的一个资源那么通过这种不同的组合然后我们能可以实时的动态来调整你用什么样的资源然后达到你想要的这样一个大模型的需求然后你就可以看见一个就是说用我们EdgeShot来做的它可以通过把它部署在多个这样的一个节点还有一个就是单个节点所以你们看见左手左手这边它可能用了10个节点你看到速度就快很多那边那个就是用单个节点单个节点的速度就很慢精度也会小很多所以就是说你可以实时的动态的来调控根据你的资源你用什么样的版本你的性能是什么样子那么在这个上面在这个EdgeShot的平台上我们可以开发各种各种不同的这种大模型的应用其中有一个应用我们把它叫做个性化的聊天聊天的这种Chatbot那我们之前大家都知道Chatbot非常有用对吧对我们的教育我们健康比如说健康的咨询各种各样的方面都有很多的应用那么我们之前做的工作实际上是用大模型来做什么就是说聊天的Chatbot它跟你聊天的时候通过你的对话 讲话它可以知道你的情感然后通过你的情感可以知道你的性格就你的personality然后根据personality它来产生相对的这种反馈跟你对话那过去我们是怎么做呢就是说这个用户输入一段话或者对话进来以后我们用大模型来进行分析来推理出来这个人现在是一个什么样的情感在通过
来进行分析来推理出来这个人现在是一个什么样的情感再通过那种情感再用大方形来推理出他是一个什么样的性格这个人是一个很内向还是一个很外向或什么样的性格然后根据这个性格第二部分我们再用这种数据局一和实时的数据来做微调来产生这样一个反馈比如说如果一个心情不好的人跟你说话如果你这个聊天机论他不知道他的性格是什么他内向是什么他可能简单就回答一句话哎呀你能不能能不能正面一点啊你不要老这么悲观啊但是你知道对有一些有这种忧郁啊depression这样的人你说这种话一点用都没有的所以呢他要detect你有这样的这样一个这种emotion的问题的时候他的回答就会产生一种像咨询师的这样的一个回答给你那么这么一个chairboard像我们在学校也是给学生去做学生现在现在的学生有很多很多这样的情绪问题一有压力他们就有很多情绪问题那学校虽然有咨询师但是他们可能应付不过来所以学校就要我们开发一个Chapel那么在这样的情况下我们比如说用这个大模型来做是可以的但是如果我们想到以后能不能把它变成一个更靠近个人的每个学生PC上的或者手机上的版本呢所以我们就刚刚用了我们的EdgeShark的这个版本这样把刚刚我讲的这个Chapel有个性化的这种个性的这种确报把它放到边缘设备上来做所以这个是我们在上面去做的那如果放在你的家里或者你的房间你的手机上你的数据是不会泄露出去的所以他的隐私是很好第二而且他的回复的速度也会很快而且你不需要连线你不需要有网络的这样连线你offline也可以去做How are you doing today?所以这是一个对话哦,所有东西都掉了,我的肺炎正在发生,我的猫跑了,外面的天气简直是恐慌。他就在分析啊,他通过他讲的这句话来分析他的情感是什么。Oh no, I'm so sorry to hear that.It sounds like you're going through a tough time.Let me see if I can help you with any of these issues.Have you tried any arthritis-friendly exercises or remedies?And have you checked the local shelters for your missing cat?As for the weather, well, at least it's not as bad as it could be, right?就是说他通过这个分析他能检测到用大模型来检测到你现在状态是什么你是一个什么样的性格然后能产生这么一个对话那么当然我刚讲这种Chad Ball不光是用在这种这种简单的对话上面实际上有各种各样的你比如说这种在家里面你可以跟这个Chad Ball的说话说我现在要睡觉了你们把我这个所有的这个各种家电给我调好调到我最希望的速度 care care因为他一直跟着你他一直跟着你这样的训练以后你的所有的你所有的跟他的对话东西都成为再训练他让他更加懂得你所以长期的来说他可以成为你的一个个人助理实际上个人助理我觉得是未来可能大模型发展的一个非常非常重要的一个阶段但是大家知道个人助理是很隐私的东西最好能放在靠近我们身边的这样的设备上这也是为什么边缘的边缘的大模型是非常重要的一个研究当然就是说我刚刚讲的这些都正在研究当中的这么一个阶段虽然可以推出一些简单的产品但实际上还有很多的这种挑战性的工作最大的挑战性工作还是我刚刚讲的在边缘设备上受限的这么一个资源那么这个你虽然刚刚讲你用模型压缩你用这种小模型它可能是低或者就是进度不够或者第二小模型的开发也是需要很多人力和资源去做的也不是这么很简单能做出来的那么第二个就是说像刚刚王老师也说了就是说你如果已经有一个大模型然后你这个已经把它布置在你的这个设备上了但是你每天产生新的数据你要学新的知识的时候你怎么去做就是持续性地去学习那么如果你是说我用这种微调Find Tuning Tuning tuning你是做不到的所以你在这方面你有没有其他方法那现在出现一个叫RIG就叫Retrieval Augmented Generation它实际上就是说不是说每一次你有新的知识新的这种需求的时候我就重新训练一遍重新全规模的这种微调一遍它是一种新的一种方式就是说你每来产生你问一个问题或者有一个输入的时候他去查询他查询完以后产生一个新的答案然后把这新的答案作为这个答案告诉你但是他每次查询这个时候也作为你的微调的一部分所以这个可能也是未来一个可以解决的一个方案那么后面还有一个当然这不光是针对边缘大模型对所有大模型就是它产生的这种这种结论或者这种结果它是不确定的你输入稍微改一改那它后面出的结果也可能不是不太一样所以你怎么可控它也是刚刚王老师讲的你怎么去可以控制它那么现在我们正在做的还有哪些一些比较重要的一些研究方面的方向呢那么刚刚就我讲的就是说在协同式的边缘的大模型的这种推理上实际上还有很多优化的东西可以去做比如说我有很多我不光是十个五十个我也上百个上千个以后整个城市的所有边缘的这种设备能不能连起来那连起来这些边缘设备共享资源的时候它的资源的管理是怎么去做是集中式的是分布
请不吝点赞 订阅 转发 打赏支持明镜与点点栏目与人文和自然科学院院士香港理工大学研究生院院长曹建农先生普林斯顿大学教授欢迎您普林斯顿大学教授普林斯顿人工智能创新中心主任王梦迪女士上海市普陀区数据局副局长尹新先生欢迎伊利消费者运营数字化负责人范文竹女士欢迎京东探索研究院院长京东科技人工智能业务负责人HFE Fellow何晓东先生欢迎京东云副总裁京东云LINE4产品研发部负责人龚逸诚先生那说到生成是人工智能作为人工智能领域的一颗璀璨星星它能够模拟人类的创作过程生成前所未有的文本图像音频甚至是视频内容并且在数据分析 模式识别 创意生成等多个维度展现出惊人的能力技术的蓬勃发展也会让很多人感到忧虑大模型是否可控 生成式AI是否可控呢接下来的时间就让我们用热烈的掌声有请普林斯顿大学教授 普林斯顿人工智能创新中心主任王梦迪女士来为我们揭秘探讨大模型时代控制与智能掌声有请大家早上好非常非常荣幸今天应何晓东和院士的邀请来分享就是我们在学术界从大模型角度做了一些学术前沿的研究在大模型时代呢我们今天我想讨论的问题是如何让大模型可控今天正好我第一个讲所以可能我们稍微回顾一下历史首先就是什么是人工智能或者什么是智能这个概念其实是在上个世纪40年代 50年代有非常多的这方面的讨论一个公认的智能的一种定义是首先所有的机器和生物都是智能系统都是控制系统那么什么叫智能呢只要我有一个系统然后有一些不管是生物 是机械 是电子还是什么样的方式能够去了解到这个机器的状态并且反馈给这个机器或者说这个生物体让它维持在一定的状态轨迹上或者简单可以是简单如钟表或者像动物像人类这些都是智能系统本质智能是控制系统然后这个概念最早是来自于1948年出版的控制论然后这个诺贝尔·维纳他是一个美国的数学家他也公认是认为是控制论之父然而当我们讨论控制系统的时候其实越精密的仪器比如钟表或者等等其他的这些机械仪器当它们越精密的时候它们的智能其实越狭窄当我们现在讨论人工智能的时候控制已经不是传统意义上的我可以在任何一个复杂的系统或者复杂的环境里面通过大规模的神经网络去学习环境的信息去学习环境的世界模型并且不断地迭代对于环境的价值函数和在环境里面执行的这样的一个动态决策那么这就是强化学习然后关于强化学习我觉得就是第一次让全世界人都知道强化学习是什么应该是2014年的这个AlphaGo这是一个双智能体对抗的强化学习然后呢在今天强化学习已经到处都是了比如我们如果去玩一个游戏去玩这个王者荣耀那么我们对战的AI就是一个强化学习的算法这个强化学习算法他们也可以互相对战可能如果他们这个训练的好的话可能比人类还要厉害然后机器人巨声智能和自动驾驶这也是我们已经几乎在生活中快要接触到的强化学习然后另外一个我觉得很有意思的是可控和巨变如果我们看2022年人工智能技术最大的突破我想基本上每个人都会认为是2022年底的差GPT但是很有可能差GPT不是那一年最大的突破这是2022年DeepMind在Nature发表一篇论文它用强化学习能够非常精确地把核聚变reactor里面的等粒子场控制在一个非常高的精度而核聚变的控制这被公认为是自动控制最难的问题因为它本质是一个
这被公认为是自动控制能源最难的问题因为它本质是一个混沌的 chaotic system然后当核聚变的控制问题能解决的话它说明我们离清洁可控的核能源非常非常接近了对 然后所有的这些都是强化学习但我们今天要讨论的是大模型那么大模型 或者我们先来说大语言模型大模型首先它可控嘛然后这里我想跟大家推荐一本神书这是Kevin Kelly在1994年整整30年前出版的一本书叫《失控》这本书其实是从控制论开始介绍起并且这本30年前的书现在其实是不管是互联网的从业者还是人工智能学家很多人都读过这本书并且从它里得到启发这本书在30年前就预测了大规模神经网络并且它预测了当有很多很多简单的这些元件组合在一起的时候会有超出这个底层元件不能理解的智能在高等级涌现所以涌现emergence这个词最早来自于30年前的这本书这本书预测了就是后来的科技发展包括神经网络包括涌现等等但这本书最后留了一个开放式的结局它说当这种涌现的智能超越了人类所能控制的范围那时候该怎么办所以这本书预测的这个结局在现在30年以后已经是我们现在的处境了2023年OpenAI发布了 Intelligence Intelligence那么这也就是失控这本书预测的未来的结局那么在这个时间点如何能够保证这样一个AI系统比人类聪明的AI系统还能够为人们所用还能够被套上缰绳这其实就是这本书没有预测到的这也是现在大模型领域最重要的研究问题那么这个问题就是现在被称为super alignmentalignment意思是对齐如何让人工智能让大模型和人类想要的功能人类想要的偏好来对齐那么如何对齐然后现在最前沿的方法当然它远远不是最好的方法现在最前沿的对齐方法是通过人类的反馈去进一步的微调来对齐一个已经预训练好的大模型然后对齐的方式是强化学习它大概分成几步基本上我有一个预训练好的大模型之后我们后面会做这个supervised fine tuning就是进一步用人提供的数据来进行用用户的数据fine tuning然后呢我们需要收集真实用户的这个反馈数据这个反馈数据可以是一个点赞或者可以是一个踩或者这个反馈数据可以是就是我给他几段不同的这个生成的图片或者是语言让用户选择他更喜欢哪个就是这样非常非常简单的在互联网逻辑中并且在这个基础上最后一步是用强化学习算法来微调大模型把大模型的生成过程理解成是一个动态的策略并且在这上面进行微调来更好的向人类的喜好对齐所以这是一个基本的技术思路然后这个算法有非常非常多的变体那么说到就是这种对齐的强化学习算法强化学习算法虽然有很多年的研究但是还有非常多的不足就举一个例子我们先不说大型模型我们就说机器人或者说这种巨深的这种控制问题如果我只是单纯地收集人类数据用普通的方法去训练一个奖励函数然后直接用强化学习的话其实这里面会有好多的地方容易出错一个最常见出错的地方在于我收集的数据并不能真正覆盖我们想要的那个最优策略它对应的概率分布所以这里面会造成一个out of distribution的shift然后这件事情我们在一个机器人的一些场景里面去做了这个测试用最好的对齐强化学习算法会发现这里有一个很大的一个能达到的实际的奖励和真正的理论最优有一个非常大的gap然后为了解决这个问题呢我们之前有一系列工作然后最近的是去年的一个工作我们的解决方案是要做一个嵌套式的双层强化学习我不光想对齐最优策略我还想对齐数据的分布当我同时做这两件事的时候一个接约性的提高所以这个是对齐乔化学习在机器人然后进一步在大语言模型大语言模型的乔化学习和机器人的乔化学习有很多不同的地方有很多就是跟大模型跟transformer model本身一些具体的技术问题要解决然后这里有很多不同场景然后我自己的研究组我们最近在这里面希望能够做出很多不同的工作吧然后比如从对齐角度一个问题就是说我有一个大语言模型我可能要在不同的场景对不同的这个用户的对不同的user group要进行对齐如果我对每一个user group都重新微调这个大模型的话那么我就需要微调很多个模型但我们其实想要的是最好是一个模型它能够同时去解决不同的目标但是从强化学习角来讲这里面有很多技术上的难点所以我们专门设计了一个针对多目标对齐尤其是少数的用户不会被大部分用户就是少部分用户的一些小众的喜好不会被大部分用户直接淹没掉而是能够让强化学习算法同时顾及到不同人的不同的喜好和不同场景的不同目标函数然后就是在不同的一些数据的在不同对应的数据场景里面比如这个对习算法可以有很多很多变化比如说如果我们不是仅仅做一个单智能体的强化学习我们可以引入一个adversarial critic来做这种对抗式的学习
智能体的强化学习我们可以引入一个adversarial critic来做这种对抗式的学习这样也可以提高对齐的效率然后同时我们最近还刚发布了一个在线实时收集数据的在线强化学习对齐算法然后这里有些图吧应该比最好的这个迭代式的DPO效果也能提高是30%到70%之间然后这是对齐然后强化学习在大语言模型除了对齐其实还有其他其他的有很多可以其他的用法比如一个用法叫Speculative Decoding这个概念是2023年的Google Demand提出的这个用法是我不是为了对齐人类了我是为了能够加速大模型的推理因为大模型我每推理一次我基本上是要在整个的模型上做一个forward pass它是很贵的一个可能的解决方案是我们可以把一个大模型和一个小模型组合起来或者说一个大模型很多小模型组合起来那么当我想生成任务的时候我这个推理我让小模型先做然后大模型只要监督就好了如果小模型做的不对大模型就打过去说你重做就像是你要学生干活然后老师带着这个学生然后这样可以节省这个老师的时间那么这件事情呢那么怎么样去协调大模型和小模型什么时候小模型让大模型去监督希望这个整体的suitable的更好这个是叫special decoding然后我们刚刚发布的一个用强化学习来协调大模型和小模型之间的这个生成和间度的这个双向循环然后我们可以把推理速度提高超过两倍多对去解决更复杂的推理甚至是coding甚至是数学定理证明等等等等的问题就是希望能够找到复杂场景那个真正的最优策略它对应的奖励函数叫Q starQ star这个词最早也是就是open AI人讨论了然后漏出来的一个词所以大家现在就用Q-Star来指代这个理想中的有推理能力的最强大模型我们最近一个工作是说如果我从一个已经对齐或已经优化好的模型出发我们其实可以在这个de-coding的层面再去训练一个轻量级的强化学习的Head然后在这个基础上我们可以把一个原来已经对齐的模型对齐到一个新的更复杂的一个奖励函数去寻找一个新任务的这个所谓的Q-Star对所以我们这里说的是大模型大语言模型的可控当我们说控制的时候我们可以理解成我已经有了一个非常强大的一个预训练模型你想象就是我们来了一个一个外星人他非常非常的聪明但是我们需要让他学会怎么按照我们人类想要的方式来做事情解决我们想解决的问题在不同场景不同的离线再线一些各种各样不同的情况下怎么做这件事情这些都是怎么样控制大语言模型的生成那接下来换一个话题接下来就讨论另外一个模型刚才是大语言模型现在我们讨论生成式AI我们一般说生成式AI的时候叫做扩散算法或者扩散模型那么这个模型一般认为是和语言模型并驾齐驱的两个最重要的大模型它们可能结合在一起是能解决最多的是能够最大程度上推进人工智能的发展那么这种生成式AI是否是可控的然后先简单介绍一下这种生成生成式AI它的一个非常非常短的历史这个概念其实来自于2013年然后VE这个模型是用来把高维的数据压缩在低维然后再重新展开那么作为这种压缩模型的一个副产品然后人们很快发现说这个Decoder很有意思因为我可以完全不要这个Encoder我直接在Decoder上输入一个随机的一个高维的向量然后这个Decoder也能生成一个像模像样的图片这是最早它作为一个副产品然后另外一个当时曾经火过很长一段时间的模型叫做这个GANS对抗是GenerativeSero Networks然后对抗是这个GANS最早也是用来做预测做Classification但是同样它因为有一个对抗的结构在里面所以它带了一个这个Generator一个生成器然后同样就是这个生成器发现比Classification更有意思我发现我们可以生成一些现实中没有但虚虚如样所以现在我们说生成式AI的时候基本上是一系列算法能够从噪声变成数据然后可以做视频可以做音频可以做文字可以做很多很多其他应用等等所以这些叫生成式AI所以说我们虽然可以想象,即使我们可以去训练一个非常大的非常非常大的神经网络,直接从噪音变成数据,但你可以想象因为这是一个很复杂,并且它的landscape很崎岖的一个转化过程这个训练就几乎变成了不可能完成的那么扩散模型是完全不一样的扩散模型以及代表的新式的生成式AI完全改变了人工智能学者对生成的定义扩散式模型天然就是一个控制过程我们是需要找到一个控制的可控的状态轨迹从初始的噪音最终生成一个图片这个天然就是控制过程那么这时候我们的神经网络并不是说我要训练一个大神经网络从零直接到最后这是一个unet而是说我要训练一个神经网络它可以调控这个一步一步的从零一直到data的这样一个过程所以我们现在学习的是一个控制过程它的drift function所以换句话说生成式AI天然就是一个控制问题并且我们可以在这样一个基础上让它更加的可控对然后多说两句扩散模型Diffusion Model因为我自己是偏数学出身这个基本上是所有技术学习模型里面数学上最优美的一个因为它本身
这个是基本上是所有基础学习模型里面数学上最优美的一个因为它本质上就来自于我要对一个随机微分方程求逆因为从噪音生成数据是一个很难的问题但是从数据生成噪音是个特别简单的所以它的方法是我先找到一个非常非常简单的从数据变成纯白噪音的一个过程然后用它的这个数学原理去找到它的逆过程然后用训练的方式找到逆过程的这个draft function然后我们组也一直在做一些理论工作我们去年是基本上是证明了关于扩散模型的第一个统计理论并且我们证明了为什么扩散模型即使它从一个非常高维的白噪声出发能够生成在一个隐藏的低维流行上面的数据这也就解释了为什么我们可以从噪声生成图片从生成视频等等因为不管是图片还是视频它们其实都是有一个内在低质的一个隐藏的流行结构然后更进一步就是说我们可以让扩散模型更加的可控如果是从一个数学优化和求解器的角度来想这件事情就是说任何一个优化问题往往我们在想一个优化或者说是一个设计问题的时候我们可能是有一个目标函数然后这个目标函数有一个landscape然后我们要找一个算法可能是这个模拟退火可能是梯度可能是蒙特卡罗我们想说的是我能不能直接生成一个优化问题的最优解这件事情是可能的我们设计了一套通用的解决方案你可以给我任何一个已经训练好的扩散模型它已经是一个可控的过程我们可以在上面加一个额外的控制量它是某一种基于T2的规定guidance它可以同时保证我可以优化任何一个新的目标函数还能够保证我不会损失掉我已经学过的这个数据本身的低维流行结构也就是说我们可以利用扩散模型的泛化能力去从数据里面找到我们想生成的目标的内在的几何结构并且用非常轻量级的方法把任何一个扩散模型它生成我们想生成的新的问题的解那么这件事有什么用呢除了这个生成视频和美图除了这个手机上我们能看到我们经常接触的应用生成式AI其实是可以生成很多很多非常重要的我们想象不到的生物学上的设计工程上的设计等等比如我们普林斯顿我们刚刚成立的这个AI创新中心我们在积极地用扩散模型想去生成电路图的设计然后想去生成芯片的设计 Demand Paper然后因为晶体结构是需要有对称性需要满足很多托普上的逻辑的他们这些晶体结构都是可合成的然后这篇论文还估计如果他们没有用这种生成式模型而是用传统的方式去找的话可能人类科学家需要800年才能把这些所有的晶体结构都找到不是一个人类科学家是所有的人类科学家然后另外一个生成式AI应用式我们组今年年初在Nature Machine Technology上发表的我们发表了一个基于语言模型的MRA的生成模型那么为什么要生成MRA呢首先我们收集了大量的这种MRA的数据跨不同的物种这样我的语言模型就也能够学到这个MRA数据的内在逻辑然后我们在生成的时候我想生成翻译效率更高的MRA那么翻译效率为什么重要呢就任何一个MRA疫苗如果我们把它打一针打到身体里的时候它必须要翻译成蛋白质才能有用那么它多大程度上它是20%翻译成蛋白质还是90%翻译成蛋白质就直接决定了这个疫苗有没有用所以我们找到我们的MRA语言模型生成的MRA我们找了合作的试试验室进行了一个闭环的验证比现在临床上用的最好的MRA疫苗还要效率提高了30%多然后另外一个特别有趣的事情就是我们在讲强化学习强化学习本身可以用来微调扩散模型但是用扩散模型 equation而是直接从数据里面去找到一个机器人系统它内在的几何结构然后生成一个比数据里面更好的一个新的控制机器人的策略这种叫diffusion policydiffusion policy应该已经是所有的机器人领域 巨神智能领域都在用的一个技术对所以最后总结一下就是不管是扩散模型还是大语言模型如何让它可控一是让它安全二是让控制它满足我们具体的要求解决特定的优化问题我觉得这个是大模型通向真正能让我们能用通向AGI的一个必然的一个必经之路好 谢谢大家谢谢谢谢王茂林女士带来的精彩分享请您回席落座稍作休息那近年来随着人工智能和大模型技术的飞速发展边缘计算和纳模型技术呢正在深度融合引领着智联网进入到一个全新的发展阶段可以说这给我们的工作和生活带来了前所未有的变革那接下来的时间呢我们有请到的是欧洲人文和自然科学院院士香港理工大学研究生院院长曹建农先生带来边缘大模型赋能智联网的应用探索与实践的主题分享掌声有请大家早上好
谢谢大家早上好首先感谢京东论坛的邀请然后我今天给大家讲的是大模型如何在边缘可以提供给更多的比如说现在的人工智能物联网靠近我们很多大数据深层边缘的终端怎么能得到更好的应用那么刚刚那个王老师讲的大模型有很多各方面的应用包括可以在控制方面可以生成方面都有很多的应用所以现在我们是一个大模型的时代无论是国外国内我们看见大模型的研发和应用都如火如荼而且是热火朝天那么有各种各样的应用我刚刚已经王老师已经讲了很多了但是跟我们个人当个人或者我们家庭需要更多的应用是什么呢比如说是这种工厂的控制我们家庭里面的人的agent对话或者小朋友的辅导学习那么这个就需要很多人工智能或者是大模型深层模型在我们更靠近我们的这一端来进行研发那么现在为止基本上百分之百的大模型这样的一个训练它需要很多的计算资源那么云上的数据中心大量的GPU提供这样的一个设施来进行训练但是它也有一些局限性那局限性是什么呢大家可以想到就是说不是所有的这种企业这种公司包括我们个人都能access就是得到这样的一些资源因为毕竟资源是受限的第二就是说我们有很多我们自己的这样的数据我们也不愿意说把它上载到云上然后来使用大模型为什么因为大模型可能收集你的数据以后那你的适应性就会受到一些影响对吧那么第二就是你可能要上到云上的时候你需要网络然后你有很大的食言那对于一些实时性非常强的行用比如机器人的控制然后机床的这种实时的监测然后导航无人驾驶那这个食言可能就不能满足我们的要求所以有什么方法呢那实际上大家可以知道在过去差不多十年当中这个边缘计算实际上是一个非常非常热门的话题大概从2021年开始那它是一个网络那么靠近网络的边缘这一端就是靠近我们用户的这一端那么如果我们把云上的计算当时在云上需要的计算能够把它push到网络的边缘这一端靠近我们用户靠近我们产生数据和使用数据的地方那我们就会克服刚刚我讲的这些限制比如说你的数据不需要上载到云上不需要上载中心服务器上然后你的数据大量的数据特别是想一想我们现在工业互联网物联网它有很多很多的传感器产生很大量的数据如果你都通过这么一个网络上载到云端它占用很多网络带宽有很多实验所以如果在靠近网络的边缘这一端来使用它来计算它那你就不需要有这样的大量的实验那么还有一个就是说可能这个物联网因为我们做人工智能物联网的话你的现在是很多成千上万上百万的这样的传感器这样的设备如果你要都把它连到云上它的可扩展性也会受到影响所以边缘计算近年来就成为了一个非常热门的研究研究的一个领域那么过去讲边缘计算是讲计算就是很多大型的这种任务你怎么把它分解 4G包括我们的车包括我们路边的Uni包括现在越来越多的这种边缘的设备边缘服务器都是我们讲的这个边缘计算的一个结点那么因为人工智能的兴起所以这个边缘计算就把这个计算扩展到人工智能模型的训练和推演上所以边缘计算现在就变成一个叫边缘的AI所以过去叫边缘计算现在大家听见叫边缘AI那边缘AI就等于说你可以把一个比较复杂的人工智能模型在云上训练好然后你把它部署到边缘的设备上来那么推演这个推演推理比这个训练所使用的资源要少很多所以你可以做到把人工智能模型放在边缘设备上来推理那么实际上我们应用大模型的时候大多数都是利用实时产生的数据把它Fill到这大模型上然后通过它的推演然后你得到你想要的结果模型实际上已经训练好了那么边缘的AR在过去几年也有很多很多的这种研究很多这样的落地那么随着大模型的兴起那现在就在想那么大模型也是一种特殊的人工智能的模型但是大模型我刚刚也讲了它的数据量它的参数它需要的资源远远超过一般的人工智能模型能不能把大模型
它需要的资源远远超过一般的人工智能模型能不能把大模型也能部署在边缘设备上让我们来使用呢那这个就是我们现在在研究的这么一个话题所以如果刚刚我们在讨论的这种大模型这种基础模型能够部署到边缘设备上我们会有很多现在这种物联网想得到的应用的好处得不到的这样的一些优点都会得到为什么呢因为生成式或者是基础模型大模型它有很强的这种生成能力它有很强的这种理解能力那过去的AI模型它可能只是做一个决策或者做一个简单的任务针对一个特别的任务但是这个大模型它有一个泛化它有一个很朴实的这么一种理解能力所以一般过去的AI模型做不到的那么通过大模型它就能做到比如举个例子过去说大家都想来观察或者监察我们人的这种行为我们的健康那么你要专门做一个AI的模型你去做它可能是可以但是它泛化能力很少比如说我观察你的数据观察你的行为但是我把它布置到另外一个家庭另外一个人他可能就很难去学习到但是大模型为什么它是通过我们所有人很多人的行为来学习到的那么它因为又有很多大量的数据的分析的能力所以它对我们数据的这种观察一般复杂的这种模式的理解和推理能力也是一般的人工智能模型做不到的所以如果我们能够把大模型放到边缘上来做那我们就会对现在的物联网实时操作普通家庭需要的应用带来很多变革性的变化那么我们能不能做到呢因为很需要去做但我们能不能做到把大模型部署到边缘设备上来呢所以现在第一个大家想到的就是大模型如果太大你放到一个边缘设备边缘设备毕竟是它的计算资源很少的很受限的那么大家就会想到那我能不能把这个大模型把它变小现在也说就在我们的手机上现在手机上放上大模型放上这种生成模型已经也有了但是它是怎么做到了呢它把模型压缩第一个所以它技术就是做模型压缩那模型压缩也有几种方式它第一种方式就是说我就剪裁这种很大量大规模的神经元网络深度学习的网络里面的不需要等等的这种权重我就把它删掉尽量的 没用的我把它删掉这种叫第一种 叫剪裁第二种呢就是说知识的增留就是说大模型我把它删掉这个叫针流就是从一个我们所谓的老师型的模型把它变成一个学生型的模型这是第二种那么第三种呢就是说量化量化什么就是这种深度学习大模型里面它有很多这种参数参数的表示的精度比如说我可以用32位来表度我把它压缩到4位那这个精度把它压小压小以后整个的大模型的尺寸size就可以成倍成倍地减少那这样做完以后我现在我把一个大模型可以把它boost到一个中端或者边缘的一个设备上来做这是第一种方法第二种方法就是说那我不要大不需要这样的大模型但是我觉得小模型可以不可以小模型就是特别就是特定的一个任务然后也是通过大数据训练但是数据可能没有那么大量然后的参数也没有上千万亿上百万亿这样的十几亿或者就可以了那么这个小模型比如说出现了马克思的这个FiveStreet它的小模型大概只有3.8就是38亿这样的一个参数但是呢它取得的效果呢甚至好过一般的这样的一个大模型因为它是特地就是说在一个单个的靠近我们终端上比如我们手机上能不能部署一个大模型那么虽然大家可以去做我估计以后可能也会越来越多的这样的这种应用会出现但是它毕竟还是不太够的第一就是说大家可以看见就是说这张表里面显示的就是说NAMA2的这样的一个不同尺寸的不同参数的这样一个模型需要占到存储的需求和我们现有的这种边缘设备上的存储的这样的一个能力来比的话那么实际上边缘设备的存储的能力远远不够来存这样一个既是压缩过的或者训练好的小模型第二就是说如果你不是特别来开发这样的模型的话你就是把一般大模型压缩来做的话也是很难把它放到一个单个的一个这种边缘设备上来的那么还有一种什么方法呢那就是说那我们能不能用协作式的边缘计算就是说你一个节点不够但实际上我们平时的这种场景当中我们有各种各样的这种边缘设备你在家里面你可能你有这种你的家电设备每个家电设备可能就是一个边缘设备它也有这种计算能力比如说你还有你的车然后在工厂里你有机床有车然后你还特别自己来设计制造这么一个边缘终端接点那么如果这些边缘终端
如果这些边缘中端或者这些节点能够互相的协作把他们的资源共享出来然后共同的来完成这种大模型的推理任务那实际上效果会更好呢所以这个就是我们提出来的一个叫协作式边缘计算这也是我们去年在香港拿到一个很大的项目就几间大学一起合作来做这么一个系统这么一个系统就是说你已经有了不同的这样一个边缘的设备和边缘的系统然后你怎么把它能够通过网络连起来然后把它们上面的资源能够抽象起来建成一个可共享的资源池然后你把一个大型的模型把它分割分割成小的任务把它调度到不同的这样协作室的边缘设备上来能不能达到我们想要的结果所以通过实践实际上也有很多学术界的研究通过实践觉得这是一种很可行的方法那么具体你是怎么要去做呢就是我们就设计了这么一个专门针对于协作式边缘计算来做的这么一个大模型的部署调度的这么一个软件系统我们把它叫做Edge Shut这个边缘ShutShut实际上就是你把一个大的东西把它分成一个一个小的东西叫Shut它也可以考虑多个边缘节点它甚至可以考虑你如果有云的网络的存在我可以把云上的资源也能融入进来你如果有终端你有手机我也可以把终端融入进来就等于说你有一个易购的资源的各种各样的设备我都放在一起来进行调度然后它还有一个方面就是说根据我已有的这样易购的资源然后我来把一个给定的模型我把它来分割用最优的方法来分割那么就是存在两个优化算法一个就是说你怎么分割它第二分割好的这个任务怎么把它部署到你已有的这个有易购的资源的这种边缘甚至云的设备上去做它所以我们把它叫做EdgeShot然后这个平台还有一个好处就是说它保证在分割的时候分割的时候数据Embedding的这一层是一定要落在当地来做的为什么它不愿意把这个数据把它泄露出去就是我有数据隐私的这一部分我保证我不会把它分配到其他的就是不是我能控制的这种设备上所以这个你也可以根据你的适应化要求根据你实时化要求来动态的比如说我们做的还是一个Lama2的这么一个大模型Lama2它实际上有三种版本是根据它的参数它最小的那个就是说有70亿个这样的参数第二就是说130亿然后就是700亿这样一个参数这样一个full scale的这样一个大模型然后你根据你现有的已有的这种边缘设备或者终端设备或者有云设备的不同的配置你来决定你要用哪一个这个版本的因为你参数少的版本你自然的性能就不好或者你的精确度也不好所以我们可以根据它不同的配置来决定你用哪个版本然后同时呢我刚刚也讲了就是说你有什么样的这种设备如果只有一个单个节点的设备或者你是有多个边缘节点你有十个五十个一百个的我们把它第二种第三种就是说你不单有边缘节点你可以甚至有连到云上去的云服务器这个时候你能不能也能充分运营到云的这样的一个资源那么通过这种不同的组合然后我们能可以实时的动态来调整你用什么样的资源然后达到你想要的这样一个大模型的需求然后你就可以看见一个就是说用我们EdgeShot来做的它可以通过把它部署在多个这样的一个节点还有一个就是单个节点所以你们看见左手左手这边它可能用了10个节点你看到速度就快很多那边那个就是用单个节点单个节点的速度就很慢精度也会小很多所以就是说你可以实时的动态的来调控根据你的资源你用什么样的版本你的性能是什么样子那么在这个上面在这个EdgeShot的平台上我们可以开发各种各种不同的这种大模型的应用其中有一个应用我们把它叫做个性化的聊天聊天的这种Chatbot那我们之前大家都知道Chatbot非常有用对吧对我们的教育我们健康比如说健康的咨询各种各样的方面都有很多的应用那么我们之前做的工作实际上是用大模型来做什么就是说聊天的Chatbot它跟你聊天的时候通过你的对话 讲话它可以知道你的情感然后通过你的情感可以知道你的性格就你的personality然后根据personality它来产生相对的这种反馈跟你对话那过去我们是怎么做呢就是说这个用户输入一段话或者对话进来以后我们用大模型来进行分析来推理出来这个人现在是一个什么样的情感在通过
来进行分析来推理出来这个人现在是一个什么样的情感再通过那种情感再用大方形来推理出他是一个什么样的性格这个人是一个很内向还是一个很外向或什么样的性格然后根据这个性格第二部分我们再用这种数据局一和实时的数据来做微调来产生这样一个反馈比如说如果一个心情不好的人跟你说话如果你这个聊天机论他不知道他的性格是什么他内向是什么他可能简单就回答一句话哎呀你能不能能不能正面一点啊你不要老这么悲观啊但是你知道对有一些有这种忧郁啊depression这样的人你说这种话一点用都没有的所以呢他要detect你有这样的这样一个这种emotion的问题的时候他的回答就会产生一种像咨询师的这样的一个回答给你那么这么一个chairboard像我们在学校也是给学生去做学生现在现在的学生有很多很多这样的情绪问题一有压力他们就有很多情绪问题那学校虽然有咨询师但是他们可能应付不过来所以学校就要我们开发一个Chapel那么在这样的情况下我们比如说用这个大模型来做是可以的但是如果我们想到以后能不能把它变成一个更靠近个人的每个学生PC上的或者手机上的版本呢所以我们就刚刚用了我们的EdgeShark的这个版本这样把刚刚我讲的这个Chapel有个性化的这种个性的这种确报把它放到边缘设备上来做所以这个是我们在上面去做的那如果放在你的家里或者你的房间你的手机上你的数据是不会泄露出去的所以他的隐私是很好第二而且他的回复的速度也会很快而且你不需要连线你不需要有网络的这样连线你offline也可以去做How are you doing today?所以这是一个对话哦,所有东西都掉了,我的肺炎正在发生,我的猫跑了,外面的天气简直是恐慌。他就在分析啊,他通过他讲的这句话来分析他的情感是什么。Oh no, I'm so sorry to hear that.It sounds like you're going through a tough time.Let me see if I can help you with any of these issues.Have you tried any arthritis-friendly exercises or remedies?And have you checked the local shelters for your missing cat?As for the weather, well, at least it's not as bad as it could be, right?就是说他通过这个分析他能检测到用大模型来检测到你现在状态是什么你是一个什么样的性格然后能产生这么一个对话那么当然我刚讲这种Chad Ball不光是用在这种这种简单的对话上面实际上有各种各样的你比如说这种在家里面你可以跟这个Chad Ball的说话说我现在要睡觉了你们把我这个所有的这个各种家电给我调好调到我最希望的速度 care care因为他一直跟着你他一直跟着你这样的训练以后你的所有的你所有的跟他的对话东西都成为再训练他让他更加懂得你所以长期的来说他可以成为你的一个个人助理实际上个人助理我觉得是未来可能大模型发展的一个非常非常重要的一个阶段但是大家知道个人助理是很隐私的东西最好能放在靠近我们身边的这样的设备上这也是为什么边缘的边缘的大模型是非常重要的一个研究当然就是说我刚刚讲的这些都正在研究当中的这么一个阶段虽然可以推出一些简单的产品但实际上还有很多的这种挑战性的工作最大的挑战性工作还是我刚刚讲的在边缘设备上受限的这么一个资源那么这个你虽然刚刚讲你用模型压缩你用这种小模型它可能是低或者就是进度不够或者第二小模型的开发也是需要很多人力和资源去做的也不是这么很简单能做出来的那么第二个就是说像刚刚王老师也说了就是说你如果已经有一个大模型然后你这个已经把它布置在你的这个设备上了但是你每天产生新的数据你要学新的知识的时候你怎么去做就是持续性地去学习那么如果你是说我用这种微调Find Tuning Tuning tuning你是做不到的所以你在这方面你有没有其他方法那现在出现一个叫RIG就叫Retrieval Augmented Generation它实际上就是说不是说每一次你有新的知识新的这种需求的时候我就重新训练一遍重新全规模的这种微调一遍它是一种新的一种方式就是说你每来产生你问一个问题或者有一个输入的时候他去查询他查询完以后产生一个新的答案然后把这新的答案作为这个答案告诉你但是他每次查询这个时候也作为你的微调的一部分所以这个可能也是未来一个可以解决的一个方案那么后面还有一个当然这不光是针对边缘大模型对所有大模型就是它产生的这种这种结论或者这种结果它是不确定的你输入稍微改一改那它后面出的结果也可能不是不太一样所以你怎么可控它也是刚刚王老师讲的你怎么去可以控制它那么现在我们正在做的还有哪些一些比较重要的一些研究方面的方向呢那么刚刚就我讲的就是说在协同式的边缘的大模型的这种推理上实际上还有很多优化的东西可以去做比如说我有很多我不光是十个五十个我也上百个上千个以后整个城市的所有边缘的这种设备能不能连起来那连起来这些边缘设备共享资源的时候它的资源的管理是怎么去做是集中式的是分布
讲资源的时候它的资源的管理是怎么去做是集中式的是分布式的还是一种层次式的那这个东西值得去研究的第二就是我刚刚讲的就是说我有资源了然后有一个大的这个模型进来推理啊然后这个微调啊我怎么把它切分最优的切分把它部署下去而且这个是动态变化的因为资源是随时变化你怎么做这个动态的资源的优化第三就是说数据隐私因为你要共享数据在不同的边缘设备上数据的来回的这种transmission的时候你怎么样能保证数据隐私和安全不受影响第三就是我们现在在做的也是刚刚拿到的一个研究项目我们刚刚讲的就是异构呢只是说软件层上面的或者这个计算资源但是硬件上的异构现在怎么去把它开发出来现在还没有去做你比如说现在不光是GPU对不对FPGA然后这种特殊的AI加速的芯片或者这种硬件的东西出来之后你的任务调度到一个节点上的时候你怎么把硬件的易构性充分地利用起来让你的性能会变得更好这也是现在正在研究的第二就是我刚刚讲的你的微调那么微调大家知道就是说你是需要把所有的数据都能共享然后有一个集中的中心的一个节点来帮你来做微调或者重新训练对吧我不共享数据我来训练你当地的这样模型然后呢把这个模型再来把它aggregate起来那么这个就叫Federated就联邦微条这也是现在大家正在做的一个可以做的研究方向好的我今天就分享到这里谢谢大家谢谢谢谢曹院士带来的精彩分享请您回席落座我们看到近年来随着基础人工智能和大模型技术的蓬勃发展也让AIJC能力如何更好的与产业结合成为当下研究的重点那京东云作为京东对外提供技术和服务的核心平台在多模态大模型巨深智能等领域有诸多探索并且也在内容生产服务 营销等领域实现颠覆式创新所以接下来的时间让我们有请京东探索研究院院长兼京东科技人工智能业务负责人IEEE Fellow何晓东博士带来AIJC基础模型发展与产业创新的主题分享掌声有请首先特别感谢各位嘉宾今天来临我们的会场也特别感谢刚才那个曹院士还有王教授的精彩分享给我们介绍了最前沿的这个技术的发展包括最前沿的大模型包括边缘计算今天到我这个环节我其实想跟大家更多谈一谈在产业方面的落地和应用因为我们知道产业和研究其实是整个学科和整个领域的两条腿和两个轮子协同发展 互相促进才能让这个领域走得更远首先我们经常说到这个大模型也说到AIGC这是今天这个论坛的两个主题和两个主题词这里面其实是有一点点区别AIGC本质上更加像是一个应用的词它本质是一个GC就是内容生产内容生产这个词其实掰开来看就是包括早期我们生成比如说新闻 电影 短视频都是内容生产但早期的时候内容生产其实是我们叫做职业生产叫做Professional GC往往是需要专业的人士导演 演员比如说去生成这个电影去拍这个电影需要记者去采这个新闻好处就是质量很高坏处当然就是成本很高效率偏低所以就进入到这个叫做叫做UGC用户职业生成的阶段比如说现在抖音上的很多视频就是用户自己传上去的一下子内容很丰富但问题是这个质量是良秀不齐的有高有低所以我们就一直在想能不能用人工智能把我们生成的内容解决这个效率和质量的两难这个idea和这个理念其实很早就有了但是在大模型出来之前人工智能生成的内容效果很低或者说效果不好用户开始能够接受这样的内容这样的内容这个AIGC才开始大行其道另一方面呢这个AIGC最开始我们都知道是从这个ChatGPT这个活起来以后大家听了比较多ChatGPT主要还是一个文字内容生成但实际上我们现在也进展到这个语音 音频内容生成这个图像和视频内容生成所以才包括最前沿的我们叫做综合性的内容生成包括数字人它包含了视频生成包含了文字语音生成一起提供一个综合性的人际交互的服务从另一角度来看我们也知道AIGC虽然是这两年才火起来的包括大模型但实际上从技术角度刚才包括了曹老师王老师也提到从技术角度其实是走过了一个比较长的历程回溯到2013年那时候开始用神经网络去模型就建模这个羽翼应该是到2014 15年提出这个注意力机制attention机制是这个Transformer的最核心的一个理念再到Transformer提出这个多头注意力机制可以大功能并行计算提高这个计算效率到或者BERT GPT到CHART GPT同时这个虽然语言早走一步图像也其实在这个生成这个领域迅速跟上最开始在GAN是2014年15年提出来的刚才那个王教授也提到了这个早期的一个生成技术然后进一步的包括Attention也开始在GAN上面在这个生成上得到应用然后Diffusion是刚才提到的最后还是这个Solar是以今年年初的这个Solar为代表这一系列其实意味着说整个这个大模型
连出的骚扰代表这一系列其实意味着说整个大模型技术其实是有一个很深的技术趋势通过这个过去我们也看到未来还会进一步的发展待会我们可能讲到包括京东我们在这方面其实也做出了我们的这个应该说是比较一些突出贡献我们的一些重要的论文在这个领域也是在这个行业里面比如说引用超过了将近6000次是一个领域的一个也算作出京东自己对这个行业的贡献另一方面我们谈到产业就避免不了谈这个价值毕竟这个技术最后变成这个产业价值才能够为普通的每一个用户真正可以得到应用这里面其实也是一个微小曲线最高价值的两个区间在左边和右边左边是我们叫做基础核心技术刚才提到的一系列的以这个代表性的技术像这个Transformer像这个GPD这样的技术能够使得整个这个效果和使整个人工智能前沿发展能够得到一个跃迁整个产业也从中得到一个很高的获益另一方面大家谈的比较少的其实也是非常关键的就是颠覆性的应用和产品技术最后要通过产品和应用来定义真正的时代我们知道其实每一个技术时代我们都会找到一个真正颠覆性的产品来真正定义这个时代特别是对普通的用户来说跟你俩是这样比如说我们想到移动就会想到手机就会想到智能手机没有自动手机大家可能觉得如果只是打电话的话可能就想不到移动这个时代我们看到这个个人计算低价竞争像这个简单的集成坦白说可能大家也看到很多这种新闻但我们认为这个可能是一个低价的区间反而是我们应该需要避免的京东我们其实整个布局其实可以画一个很复杂的框图但我其实没有画特别复杂本质上来说一方面在基础技术上在研究平台上我们打造围绕一系列的内容我们打造我们的基础能力同时在产品系列我们也是从客服到营销到数字人到ARGC一系列的应用希望能够再探索高价值的颠覆性的产品和应用据说目前其实今天大模型可能谈了很多其实从ChartGPT从2022年出来以来到现在大概过去了一年半还多了下一代大家还要讨论更大的模型和更多的计算力但某种意义上其实大家在想大模型某种意义上进入一种应用的就是颠覆性的应用大家还是寻找所以我们有点就是说有点进入一个平台期虽然这个榜单上的数字是越来越高但真正的大规模的影响每人的就是每人日常生活的这个应用其实还在探索之中这就是我们说我们可以目前可能更加是需要一个颠覆性的这个产业应用目前的很多应用还是在原有的APP上做一些间接性的改进包括像对话机器人之前我们知道有Siri有小兵还有各种各样的Echo现在确实对话比原来好了很多真正的实际上突破第三个点我们认为现在大模型的幻觉就是幻觉率还是太高大模型的幻觉就是说大模型可能生成一些不符合事实的错误的不可控的或者是不符合规范的这样一些内容目前大概大模型的幻觉率还是在将近10%到20%大家如果用GPT的话你会发现你跟他聊大概聊个10句话左右就会有一句话是有点莫名其妙的是吧 说是而非的这样的幻觉率在一般的不关键的这个普通应用中可能大家不觉得觉得也很有意思但是在关键的商用里面这是不能容忍的比如说我们不能想象我们给一个著名的大V和这个著名的大V做了一个数字分身他在直播的时候是吧 每个一分钟他的这个面部就扭曲一下是吧 每个半分钟他的手臂就多出个手指头来这是商用上是不可接受的所以如果我们认为这个大模型会是下一代人工智能产业的基石那么大模型本身的幻觉率就变成一个非常关键的因素因为它每迭代在这个基石之上建造一个大厦的时候你每盖多一层楼那么它的底层的所有错误就会被进一步传导甚至放大所以怎么进一步地压降这个大模型的幻觉其实是一个工业界特别产业应用的时候面临一个很重要的问题简单介绍一下京东的大模型京东的大模型我们叫做延熙大模型取名叫做言下之意这个心有灵犀就是说希望我们的大模型能够直接就是说能够跟人更好地沟通大家在网上能够爬到的这种所谓的能够购买能够获取的公开数据以外我们还有30%的这个训练基座数据是京东自己多年的这个业务运行的过程中成立下来的零售 物流 金融 健康等等数据有这些数据使得我们大模型可以在这些行业理解得更深有更深度的行业知识和专业知识真正能够走到产业的需求不会出现所谓的叫做外行看得像内行内行看得像外行的这种情况就是说现在是外行看大模型生成东西就觉得很酷但内行一看就知道这个生成东西是有问题的有瑕疵的 不能用的这是所谓的外行看得像内行内行看得像外行这个是在产业应用中是我们需要避免的这一简单介绍一个工作是我们在21年发表的工作就是怎么样把这个行业知识注入到这个Transformer这个基础模型里去能够更高效地提高它对特定行业的专用的这个我们叫这个专业性专业知识的这个准确率比如说这个实体属性抽取这个其实是这个Benchmark和这个测试集是我们内部用的非常多或者是说在行业里面用的很多的一个数据集它是用在什么地方呢用在比如说客服什么叫实体属性抽取呢比如说如果一个顾客来咨询一个冰箱的属性比如说他买个冰箱的时候那么这个冰箱的大小 价钱 颜色 耗电的等级这些就是这个冰箱的实体的属性咨询这些属性当然是不能错的因为用户问这个冰箱的时候那么你就希望得到正确答案
这个属性当然是不能错的因为用户问这个冰箱的时候那么你就希望得到正确答案怎么样从这个比如说从这个商品的说明书里面真正的把这些死性把这些关键的属性抽取出来回答给用户其实是一个很关键的问题在这里面我们需要一个非常高的准确率普通的这个Transformer其实达到了83%的准确率换句话说它的换绝率大概比如说在17%左右是不可接受的甚至人工交证都成本太高那么经过我们叫K-Plug这样的模型进一步地把它强化以后它的正确率会极大的提高错误率基本上只有原来的四分之一这样的话再加上一系列产品设计的一些保护保障那么它就可以做到真正可用的水平再比如说在京东我们其实在多个场景已经大规模使用延续大模型包括文案营销文案征程智能客服 数字人 商品图 智能诊疗物流仓储等等我也同时不光服务我们自己我们服务了大量的在京东上的合作商家合作伙伴他们咨询就是他们给他们的客户提供咨询 提供服务提供导购营销的时候也会用到京东的这个延熙大模型整个来说我们还进一步地建造了延熙AI开发计算平台可以更好的构建智能体在这个基础大模型之下让我们的合作伙伴能够快速地 便捷地构建各种各样的智能体包括导购智能体服务智能体咨询智能体这样更好的高效的为我们的合作伙伴给他们的用户提供服务提供便利性这个大模型进行生成极大的降低了这个商家在为一个新的商品上架需要的这个营销的这个努力需要的营销的成本同时我们的生成针对这个专业度丰富度这个忠实度这几个点因为这是电商生成文案的时候是必须要保障的经过我们的模型优化也可以做到非常高的可用性这里给大家看一段简单的视频综合地展示基于延期语言大模型的一些应用中文字幕志愿者 李宗盛你好 有什么可以帮到您的吗中文字幕志愿者 李宗盛根据用户行为和会员情况我们可以看到该美妆品牌旗舰店的用户和会员在过去几个月中刚才是我们的一个相对是怎么应用大模型的一个体验然后我刚才提到其实刚才很多这个展示还是基于语言的比如说对话 咨询 导购等等但是在技术上我们看到这个大模型这个技术已经进化到多摩泰技术了所谓多摩泰就是说不但理解这个文字不但能够生成这个文字回复它能够理解图像生成了一个生成这个图像生成这个视频这就是我们在多摩泰这个阶段再进一步我们说是可以走入到这个巨神智能阶段大模型真正从数字世界走入实体世界刚才曹教授也介绍了一下在AIoT方面在真正的边缘计算方面和多模态生成方面的一些具体的展示应用展示简单的生成海报在内部说了可能大家看了很多了最近一键生成AI图片很正常右边是我们商品图创作比如说简单一个商品图在不同的层次在不同的背景上怎么样能够生成不同的渲染最后能够帮助商家用一个简单的商品图生成一系列的营销图案最后把它上架进动再比如说这是一个我们应用的情况比如说怎么生成商品商项页请不吝点赞 订阅 转发 打赏支持明镜与点点栏目感谢观看
请不吝点赞 订阅 转发 打赏支持明镜与点点栏目当然我们还进一步生成声音刚才我们看了文字 看了图像其实声音也是一个非常重要的媒体和媒介人类在沟通的时候当然都是通过语音的但其实人类沟通的时候不但通过语音传递信息很多时候是通过声音来传递一个情绪这里我们给大家看看和听一下用AI生成的带有情绪的和能够传递情感的声音时间关系我就放一两个例子吧喂喂 你好我是这个京东护肤的颜值守护官宝宝最近换季了咱们娇嫩的皮肤要开始干燥你是机器人吗那还挺聪明的嘛我是关心你再忙也别忘了好好呵护自己不然我会心疼的我听说欧诗曼在双11有小美和新品体验妆入会仅需9.9元就可以解锁两支1.2毫升的磁泡精华待会会有店铺活动短信发送给你有兴趣的话记得尽快去了解一下对你的声音更感兴趣所以我们也是通过这个AI技术AI技术打造了这个我们叫做语音这个声音内容供应链能够更好地通过声音帮助品牌帮助我们合作伙伴能够产生与用户情感共鸣的这样的声音时间关系下面几个可能我就不播了再下一步我们看到这个这是通过智能明星我们看看视频外呼然后再往下我们看到就是我们有了文字声音图像视频各种生成能力综合起来看我们能不能创造一个颠覆性的一个新的产品叫数字人数字人这个产品换句话说在五年前我们是不存在的因为这个技术达不到但是经过甚至在三年前我们都没看到合适的数字人甚至在两年前我们都觉得数字人太傻是吧但今天其实基于一系列这个大模型的驱动我们看到这个数字人已经开始真正走入生活了我们也在京东也是通过这个点线面的尝试给大家呈现一下我们今生的这个数字人先给大家看一个去年的这个效果顶级拆GPT成为了业界锐利的焦点首先何博士能为我们简要介绍一下拆GPT是什么吗当然可以拆GPT是一种基于GPT模型的人工智能它是由OpenAI团队开发的旨在实现与人类类似的对话和文本生成能力CHAT GPT能够理解自然语言进行智能回应并在各种场景中提供协助为了心中的理想主义有之其不可为而为之的孔明有之死绝不放低内在坚持的成功他们虽然没能实现大团圆但他们的坚持和执着真的让我们感受到了理想主义的浪漫让我们的心灵受到了极大的震撼再来说说这个呢是我们四月份的时候在全网真正这个推出的这个产品直播就是刘强东总的这个数字分身刚才这一个片段是他在这个为京东图书带货刚才谈这个三国演义正好就是为这个四大名著带货的时候你们看到的这个一个应该说是纯虚拟合成的这么一个数字片段我们未来还是希望就是数字人本身会作为一个数字员工这么一个形态真正的走入各行各业真正走入各个场景为我们就是说为我们各个有需要的这个场景提供这个7成24小时的这个高体验的交互性的这样的这个体验再比如说今年618的时候我们其实已经在全面开花我们做了一系列的这个这是我们的这个说了这么多那AI数字人Sandy能不能请你在现场为大家演示一下在今年的京东618你能够为消费者在哪些场景提供哪些服务呢好的 没问题接下来我将为大家展示在今年的京东618我能为大家提供的服务我现在所在的是陕西同川樱桃种植园这里的初夏樱桃买过的朋友都说比进口车梨子还好吃现在下单源头直发到家还有百亿农股优惠Sandy你好根据你的需要为你推荐这款智慧屏电视轻轻一碰就能投屏用它和家人一起追剧看欧洲杯手机直接变剧目现在下单还有已就换新补贴哦对于职场女性来说化妆品要精简百搭这几款玫瑰色系豆沙色系的口红不张扬且百搭无论是通勤还是聚会都能轻松驾驭您好我非常理解您急切的心情关于您提到的先享后付服务是京东为用户提供的一项便利服务即零元下单满意再付款您在开通过程中遇到任何问题请随时联系我们经过刚才的展示大家对我的表现还满意吗非常好啊谢谢AI数字人生这是我们的那个京东的CEO给他做的这个分身然后在618的一系列的场景上得到应用不光是给京东自己做我们给格力像董明珠 董大姐给包括一系列其他品牌都进一步的数字分身制作这样更好的我们知道企业家现在都说企业家要导致自己的IP
这样更好的我们知道企业家现在都说企业家要打造自己的IP那企业家怎么打造自己的IP呢企业家又很忙平时包括沟通能力包括中间那个韩国我们说LG的韩国中国区总裁李东善他不会说中文他很来在中国更好地打造他的IP因为他需要跟用户沟通所以我们经过给他打造这个数字分身以后比如说董大姐就可以跟大家更多时间分享他的故事比如说LG的这个李东山总裁就可以通过中文跟他的顾客进行沟通和交流我要不得他听听这个听听这个解如何发展也让我们在这个过程当中体会到品牌的价值我们格力认为品牌就是用质量技术担当责任来支撑那么现在是一个大数据时代我觉得我们就用数字说话2023年格力电机营收超过2000亿我们给国家贡献的税收仅180亿我们获得的利润是290亿给股民分红超过120亿接下来我可是知道企业如不能放那个中间那个韩国那个总裁不能放吗接下来我可是知道我想大家听一听这个对这个是更依赖超高分辨率现在的AI技术完美地保留了李总裁的特有的音色但是他确实不会说中文但通过中文的能力现在让他可以用他自己的口音但是用中文可以跟他的消费者进行更好的沟通和交流时间关系我其他结果可以接下来我可是对 所以刚刚过去618我们看到了就是说整体的这个我们以为5000家的这个客户提供的这个数字人的服务整个直播时长在618我们播了将近40万个小时整个成交量在618期间就达了10亿观看人数加1亿包括这个人的评论也超了500万给了我们大量的反馈进一步帮助我们把数字人把这个AIGC技术做得更好很希望我看对所以最后我们还希望有大量的规模化产业应用也会马上开启这也是我们看到当技术出现颠覆性的时候我们就会想产品的颠覆性在什么地方当产品出现颠覆性的时候我们就会想模式和应用的颠覆性在什么地方这时我们通过这种三步走更好的把数字员工产品能够真正推向产业界真正落地最后我们还有一个推出一个叫做延期商家扶持计划帮助京东的各类品牌打造一个日不落的直播间同时也免费送一个月的速成直播和专业服务运营希望有兴趣的合同伙伴随时来找我们最后可能还有点时间谈谈我们巨神智能方面的进展能够完成一系列复杂的任务这里给大家看一个简单的展示其实左边这个是一个早期的我们叫做物流机械臂一般来说我们不会把这个称为巨神智能因为它虽然可以移动 可以抓取但它更加是按照固定编好层的方式来做这个事情的右边这个是我们最新的探索比如说在这个食堂怎么这样让机器人可以收拾餐桌所以最后我想总结一下就是在京东咱们还是觉得大模型还是应该以产业来牵引来驱动大模型技术发展真正的还要着力于打造颠覆性高可靠的AI技术产品真正让这个时代这个AI时代能够有一款可以定义性的产品让每一个普通用户从中获得受益以上是我的汇报谢谢大家谢谢谢谢何晓东博士带来的精彩分享请您落座那接下来的时间呢我们将进入到今天论坛的第二个部分一同来探寻AIJC技术的成功实现经验所以接下来的时间让我们用热烈的掌声有请上海市普陀区数据局副局长尹新先生带来人工智能助力数字城市数字化转型的主题演讲掌声有请非常感谢主办方提供这个交流机会我来自普陀区数据局我交流的主题呢是人工智能助力城市痊愈数字化转型实践今天这个活动安排呢感觉非常有意思先是从学术的大牛和技术的大咖然后到我们这种对技术可以说一无所知的用户就让我想到了我们读书的时候有一种叫科普开放日就是我们的院士老院士们会给中小学生去讲一些科学上的一些比较有意思的历史或者最新的情况小学生呢跟我现在情况差不多可能听到很多但是自己觉得听懂的很少真正听懂的就更少但是呢通过听这些自己不懂的东西让自己对IGC未来跟我们的上海的一个数字政府建设能够带来我们的未来的发展前景充满了这种热情和期待回到我们普陀区普陀呢是我们上海西北角的一个中心城区也是上海市区的西大堂我们的城区面积是55平方公里常住人口124万我们在近代是我们中国的近代工业文明的发源地之一然后在建国之初我们也是上海首批的工人新村在2010年代我们是上海重点的转型区域
在2010年代我们是上海重点的转型区域在2020年代我们又成为了上海市首批的数字化转型示范区这是我们普陀区的一些背景情况向大家做个汇报2018年以来随着亿网同管它的不断夯实从治理的精度和效率出发也从我们新设立数据局的职能出发都对我们三网融合这件事有着更高的要求依托人工智能能工智能呢,我们目前已经解决了很多像日常办公啊,政务服务啊,城市治理等方面比较确定的重复的稳定性工作上的需求。随着ChatGPT4和更高版本的持续发布呢,我们也逐步看到了构建全天候智能化的一个服务的一个可能性。比如说,我们构建了叫政务处置小助手,还有一个呢叫居民服务数字社工,还有一个叫数字营商电小二。三个应用其实功能都比较相似只不过它面向对象不同政务处置主要是针对于业务的派单像居民的一个数字社工主要是针对我们基层居民服务而营商店小二主要针对我们面向企业的很多比较有特色的比较繁琐的服务通过这些智能化应用我们也在各个方面各个条件使得我们实现几方面目标像环境更宜居生活更幸福治理更高效程式更智能我们在体验AI带来这种像文稿编写图像生成这种非常直观的这背后呢我们也有体会可能是一些更长远的问题比如说像语调库知识库 要素库等等它也存在一定的不足在城市治理智能化应用落地的过程中呢我们需要的到底是什么呢可能是先进的智能的算法或者说是一些大模型平台或者是一些智能化应用但是我们感觉针对之前说到三方面问题我们需要的更可能是数据的规级整合数据要素的建设和数据场景的赋能在AI应用中我们始终把减负增效作为重要的目标我们在实现意见制收基础上有持续加大辅助治理辅助办公的投入未来我们的基层同仁可能在依托我们的很多智能终端基础上就应用我们AI的一些智能的应用可以实现边聊边学边聊边办而且在过程中也比这种非常有经验的老法师他能实现更加办得准更高效在原有的我们的三网一网同伴一网同管和一网协同基础上我们也在不断增加知识库的积累在日常的管理服务中基于要素内容沉淀治理动作和服务特点然后依托AI进行一些流程的优化基于城市的要素治理我们更加关注的是我们基层服务的一个底层逻辑以城市治理要素体系为基础形成我们的一个城市的智能化的应用并且构建一个底层标准的数据要素体系为未来我们的数据治理数据流通并和基于知识的智慧城市应用框架提供更加丰富更加扎实的基础知识库同时呢我们也试图去构建一个城市治理和服务的一个模型的训练的基础平台重点围绕我们街镇居村的一些基层的治理服务需要开发的训练场景通过构建智能服务中枢呢可以实现智能模型的协同纳管完成资源的共享和优化利用避免重复建设基于场景呢我们也在不断地引入相关的模型能力进一步提升我们三网的智能化水平基于智能中枢为我们基层工作不断提供智能化的辅助工具还有一块呢就是我们的未企服务方面这一块呢是尤为复杂因为营商环境也是我们政府更为关注的一个领域针对企业它复杂多元而且在不断变化的各种应该说奇奇怪怪形形色色的各种诉求吧为了提升我们服务的精度和效率同时也减少我们人工派单的一个分析和一些研判的投入我们也通过模型训练等智能手段加强语意理解自动分列标识对诉求进行智能的派单结合各个部门的智能相对精准地把任务派给相关的部门但这只停留在派单的环节后续的处理环节这块可能是更为复杂实现了全天候的一个智能的在线咨询和事项办理这么一个功能通过沉浸式的服务引导企业也可以就是跟我们AI有疑就问边聊边办全天都能完成这个企业的一些基础的服务当然我们数字政府建设它最重要的基础还是一个安全底线的保障在数据安全隐私安全和分级授权等等方面我们也期待着AI能够扮演更大的作用像刚才我们也听了几位专家的一些介绍我们普陀区围绕一些前沿的一些学术的研究和一些技术的落定我们也打造了很多新的平台比如说我们在长峰引进了上海清华国际创新中心可以供国内的一些前沿的学者做一些探索我们在我们的真如呢也引进了海纳工程院网建院社的团队来赴能我们的海纳数字化转型小镇的建设我们同时也配套了很多相关的政策资金 应用场景的资源和办公的场地 研发的场地也欢迎我们的机构我们的企业 我们的专家能够来到普陀来到我们这一片创新创业的热土来开创我们AIGC相关的新事业谢谢大家谢谢尹新先生带来的精彩分享我们看到大模型和AIJC的应用落地其实是离不开数字基础设施的支撑那先进算力如何去支撑
是离不开数字基础设施的支撑那先进算力如何去支撑大模型发展企业又该如何快速地去训练出属于自己的产业模型呢接下来的时间呢让我们有请京东云副总裁京东云赖斯产品研发部负责人龚一诚先生带来京东大模型发展及场景应用最佳实现的主题演讲掌声有请大家好我是来自于京东云的龚亦诚下面我给大家分享一下京东大模型的整个应用场景的最佳实践刚刚何博士包括曹验士其实前面也提到很多在大模型的应用这一侧相关的落地其实刚刚过去的618京东其实在大模型的整个AI机子的应用场景里面我们其实有数百个这样的AI机子相关的应用包括刚刚何博士提到的数字人的包括我们零售商品的文生图包括我们在金融里面的财富场景的包括我们在营销增长场景的后面我们会有个短视频来去示例一下其实今天包括我们的员工的研发我们的代码我们代码的自动生成我们目前的话京东机器巧扣的我们的整个代码的自动生成的采纳率也将近20%多的代码的采纳率等等嘛实际上整个大模型的话在京东内部的话已经大范围的去做相关的应用场景的一些落地这个其实前面也提到了刚刚何博士提到的整个在数字人这一块的话一些相关的数据其实刚刚那个曹先生也提到了其实面向不同的公司来讲的话很多在产业方面的数据其实很多时候都落在公司的各个公司或者各个行业的内部的相关的场景里面其实并不是说今天在一些基础模型一些泛化场景里面可以去直接去获取的那这个也是为什么很多企业在一些行业一些具体的场景里面会结合自己的一些行业数据去做一些行业模型的这样的落地的这样一个维度那实际上从京东我们基于大模型去推进的过程中我们去看的话我们大概经历过三个阶段我觉得我们从21年的时候包括当时京东的探索研究院就是何博士这边可能带队从21年其实我们在重庆当时就建了一个亚洲最大的一个superportA100的超弹中心其实当时就在做这个基础大模型那之后的话我们又开始进入到这个行业模型的阶段因为集中有大量的这样的产业的数据我们做的这样营销增长的大模型我们的那个医疗问诊大模型等等非常多的这样的行业模型的这样场景因为我们发现基础模型整个泛化当然在具体的行业场景里面其实缺乏这样的专业度所以我们又面向了进入到第二个阶段做了非常多的这样的行业模型这样的场景那当然了到今天为止的话其实让我们又进入到第三个阶段我们进入到智能体的阶段那目前在京东内部的话我们有非常大量的这样智能体落地它非常碎片化的这样各种各样的这种场景包括OI场景的实际上就是跟企业端最终去落地的时候这个链接的过程实际上就是智能体我们最希望去看到的真正能去解决企业端的问题的这样一个对那当然了从整个大模型的落地逻辑里面其实刚刚我们也提到了那从最早的我们最远理解了环境的感知等等就是说今天智能体落地的时候包括何保时刚刚提到的巨神之轮等等其实更多的说无论是机器人的形态也好还是今天来讲的话来自于这种机器人的软件的形态或者是不同的形态来好更多的也是去感知外界的环境无论是视觉 语言包括这种交互式的这种方式的其实不同的方式去感知外界的一些不同的输入然后最终结合模型结合一些企业端的一些规划 行动 执行等等去完成整个大模型在企业端的最终的落地那其实前面因为刚刚几位专家都讲了非常多刚刚在应用测的这样的落地的场景我在这个地方稍微展开一点去讲一下京东的沿西的AI开发平台那这个是个什么逻辑呢其实今天我们看到无论是说我们今天做基础大模型做模型的行业模型做模型的微调以及最终走向智能体这个形态来说传统意义上来讲其实每家企业去做这件事情本身的门槛其实还是非常高的门槛还是非常高的比如说今天我要想去做整个基础模型的本身的训练来好整个算力的调度资源利用率的提升等等吧我觉得这里面有非常多的事情其实对很多企业来讲的话今天我其实更多的目的其实为了打造这个基础模型其实我并不关心说我的基础说是我要花大量的人力跟资源去构建这样的相关的事情所以说从我们整个延期的智算平台教育来讲的话我们可能分成三个层面我们第一个层面就是在基础设施这个层面的话我们有一个智算平台这样一个面向高性能计算场景的这样一个智算平台那主要我们底层的话我们像传统的NV的这种GPU的系列的GPU包括国产的生腾摩瓦先程了随缘了等等我们全部基于我们的智算平台我们全部可以兼容这样的不同的GPU的整个系列的型号从底层的DNC网络从IB到ROCKY不同的DNC网络我们完全是兼容的我们结合自己的FQI的高性能的智能网卡我们可以完成整个高性能的全高性能的整个高性能计算的形态然后同时从存储维度的话结合我们的云海的统一存储引擎那今天其实我们看到做模型训练过程中全生命组织开发过程中你会发现有几个阶段从数据的准备数据的清洗到模型的整个训练到最后的部署等等吧其实你会看到在不同的阶段对数据的依赖其实不一样的在数据准备阶段可能整个数据的来源还是非常多的数据的成本他关心数据的成本吞吐等等这些以来的场景那在模型训练阶段更多关心的是小文件的大量的并行的读写对并行文件的整个依赖那实际上从我们高线
并行的读写对并行文件的整个依赖那实际上从我们高性能的存储角度来的话我们其实是一个统一引擎就是底层我们一个存储引擎我们面向上面的话我们支持不同的存储的类型从对象文件块并行文件到HDFS这样从企业端去做数据准备数据流转以及数据模型的基于数据的模型的整个训练的话整个数据的流转会变得非常容易而且是原生形态的这种原书记级别的数据的流展当然从这个底层基础设施的层面我们更多的是希望能基于延期平台帮真正去做模型相关的这种企业去解决它在基础设施这个层面的相关的问题当然这是我们的T1这一层那我们的TR这一层的话就是我们的模型全市民主体开发的这一层那这一层我们是从整个模型的刚刚提到的从数据的准备到模型的开发微调模型的部署等等吧这评测模型的评测我们全部都做到我们的TR这一层那这一层的话其实更多的是面向一些行业场景的其实今天很多企业可能我们今天我们希望拿到一些开源模型或者商业化模型我们可能想去做一些微调那这种场景下的话其实我们基于这个平台的话我们就可以很好地完成这个工作你并不需要自己完全去构建这个过程而且整个模型的微调进段的话我们支持有代码跟无代码的方式你既可以支持loadbook做一些代码的微调同时完全可以支持无代码的直接就上传你的数据然后从界面上做一些这个配置然后可以自动做无代码的这样的基于整个平台的话完全是一个界面化的可视化的操作就可以完成非常基于模型的全身命中的这样一个过程那这里面当然我们也支持一些高级的功能因为刚刚曹燕时也提到了我们今天看到有很多大模型包括一些小模型的场景那基于我们的平台的话我们既可以做小模型的放大我们也可以做大模型的缩小并且我们也支持不同模型的拼接把不同的模型基于模型的本身的宽度然后做拼接然后去提升整个模型本身的综合能力那这种非常高阶的能力的话我们基于我们的平台都可以完成这样可以满足我们不同企业在模型面向不同场景不同规模不同侧的这样不同的能力这个就是基于平台这一层我们就完成模型的全生命主体相关的一些工作当然往上这一侧的话刚刚讲的模型这一侧的话平台的开发这一层我们几乎把所有的主流的开源模型以及绝大多数的商业化模型都入驻到我们的平台上了那时候我们基于模型我们相关的一些开发者的话那就更加容易去基于模型做相关的应用的创新了那再往上这一层T3这一层实际上就是我们的智能体的构建平台那这一层更多的说我们今天我们看到很多模型那真正往企业端去做最后一公里的这样落地就模型到企业应用的最后一公里的这个打造来这个落地来好的话到流程的编排了最后到本身的各种插件的管理链接等等吧这一系列的事情实际上我们基于我们的智能体平台的话都可以帮助企业去完成模型到企业端的一些落地的应用这样最终的落地当然我们本身智能体平台上面也落地了非常多的一些行业比较通用的一些模板包括一些客服场景的营销场景的等等吧就是不同场景的这样通用的模板这样可以助力企业端的话结合自己的应用场景的话可以选择不同的模板然后快速构建企业端落地相关的一些智能体的应用那这个其实刚刚讲的就是沿西这个智算平台的话我们从沿西的整个开发平台角度来看的话我们分三个层面这三个层面其实更多的是说面向今天不同企业端面向整个大模型它的整个来讲的话一个不同的这样的需求其实刚刚提到的T1这一层更多的是面向整个支算面向支算中心的可能今天大家做支算中心做算力运营的过程中我们面向算力对外去提供服务的时候从单集群的千卡到万卡级别的集群的整个运转到单卡包括推理各种场景的不同的这种系列的基于这种资源层面的超算的这种场景的TR这一层面向整个基于模型做前生命中期的这样开发的这样的企业T3这一层的话更多的是面向企业端基于大模型做应用落地的这样的一个创新那这前面我提到的整个我们现在这个技术设施这个层面的不同的这个系列我在这个地方就不展开讲了当然了我们今天说的所有的这一切的无论是刚刚我提到研究智能平台也好不同的这个技术设施的相关能力也好最终我们都是希望能够围绕模型真正去用起来那前面其实何保时其实也介绍了非常多京东在整个模型的AIGC的整个应用这一侧的落地的相关的一些非常多的场景那这个地方有一个简单的PVD以京东沿西AI开发计算平台为底座领航者AI增长运营平台成为大模型智能体落地范例智能体可以根据目标和当前状态制定一系列行动步骤完成诸如营销活动这样的复杂任务分步骤拆解在营销费用申请环节大模型智能体在业务可视化区域召回打开费用申请表单系统界面根据指令自动完成表单填写在营销活动搭建环节大模型智能体在过往优秀案例的经验学习上自主实现活动页面的多图层多楼层架构搭建借助文生文文生图图生图等生成式AI能力实现活动页面快速高效的制作并可以对局部再次进行灵活的优化调整在营销策略配置环节大模型智能体调用领域算法模型实现大小模型协同确保所生成营销策略是严肃的专业的可解释的在营销数据分析中
营销策略是严肃的专业的可解释的在营销数据分析环节大模型智能体实现自然语言到查询语句的转化快速输出分析报表及数据看板对这个其实就是我们在营销增长这个场景里面基于智能体的大模型的智能体的这样一个落地的应用实际上从大模型在企业端落地的时候其实我们看到当然刚刚提到也有三个阶段就是在智能体这个层面的话从第一个阶段做模型的就是智能体跟现有场景的一个融合那实际上今天我们看到的很多企业但无论是OI场景还是说相关的刚刚我提到营销增长等等不同的场景其实绝大多数场景今天看到的都是一个融合场景就今天我现有场景里面可能我结合大模型做了一些局部的一些提升比如说我的这种就是说是一些企业内部的一些住宿这种形态的实际上更多的时候我可能助手帮我完成的是我对整个自然语言的不同的理解最终我去链接的时候我还是跟企业端的不同的应用去做打通的我觉得这是第一种形态的第二种形态的话就是完全重构形态的就是原来现有的方案里面的话可能我采用的是一整套的方案今天可能我这个方案我是完全颠覆的完全基于大模型去驱动的比如说何博士刚刚提到的数字人的场景里面的可能他就完全是现有场景里面完全通过大模型全部是重构这种形态的那第三个其实刚刚何博士在下面其实也提到了其实大家更多的还是希望去探索原生的原生的会更加的有冲击力这个也是目前京东我们在内部去大范围去寻求相关的这样的一个创新的落地的一个目标那当然了其实前面提到了其实目前的话刚刚落地的其实应用场景里面的话从营销增长到我们的智能财富顾问了等等吧其实非常多的包括刚刚前面提到的我们整个交易code交易code是我们京东内部的整个基于大模型的整个代码生成的这样的一个能力那目前的话我们在京东内部的话数万名的员工都在使用这个交易code去做相关的代码的生成那目前整个代码的生成的采纳率也将近有20%多的这样的采纳率也取得非常不错的这样的效果那其次就是像我们也在对外做我们相关的一些这个能力的复栏包括我们给头部的一个新能源的这样的头部公司在结合这样的一个大模型加这个知识图谱去做它的整个这个知识中台覆盖到它整个新能源的整个电池的芯片的设计到整个全生命周期的这样的一个核心的一些组件的这样的设计全知识的这样的一个中台的这样的能力包括我们也为一些这种一个头部的航空公司去提供整个机器大模型在智能体端落地他的为员工去做一些这种咨询跟信息的这样的一个检索增强的这样的不同的场景的落地当然了就是说今天其实无论是在京东内部还是在我们服务web企业端的时候其实我们看到整个大模型它处在在不同的应用端的话可能它的改造的这个或者说它所占比它是不同的阶段从融合还是重构还是今天来讲的话我们真正去做原生的这样的大模型的这样的应用那实际上今天来讲的话其实我们落地刚刚看到的像刚刚PPT里面展示的其实在不同的方向的话其实目前都有大量的这样的创新了那当然了最后我们也是希望在京东我们希望是来自产业更多的也理解产业最终那我们也是希望能够服务产业融入到产业的过程中最终去期待在大模型机缘AI机器的整个场景里面能够蓬勃发展京东能够贡献一部分力量以上是我的汇报谢谢谢谢谢谢龚一诚先生带来的分享请您落座其实很多人都说今年是AIJC的应用元年那当AIJC遇上医疗这两者又将会碰撞出什么样的火花呢它能否成为医生诊断的得力助手能否帮助患者更便捷地获取个性化的健康建议又或是优化医疗资源分配提升医疗服务效率与质量所以接下来的时间让我们有请京东健康智能算法部负责人王国新先生带来AI技术重塑医疗健康服务的主题演讲掌声有请谢谢然后以及龚先生的一个演讲把我引到这个应用这个环节来实际上医疗应用我相信我们大家是最不陌生的一个应用我们绝大部分人都是医疗服务的一个消费者但是医疗应用我们都知道其实有一个所谓的不可能三角如果用咱们大白话讲就是又便宜又方便的然后质量又好的医疗服务这个世界是不可得的至少在医疗经济学的角度它是不现实的但是如果我们回望过去一百年的历史我们不用太早回望过去一百年的历史有一个关键因素让这三个不可能三角不断地在发生变化这个关键因素就是技术如果我们回想所谓的二十世纪的三大医疗变革第一个医疗变革是化学药就是说从以抗生素的代表为出现的化学医疗的出现六七十年代以X光和CT为代表的这种影像医学然后再以我们最近90年代为代表的这种基因和小分子的医学然后都让这种技术为整个医疗环境的一个变化插上了一个翅膀所以其实站在我们的视角我们是怎么判断这个人工智能这件事情也认为当前的人工智能也是这么一次最大的机会这个人工智能能让医学走出这个不可能的三角为我们提供这个低成本高质量然后以及是犯债的这么一个医疗服务这也是围绕着这个观点京东健康在过去两年的时间在整个大模型的方向制造制定了这个AI驱动的产品和解决方案的计划这个PPT怎么看呢首先第一点先看我们的技术底座我们在我们的技术底座在精一千寻的医疗大模型上
先看我们的技术底座我们在我们的技术底座在精一千寻的医疗大模型上构建了整体的医疗底座融合了这种多模态的这个临床推理的数据患者诊数据诊疗方案数据在这个基础之上我们构建了智能问诊智能诊断智能问答然后数字疗法等各种产品再把这种产品构建成四个核心的解决方案即面向患者的解决方案面向医生的解决方案面向医院的解决方案以及最后面向企业的解决方案这些解决方案一起和我们所有的合作伙伴共同构成了面向患者的健康监测诊断到治愈这么一个整体健康流程的方案这是我们AI驱动过去两年一个最大的工作说到我们这个大模型我们的精英签询大模型是做在延熙的通用大模型的底座平台上的这其实是我们在2003年7月份在北京发布的一个模型大概过去一年了业界的朋友在问说我们京东健康到底做了哪些事情在这个模型上做了哪些工作我想用一个核心的三点来去描述我们具体做的工作第一个工作是我们不断地在提升医学的安全围栏因为医疗的通用领域刚才何博士其实提到了因为大模型它本身是个概率模型它必然充满着幻觉如果在某些场景中它的准确度是95%可用的话在医疗场景应该至少是99%级以上才可用进行据实和据达比如说我们今天是一个医生应用的场景那它的据实据达率其实可以做得很低因为我们是一个人机协同的环境有医生为我们把关但如果我今天一旦是面临一个C段的用户场景面对一个C段的建议的时候那我们就必须守好我今天的诊断的准确率我今天能做诊断还是能做建议我还是能帮你进行一个更有效的一个医学资源的链接这是我们今天做的第一个工作第二个工作是所有的医疗条件医疗环境我们不论再怎么讲我们不论再怎么数字化实际上医疗还是以线下医院为核心为服务的这么一个医疗体系的在这个医疗体系中精英圈群过去一年做了一个很重要的事情这跟刚才曹博士的议题稍微有点像就是讲怎么把一个更大的模型和我们整个的医疗的信息化系统进行有效的结合而医学医院绝大部分医院它并不是一个有效的算力中心所以在这个过程中我们如何有效的去更好的使用院内的资源和院内的数据化系统更新有效的结合这是我们干的第二件事情第三件事情而这件事情是一定面向未来的而且是大模型真真正正特别医疗大模型真真正正能不走进C端成功的一个关键就是我如何把一个出厂设置的大模型能够和我们患者自身的个性化数据进行一个深度的结合我们都知道我们国家有大数据局如何去大模型和如何和我患者自身的个性化数据进行一个有效的整合进行持续的长周期的学习同时保持隐私这三件事情凑成了我们过去一年最核心要解决的问题总结来看第一个就是让模型学会它什么不会第二件事情让模型学会跟线下的数字化的系统进行有效的结合第三件事情就是让模型尽可能的除了利用它的世界知识之外还要广泛的应用个人的数据这是我们过去一年的工作下一页讲的就是我们今天都讲一个大模型然后大模型走向应用我相信今天在座的各位除了大模型本身的研究学者之外更重要的都是在了解咱们一个大模型怎么去改变我们现有的业态咱们怎么装在我们的业务场景中我这里其实讲了一下就是汇报一下我们京东健康对这个问题的思考实际上我觉得总结为三句话第一句话是要尽可能地贴近你的场景的数据训练或者话说如果用医学的角度我们就是要用科学实证的可靠数据实际上在我们整体模型的训练过程中我们发现有一种数据的价值是最大的那就是教科书这种数据本身对模型的效果最大同时如何去在您的这个业务场景中或者叫什么叫垂直的大模型或者通用大模型我觉得垂直大模型白话讲就一件事情它要讲这个行内人说的话它要获得这个行内人的认可在这个过程中用医学的角度就是我们要符合行政医学的指南共识的要求我们所有的结论要符合医学推理的要求这件事情是一个关键所以在因此在我们的大模型的持续训练过程中实际上我们引入了大量的医学知识专家系统专业工具和业务系统在这边举了一个肥胖适应症的这么一个案例来表示当一个医生看到了一个肥胖病患者的时候他的推理路径那么当你的大模型跟你的医疗场景结合的时候也得符合医院医疗的这么一套推理思路才有可能被这个行业的专家所接受咱们才有可能进行产业数字化的智能化的一个转型最后一点我觉得也是比较关键的一点就是识别合适的场景今天我们都说大模型今天技术在往前跑但是场景还在后面追一方面当然由于刚才提到的一些它的幻觉性的问题它的计算成本的问题它的隐私性的问题还有一点就是我们如何在这种复杂的环境中挖掘到合适它的场景所以我们在这里就是说在医疗这个环境最关键的是要回答我们能不能找到遵循行政实践的这么一种人机协作的流程而这种流程如果能被我们这个行业降本提效也好或者能被我们行业的增长也好所接纳
业务行业降本提效也好或者能被我们行业的增长也好所接纳这件事情我相信是一个大模型能不能在一个产业落地的一个关键然后这里就说到场景后面的PPT就比较轻松我们主要讲讲我们今天落地的一个实践场景跟大家汇报也希望是一个对这个行业的一个抛砖引玉第一个场景是实际上我们在多个三甲医院已经持续落地的一个场景我们今天都有看病的我们大家绝大部分人应该都多多少少会有一些去医院的一个经历大概都是排队两小时看病五分钟这是一个常见现象然后这两小时过程中可能进去医生跟你说几句话就开一个单子把你拉走了去做个检验检查去验个血验个尿明天空腹这是很常见的一个现象所以诊前的智慧服务就是我们大模型切入的第一个场景它的逻辑就是在您挂完号报到的时候我们就有一个数字的医生助理出现然后这个患者就可以跟这个专门科室的医生的数字助理去讨论你的病情上传你过去的家族史上传我们的最新的检验检查而这个数字医生在和医生的辅助在我们没有见面的同时就可以开具检验假驾单就可以开具入院单然后从而当您下次入院的时候可以带齐这样的信息数据或者是提前做好检验检查再去见医生从而极大的降低了在院内的等待时间降低了院内的拥挤程度同而提升了患者的满意度这是我们找到的第一个比较合适的一个场景第二个事情呢就是咱们都常说这个跟医生AI的结合是更好的成为一个医生的co-pilot可能是我们的一个目的这件事情一方面当然是可以提升的降低医生的工作强度降低漏诊率提升我们医疗服务的一个重要性另外一件事情我可能作为一个人工智能方面的人工智能方面的从业者也提一下这件事情对人工智能的意义实际上我们自己本身是一个数据大国但不是一个数据化大国我们绝大部分诊间的数据我们绝大部分诊疗的数据在我做医学模型的时候实际上都是严重缺失的而这种诊间的这种医疗AI的产品能极大的提升诊间对话医生决策医患换教的一个数字化水平而这种数据的所谓的沉淀会成为我们这个行业或者成为这个医院甚至成为这个医生以及这个患者未来的个人数字账号中的一部分然后从而来加速我们这个行业的迭代这是从应用走向我们的产业发展然后第三件事情不好意思第三件事情就是当然我们看完病了绝大部分我们的患者大部分都是慢性病患者看完病了欣喜地回到家但是所有的慢性病治疗有个最根本的逻辑就是脱落率什么意思呢医生跟大家说早点睡我相信咱们在座的各位可能都没完成医生跟大家说这个药一定要每天吃三粒然后持续一个月我相信大家坚持一周的人都很有限同时我们跟IoT的端测的智能化设备进行一个打通将我们的真实的身体数据进行一个院内的流动当然是在患者允许的情况下这样就能把咱们医疗的数据从院内和院外进行一个全面的打通从而这件事情帮助医院了帮助医院更好的了解患者帮助患者更好的康复同时它其实能极大的降低我们的医保费用降低我们的发病二次发病率这是我们诊后的管理当然我们还有一个不好意思当然我们还有一个纯C端的医用产品叫ABC健康这是我们的一个小程序大家可以使用这是我们在AI未来的AI的一个C端的探索我们的一个目的就是希望能做一个产品当然他还在迭代虽然能做一个产品让这个产品成为我们每一个人最佳的健康的建议者以及再次行动的建议者当我们身体出现疾病当我们出现不适的时候可以通过ABC健康可以通过京东健康的服务从而去连接各种医疗资源这是我们的目的然后最后也是希望在一个AI这么一个大背景下不论是巨声智能也好人工智能也好所有的这些技术最终落地到产业的时候能够推动我们中国医疗产业的明天谢谢各位非常感谢王国勤先生带来的精彩分享请您落座我们看到其实在今年京东云延新数字人更是服务了超过5000家的品牌商所以接下来的时间呢就让我们用热烈的掌声有请一力消费者运营数字化负责人范文竹女士带来AIGC成为生产力的主题演讲掌声有请有请各位领导同事还有各位同行们大家好我来的这个可能应该是整个产业链算是最前沿的应用端了刚才很多嘉宾都提到了应用以及实际的场景那其实呢一例就是算是消费品类代表最前沿然后我们到底实际去怎样应用AI的能力也是希望跟大家做一个简单的分享吧包括我们现在的一些功能上的试点试水也好还有我们的整个能力上的一个布局也希望跟大家一同探讨一下看我们在整个消费品甚至是零售以及营销这个行业里面到底有什么新的机会点那简单介绍一下伊利这个集团相信大家都不陌生可能是个喝过牛奶的消费者都知道我们这个品牌那其实呢我们已经是做到了全国乳业的第一名以及世界乳业的前五然后那最新的财报数据呢也是表现得非常亮眼那其实除了大家耳熟能详的
财报数据也是表现得非常亮眼其实除了大家耳熟能详的一例品牌以外像经典MC 乔乐兹然后冰淇淋 奶酪冷饮酸奶甚至是矿泉水类以及宠物食品其实整个集团都在涉猎这样一个乳制品的集团到底我们来这样一个人工智能大会是做什么呢一方面其实是跟大家分享一些我们实践另一方面其实我也是带着任务来的因为我所在的部门叫做伊利的数字科技中心是起到了一个带领集团以及赋能整个集团在做整体数字化转型这样一个工作那我所分管这边呢是在营销和消费者运营的领域那也是因此我们也是这次带着任务来然后一方面是跟大家分享探讨另外一方面大家朝后看到我们分享就会看到啊我们在现在的能力应用上还有很多很多的空白也是希望借此机会能够找到更多的合作伙伴包括跟京东云的同事们然后我们共创更多合作的一个机会点那今天我们分享的主题叫做AIGC成为生产力其实一年前我们听到这个AIGC它基本上是停留在我们集团的领导层的会议上每次我们跟领导一说这个我们的SORA生成的视频多么的酷炫然后我们行业内有什么应用然后领导就会再问我们说然后呢就是跟因力到底有什么关系那一年过去了我们现在可以勇敢地站在我们集团的会议上跟领导汇报说那借助外部的一个新制生产力能够助力伊利集团然后去进行整个这个引领乳制品行业发展这样的一个新型的一个动力那简单介绍一下就是刚才所说的一些产品线也好不论是阿姆西啊金点啊还有像我们的冷饮畅清以及婴儿营养品以及成人养品包括这个奶酪品类现在都是在做全产业链的一个布局的那整个集团的愿景呢是希望能够成为全球最值得信赖的健康食品的提供者那所以看到这个我们的这个宏大的愿景到底跟Ni有什么关系啊跟大家分享一个数据就其实我们应该是在2021年开始才陆续去布局我们的品牌内容营销数字化的产品和中台那也是看到了现在可能每天将近十多亿的消费者是在新媒体渠道去进行信息的一个获取那我们跟消费者接触的模式就从原来的可能我们做一次广告我们做一次活动的赞助变成了我每天需要分发数十万甚至是几百万各个触点各个消费者所能看到的地方的内容去用我们的内容结合我们的产品打动我们的消费者因此我们也是把它定位为一个大势所趋就是原来我可能一年生产几条视频那尤其是我们现在可以看到的趋势小红书也好抖音也好那不再是一个单纯的媒体我们是把它定位成一个媒体结合的一个阵地它是承载我们整个用户生态然后去链接消费者的一个生态平台那这两个平台上如何发力以及像内容电商和平台电商如何发力就是我们现在需要探讨的一个能力了那因此呢对于我们内容运营同学和品牌营销同学的能力要求也从原来的我们可能基本上都是靠个人苦思冥想想创意想demo想文案那到现在基本上我们的大数据的应用以及我们的数据是武装到我们每一个内容运营同学的从头到牙齿了基本上大家从一开始的内容数据洞察以及趋势的分析再到最终的应用以及效果的反馈和评估全面在借助AI能力升级那其实我们刚才所介绍到的就是2021年开始我们自建了一套数字内容中心其实也是结合了行业内数字化可能算是1.0时代大家都听过的一些产品比如DEM也好呀CMS也好呀去搭建这种基础化的线上线下一体化的内容管理平台以及内部的审核分发的流程然后从今年开始原来没有的功能去进行一个加强那现在我们的这一个数字内容中心版本其实已经算进化到2.0了是从内容的洞察生产 创意 制作我们的分发审核然后一直到数据的回流和数据的自动分析和洞察建议形成了一个完整的闭环这其实是可能终端产业链的品牌营销同学他到底怎么能用AI的一些小小的算是启发吧也是我们在这半年之中摸索出来的一条能怎么用AI工具的一个链路那这个后来呢我们又自己卷了自己一下这一套平台是我们自己搭了一个壳那其实这个壳是借鉴了我们自己归纳出来的到底AI在品牌营销端能用到哪四大场景那有哪些核心的工作流程那最核心的就是左边所看到了从洞察 创意 制作还有管理分发和价值评估这个其实是终端品牌营销同学最痛的痛点之一也是所有事情都规避不了的这个五大流程那我们数字科技中心做的就是找遍市面上基本上所有的AI工具然后一个一个往里填如果有的话那就把这个工具塞进去如果一个工具不够我们就搭配一条的产业链上的工具然后给到我们内容运营同学一个支持然后上面所看到的呢是我们自己梳理出来的一个品牌营销如何在现在这个时代上借助AI工具也好数字化工具也好去做内容营销的一个
时代上借助AI工具也好数字化工具也好去做内容营销的一个小小的方法论会看到它基本上涵盖了我们能听说的所有的社交媒体的触点以及媒体触点上所有面对消费者可以分发内容的这个渠道那在每一个渠道上都有不同的内容的知识内容的格式内容的规格然后我们再去匹配不同的这个训练好的模板然后大批量的去生产内容供给到前线然后再通过我们下面所说的一整条闭环的流程然后帮助内容运营同学去提效打造这套工作台之后呢给大家详细介绍一下就是在工作流程之中具体我们都干了什么那首先是内容洞察的这个流程提效洞察这个其实市面上有无数的就是在1.0时代的数字化工具可以让我们用不论是这个social listening也好呀还有这种大数据洞察雷达也好呀就是伊利其实并不缺这样的一个工具但我们的内容运营同学缺的是工具真的特别的多就是没有人能从每天好几百条的这个趋势以及热点之中一眼就看出我们今天到底要跟哪一个热点哪一个热点是跟我们最匹配的我们去跟是没有风险的所以在这种需要大量的重复去判断甚至要跟经验去判断的这种过程之中就毫无疑问AI是可以帮助我们解决问题的那我们就引入了一些行业上的这种洞察工具 listening还是跟阿姆西适合还是跟金点适合会基本上一秒钟的时间给到我们一个相对符合的答案那内容运营同学有了这个答案就可以有地方使得去进行一个下一阶段的一个思考而不是在像茫茫大海捞针之中一样我一个一个去试到底哪个热点是我今天有可能去爆掉的一个素材也就减轻了我们后链路一个试错的成本那在第二项呢就是内容策略和创意工作流程提效这个其实说老实话我们现在也没有摸索出什么特别靠谱的这种闭环的应用我们其实是希望可以借助这个生成师的AI一战式帮我们的同学打造出来这个灵感生成比如我们看到了一个热点说今天有一个什么小猫这个跳舞火了其实我们的内容运营和品牌营销的同学希望下一秒可能就看到一个一模一样的小猫拿着一个伊犁的牛奶然后再跳一样的舞但现在可能我们能做到的是呢慢慢地去训练它先给一个demo然后我们后面再通过无数的这个模型的训练和图片的训练让这个小猫真的能跳起舞来然后达到这个用土生视频这样一个能力那所以呢刚才看到就是我们堆的工具比较多就是应用比较多的一个板块其实就是内容工生产工具上的提效这可能也是现在真的解放了无数00后的内容运营同学像数字人口播也好我们的直播切片也好然后AI的混减批量出视频然后文生图然后外呼关怀然后还有文本和语音的自动的合成然后以及我们基于脚本的数字人直播可以说是常态化的植入了我们所有的营销的全链路基本上像这个混简视频每一个内容运营熟练的工种的同学他可能一天原来只能生产出三四条就已经到头了一个月基本上是80到100条的一个效率但现在基于优质的模板可能一个小时他能生产出100条那我们这100条去干什么呢就是原来我只能通过一个抖音账号去进行发送但有了这种AI的混解我现在是可以动用我们数以万计的导购然后我们的员工然后我们经销商和门店的触点然后一天就向外发送将近上千上万条的一个视频然后每一条视频去传达不同的产品不同的理念然后去对于我们的消费者然后提供不同的内容然后去带动我们产品的销售那举个例子这个数字人直播这一侧这点其实基本上是我我刚来伊犁就是2021年开始就用到了从最早的这个可以看到第三个是我们当时用了一个QQ星的卡通人从我们的店那边去扒下来的那虽然说让他完全替代掉真人我觉得短时间不是很现实啊但是我们用他干什么呢就是白天我们用我们的真人主播然后不断去测话术然后去测最好的表达的方式然后那晚上我们就把这个能促进成交能促进转化和停留的这个话术留下来然后反向去训练给我们的数字人我们也应该是乳制品或开销行业就是这种数字科技中心里面少数自己有训练师团队的这么一个部门现在我们将近有四五个训练师的同学他们其实不做纯营销的事情他就是根据我们最优质的话术去不断地训练我们各个平台直播所用到的数字人然后每天去积攒我们最优质的一个画数然后再去打磨所有的通用的画数库和定制的画数库那大概成本降低到什么级别呢算过一笔账啊如果是真人直播可能最少最少一个小时也要一百块钱但现在的这个直播每天晚上基本上可以24小时开着只要没有真人直播的时候就可以去应用那可能也就四五块钱一个小时而且它可以规避的是这些直播新手然后或者是说错话带来的一个风险那反而在某种意义上可以帮我们规避一些不确定性那确定性就交给那些很成熟的主播在白天去沉淀更好
那确定性就交给那些很成熟的主播在白天去沉淀更好的一个方式去应用那刚才我看那个京东云的老师可以分享到的是LAS的应用然后在我们数字科技中心就是我们其实算是从SaaS开始用然后我们部分技术同学他们可能会用PaaS级别的这个服务然后那我们前端的运营同学就是基于这些很成熟的软件工具在不断地去梳理出来运营的模板运营服务流量服务甚至是咨询服务然后再带着我们后面一系列的这个技术人员我们的算法底层的人员然后去为我们的品牌营销同学提供全方位的一个解决方案就有点像在伊利数字科技中心就是内部搭了一个这种整合的解决方案提供的专家的一个团队了然后那可能我的品牌营销的同学他根本不需要知道说啥叫算法啥叫算力他只知道说这一套东西我们是给他打磨出来的一个成熟方案然后用就好了然后在这个内容生产的图生文生图和图生视频端呢其实也有一些探索啊我们除了应用一些成熟的软件和我们合作伙伴以外呢然后自己也会有自己的算法训练师然后还有我们的这边的运营同学去试试面上所有的开源工具刚才我提到了我们还有一个宠物食品线可能大家不太知道宠物品牌叫医保就是小猫小狗因为说实话原来我们拍小猫小狗特别的废猫就你这个一天可能他万一不开心了猫主子他就不跟你玩了对吧那现在呢我们直接训练出来了一套我们的猫模特我们的狗模特你想让它干啥干啥你想让它拍几条就拍几条然后右侧是我们基于这些训练好的图片然后用图生视频让它可以动起来的一个广告片那原来做这种真猫真狗的拍摄的广告片加上整个场景搭建可能要10到20万一条但这种类型的图生视频的广告片基本上算下来加上我们自己的人可能也就两三万一条所以这个的成本结降对于整个品牌营销来说是十倍以上的一个效率提升和成本结降还是非常的显著的我们同学能帮忙点一下吗这个右边这个小狗哈喽你眼中的狗狗在疯狂开家狗狗眼中的自己在认真收纳你眼中的猫咪在打浪花瓶当你眼中的自己在擦拭桌子别忘记他们只是个毛孩子四个二堂节毛孩子也有自己的专属能耐伊利乳液研究过联合研制所有小猫小狗全是AI的都不是我们这个拼来的真猫真狗所以这的确在这个内容生产方式上对于品牌营销人来说还是一个颠覆性的改变所以接下来其实我们在这个领域吧还会进行持续的一个探索不光是开源也好呀还是我们跟合作伙伴自己去摸索也好呀然后需要把里面有一些针可能还有一些这个抽盲盒的感觉啊然后慢慢给它固化下来让大家看着不那么跳戏然后另外就是在这个AI外呼能力的一个应用了外呼其实大家说到可能想到第一耳是这个骚扰电话但其实现在我们并不把它定义为说加粉或者是用户运营的一个工具我们是怎么用它呢可能我们会请我们的明星代言人授权给我们它的声纹或者是我们自己找我们内部的专家比如说我们的领导人我们的一些营养师也好就是带有很温度的这种语音语调然后全程录下来采集下来那接下来就不断地用这些特别好听的或者是我们现在MC代言人迪丽热巴小姐姐我们的伊丽代言人李线的声音然后去给我们消费者在生日的时刻然后打上一个很关怀的电话发挥不了我们技术上的一个能力所以我们更希望把它当成一个体验式的关怀式的一个工具能够真正用AI技术合成更多好玩的有意思的电话如果说之前关注过经典的这个消费者啊可能大家会看到说即将应该开始吧然后会有更多的这个明星语音的一个预约之前就已经做过像王心凌也好呀还有我们刚签的那个浪姐的一些这个明星代言都是可以体验到的然后最后的一点呢倒数第二点呢就是我们关于内容管理工作流程上的提效这块其实我觉得它还挺留在数字化的时代还没有进入智能化的时代现在我们依旧有很多的素材是靠AI打标抽条和人工打标结合去开展的我们现在应用的部分AI打标的工具可能对于我们图片和视频抽条打标的准确率大概能到95%就已经算很不容易了可能在服装领域会更高一些但是食品行业的确还有一些不确定性以及难以让AI学出来它到底是什么样的场景这样的一些视频和图文所以接下来我们可能大力发展的就是在AI合规检查然后对于伊利每天生产出的十几二十万条素材然后我们需要自动
产出的十几二十万条素材然后我们需要自动让它去抽调出来我们的素材然后再去匹配上最合适的标签然后再根据这些标签和数据表现的反向沉淀出来到底哪些类型标签的内容是我们下一步能够产生更好转化的内容我们就继续照着这个标签去生产内容那些不好的标签或者是表现不好的内容就慢慢淘汰掉这其实是发挥AI能力最大化的一个方向对然后最终我们整个分享就到此为止了说实话今天整个分享非常技术我们站在这边就属于班门弄斧但的确算是产业链前端行业应用的一个小小的先行者然后后续也是希望跟各位大咖们各位专家们我们共同可以在营销端以及服务消费者端探索出更好的模式好 谢谢大家谢谢 谢谢范女士带来精彩的主题分享请您稍作休息最近呢 各位领导 各位嘉宾朋友要送给大家首先要非常感谢今天给我们带来分享的所有嘉宾朋友你们的分享就像高贤 启后坤 金玉 换新生给我们带来了关于AIJC更多新的思考启发了新的可能那其次呢要非常感谢我们现场以及线上的所有嘉宾朋友你们的认真聆听和热情参与让今天的活动变得更有活力和价值那么最后呢要非常感谢为此次活动付出的所有幕后英雄你们的辛勤付出和默默奉献以及对于每一个细节的认真把控才能够让今天的活动得以圆满呈现所以我提议让我们最后一次把最为热烈的掌声送给此次活动也送给优秀的自己谢谢大家我是主持人李静期待在今后的活动当中和大家再次相会开启更多的精彩内容谢谢
