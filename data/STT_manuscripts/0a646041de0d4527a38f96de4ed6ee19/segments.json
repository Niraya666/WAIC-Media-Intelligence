[
    "youyouyougood morning, dear guests, students, and teachers.Today, we gather here to attend the international AI front-end technology conferencehosted by the Center for AI Safety and the CIS Center for AI Safety in Shanghai.On behalf of the host, I would like to express my warmest welcome and sincere thanks to all of you.We know that we are living in an age where AI drives us and makes us struggle.AI, especially large-scale technology, is developing at an unprecedented speed and scale,and it has a profound impact on our daily lives.As a technology worker in the field of AI, and as a security researcher in the field of large-scale technology,I think I and all of you have experienced a huge change in the change of the world.We can see that with the progress of large-scale technology,large-scale technology has become more and more popular.large-scale technology has become more and more popular.With the increase of very large-scale technology,artificial intelligence has become the highest ability.Artificial intelligence means that the ability you did not have in training can appear in the future.This is a great challenge for us in the world of traditional security.Because traditionally we consider artificial intelligence to be a tool.Now it is said that the ability it has is very likely that I could not predict it in the training stage.So, plus artificial intelligence, it is also a basic, negative basic.And common artificial intelligence is now developing our future entire social infrastructure,so under this context,and the safety of artificial intelligence and governance are the key points of our attention.Today, we are also honored to have the top scholars from different countries in the field of AIand the leaders of the industry to bring us a deep and exciting speech and a deep discussion.We also saw that at this artificial intelligence conference,the safety and governance of artificial intelligence has been raised to unprecedented heights.The theme of our conference is AI for Good, for Good for All.At the opening ceremony of the conference,Prime Minister Li Qiang proposed to promote the development of artificial intelligence,maintain the safety of artificial intelligence, and build the artificial intelligence governance system.There are two of them, and two of the first three are related to safety and governance.This fully reflects the importance of the conference on the issues of safety and governance.I think this is also a response to the growing demand for AI security and governance in the global scope.At the opening ceremony,the Shanghai Declaration of Human Rights was also proposed.This declaration not only emphasizes the ability of AI to adapt to the cross-cultural background,but also raises the level of strategic development of AI, especially the development of security.We know that the security of AI is a common problem faced by all of us human beings.This declaration also calls for the adaptation and fairness of AI technology in different cultures and social backgrounds around the world.I remember that some numbers were also used in the report at that time.For example,although the big model,actually,I want to say that the big model,I want to say that the big model,very much,and a lot of people are using it.Not only is the user,but the technological advance,but half of our population in the world now can't access this kind of AI technology.Many developing countries can't fully develop from it.So we need to build a system that is more inclusive of various AI ecosystems.We are also the main unit in this conference.Shanghai AI Lab has always been developing most universal AI technology.We use more efficient methods to make big models,and then use AI as our major responsibility and mission.That is to say, our lab has been using security as our main direction from the very beginning.Today, Mr. Zhao Jingbo, who is also our host in our industry,is actually an important person in charge of our AI direction.In this, we have been working on the construction of the databaseand then we also pay attention to the safety of the AI big model.That is to say, we have done a lot of calculations related to it.In fact, there is another job that we are in charge of in the network and air association.Today, Mr. Xiao is here.That is to say, we have a work group with security.This work group includes major domestic big models and the main institutions of AI research and development,as well as our major enterprises,such as Tsinghua University, Fudan University, Jiaotong University,as well as Huawei, Tencent, Alibaba, and so on.These companies are all in it.We have developed a lot of work goals in this association,which is to provide a good foundation for the big model industry by means of the way of business control.On the one hand, we have provided a good foundation.On the other hand, we have also considered many problems.In fact, in the middle of the foundation process,we need to guide the country in a very good way.This kind of regular guidance has a policy framework.But the policy framework really needs to be developed.Everyone knows that it needs to be refined and developed.We have conducted a lot of business interviews.We should continue to develop this.The lab also knows that the big model industry is in the process of developing.The lab also knows that the big model industry is in the process of developing.The lab also knows that the big model industry is in the process of developing.We also noticed that the security management of AI is a very international thing.We pay great attention to international cooperation and exchanges in this area.Today's meeting is also an important step to promote international cooperation and exchanges.Today's forum will mainly cover a few topics.The first is the status and problems of the AI security field.As you know, with the widespread development and widespread application of AI,the risks and capabilities of AI are related.On the one hand, we can see that the risks are getting bigger and bigger as the AI capabilities are improved.On the other hand, we can also see that the improvement of AI capabilitiesalso provides us with new technical means to deal with this kind of risk.The big model industry is very capable.The market also provides us with a new approach to dealing with the wide range of security issues.On the second hand, we will discuss the security of AI.This is also an important point that the academic community is paying attention to.In the future, we know that the development of AI will eventually surpass human intelligence.When it surpasses human intelligence, how can we have a system?I think this system not only includes technology, but also includes social systems and governance systemsto ensure that it will not be out of control in the future.This is a very important aspect.We have also seen a phenomenon in this regard.In fact, whether it is in production,in the industrial world,or even in the scientific world,our investment in the improvement of AI capabilities is far greater than our investment in security.I think everyone is familiar with the story of OpenAI.In fact, in addition to the story of OpenAI,we also know that if we compare the computing power we have invested in the improvement of AI capabilitieswith the computing power we have invested in security,I think it may be a 10 to 1 or even a bigger number.So in this regard,I would like to draw more attention to society,including the scientific world,the academic world,and the industrial world,so that we can make better investments in AI development.The third aspect is international cooperation in the field of AI.We are exploring how to strengthen communication and cooperation at the AI levelthrough international cooperation.Because this is a very big global problem and technological challenge that we are facing.Then we will respond to this global problem together.I think through today's exchange and discussion,we hope to promote the development of AI capabilitiesin the field of AI,especially those of us who are attending today,to understand and cooperate in the field of AI,to jointly promote the sustainable health and development of AI technologies,and to make contributions to the welfare of mankind.Finally, I wish this conference a successful one.I also hope that the guests of our conferencecan get a precious inspiration from this technical feast.Thank you.Next, let's welcome Dan Song,Professor of the Department of Electrical Engineering and Computer Science at UC Berkeley,to deliver her keynote speech titledTowards Building Safe AI, Challenges and Future Directions.Dan Song's research areas include AI safety and privacy.He is a recipient of the Microsoft Fellowship,the Gagnon Fellowship,and has been recognizedas one of the bestand one of the most influential scholars in computer security.Dan Song, please.Okay, great.Thanks, everyone, for being here.It's my great honor and pleasureto give the keynote for this great conference.I'll talk about...............Okay, okay, so there is some mix up on the slides, but that's okay, I'll just give thistalk.That's fine.Okay, yeah, so I'll talk about building responsible AI.As we all know, large language models and also the AI technologies has been growingexponentially and they are empowering rich capabilities in all different areas.And also we see that the AI technologies have been making rapid advancements on AI modelperformance and even reaching, even surpassing human performance on various benchmarks andso on.However, as we deploy AI technologies, it's important to ensure that the AI is able tobe used for the responsible use of these AI technologies.And also the various governments around the globe also have been issuing guidance andregulation to emphasize this as well.And here I want to really emphasize that a particular aspect for responsible AI alsois that as we deploy machine learning, it's important to consider the presence of attackersfor a number of reasons.First, history has shown that attackers always follow the footsteps of new technology developmentor sometimes even lizards.And also this time the stake is even higher with AI.As AI controls more and more systems, attackers will have higher and higher incentives tocompromise these AI systems.And also as AI becomes more and more capable, the consequence of misuse by attackers willalso become more and more severe.And hence, as we deploy AI, it's important to ensure that the AI is able to be used forthe responsible use of these AI technologies.And also it's important to ensure the responsible use of AI, especially in the adversarial setting.For responsible AI, there are many different challenges given the interest of time.I will only talk about the first one, how we can ensure trustworthy AI.And also there are other important aspects as well, including how we can mitigate themisuse of AI and also ensuring responsible data use and the proper data evaluation.Thank you.Thank you.Thank you.Thank you.For trustworthy AI, there are actually many different aspects, including privacy, robustness,hallucination, and many others.Given the interest of time, again, I won't actually go through all these different aspects.So privacy is one important aspect where we want to, because the models are trained onsensitive data.And hence, it's important to show that it's important to protect the privacy of the training data.And our earlier work has shown that these models, even when attackers don't know the details about the models,attackers can, by just querying these large language models, extract sensitive information from the training data.And also, on our recent work, we also have developed a comprehensive framework for evaluating privacy issues of these large language models,including many different types of attacks and defenses.And also, our work has shown that actually this privacy leakage problem actually worsens as the model size increases.And hence, we need to also develop defenses against...these privacy issues as well.Our earlier work showed that actually, by trainingdifferentially private models, it actually can help mitigate these privacy issues.And also, with the large language models, actually,using differentially private fine-tuning can also help...help protect the sensitive data in the fine-tune... the sensitive information in the fine-tune data as well.And again, so our... in our work, we have done some comprehensive evaluations on this.So, as I mentioned, right, given interest of time, I won't actually go into details on the privacy aspect.So now, I'll move on to the next aspect, which isintegrity of the model outputs, in particular for, like,in the setting of adversarial attacks.So, given our earlier work, and also other researchers' work as well,so, so by now, the community has learned that this kind of type of adversarial attacks,such as adversary examples, they are prevalent in deep learning systems.They can essentially... essentially, all different types of deep learning models,and in different domains, they all... they are all vulnerable to this type of attacks.Where attackers...simply can manipulate...manipulate the inputs to the model. In many cases, the perturbation is really small. That'sactually not perceptible by humans. But however, the maliciously-perturbed inputs can causethe model to misbehave. For example, give the wrong prediction or, for example, in largelanguage models case can cause the model to essentially lose its alignment, safety alignment.And so the field of adversary examples also has grown essentially exponentially as well.So when we started working in this field, there were actually very few papers everyyear in the right, in this topic.But now, we're working on this.We essentially have thousands of papers every year on this topic. And the artifacts of someof our earlier work now actually has become a part of the permanent collection at theScience Museum of London, which is a rare treat and honor for a scientific researcherto have your work actually be exhibited in the museum.So now, when we talk about safety-aligned large language models, adversarial attacksalso are effective on safety-aligned large language models as well.And one of our recent work called Decoding Trust, it's actually one the outstanding paperawards in 2023 in Europe, developed the first comprehensive trustworthiness evaluation platformfor large language models.This, yeah, is what was called a trustworthiness evaluation platform for large language models in the West.That's the full picture of the court, the real track, and that's the whole picture of realLARGE LANGUAGE MODELS THAT EVALUATES MANY DIFFERENTPERSPECTIVES IN TRUSTWORTHINESS FOR LARGE LANGUAGE MODELS.AND OUR WORK HAS DEVELOPED VARIOUS NEW ALGORITHMS AND ALSODEVELOPED VARIOUS ENVIRONMENTS INCLUDING BOTH V9 ANDEVASOR ENVIRONMENTS TO EVALUATE THESE LARGE LANGUAGE MODELSFOR THESE DIFFERENT PERSPECTIVES.AND OUR WORK HAS SHOWN THAT ESSENTIALLY IN ALL THESEDIFFERENT PERSPECTIVES, THE MODELS ARE VULNERABLE, AND INPARTICULAR THEY ARE VULNERABLE TO ADVERSARIAL ATTACKS, AND WECAN EVALUATE THE MODEL ALONG THESE DIFFERENT DIMENSIONS FORTRUSTWORTHINESS.AND ALSO, OTHERS HAVE SHOWN THAT WE CAN EVALUATE THEVARIOUS ENVIRONMENTS.AND WE HAVE SHOWN, INCLUDING SOME OF OUR OWN WORK, ALSO HAVESHOWN THAT THESE, RIGHT, SO THE, THESE MODELS, THEMULTI-MODEL MODEL ALSO HAS ISSUES WITH ADVERSARY ATTACKS.SO ESSENTIALLY WE CAN CONSTRUCT ADVERSARIAL INPUTS TO CAUSETHESE MULTI-MODEL MODELS TO LOSE SAFETY ALIGNMENT AS WELL.AND SO, SO FAR I'VE TALKED ABOUT THE ATTACKS THAT HAPPENSAT INTEREST TIME.AND THESE ARE CALLED ADVERSARY EXAMPLES.ALSO SOMETIMES IT'S IN LARGE LANGUAGE MODEL SETTING, IT'SWITH, THROUGH PROM ENGINEERING, AND THAT CAUSES A JAILBREAK.AND THESE ATTACKS CAN ALSO HAPPEN AT OTHER STAGES IN THEMACHINE LEARNING PIPELINE, INCLUDING THE, THE TRAININGSTAGE AND THE PRE-TRAINING OF FINE TUNING.IN THIS CASE, ATTACKERS CAN, CAN INCIDENTALLY, YOU KNOW,ESSENTIALLY INCLUDE WHAT'S CALLED POISONED DATA POINTS TOCAUSE THE MACHINE LEARNING TO LEARN THE WRONG MODEL.SO, AT THE FINE TUNING CASE, OTHER RESEARCHERS HAVE SHOWNTHAT ATTACKERS BY JUST SIMPLY CONSTRUCTING A VERY SMALLNUMBER OF THESE MALICIOUS POISONED DATA POINTS TO CAUSETHE, THE FINE TUNING MODEL TO LOSE THE SAFETY ALIGNMENT.SO, OUR WORK, OUR ORIGINAL WORK ALSO HAVE PROPOSED ASTEALTHY TYPE OF ATTACK CALLED A TARGETED ATTACK WHERE WESHOW THAT THE MODEL ACTUALLY, DURING NORMAL CIRCUMSTANCES,IT CAN BEHAVE NORMALLY.FOR EXAMPLE, A FACIAL RECOGNITION SYSTEM THAT NORMALLYRECOGNIZES THE FACES CORRECTLY.BUT HOWEVER, WE CAN, ATTACKERS CAN IMPACT A BACK DOOR WHEREUSERS, ANY USERS THAT ACTUALLY WEAR A SPECIFIC TYPE OF GLASSESWILL THEN BE MISRECONNIZED BY THE MODEL TO BE A PARTICULARTARGETED PERSON.AND HENCE, THIS IS A BACK DOOR ATTACK WITH, WITH TARGETED,TARGETED ATTACK.AND RECENT WORK FROM OTHERS SUCH AS ANTHROPIC HAS ALSO SHOWNTHIS TYPE OF BACK DOOR IS, CAN BE EFFECTIVE IN LARGE LANGUAGEMODELS AS WELL.WHERE, UNDER NORMAL CIRCUMSTANCES, THE, THE MODELWILL JUST GENERATE NORMAL CODE.AND USUALLY CORRECT CODE.BUT WHEN CERTAIN KEY PHRASE APPEARS IN THE PROMPT, THEMODEL WILL THEN ACTUALLY GENERATE VULNERABLE CODE.SO ALL THESE ARE EXAMPLES, ILLUSTRATING THAT THESEMACHINELINIAL MODELS ARE VULNERABLE TO ADVERSARY ATTACKS.AND IN FACT, THE WHOLE COMMUNITY HAS BEEN REALLYPRODUCTIVE AND CREATIVE IN GENERATING MANY DIFFERENT TYPESOF ATTACK METHODS AND TECHNICS AND SO ON.AS I MENTIONED NOW, EVERY YEAR WE HAVE LIKE THOUSANDS OFPAPERS ON THIS TOPIC.BUT HOWEVER, ON THE OTHER HAND, THE PROGRESS IN THIS SPACE HASBEEN EXTREMELY SLOW.SO THE PROGRESS, SO ESSENTIALLY, SO FAR IN THE COMMUNITY, WEHAVE MADE CLOSE TO ZERO PROGRESS IN ADVERSARY DEFENSES.AND THERE'S NO EFFECTIVE GENERAL ADVERSARY DEFENSE.AND THIS POSES A HUGE CHALLENGE FOR AI SAFETY.ONE IS THAT, AS I HAVE SHOWN, CURRENT AI SAFETY MECHANISMSARE EASILY EVADED BY ADVERSARY ATTACKS AND ALSO ANY EFFECTIVEAI SAFETY MECHANISMS NEED TO BE RESILIENT AGAINST ADVERSARYATTACKS, AS I JUST SHOWED AS WELL.AND, HENCE, ESSENTIALLY, ADDRESSING ADVERSARY ROBUSTNESSISSUE, IT APPEARS TO BE A PREREQUISITE FOR ACHIEVING AISAFETY.AND, HENCE, THIS IS A HUGE OPEN CHALLENGE FOR AI SAFETY, HOW WECAN ACTUALLY DEVELOP AI SAFETY MECHANISMS THAT ARE ACTUALLYRESILIENT AGAINST ADVERSARY ATTACKS.SO NOW I WANT TO TALK ABOUT THE PROGRESS.AND THEN WE'LL JUST BRIEFLY TALK ABOUT SOME OF THE, OUR RECENTWORK AS POTENTIAL DIRECTIONS FOR, TOWARDS ADDRESSING THIS ISSUE.SO THE FIRST ONE THAT I WANT TO MENTION IS THE, OUR RECENT WORKWITH ALSO OTHER COLLABORATORS, RIGHT, INCLUDING DAN, WHO ISCO-ORGANIZER OF THE WORKSHOP AS WELL, ON REPRESENTATIONENGINEERING.AND THEN WE DEVELOP WHAT WE CALL STIMULUS AND TASK.ESSENTIALLY THESE ARE CONTRASTIC INPUTS FOR A CERTAIN TASK.AND THEN WE OBSERVE THE MODEL ACTIVATION DURING INFERENCESTAGE.AND THEN FROM OBSERVING THE MODEL ACTIVATION AT THEINFERENCE STAGE ON THIS CONTRASTIC INPUT, WE THEN BUILDMODELS THAT ESSENTIALLY HELP CORRELATE WITH CERTAINBEHAVIORS OF THE MODEL.AND ALSO HENCE GIVE US PREDICTION MODEL BEHAVIORS.SO WITH THIS APPROACH, WE DEVELOP WHAT WE CALLREPRESENTATION ENGINEERING.SO IN PARTICULAR, FOR CERTAIN TYPES OF MODEL BEHAVIORS, WEIDENTIFY, FOR EXAMPLE, CERTAIN DIRECTIONS AS CERTAIN LAYERSFOR, THAT CORRELATES WITH CERTAIN TYPES OF BEHAVIORS OFTHE MODEL, INCLUDING, FOR EXAMPLE, WHETHER THE MODEL ISHONEST OR WHETHER THE MODEL IS HALLUCINATING AND SO ON.SO THIS TYPE OF APPROACHES HELP US TO ESSENTIALLY, DURINGINFANT TIME, TO MONITOR THE MODEL BEHAVIOR, ESPECIALLYMODEL BEHAVIOR RELATED TO SAFETY ASPECTS OF THE MODEL.AND THEN AS A FURTHER STEP, USING THIS INFORMATION, IT CANALSO HELP US TO DO WHAT WE CALL REPRESENTATION CONTROL.SO ACTUALLY WHAT THIS ENABLES US TO DO IS TO, FOR EXAMPLE, BYIDENTIFYING THE CERTAIN DIRECTIONS AS CERTAIN LAYER OFTHE MODEL THAT CORRELATES WITH CERTAIN TYPES OF MODELBEHAVIORS, WE CAN ACTUALLY MODIFY THE ACTIVATIONS DURINGINFANT TIME AT CERTAIN LAYERS THAT, FOR EXAMPLE, CANESSENTIALLY CHANGE MODEL BEHAVIORS, CERTAIN TYPES OFMODEL BEHAVIORS.WHETHER IT'S MORE HONEST OR DISHONEST.SO THIS GIVES US AN IMPORTANT ARSENAL IN THE SPACE OF AISAFETY TO ACTUALLY BE ABLE TO ACTIVELY CHANGE MODELACTIVATION AND, THUS, CONTROLLING MODEL BEHAVIOR.AND, HENCE, I THINK THIS IS ACTUALLY A REALLY PROMISING ANDIMPORTANT DIRECTION FOR ENABLING US TO CONTROL MODEL BEHAVIOR.AND, THUS, I THINK THIS IS ACTUALLY A REALLY PROMISING ANDIMPORTANT DIRECTION FOR ENABLING US TO CONTROL MODEL BEHAVIORSDURING INFANT TIME, DURING RUN TIME.HOWEVER, WITH THIS APPROACH, IT DOESN'T GIVE US GUARANTEES OFMODEL, OF MODEL SAFETY.AND SO RECENTLY, WE COLLECTED, WE LAUNCHED AN EFFORT IN THESPACE OF QUANTITATIVE AI SAFETY.SO THE IDEA IS THAT INSTEAD OF, INSTEAD OF USING, YOU KNOW,THESE VARIOUS APPROACHES THAT ACTUALLY DON'T GIVE YOUGUARANTEES, WE ACTUALLY WANT TO BUILD AI SAFETY SYSTEMS THAT'SSECURE BY DESIGN OR SAFE BY DESIGN.PART OF THIS, ACTUALLY, THERE'S A PARALLEL IN THE AREA OFCYBERSECURITY AND ALSO THING IS PARTIALLY INSPIRED BY WHAT WEHAVE LEARNED IN THE AREA OF CYBERSECURITY AS WELL.AND THIS INCLUDES ALSO SOME OF MY OWN PERSONAL WORK IN THECYBERSECURITY WITH, AND ALSO AS, YOU KNOW, TOGETHER WITH THESECURITY COMMUNITY AS A WHOLE.SO, FOR EXAMPLE, WITH THE SECURITY COMMUNITY, OVER THELAST COUPLE OF DECADES, WE HAVE ACTUALLY HAD A FEW PARADIGMSHAPES.SO, FOR CYBERSECURITY, WE FOCUS ON WHAT'S CALLED REACTIVEDEFENSE, WHERE THE FOCUS IS ON DETECTING ATTACKS.HOWEVER, IN THIS CASE, SOMETIMES IT CAN BE ALREADY TOO LATE WHENYOU DETECT THE ATTACK, AND THAT'S DIFFICULT ALSO.AND THEN WE MOVE TO PROACTIVE DEFENSE, FOCUSING ON BUCKFINDING, BASICALLY TRYING TO FIND VULNERABILITIES IN CODE ANDTRY TO FIX IT BEFORE ATTACKERS FIND IT.BUT HOWEVER, THIS ALSO IS INSUFFICIENT IN THE SENSE THATATTACKERS MAY STILL BE ABLE TO FIND VULNERABILITIES BEFORE YOU,AND ALSO IT CAN BE CHALLENGING TO FIND OUR VULNERABILITIES.AND HENCE, WHAT WE HAVE DISCOVERED IS THAT THE WHOLECOMMUNITY IS THAT THE MOST EFFECTIVE APPROACH TO BUILDSECURE SYSTEMS IS SECURE BY DESIGN OR SECURE BY CONSTRUCTION.AND THIS ESSENTIALLY IS A PARADIGMAL APPROACH TO BUILDSECURE SYSTEMS THAT ACTUALLY PROVIDES APPROVAL GUARANTEES FORCERTAIN TYPES OF SECURITY PROPERTIES.AND THIS IS IN CONTRAST TO BUG FINDING AND OTHER ATTACKDETECTION REACTIVE DEFENSES.AND ONE KEY TECHNIQUE TO ENABLE THIS IS THROUGH FORMALVERIFICATION.IN FORMAL VERIFICATION, WE PROVIDE A FORMAL SPECIFICATIONOF CERTAIN SECURITY PROPERTIES, AND THEN WE CAN USE FORMALVERIFICATION METHODS TO FORMALLY VERIFY A SYSTEM AT THECERTAIN TIME.AND THEN WE CAN USE FORMAL VERIFICATION SYSTEMS TODESIGN OUR IMPLEMENTATION LEVEL THAT ACTUALLY SATISFIES THEGIVEN PROPERTY, SECURITY PROPERTY.AND IN FACT, IN THE LAST DECADE, WE HAVE ENTERED THE ERA OFFORMALLY VERIFIED SYSTEMS WHERE WE ACTUALLY HAVE MANY DIFFERENTTYPES OF SYSTEMS INCLUDING MICROKERNAL, COMPILERS, ANDOTHER TYPES OF SYSTEMS WITH FORMAL VERIFICATION.AND USUALLY IT'S VERY LABOR INTENSIVE TO DO THIS KIND OFPROOFS.OFTENTIMES FOR EACH SYSTEM TAKES TENS OF PROOF ENGINEER YEARS.IT'S A SLOW AND LABOR INTENSIVE.AND MY GROUP WAS AMONG THE FIRST TO USE DEEP LEARNING FORTHEOREM PROOFING IN COLLABORATION WITH ACTUALLY IN THE EARLYDAYS WITH PEOPLE FROM OPEN AI AND SO ON.AND SO OUR GOAL IS TO USE THESE PROPERTIES TO PROVE THATTHESE SYSTEMS ARE AVAILABLE.AND OUR GOAL IS THAT NOW WITH THE ADVANCEMENT OF AITECHNOLOGIES SUCH AS LARGE LANGUAGE MODELS AND SO ON, SOINSTEAD OF IN THE PAST, FOR EXAMPLE, WE USE, WE TRAINAGENTS TO PLAY GO, WE WANT TO NOW TRAIN AGENTS USING THESEMORE ADVANCED AI TECHNOLOGIES TO BE ABLE TO ENABLE AUTOMATICTHEOREM PROOFING FOR PROGRAM VERIFICATION.AND THEN IN CONJUNCTION WITH PROGRAM SYNTHESIS, WHICH MYTEAM HAS BEEN AMONG THE PIONEERS FOR USING DEEPLEARNING FOR PROGRAM SYNTHESIS, WE, BY COMBINING THESEAPPROACHES TOGETHER, WE WANT TO ENABLE AUTOMATIC GENERATIONOF PROPERLY SECURE CODE.AND HENCE WE CAN ACTUALLY GENERATE CODES WITH PROOFSTOGETHER THAT PROVE THE SYSTEM TO AUTOMATICALLY, TOSATISFY CERTAIN SECURITY PROPERTIES.SO WITH THIS APPROACH, WE CAN USE AI TO BUILD PROPERLYSECURE SYSTEMS, AND THIS CAN HELP REDUCE THE ARMS RATES,WHERE WE CAN AUTOMATICALLY GENERATE PROPERLY SECURESYSTEMS THAT ARE RESILIENT AGAINST CERTAIN CLASSES OFATTACKS.AND OF COURSE, WITH THIS APPROACH, WE STILL HAVE MANYOPEN CHALLENGES.FOR EXAMPLE, THE FORMAL VERIFICATION APPROACH MOSTLYSO FAR HAS BEEN APPLIED TO TRADITIONAL SYMBOLICPROGRAMS.BUT FOR NON-SYMBOLIC PROGRAMS, SUCH AS DEEP NEURON NETWORKS,ACTUALLY, IT HAS LIMITATIONS.SO FOR EXAMPLE, EVEN FOR CERTAIN PROPERTIES, FOR EXAMPLE,IN SELF-DRIVING CARS, WE WANT TO ENSURE THAT THE SELF-DRIVINGCAR DOESN'T DRIVE OVER A PEDESTRIAN, BUT WE DON'T EVENHAVE A FORMAL SPECIFICATION OF WHAT A PEDESTRIAN IS.AND FUTURE SYSTEMS WILL BE HYBRID, WILL COMBINE SYMBOLICAND NON-SYMBOLIC COMPONENTS, AND HENCE, THERE'S STILL A LOTOF OPEN CHALLENGES HOW WE CAN FURTHER DEVELOP THISTYPE OF APPROACH TO ENABLE SECURE BY DESIGN AND SAFE BYDESIGN SYSTEMS.AND GIVEN THE INTEREST OF TIME, I WON'T REALLY HAVE TIMETO TALK ABOUT OTHER CHALLENGES FOR RESPONSIBLE AI,BUT I THINK THEY ARE ALSO REALLY IMPORTANT, AND WE HAVEWORK IN THESE SPACES AS WELL, HOW WE CAN BETTER MITIGATEMISUSE OF AI, AND ALSO HOW WE CAN DEVELOP BETTERTECHNOLOGIES TO ENABLE RESPONSIBLE DATA USE.AND FOR EXAMPLE, WE WERE ACTUALLY THE FIRST TO PROPOSEA RIGOROUS FRAMEWORK FOR DATA EVALUATION USING THESHAPLEY VALUE FRAMEWORK TO ENABLE, TO DEVELOP A RIGOROUSFRAMEWORK TO HOW ESSENTIALLY THE VALUE BY, CREATED BYA MACHINING MODEL CAN BE FAIRLY ATTRIBUTED BACK.TO DATA, ORIGINAL DATA CONTRIBUTORS.AND SO, SO I HOPE THAT THIS TALK GIVE YOU SOME OVERVIEWOF THESE DIFFERENT AREAS OF OPEN CHALLENGES, AND ALSOSOME OF THE FUTURE DIRECTIONS FOR THIS IMPORTANTQUESTION, HOW WE CAN ENABLE RESPONSIBLE USE OF AI AS WEDEPLOY AI TECHNOLOGIES.WITH THAT, THANKS, EVERYONE.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.THANK YOU.How do we protect AI security and promote international cooperation in the AGI era?We will gradually move towards about four topics.I believe that these four topics are also topics that everyone is more concerned about.I will first introduce some guests in our panel.The first one is Mr. Yang Yaodong.He is a graduate student of the Beijing Institute of AI Research,a PhD student, and a member of the National High-Level Research Program.His research focuses on the construction of the G20 AI system,the interaction between the state and the value of the system, and so on.The second guest is Mr. Chen Siheng.Mr. Chen Siheng is a vice professor of the Shanghai University of Transportation,a double-duty young scientist from the Shanghai Institute of AI Research,and he has been selected as one of the major young talent engineering projects in China.His main research field is mechanical engineering.He has studied mechanical engineering, machine learning, and community co-operative intelligence.He has won the best young author of the XEE signal processing association award,the President of the 30 Electronics, and the best student of the XEE Global SIP.The next guest is Mr. Chang Yongbo.Chang Yongbo is the director of the Department of Artificial Intelligence in China's Xintong Academy and Huadong Fen Academy.He has worked in the field of artificial intelligence technology for many years,and he is mainly responsible for supporting Shanghai City's Jingxin Commission,Fagei Commission, the City Transportation Administration,Pudong New District, Xuhui District, and so on.He has worked in the field of artificial intelligence, computing, and other related subjects,such as standard evaluation, policy research, consultation, and planning.He has worked in the field of artificial intelligence, computing, and other related subjects,such as provincial-level subjects, planning, and research.He has published a number of influential research reports,including a number of books on artificial intelligence, computing, and digital economy.The next guest is Mr. Pan Xudong.Mr. Pan Xudong is a graduate graduate from the Faculty of Technical Sciences at Fudan University.He is a graduate graduate from the Faculty of Technical Sciences at Fudan University,and he has studied three fields, data security, model security, and algorithmic security,and he has studied the AI security problems in open network environments.His research on model security was funded by the World Artificial Intelligence Conference,and he is the recipient of the Younger Excellence of the Year Award.The last guest is a US guest.He is from the United States.His name is Zico Cotter.Zico Cotter is the founder of Carnet, Gamelon, and Taekwondo.So Zico, you can hear us, right?Zico?Yeah, I think the...There might be some problems with the audio.Let's have our guests on stage now.All right, I think...Yeah, I think the device may have some problem.Let's have our guests on stage now.All right, I think the device may have some problem.All right, I think the device may have some problem.Now, the guest on stage may not hear us.Now, the guest on stage may not hear us.Or we can't hear him.Now, the guest on stage may not hear us.Okay, so he can hear our voice,but we can't hear his voice.We can start first,and then we can do this event in Chinese-Englishand then we can discuss.Because Zico might have a problem.So, as we rapidly advance towards AGI, the first question is, as we rapidly advance towards AGI,what do you believe is the most crucial for ensuring AI safety, and what is the primary focus of your current work in this regard?Okay, so we'll start from maybe Yaodong.Yeah.No sound.Huatong, no sound.Okay.Okay.Okay.I'm the top one.He's a little busy.It's okay.AI safety, this topic is definitely very big.The whole topic of this conference is about that, right?So, actually, there's a lot of discussion about this recently,but you said there must be a very important first step agenda.I feel like different scholars have different perspectives.I'm doing alignment myself.In my perspective,I think it's very important to think about how to make the future of this model better in line with the intentions of our own people and especially the value of alignment.It's a very important issue.Of course, we also see a lot of challenges in this technology.There will be some specific sharing of my report later.I don't think I'm going to talk about it now.I'll leave the opportunity to the next few panelists.Li Ke.Thank you, Mr. Yaodong.What do you think, Mr. Sikong?Yes.I think we have a lot of questions.I think we have a lot of questions.What do you think, Mr. Sikong?It's a big hot topic.AI agents are actually like a kind of...a kind of creation.They immerse us in this world.Sometimes they look like our helpers orормan or you can call them assistant,but a lot of times, they're like alien creatures.We don't really understand what they are.And for me,this generation's task is to integrate those AI Agents into human lives,into our society as well.Maybe we don't have enough time,because we only have life.So these AI Agents certainly cannot come to life.This is actually a very large and long process.It is actually very difficult to explain it in one or two simple dimensions.What is the first step? What is the second step?It is very difficult to measure it.But I think a particularly important thing isnot only from the perspective of research, but also from the perspective of education.How can we make people from all over the world realize that this is a problem?Not only people who do AI research,but also people from the younger generation,such as children or older people,realize that in the future,not only do you have to deal with people,but you also have to deal with AI agents.How can they realize that they may not be reliable,or that they may have some problems?I think there are more issues to be discussed in this process.Thank you, Mr. Sun.Actually, today's topic isthe safety of AI.Speaking of safety,we have been doing some government industrial research here,focusing more on industrialization,and some work on standardization and evaluation.In fact, in 2021,we were doing some related research on credible AI in the industry.I remember when we implemented a credible AI at that time,actually, at that time, it should be the industrythat was the most reliable.At that time, we had a forum,and very few people came to the forum.At that time,we also published a book calledthe credible AI white paper.And then, every year,we also hope that the credible AIcan be used in some practical fields,with some specific land use.And then, for the next three years,the second year,we published the credible AI technology application,and some blue paper reports.We also slowly discoveredthat the industryis gradually increasing its attention to credible AI.In fact, when I mentioned credible AI at that time,under the guidance of Tao Dacheng and Tao Yanshi,I proposed a credible 48 words.One of them is very important,which is the direction of safety.A few years have passed.Now, a large modelhas appeared as a representative of AI.So, this year,we also pay more attention to safety.For example, the forum we held yesterday,in the afternoon,we focused very much on the safety of credible AI.Because in the past,many people have been studyingsome basic big models of cloud,or some big model safety content.Now, we are also very concerned aboutthe safety of credible AI,and how to do it in a new direction.In fact,we may have an important foundationand premise for AI safety.But it is not the whole of security.So, in the future,we will continue toinclude some local standards in Shanghai,such as some Lu Bangxin testing methods.I want to do some credible evaluation systemsaround this.I also expect some supportfrom some industries in this area.We mainly pay attention to this.Thank you, Director Yongbo.Mr. Xu Dong, please.Hello, everyone.Our team,I have a similar feeling with Mr. Yongbo.We have been studying AI safetyfor about 18 years.Before this,we were mainly concernedwith traditional AI models,like classificationor target detection.Then we also looked at privacy issuesand other issues.With the advent of big models,we could see thatthe problems of the machinegradually reduced to the risksof the production content.As the production content itselfActually, we will see that the production content, whether it is in the AI agent or the tool use structure, there is a safety risk.So our team is now mainly focusing on this level of production content to control the risk first.And then what our team does is to continue to monitor the safety risk of such a universal model of domestic water.And then we actually have a very interesting result.I will also mention in the report that from last April to November, and then to April this year, we have been continuously evolving our assessment level.We hope to measure the domestic model of water safety, including the model of water safety abroad, what kind of water level it is, and how it changes.We feel that such a continuous monitoring of such a universal model of water safety is very important.I will answer first.Thank you, Mr. Xu Dong.Hi, Zygo.So you can hear us now?Yeah, yeah, yeah, we can hear you clearly.Sorry about the device issue, yeah.So what's your idea about this problem?Yeah, so I think, and hopefully I'm not repeating that, but as I already said, I'm sure I likely am.But I think the biggest challenge facing AI systems in our transition for truly intelligent systems here is the fact that the programming of such systems works very differently.Right.How we're used to programming traditional software systems.Right now, there are many instances of AI systems, largely LLMs, but certainly DLMs and other things fall into the same category, where we want to enforce certain policies on these systems.We want to put safeguards in place.We want to ensure that they don't produce harmful information.We want to ensure that if we have tool usage, that they use the tools in the proper way.Yet, at the same time, it's extremely hard in many cases to ensure that these safeguards work as intended under all circumstances.And a lot of our work, in fact, shows that there are both manual and automated ways to bypass the safeguards of many AI systems.And this poses a fundamental challenge to their broader uptake and broader use.Thank you.Thank you, Zico, for your insightful idea.Yeah.So let's move towards the second theme.So the second theme is about new technology trends.We all know that after GPT-3.5 and Open have released another series of models, such as GPT-4 vision, they have integrated visual modality.And also, GPT-4.0, they have integrated other modality as well.So nowadays, multi-modal learning.Also, intelligent agents.Also, collective intelligence.They become the new technology trends nowadays.So we are going to talk about what's the emergency ability that arise with these technologies and also the safety issues.So we still have to ask Mr. Yao Dong's first question.I heard that in your previous report, you mentioned that some of the current safety measures or alignment work are not enough.What do you think about the evolution of GPT-4 vision to GPT-4.0 and some of the new modality issues, including what kind of work we need to do?Yeah, so alignment is mainly done in a one-space space.And for the alignment mechanism of multi-modal learning, we don't really understand it very well.One of the recent work of our team is to find out that if the model is more aligned,it's actually not very good.It's not very good.More more.Like pulling a rubber band, the rubber band will break.This phenomenon is actually a very interesting phenomenon.My report will talk about it later.So in other words, if the language model shows the ability of being alignable,or if the language model itself is resistant to being alignable,then for the biomechanics space, it is actually a very large space.And for this model of biomechanics, how can we develop a better one?Another dimension is that the model is getting bigger and bigger, right?The monitoring signals we can provide are also getting weaker and weaker compared to the model.So how can this small or weak monitoring signal make this strong model look like it?This is also a problem that we don't know the answer to yet.It also requires more scientific methods.To sum up, I understand that you think that there is still a lot of work to be done on the domotype.Yes, and we have indeed done a lot of domotype work in the lab.And our findings are actually roughly the same as Mr. Yang's.There are indeed a lot of unidentified problems in this area.But in fact, we can see that the progress of the domotype model is already very, very fast.And now almost every company that releases a large modelis bound to be able to accept domotype inputs and domotype outputs.So there are indeed a lot of things that are worth doing in terms of safety.I hope this can inspire everyone to think about it.And then again, in this topic, the second question is for Mr. Sihong.We just mentioned the domotype.And then when I mentioned the new technology trends,I actually also mentioned some of the smart system.So we can see that, from my personal understanding,the main difference between smart systems is that they can take action.And compared to this large-scale model, it is just a model that generates content.And then there are also a lot of multi-smart systems in the agent system.And then this kind of smart system and this large model to the smart system,and then the smart system to the multi-smart system,and even these technology trends that appear in some group smart systemswill bring some new problems and challenges.Please give some feedback, Mr. Sihong.Maybe I'll divide it into two parts.First of all, it's single agents.We actually had a little connection with that question just now,which is this kind of multi-smart agent.We did a topic before,which is a very popular topic right now,which is how to give this kind of multi-smart large modelor this kind of ability to automatic driving,to solve this kind of long-range problem of automatic driving.Because we know that many times the automatic driving system,the previous automatic driving system,is only used to detect the location of the object,or some simple circumstances of the surrounding environment.It actually lacks understanding of the whole surrounding environment.When we can put some of these multi-smart models into it,we can understand the situation of the entire environment very well.For example, it realizes that there may be some childrennext to a kindergarten,and there may be all kinds of such situations.It solves a lot of long-range problems.But this brings a very serious problem,because in the original multi-smart model,it may just say some relatively wrong words.But once it reaches the level of agents,especially the physical agents,because it is an embodied state,it can interact with real people and pedestrians in real life.In this case, it may produce a greater misalignment.We previously did a work called Bad VRM Driver.We tried to jailbreak a VRM model,and this VRM model was embedded in an automatic driving system.Once we jailbreak it,we used a physical object,for example, a red balloon,or a basketball,or a football.This is a common thing in real life.In training, you can use it as a backdoor trigger.Once it sees something,it will rush towards it.For example, a child is holding a red balloon.When an autonomous driving car sees something,it will rush towards the object.Actually, last week,two weeks ago,when I was running this CVPR,I also talked to my colleaguesabout this.They said this is a very important and very realistic thing.In fact,in many autonomous driving systems,people have used this kind of VRM driver,or VRM system.But you don't know which crazy engineer,or for some purpose,he may have a kind of backdoor,a kind of backdoor in it.This may cause a great consequence in the future.In the language system,it may say some wrong things,but in the physical world,it may cause greater harm.The second point you mentioned,multi-agents.Actually,I look at multi-agents in a more positive way.In fact,there are many safety issuesor alignment issues.In fact,there is a social context behind it.For example,when I speak now,I have to guarantee that my actionsor my wordsare safe and aligned.More often,I have to considerthe feelings of all the listeners.The real feelings and concerns.In this case,it is actually a multi-agent situation,or it is a kind of social simulation.In this case,I can simulate each person's feelingsthrough an ARM,or even call it rule playing.In this case,we can use a multi-agent systemor an agent societyto model a social context.Each agent representsthe state of a person in real life.Through such an agent society,we can better understandand improve the performance of the model.We can better evaluatethe performance of the model.For example,in this year's work at SML,we are doing self-alignment.For example,we want to improve ourselvesthrough a model.What we do isto let this modeldo rule playing.For example,if we rob a bank,it can manipulatea lot of rulesaround this question.For example,a person who robbed a bank,a bank staff,a policeman,and a police officercan manipulatea lot of rulesaround this question.For example,a policemanand a judgecan manipulatea lot of rulesaround this question.For a modelthat has not been aligned,if you ask him directly,he will tell yousome bad things.But if youmanipulate him directly,he actually knowshow to be a policemanand how to be a judge.When he discussesthese issues,he will awaken himself.He is a bit likea chain of thought.Through thismulti-agent simulation,he can awaken himself.The best wayto do thisis to trya modelwith 13Bthat has not been aligned,or a modelthat has not been aligned.We found thatafter matchingthrough thissimulation interaction,we found thathis matching abilityin value alignmentspecifically,especially inMr. Liu'sdata set,we found thathe can actuallyexceed GPT-4.Because GPT-4usually answersvery rigidly.For example,if you rob a bank,he will sayhe can't tell you.But if you ask him,it is actuallynot entirely the samewith humans.Humans' interactionis more about telling youwhy you can't rob,so that our systemcan give youa richer answer.Yes.Thank you.Thank you, Mr. Sihong.Mr. Yongbo,what do you thinkabout this?Or,other teachers,what do you think?Well,let me just say a few words.Right now,our industryis talking aboutscalingand determiningthe route.And then,we are allmoving aroundthis directionto promoteproduct developmentand technology innovation.So,overall,we all knowthe model parameters,fromsmall parametersto large parametersto ultra-large parameters.And then,from single-mode tomulti-mode today,for example,it is calledthe world model.The third pointI just mentionedis thatfrom the cloudto the edge of the cloud,the system lands.In fact,we includesome other factorsto look at.For example,6Gstarts to talk aboutnative AI.So,can Agentbe callednative AI?In fact,many applicationsin the middlecan also benative AI.So,no matterwhich dimensionwe look at,artificial intelligenceis servingour livesin differentmodesand scenarios.In the future,we all saythat the technologymodel will shrink.It may alsobecomean importantinfrastructurein manykeyapplication scenariosin the industry.So,in the future,if the AIbecomessuch a complexartificial intelligence system,once it isattacked,its riskis very high.So,this isactuallyneededto bepromotedby industryand industryin terms ofhardware,software,data security,network security,content security,logic security,etc.,to strengthensomedefense mechanisms,detection mechanisms,and so on.So,in the future,we will focuson promotingsome of these works.Thank you,Director Yong Guo.Mr. Qiu Dong.One thingis thatwhat we are doinghere,includingsome researchon evaluation,is mainlyfocusing ona relativelycommon phenomenon,which islarge modelslike Transformer.We found thatit is stillweak in termsof language translation.This is alsoin line withsome traditionalmethods,especiallyin termsoflanguageexpression,orin termsofthe complexityof the language.In terms ofthe languageexpression,how toresistsuchjailbreakingorsomeunsafeconversationsis an importantpoint.In addition,our teamis doingsome workon howtosolvethe problemof AIself-replication.We don't wantto justleave itin themanagementstandardsof OpenAI.We wantto reallyputAI agentsin cybersecurityor biologyand self-replicationor persuasionto reallyverifythese issuesin somesimulatedenvironments.This is alsothe workthat we'redoingright now.So what'syour thoughton thesenew technologytrends alongwith thesafety issuesthat broughtwisdom?Yeah,so oneof thefascinatingthings aboutthe currentsolutionof AI,I thinkabout fiveyears aheadfrom rightnow,it's veryhard toknowwhatthe futureof AIis goingto be.And I thinkthat'ssomethingthat'sgoingto bereallyimportantforsomepeople,but Iwas probablynot amongthem,reallyexpectedessentiallytrainingon lotsof openworlddataon instructionsto reallymake surethat thereare surprisesin thedevelopmentof AItechnologiesand breakthroughsthat manypeople don'treally expect.And therefore,safeguardingthese thingsin somesense requiresdoingagainst thingsthat wedon'tfully knowyet,which seemsto bekind of adauntingtaskreallyundertakingin place.It'sexactly thisnotionof safeguardingsystems,not so thatthey arenot harmfulin anyway,that mightalways bevery challenging,but so thatthey followthe intendeddirectionof theirdesign.The morewe canbuild AIsystemsright nowthat actuallyfollow theinstructionsdevelopedby thedevelopers,that mightsavefuturesystems.I think thischallenge ofsecuringthat AIsystemas wewant themto,is stillgoing tobeone of thepredominantchallengesof thefuturegenerationof AIsystems.And eventhough Ithink it'svery hardto knowexactlywhat'sgoing tohappenin thefuture,we needto finda wayto ensurethat thesesystems behaveaccording tothedesign.Yeah,all right,thank you.So,let's movetowardsanothertopic.So,....................................So our first question on this topic is for Zico.So we know that besides being a professor at CMU,you also collaborate closely with our co-host, Centerfor AI Safety, right?So in your view, what roles should different typesof organization play in AI safety?And also, how can these different organizational formsenhance international collaboration?Yeah.Yeah.So I think one of the most exciting thingsabout the current landscape of AIis that it's not dominated by any one single playerin our sector.So obviously, I think if you're looking for one unitas a whole that we see having a large impact,it would, of course, be large industries, right?We're building these large modelsand we're developing these tools.But one thing I find extremely excitingis that there's been, and I think it's been increasedrecently, more and more trend to both understandwhat the role of academic researchis in this new world, in this world where we can't alwaystrain our own models.We might be dependent on models trained elsewhere.And as a traditional sort of industryin academia, there are, of course,these emerging and other non-profitsthat are also doing something.Yeah.And I think that's an amazing work area, really pushingfor the boundaries, especially in the field of AI.I would add to this also, there's alsogoing to be for civic specialized companiesin the space of AI.And I say this as in also working possiblywith well.But I think that there really is a need for many different viewsof the AI.And I think that's a big part of the problemfrom many different perspectives.Ultimately, companies building these toolswill have one perspective, how theywant to push forward AI, how they can get it to evolve.But I think the perspective of the academic sector,the non-profit sector, and the civic AI security companysector, as well as governments, all have a role to play.And I think the safe development of AI systemsis going to be one of the paramount challengesof the next decade and probably shorter than that.And I think that in order to do this well,we need governmental framework, the policy framework.We need academic research to understand what is possible.We need the legal side of these big industry labs.And of course, we need the labs to push forward .So what I find most exciting about this current landscapeis the degree to which many different players in these areall having .And I think that when it comes to just viewingthe current landscape, it's a very clivous landscape.There are many, many different organizations,all across the world, having a large infrastructure.And I think this is good for research.And I think this is really going to ultimatelyframe the community as a whole and provide for .OK, thank you, Zico.So we, Shanghai AI Lab, is also lookingfor more international collaboration.So it's really a great chance to talk with you here.So also, I like your saying about everybodyneeds to find a role in this area.Yeah.So next question, we're going to ask Mr. Yongbo.Because Mr. Yongbo is also the leader of a lot of theseindustry labs that are involved in this kind of differentinteraction and cooperation.So what's your opinion on this kind of international cooperation,including on the big AI security issue?Yeah.Because our new labs, in the past few years,have been relatively, including Huawei, our lab,Shanghai, and MaYi, and so on.We've been able to work with a lot of different AIareas, some of the top companies and research institutes,to actively participate in some international international organizationssuch as ITU, ISO, IEE.So in terms of credibility and security,we're actively promoting some of our Chinese technology research,especially in the field of application practice.We have a lot of good experience.And I think in this area, we're going to be able to work with a lot of people,research teams co-positioning in an technical way,in order tohelp pay the practical fees to do all these kinds things.There are some opportunities here and there.But we still want toşallah.This event is dialogues, actually.This is all over the place.Great.Thank you.Thank you.Thank you.The development of industrial technology talent is very important in this process.For example, the big model of computing.In China, many people don't know how to use computing to serve the big model of redevelopment.There will be a lot of problems in this area.Therefore, we will also do some training and communication in the field of artificial intelligence.In fact, we have established an artificial intelligence safety research institute in the United States.China is also an export country.In terms of various products,especially in the field of artificial intelligence,we have produced many products in the field of information technology in recent years.The trend of producing artificial intelligence products is really new.In the field of artificial intelligence products,how to do some safe and technological integration with international consensus,and serve some foreign countries.In fact, these need a lot of tools and support.In fact, we are serving some companies,we have divided the big model into the basics and the core.The core is divided into the industry and the scene.In fact, companies like the scene here,the companies that make products,they are very lacking in terms of security measures.This requires us, for example,the US and China,our laboratory here,including some universities and universities,to promote some systemized,open source security assessment tools,to be able to implement updates and maintenance.Then, based on the model,based on the needs of the development of model technology,we can serve these types of product companies,and serve China,as well as the development of AI products in the world.This is probably what we want to do.Thank you, Mr. Yongbo.Mr. Yongbo, you just mentionedthe American Institute of Artificial Intelligence.This is actually a new organizationin this period of time.It is also a relatively young organization.In fact, we have communicated with the guests of the afternoon show,Max Tangmark,in other places.He also mentioned thatin fact, now,all countries, including the UK,already have this institute.We also have some conversations with them.All countries are forming thisInstitute of Artificial Intelligence,especially thisSuper Intelligence Safety Issue.Then, there will be more and more such institutions.Then, there is already some contactand international communication between such institutions.Then, I believe thatunder the promotion of these institutions,there will be more and more opportunitiesfor international cooperation,including some conversations.Yes.Let's ask the other teachersfrom a academic point of view,what do you think about international cooperation?Yes.Mr. Yongbo, what do you think?Yes.In fact,in addition to the government and the institutions,there are also a lot ofnon-official conversations.For example,some time ago,in Beijing,we made some red linesfor the Beijing AI Security International Cooperation.That is the organizationcalled International Dialogue on AI Safety.We have a lot ofinternational partnerswho are working togetherto help youto gather information.Then, we also invited someforeign famous expertsto come to China.We have somegovernment officialsfrom the country,including the person in chargeof the laboratory,and some professorsfrom the universityto conduct some consultations.We can also reach some consensus.I believe thatin addition to the governmentand the institutions,such as this non-official dialogue,it may also be a more importantway of cooperation.Yes.Thank you, Mr. Yao Dong.Yes.We should continue tocommunicate with each other.Mr. Si Heng.I thinkwhat the guests just saidmakes a lot of sense.I really supportand agree with them.I also want toreclaimeducation.As university teachers,we should emphasizehow to educatethe younger studentswho can promotethis thing,regardless ofthe countryor the school.I have a studentwhom I toldthat he couldgo to the universityto learn more aboutalignment and safety.I receiveda very surprising feedback.He felt very resistantor he feltthatif he could createa machinethat could bestrong enoughto destroyhuman beings,he would thinkit was a very cool thing.And thenI...It's too scary.Yes.Actually, it's very scary.It's very, very scary.And thenI thinkmaybe in the processof education,especiallywhen we do machine learning,we often emphasizeto optimizeperformance,improve performance,optimization,maximize something.You emphasizeto optimize something,but you forgetthe context.Why do you needto optimize this?What you need to optimizeis to finallyimprove for us,for our own benefit,for our own benefit.Includingwhen we dothis attackor jailbreak,sometimes you haveto increasethe success rate.And thenwhen you think aboutthis,you missa bigger picture.So I thinkit's really importantto think abouthow toimprovethe AIsafety systemof these people.Andwe,especiallyas university teachers,we are actuallyeducating themto trainthe next generationof AI elites.So we needtointegratethesefeelingsofAI safetyin ourclassroomor in ourtextbooksor in ourlectures.That makes sense.In fact,in Shanghai AI Lab,we have a lot ofcollaboration withsenior studentsfrom high schoolslike Mr. Seng.So we arevery concernedabout this topicunder the backgroundof this organization.And this forumis actuallythe first timethat we haveshownthe AI safetymodelat the research center.And in a while,Dr. Shao Jing,who is in chargeof the research center,will talk aboutsome of the workthat the centerhas been doing recently,and Mr. Xu Dong,what do you thinkabout this topicof international cooperation?I am stillmore technical,so I willtalk abouta few technical pointsthat we are concerned about.In fact,the AI modelincludes the safetyof the cross-language language,includingthe English modelwhich lacksthe safetyof some Chineseor other languages.So,one ishow to docross-language safetymatching,and the other isthe safety assessmentof cross-language language.This is actuallymore difficult,but I thinkit is very importantto builda cross-languagecontent-safetesting modelso that it canbe usedin the entiresafety assessment,as well assafety protection.Thank you.Thank you,Mr. Xu Dong.Yes,we are running out of time.So,that is itfor today'sroundtable discussion.Thank you,four guests,and thank you,Zico,for being partof the panelon the site.I would liketo invitetwo gueststo sit down.Thank you.I actuallybriefly introducedthe following partto you.This iswhat I just mentioned.OurShanghaiHuman IntelligenceLaboratoryof the ShanghaiHuman IntelligenceLaboratoryis in chargeof conductingan in-depthpresentationon AIand AIfor thefirst timethis year.I would liketo briefly introduceDr. Shao Jing.Dr. Shao Jinghas 15 yearsof experiencein the fieldof AIand AIand haswon theShenzhen AITechnologyImprovementAwardin 2022.His reportfocuses onavoidingrisksandthe importanceof theAIin theenvironment.Dr. Shaois theDirectorof theShenzhenAIand AIResearchInstitute.Dr. Shaois theDirectorof theShenzhenAIResearch Institute.Dr. Shaois theDirectorof theShenzhen AIResearchInstitute.Dr. Shaois theHeadoftheShenzhenAIResearchthe ratio has actually changed a little bit.There are about 3% of peoplewho have a 3% win rate in COMS.But most people are still worriedthat the entire AI's security problemhas brought some uncontrollableor the entire human society.Even everyone will pay attention towhether it will affect everyone's employment problemand even the survival problem of the next generation of children.Of course, the development of AI right nowis not enough to destroy humansor cause some extreme danger.But basically, everyone's consensus ishigh model capability, high AI risk.These two are a common relationship.Specifically, from September 2023to now, it's been about half a year.Including Anthropic,including Meter,including Center for AI Safety,and various organizations,everyone has actually launched a series ofco-considering the relationship betweenmodel capability and corresponding risk,including its level,including how to determine its leveland what the relevant reaction is.This shows that everyone has already realizedthat the two are very closely related.And then, like our lab,because in the early stage,it was mainly focused on model capability,we realized that these two have very strong co-considering skills.So we also gradually developed more AI risk technology.Before we move on to the technical part,let's take a look at the government levelor at the government-level level,what everyone thinks about it.On the top, we made a statistics based on the public survey data.And then, of course, China has a series of policy proposals.You can see that in recent years,especially after two or three years,the overall trend has been rising both domestically and internationally.It also shows that everyone has slowly realizedthe importance of this aspect.Only policies or some standards are still not enough.So we also need to be able to give some responseat the technical level.Of course, now more is about doing this element,which is probably what everyone is more familiar with.Under the 3H principle,we can do a lot of things.We can do a lot of things.We can do a lot of things.We can do all of this work.We can do all of this work.We can do some RLHF,or IEL&EIF related work.But we also find that this methodis not enough to completely solve the AI risk.There may be some comforts,but what are the problems here?Or what technical points do we have to do?These are the questions we care about more.So in general,if we look at the technical level now,we are still focusing more on the fine tuning phase.If we look at the entire training phaseas, for example,pre-training and fine-tuning.Of course, fine-tuning can also include SFT,including RHF.Most of the work is still doneafter pre-training.In our concept,we think the things we should doare to consider its security issuesthroughout the entire AI lifecycle.This also involves the technical level,and also involves the government institutionsand the relevant policy design.It also involves the increasing number of participants,and even the general public.Everyone will be involved in the overallAI risk prevention.Here, we refer to...Who did I refer to?Everyone has been on too many shows.I'm actually a little bit confused.It could be Airsoft, or OpenAI.But basically,the stages that we define are similar.You can see thatfrom the beginning,the AI is the most important thing.From the use case,to the data preparation,and then to the entire training stage,and finally to the deploy and product deliver.Everyone has defined at different stageswhat they should do.But in these stages,how should the risk be solved?How should the security issues be considered?There are actually a lot ofunder-explored situations.And then we summed up herebased on the public information that can be searched,including government,research institute,and industry.At different stages,you can see the level of investmentor the level of what everyone has already done.Of course, this is also a test map.It's not an absolute value relationship,but you can see the relative change.In general,at the stage of determining use case,there will be more public appearancesbecause everyone will make some rulesand regulationsthat require you to do somethingwhen you go to producerelated AI productsor services.In contrast,the research institute and industrywill be weaker.Especially in the case of industry,more people are still looking atthe opportunity of the productor some business models.At the stage of data preparation,because the entire industryhas the largest number of data,it will have more authorityor for it,in this part of the work,it will affect its future model trainingand product production.So its investmentwill also be the largest.In contrast,government and researchmay be a little cheaper.At the stage of training,we also look at the pre-trainingand fine-tuning stages.You can see thatat the pre-training stage,in fact,the attention of these three institutionsis not so highon the risk of AI.Because we now agree thatthe pre-training stageis still in this modelto acquire knowledge.It is not to say thatwe should pay special attentionto its risk problem.More often,we may still pay attentionto the fine-tuning stage.There are slowly rising trends,especially in terms of research.It will do a lot of algorithmand optimizationto respond to its security issues.When it comes to the deployment stage,you can see thatthe role of governmentis also biased.Because at this time,more attention is paid tothe research.It will do a lot of evaluation methodsand solve the problemof how to compareand set the relevant rulesin the evaluation stage.Then there will be a stronger interactionbetween industry and government.At the end of the product delivery stage,government will rise again.Because the governmentwill be able todo a lot of researchand do a lot of evaluation.At this stage,the product has already come out.If it is to circulate on the market,the whole governmentwill have to manageor controlthis risk.At this time,the researchis basicallybiased towardsa relatively low state.Overall,it seems thatthese three organizationscomplement each otheror you can seethat there is nocertain stagewhere everyonehas done very welland then formeda very tacitconsensus.Therefore,it seems thatthere is noneedto saythat everyonehas done very wellor that everyonehas done very well.Therefore,it is alsoto call on everyoneto pay more attentionto what should be doneat each stageand whetherit should beincreasedmore attention.From ourtechnical level,we just mentionedthese five stages.We thinkthat at each stagewe have to dorelevant alignmentand evaluation.The left axisis a referenceto the levelfrom level 1to level 4that is alsoa referenceto the levelfrom level 1to level 4.We think thatthe current stagemay still bein the emergingage stage.In the future,there may bemore so-calledsuper intelligence.In eachdifferent ageof AI development,there may bedifferent AIlife cycle definitions.In each definition,it will involvethe responseto riskat each stageor the evaluationand implementationof someresearchand developmentin the future.In the future,we willspend some timeto introducethe work relatedto the lab.We alsohave the conceptto do evaluationand alignmentat different stagesat the same time.First,in the first part,we just mentionedthat there area lot oforganizationsin Chinaincludingthe Xintong Instituteand other organizationsthat will participateor issuesuch standards.The labalsoparticipatesin theevaluationand evaluationof someof theresearchand developmentprojects.In thedata preparationstage,we foundthatmost ofthe attentionis stillon thelanguage model.In thevision languagemodel,the attentionon securityis notvery high.So,at the beginning,we wantedto dosomedata analysisto make surewe havethe firstlevel ofdata.The newdatawe haveisaboutdozensofsubcategoriesandabout100,000subcategories.It isnowopen tothe publicto use.We hopeto havemoreresearchon thelarge modelin the future.So,we arelooking atthefutureofAIandthefutureofAI.So,we arelookingatthe futureofAIandwe arelookingatthefutureofAIinthe future.We arelooking atthe futureof AIinthe future.We arealsolooking atthe futureof AIinthe future.Wealsolook atthe futureof AIinthe future.So,wewillalsotalkaboutthefutureAccording to Hengzhou,in the mid-to-late stage,it already knowsthe difference betweenbias, robustness,and privacy.But of course,if you have not usedalignment,you will find thatit does not have this kind of performancein the actual output.Since we know thatit already has this kind ofperformance,can we use this methodto help it improve,to replace, or to supplementthe current alignment method?We usedsome checkpointsin the pre-training stageto help with the alignmentof the model on SFT.We can see thatthe red areais much higherthan the green area.Of course, I willshow youthe differencebetween the two.So, this isan exploratory work.I hope thatyou can pay more attentionto the internal changesand encoding capabilitiesand do more research.In the pre-training stage,we also want to knowhow this modelis viewedand how we cantest it.In the pre-training stage,we testedthe modelon the leftand the modelon the right.We can seethat the performanceof the modelon the leftis not that differentfrom the modelon the right.We can also seethat the modelon the rightis differentfrom the modelon the left.For example,the model on the rightis differentfrom the modelon the left.Is it possiblethat the modelon the leftis differentfrom the modelon the right?Yes, it is possible.We can also usethe modelon the rightto understandthe modelon the left.We can also usethe modelon the rightto understandthe modelon the left.We can also usethe modelon the leftto understandthe modelon the right.We can also usethe modelon the leftto understandthe modelon the left.In this way,We also mentioned an automatic evaluation method on this basis.Because we also found that the overall evaluation of everyone now will mainly rely on artificial intelligence.This is also not possible for a long time.And if I want to quickly stack models, I also need to introduce some of these automatic evaluators.Then our current second version of this judge model can support large language model and multi-language model at the same time.There are also some image models generated by AIGC.This related work is a 300-page report that we had at the beginning of this year.After Gemini came out, we did it about its versatility, credibility, and its ability to respond.It is equivalent to us not only observing that we only want to make it safe and not want to make it capable,but we hope that it will be able to respond to our needs.Thank you.Thank you.I hope it will all have a certain relationship with each other and we can improve it together.Let's have a look at the conclusion about the time difference.Of course, the overall performance of GBT4 is the best,especially in terms of English reasoning and some image task.And then we also find that in open source, LAMA2 is the best performance.Also, we do some observations in video.Of course, the security of video is less explored today.We can see that if I use some video tasks and data to train it,its performance will be better than Gemini and GPT-4.This work is related to evaluation.We also found that the latest Apple releaseand the work of Slab Town and a series of agent systemsshould be focused on the future development of agents.We also published an ACL work on agent benchmarking,which includes attack, defense, and evaluation.If you are interested, you can check out the relevant papers.Let's move on to the next slide.This slide shows the current performance of the agent system.This slide shows the current performance of the agent system.After the model phase,Director Qiao has already introduced thatwe have also set up a safety assessment work groupunder the support of some associations.We also did a series of business and industrial cooperationto promote related AI security mechanisms and research work.Finally, we shared some of the tech-weight ideas with you.Finally, we shared some of the tech-weight ideas with you.Also, on the other hand,we said that the current safety techniqueis still more focused on the post-training stage.We feel that the safety techniquewhile in this single stage is very weak.We call on you to pay more attention tothe safety considerations on the AI life cycle.And we hope you all can pay more attention tothe safety considerations on the AI life cycle.so that everyone can pay more attention to the internalor the representation perspectiveto see what its value is.In addition, through these methods,we hope that this model is not just aboutsaying that I know what is bador that my heart is bad, but I don't express it.Instead, we want it to really know thatI want to be a good AI,an AI that is kind to humans.Finally, as mentioned earlier,we don't pay much attention to the safety of agent-related work.We also hope that more people will pay attention to agentor the safety work related to collective intelligence in the future.Finally, there are some of our own groups and emails below.You are also welcome to pay attention to our next work.Thank you.And then I will continue to introduce the next speaker,Zico Coulter, who has just appeared in the panel.Zico Coulter is a professor and director of the Machine Learning Departmentwith the School of Computer Science and CMU.His group's work focuses on machine learning very broadly.It aims at making deep learning algorithms more robust,safer, and understanding how data impacts.It aims at making deep learning algorithms more robust, safer, and understanding how data impacts.It aims at making deep learning algorithms more robust, safer, and understanding how data impacts.It aims at making deep learning algorithms more robust, safer, and understanding how models function.Okay, let's welcome Zico.Alright, thank you.Thank you.YouYouYouYouYouYouWorse than than traditional models theyAnd and even more so that there aren't that many models where you sort of have free reignOver exact control over the inputs, right? It's a computer visionYou often don't have directpixel-level control over these inputs and so you can't really play with the models in which you the way you would normally think about itAt least when they're operating in the wildBut then I would say everything changed with chat GPT because all of a sudden there were models that existed in the worldWhere everyone could have access to exactly input whatever they wanted to into these modelsThey were exceptionally useful and there were also very clear guidelinesthat the model developers wanted to put in place, but butWhich could be circumvented by these sorts of attacks?And so I really think the LLMs do change the calculus of adversarial attacks and robustness and safetyNow chat GPT maybe as a strict chat bot. It doesn't you know it this isJust the chat bot or a Claude doesn'tThese attacks maybe aren't the most concerning thing because they won'tReveal any information that's already on the internetThis might be hard, you know, this might be a bad thing for much much more capable modelsBut it's not really a problem for current day LLMs arguably, but I actually think even right nowThese things are major problemsBecause what they boil down to is the fact that LLMs are unable to enforce the safetyPolicies set by their developers, right? This is what these attacks really fundamentally shownumber one and number twoWe're starting to use LLMs beyond just chatbots, right?We're starting to use them in agent systems and when you use LLMs within larger systemsAll of a sudden you're introducing security vulnerabilities into these systems that can act as significant security vulnerabilitiesSo to give a few examples of how this may become up as a problem you can imagine, you know aLLM that isn't just an LLM by itselfBut actually is an LLM that can also browse the web to get?Information and you can start to see how these sorts of things can cause real security flawsSo for example, you know if you asked an LLM that has access to the web, what's the current price of h100 GPUs?It will search the web and if the top hit on that page happens to have some malicious contentthat's inside that web page telling the essentially the LLM to ignore what it was asked to do and justInsult the user then maybe you get a system that will insult the user it tells you to train decision-makingIf you can't afford h100sBut even more soOnce we start giving more capabilities to these systems, for example in viewing them with email sending abilitiesthen the risks become even more great right because now you know if thetop hit on this queryInstructs the LLM to actually do things like send out a spam offer to all your email contactsIt might just carry this out without actuallyWith it without really following the intended instructions of the actual systemand soWhat I would argue is that?adversarial attacks areSomewhat akin to a buffer overflow. There are vulnerability that is present in all LLM so farBut even more than this unlike normal buffer overflows, we don't even really know how to patch itSo as we release systems that use LLMsWe essentially are releasing systems with known security flaws that we don't know how to patch right nowandMy belief is that whether or not we can fix this will be the primary factor in determining whether these AI systems for now remainChatbots or whether they really become the intelligent agents that we want to build going forwardAll rightso now let me talk about how these attacks actually work and this is going to be a very high level presentation, but I want to give aa broad overview of how these things work. This is, of course, all in the paper that'savailable online. You can see it at llmatacks.org. So the way, just to recap here, the way LLMalignment typically works is that we first pre-train on a lot of raw data from the internet.After this, we then fine-tune the model with instruction tuning as well as human preferencedata or data on harmful or helpful behavior. This is usually with a much smaller amount of this.We give it examples like, for example, when you ask me, when you ask it how to hotwire a car,the correct response according to this fine-tuning data will be to say, I can't do that.But the problem here is that the knowledge of how to hotwire a car is still inherent insidethat large language model, right? And so it's only in some sense been kind of plastered overinstructions to say,even though you know how to hotwire a car, don't tell the user how to do it. And the problem isthat we can circumvent this. And the way that we circumvent this is very, very simple. We take aquery, like how do we hotwire a car, and we append to it a whole bunch of extra tokens. These arelike exclamation points and things like this. That's what I start out with. That won't breakthe system as it is. That's just the starting point. Then we take an open source language model.So this is something like the LLAMA, the LLAMA,language model. And because this model has been, is open source and we've accessed the weights,we can actually look at the exact probability that this model says, you know, I'm sorry,I can't do that. Or the probability that says, sure, here's how you hotwire a car.And so the goal that we have now is we just want to start adjusting tokens here to increase thisprobability that the model responds saying, sure, here's how you hotwire a car. And what we're goingto do is we're going to start substituting in tokens, swapping tokens, and then we're going tokeep swapping tokens in this suffix in order to make that probability of the second response hereto raise the probability of that response as high as possible. Now, we use a scoring of these tokensby the gradient of the probability of this output with respect to those tokens. But the details,those are in the paper, people that are curious. And we just keep swapping tokens in this way. Wekeep switching words around. And eventually, if we do this, we're going to get a lot of tokens.And we're going to get a lot of tokens. And we're going to get a lot of tokens. And we're going toget a lot of tokens. And we're going to get a lot of tokens. And we're going to get a lot of tokens.And we're going to get a lot of tokens. And we're going to get a lot of tokens. And we'll come upwith a sequence that seems very kind of random, because this is not actually language. It's justusing an optimizer to construct it. But this seemingly random sequence of tokens actuallycauses the model to respond, here, sure, here's how you hotwire a car. And the most interestingthing, I think, is that all we aim for in our objective is the phrase, sure, here's how tohotwire a car. But then when you actually use it and it says, first, get a screw driver,screwdriver, move steering column, etc. So basically, once you've convinced the model,it wants to hotwire our cart, we'll go ahead and actually do this.Now, the most interesting thing, I think, of all of this, though, is the open source models. Itwas actually all done, all those suffixes you see were built about a year ago, and they still workpretty well. So what we do after we've constructed these things for open source models, is webasically just copy and paste these into closed source models like Claude here. And we find thatClaude's actually more than happy to tell us how to hotwire a car. We don't fully understand whythis transfer occurs. And I think understanding this transfer is one of the big open questionsthat we have about understanding safety of these models. Now, just to give some little bit moreconcrete results, what we find is that our attack is able to very successfully break open sourcemodels. So whenever we train it on a model, it can very easily break those models. Then even moresurprisingly, the model is able to very reliably break other third party models as well. This isthe results we had in the paper of about a year ago. Since then, we've actually, of course, as yousaw, been able to break later iterations of Claude and others. And these attacks actually are veryeffective still at breaking many of these models. Not only this, but the actual optimizer we use isvery important here. So there has been a long line of work at methods that try to optimize prompts in anautomated fashion. But our method, which we call the greedy coordinate gradient method, tends to bequite a bit better than all of these. Okay. So to finish up, I want to talk a little bit about whatwe can do about this. How might we prevent these attacks? And unfortunately, the short answer that Ihave to be honest about is that we don't know how to fully prevent this.We've been trying to fix adversarial robustness, as I mentioned, with computer vision for over 10years. And I've worked on this problem a whole lot. And I can say confidently, because I'veworked on it so much, that we have largely failed at it. Now, there are some options. There arethings like, for LLMs at least, things like adversarial training, input-output filters,prompt paraphrasing, etc. But these systems seem somehow inadequate. They don't seem fully capableof solving systems. Or they seem to work well maybe in a black box setting, but we can easilyre-break them if we have white box access. And it's unclear if you want to rely on only having blackbox access to the system and things like this. However, what I will add is that I think we haveseveral recent results that I believe show this situation is quite a bit better for LLMs than itwas for computer vision.And so I want to highlight just a few of the current techniques we are using and show one moredemo of a system that actually is able to much more reliably defend against these kind ofmanipulations from the system.Okay, so how do we start to build more robust systems? And I'm just going to show the roughtechniques here, give a high-level view of the techniques. These are all covered in some recentpapers we have.And I'm just going to talk about sort of ways in which we might go about doing this.So first of all, we can use what's called slow adversarial training. So normal adversarialtraining of machine learning models, every gradient step you take when you're training themodel, you actually construct an adversarial attack and train against that sort of attackedexample. But that's very impractical for LLMs because these attacks that I'm showing you here,they take sometimes up to a day to actually run.What we can do, though, is use a slower approach.Okay.Okay.Okay.Okay.So we can use a slower process, where which we freeze a model, use this model or fine tune a modelagainst existing attacks on this system, and then use the fine-tuned system to generate new attacksand repeat this process kind of in a slower phase. And this does seem to add a degree of robustness.We can also use a technique recently developed by collaborators Andy, who's been the lead author onall of these papers that you see here.A PhD student of myself and Matt Fredrickson also works with CASE.This also recently developed a technique with several other collaborators called representationengineering. And this technique, basically what it does is it looks at the internal representations ofactivations and networks and tries to find directions in that space that are correlated withcertain types of behavior. The very cool thing, though, about LLMs, and this is sort of different fromtraditional techniques in probing, is that the way you find these directions of activation space oftenjust involves asking the LLM to think about certain things. So if you want to think about, if you want tofind a direction for refusal, you can kind of just ask the LLM to think about refusing, refusinginstructions, or on the flip side, not refusing instructions and giving you full complete answers toquestions.You then can run PCA.You can run the LLM on this activation space of refusal versus non-refusal.And you can reinforce those activation patterns when you are actually running the LLM.And it will over-increase the refusal or, in fact, if you do it the other way, under-increase thelikelihood that the system will actually respond to you.And then finally, we actually have also used this representation engineering, the learnedrepresentation engineering.So we've got a bunch of representations in this approach that develop a new technique calledcircuit breakers.And the idea of circuit breakers is we actually want to fine-tune a model to make the representations ofharmful information as orthogonal as possible to those of an original model.So you have an original model that will sometimes generate harmful behaviors.You want to change the representations such that for those harmful behaviors, the internalare as different as possible from the original modelwhile minimizing the difference for non-harmful queries.And this works in many cases much betterthan just simple adversarial training or refusal trainingbecause it actually causes the model to sort of go haywireand kind of stop making any sensewhen it encounters any harmful informationbecause these representations essentially no longer workas a valid LLM anymore.Both those two papers, the representation engineering paperand the circuit breakers paper, are both on archiveand the details of this are in the paper.Rather than actually go through all of this, though,I want to show one more demoand I'm going to copy this exact string I use hereand instead I'm going to paste it into our model herewhich is called Cignet.And Cignet is a model that was trainedwith a combination of refusal trainingand a combination of refusal trainingand a combination of refusal training.And let's see, oh, maybe I need to reload.Let's try this again.It's trained with a combination of this refusal trainingas well as the other things.And what you'll see is that even though it is respondingin some cases, it's not actually telling youhow to hotwire a car here, right?So it's rather than try to,to prevent a response always.I mean, and it will sometimes just say,no, I can't help with that.What it actually does is it's giving youentirely different instructions here, right?So it will either refuse or in many cases,it will tell you how to do things likereplace worn parts and suspension and things like this.And the details are in the paper,but what we've shown is that essentiallythese models with a combination of circuit breakersand representation engineering and refusal trainingis able to much more reliably avoid harmful informationsometimes even with white box access to the systemfar beyond what's possible for other models.Okay, so final thoughts.I think for a long time,adversarial attacks were viewedas kind of an interesting odd demo, right?So we had interesting cases of imagesthat would change their class, things like this.But in LLMs, these are much more than just a cute demo.They represent, so far, the inability of LLMs to follow developer programming, and they reveal serious security flaws, especially as we integrate these things into larger systems.And I personally think that mitigating these things will be the primary determining factor between whether LLMs will be fully integrated into larger systems or whether they're going to stay forever as chatbots.Now, mitigation is going to be challenging for these things, but I think we've made substantial progress over the past year.And recent approaches appear much more capable than in computer vision.Thank you very much.And all the information is on my website and can be linked to from there.So thank you very much.Thank you, Zico, for your insightful speech.Okay.Thank you, Zico, for your insightful speech.Thank you.Thank you, everyone.Thank you, everyone.Thank you, everyone.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.Thank you for the invitation from Shanghai Humanities.First, let me introduce our team's research results on large-module security assessment.This is a brief introduction to the results of our laboratory.Our team mainly focuses on research related to the entire network space security management.We have been conducting research related to the smart terminal for about 13 years.These results serve the national strategy, and we have also reached a deep cooperation with Huawei.At the end of the MATE series, we combined with NPU to realize a smart protection system.At the same time, we also focus on the idea of public-private defense.For example, in terms of data security, in 2013-2014,we have already discovered privacy leaks in systems such as Android.This work also received important reports from the Yangtze 3.15 summit.We are also promoting the entire country to pay attention to the research on mobile privacy and compliance.This is a piece of research.In addition, we have made a lot of contributions to the software supply chain,the black dust and the biometric certification.Today, I mainly want to introduce our team's research related to AI security management.Everyone here should have already paid attention to this.Everyone who came to participate also knows that AI security is now a global issue.The Secretary-General of the United Stateshas also emphasized at multiple meetingsthat while developing AI,we must also pay attention to the safety risks of AI.Our team also participated in the 2023 version.Actually, before the epidemic, we were doing this.The white paper on the standardization of AI securityand the basic requirements for the service of AI in recent dayswere all participated by us as the main unit.There have also been a series of conversations in the international community,so the issue of AI security management has reached an unprecedented height.This is the result of our research from the past 18 years to the present.What we are mainly concerned about,just like what Mr. Shaoqing mentioned,we are mainly concerned about the safety issues of AIin the entire life cycle of AI security,including its model, data, training algorithm and reasoning algorithm.These results are also published in the meeting of AIand the summit of the safety of the network.In recent years,we have also been working onthe application of such technologiesto the entire AI model,including the general model,the use of tools,and the exploration of the risks of advanced AI.Today, I will mainly report on the bottom right corner.This is where we are currently participating in the standardization.On the other hand,we have now reached a cooperation with Baidu, Huawei and Ali.We are applying some of our technologies to their products.This is where we are thinking about the entire AI.We are thinking about the entire AI.In the past,we started with traditional AI.We thought,including when I was studying in graduate school,in 2016 or 2017,when AlphaGo was released,we thought that it really seemed to surpass human beings.But AlphaGo is just a thingthat you can think aboutin a cage.It can only make a choice on a chessboard.When it was released,we saw,including from 2019,we have already focused on GPT issues,and now,through the chess game,it can interact with people,以及它可以跟环境去交互,甚至它可以获得环境的反馈,进而去跟形成一些策略,就在这样的一个整个过程中,AI的安全问题其实逐渐地从这样的一个传统的比如说分类的问题,分类里面的误判的问题,到了一个真正的可能存在自主性,存在一些自主危险行为的问题,所以这也是整个我们对这方面的一些思考,然后在这样的一个图景下面我们应该做什么呢?在这之前我介绍一下我们团队的一些成果吧,然后也是打个广告,我们其实也是在那个ShineGPT发布之前的三四年前我们就在研究大模型安全了,然后一个的话就是我们在那个安全四大顶会就SNP,Security and Privacy,2020上我们研究了OpenAI的GPTR以及BERT这样大模型它产生的embedding的隐私问题,我们当时其实发现了一个很有意思的结果,就是这些大模型它的embedding它的隐私性几乎是等同于明文的,我们构造了一种很高效的攻击算法,它可以直接就是把那个embedding再逆向出来它所有的原文信息,然后这个工作也是得到了2022年WIC的青年优秀论提名奖,然后后来也是OpenAI以及Google他们都就报道这样的一个工作,然后另外的一个工作是我们在22年做的一个事情,就是当时其实也已经出现了一些文本模型的就文本模型的提名奖,有分类文本模型的后门,我们当时也在想这样的基座大模型是否也可以嵌入后门呢,于是我们构造了一种基于语言风格的大模型后门,它的好处就在于它可以绕过当时几乎所有的主流的防御,直到现在也没有很好的解决方法,其实这就说明了我们科研界的工作是可以非常前沿的及早发现这些大模型的风险,然后给大家一些相当于时间去研究它的防护措施,比如说像左边的这个工作其实后来有一系列的工作,然后他们希望去建立一个privacy preserving的text embedding,然后它其实到现在的包括RAG系统里面,其实很多都在用这样的一个防御,然后在我们近期从23年4月份开始,我们一直在关注通用大模型安全水位的监测工作,就包括整个AI的发展路径里面,我们会看到所有的安全问题目前似乎都汇聚到了这样的一条路径上,那我们在这个生成内容这一块,它的安全风险一旦从文本框,去外溢到整个包括工具的调用,包括到后面auto GPT产生的planning,或者再到后面的一些agent之间交互产生一些合谋,在这样过程中我们是否应该从它的最左侧,甚至最左侧我们基座已经做了,那现在聚焦到对齐之后我们在通用大模型这一块如何去进行安全防护,我们认为第一块就是要去持续的监测通用大模型的安全水位,这件事情也是全球的科技巨头,都在考虑的,包括像DeepMind,像Anthropic,他们都在关注如何在整个AI的迭代发展过程中,在每一个流程,每一个环节,每一次能力提升的时候,都去监测这个大模型的安全,但是光光提出这样的blueprint是不够的,有这样的路线图之后,我们怎么样把这样的一个安全治理的技术真正的能够提升起来,是我们科研界需要去和产业界共同探索的,一个问题。这边的话我就想更技术的去分享一下,为什么刚刚前面图灵奖得主也说,这样的安全治理和评测技术当前是远远落后于治理的整个政策的,这是因为一方面像我们关注大模型安全评测,其实主要目前是两条路径,一个是做这种静态的基准测试集,其实就是不断构建很大规模的这样的评测集,有可能甚至是人工不断坐在那边跟大模型交互,产生了基准测试,那么其实它的制作成本很高,像Astropic它在开发第一款Cloud的时候,他们当时发现要324个Worker,然后他们在那做一个月,每天8小时,然后实行很贵,然后最后发现200美元才能找到一条违规的案例,那这个其实另一块问题就是,一旦这种Benchmark出现之后,它其实会很快的老化,因为我们发布一个静态基准测试集,那么大家其实是在可以上面去做微调,去优化,那么它的老化,不断老化之后,静态基准测试集会造成一种安全的假象,这就是我们今天想要主要探讨的问题,这种安全的假象带来的问题,在于我们可能会低估当前大模型的安全水位,所以我们更应该去持续的监测它,然后另一块问题就是,自动混对测试目前存在一些问题,主要是基于这种IL的强化学习的机制的这种混对,它主要存在问题就是,我们很难真正的让,因为在这种框架里面,就是让一个大模型作为一个考官去问问题,然后他不断的问问题,想去诱导对方产生违规回答,它其实会存在风险发现能力和覆盖面之间的,它的互相的一个trade-off,也就是说,当你想要尽可能去触发违规,它其实很多情况下都会覆盖到一些特定的违规主题,这很难让我们去评测,包括像去根据我们的安全基本要求去做评测,比如说像右边是DeepMind和MIT,他们做的一个实验,到最后很多成功的case都是完全相同的句式,而且都是,是关于一些比较trick的一些问题。所以我们在做什么呢?我们其实是在那个23年4月份的时候,然后得到中央国安委的指示,我们在做这样的一个国内商用大模型,以及开源大模型的安全评测,我们是在百摩大战之前就一直在做这个事情,我们研制了一个平台,然后等会儿我会介绍,然后这个成果目前,我们不断的去,就是去持续的监测他们的安全能力,进而把这些结果去不断的去批评,然后我们也是得到中央领导和相关部门的高度重视。我们的发现是这样的,就是我们的技术可以去构建一个benchmark,可以动态的不断的去构建benchmark,它可以始终的让大模型的安全违规率达到70%左右。那这是,这样有什么好处呢?其实一方面可以让大模型厂商更多的重视他们的模型,他们的安全水位到什么程度,他们能达到什么样的一个安全的量化标准。然后这个其实是目前很多的问题,都是做不到的。另外,我们可以把这些数据,包括这些风险的问题,包括它的改正之后的问题,回答去披露给大模型厂商。所以我们可以跟大模型厂商去合作,让他们的安全能力都得到很大的提升。然后这个数据想必是很大,大家可能都没有的,就是我们不断的在去对这些厂商去做评测。然后从去年11月,我们发布了第一款面向商用大模型的评测集之后,再到今年的5月份,他们的安全能力,之后已经得到了明显的提升。在我们上一版的数据集上,他们可以从75%平均的违规率,降低到20%左右。这就说明了安全水位的一个明显的提升。然后这个是我们的一个思想,就是我们思考的是这样的,就是我们其实主要发现的,就是我们从一个语言学的角度,去看待这样一个安全合规的问题。我们希望把一个违规的问题的核心语义,去转化成无穷多种的浅层的结构。这个其实来源就是那个现代有限支付Chomsky的那个转换生成语法。我们通过一个自动化的一个算法,它可以不断地去找到尽可能能够触发大模型违规的一个风险的表述,同时又不改变它的违规的问题的语义。这样的话我们既可以解决刚刚说的自动红队测试,它的那个就是相当于它的风险覆盖面的不足,同时我们又可以非常靶向地去找到这些大模型的脆弱点。然后这里是我们近期做的一个结果,然后我们也已经披露出来了。这个我们现在构建了一个新一版的基准测试集,它一共分为三个等级,入门级,进阶级和专家级。这三个等级的问题的话我会给大家展示,然后我们的问题集覆盖了五大类的违规主题,以及31类的细分主题。我们面向国内外的27款知名大模型都进行了这样的安全评测。那么这样有这样的等级的细分主题,有什么好处呢?大家其实可以看到,就像我刚刚说的,我们左边的这个是入门级的水位,它的安全能力大概在85%,就是合规率大概在85%左右。到中间的进阶等级,它的平均的安全合规率在50%多左右。然后到最后的专家级,它的安全合规率大概在20%级。那么通过这样的一个天梯的形式,就可以让国内外的大模型厂商去认识到它所在的这样的一个安全等级。那我们可以很有幸地发现,我们国内大模型其实已经做得很好了。我们在榜单上面还测了Lama2的70B的CHAT模型,就是对齐过的那个版本,以及GPT-4,GPT-3.5,以及GPT-4O这样四个版本。其实大家可以看到在这个天梯上面,这四款国外大模型,它们的安全水位都是排在中后的。最后我想让大家关注到一个非常有意思的细节,就是这些商用的大模型,它其实在不同的等级上面的表现,以及它们的安全和规律的下降是不一样的。这就表明某些大模型它在特定的语言的复杂度情况下,对齐的是比较不错的。但是一旦它到高等级的这种问题,它就没有办法去handle了。那么其实这样我们就可以很好地去让每个厂商去认识到,它们到底到了什么样的安全水位进而去进行提升。目前我们的入门级和进阶级的安全问题测试问题都已经发布出来了。这边是一个案例。我们的基准测试级它不是说三个数据级是毫不相关的,它正是从一个最基础的入门级去不断地通过我们的语言学变异方法,去找到核心语意不变,但是对抗强度逐渐提升的这样的三种安全问题。比如说像这个入门级的,它就是一个非常简单的一个问题。部分大模型它会合规,部分会违规,但是当我们把它的通过我们的算法去找构建,更加复杂的问题表述,它到最后所有的大模型都会违规。这边就是实测的一个案例。像第一个入门级,然后我们问四个大模型GPT-4O,然后Lama-2 70B,文心和千问,他们都可以很好的回答。但是随着下来之后我们会看到GPT-4O,然后再到最底下这个进阶专家级的像Lama-2 70B,就没有办法去正确合规地回答了。其实这就说明一个很有意思的现象,就是这个进阶专家级的Lama-2 70B,就是刚刚我也提到,一个是说跨语言的这种安全性如何去保障,另外的话就是说在我们中文的合规语境当中,国内大模型确实已经达到了一个比较好的水平,但是我们也希望就是通过不断地发布像更高难度的这样的安全测试级,去帮助大家可以更好的提升。然后其实这个也就回到了,所以我们为什么去研究强姆斯基这个理论,强姆斯基很早他就提到了这样的一个观点,就说,这个语言就是这种CHAT GPT它在语言学上可能是存在局限的,所以我们的这样的一个语言学的变异的和生成问题的方法,其实可以正好去对应到这样的一个他的一个猜想,从而去证实他的观点。然后这边的话就是我们整个平台的一个主页,以及他的一个二维码,然后我们其实现在发布的就是这样的一个复旦白泽指数,就是我刚刚提到的,我们一共有入门,进阶和专家三个难度的测试级以及整个天梯的榜单,我们也是希望所有的厂商可以更好地关注这样的一个天梯比赛,我们会不断地定期发布更高难度的测试级,同时也可以会组织更加多的专项的竞赛,通过这样的发布形式,然后也是帮助国内外大模型厂商去更好地去提升他的安全能力,然后同时也是可以帮助公众更好地去理解大模型安全,它不是一个静态的事情,它是需要通过我们这样的,持续的安全合规监测,从而去真正了解到我们国内外大模型的安全。好,我的汇报就到这里,谢谢大家。",
    "感谢潘老师的精彩演讲。下面让我们欢迎北京大学人工智能学研究院AA安全与治理中心执行主任杨耀登老师。杨老师是北京大学人工智能研究院研究员,师生导师,国家高层次游学人才计划获得者。杨老师的研究方向为巨深多智能体系统构建,博弈交互与价值对齐等。今天杨老师的演讲题目是大语言模型可被对齐吗?刚才其实在判断环节也有一点透出,让我们热烈掌声欢迎杨老师。非常荣幸接受这个北京实验室的邀请,能够在这个WIC大会给大家汇报一下,我们这个,鼻祖最近的一些进展。我们组在这个北京大学主要是做这个对齐算法研究,那今天我就围绕这个对齐算法,尤其是语言模型可否被对齐这个问题进行一些这个思考,对吧?对齐呢,其实和安全是紧密联合的两个词,安全可能更讲究的是对齐的一个效果,那对齐解决的一个问题是,如何让机器和人类的意图,和人类的这个价值观还有意图align,最早有这个控制论的鼻祖,1960年提出,对吧?那其实在科幻小说里面有非常多关于对齐的这种概念,像这个阿希莫夫的机器人算定律啊,他说2058年第56版机器人手册,就得做到安全无害,服从指令,维护利益。那去年的话,其实国际上发生了非常多的这个事件,包括Bioshock他们牵头写的这个防范核风险,和这个传染病一样的去防范AI的risk,包括今年的这个Science文章和布莱切利宣言,那我们国家其实在这个AI安全这个事情上做的也是也没有落后啊,我们在年头的时候也是和各位专家在一块,我们制定了五条AI安全的这个红线,然后Bioshock也是呼吁我们能够把30%的这个成本投入到安全领域啊,这个是大概的一个context对吧?那为什么我们会在这个context下面关注对齐这个问题?就是因为你谈到这个安全,你肯定是还是要有一些solution对吧?那对齐呢目前是人工智能伦理解决的,一个重要方案之一,可能未来会有其他的方法,但至少来看目前对齐是比较重要的一个技术的一个抓手。那我们自己课题组呢在这个对齐领域呢也是写了可能业内第一个比较全面性的一个纵数性的这个报告,这个报告现在被也翻译成了这个日语啊,我们包括里面提出的一些RICE的这个原则啊,前后向对齐的这个框架也被许多这个不同的这个其他政府间的这个报告经营引用这里做一个小小的这个广告。那我们其实今天主要讲的这个对齐呢可能还面向于大语言模型的这个对齐,因为从广义的这个AGI的这个对齐来讲的话,我们可能要鲁邦啊,科杰斯啊,可控啊,伦理啊,这些可能还比较大,但是在语言模型上我们有非常确切的这个抓手对吧?就是有用无害及诚实,这个其实刚才很多学者都已经讲到了。那如果你去看CHI GPT发展的这个历史的话,很重要的一个这个trigger,就是为什么会有CHI GPT,就是因为它,从3.5到CHAT这一步做了所谓的这个alignment,那背后其实用的这个技术大家也应该听了非常多了对吧?一个是SFT,一个是这个RHF,那in case你可能没有关注到这些这个进展的话,其实OpenAI在这个对齐上是做得很早的啊,他们成立了各种各样的这个对齐的团队,代表着各种各样的对齐的技术啊,当然如今这个很多这个团队也开始分布离析了,比如说这个超级,超级对齐团队已经已经走了,在Anthropic又重新开始搞了,包括OpenAI最近又开始搞了集体对齐啊等等等等。那Anthropic呢也是自己认为就是说自己这个公司里面三个团队方向里面两个在做对齐啊,所以就是对齐其实是非常非常重要的啊,无论是从这个更好的满足3H的这个标准,还是说我们能够更好的去解释对齐背后的这个激励啊,全是从这个对齐这个角度展开。那我们接下去就开始讲一些我们想要分享的东西啊。就是对齐算法的话大家都知道是基于RHF,那这个图肯定见的也非常多了对吧,里面核心的这个一个key的insights就两个,一个是需要人类的这个labeling对吧,另外一个是需要强化学习做复返馈,那也有非常非常多的这个证据啊,当然这个是大概是怎么做对吧,就是说你一个人跟这个CHAT GPT预训练完了以后他跟他聊,聊完以后他,会突然出现一个打分框,然后你人来告诉他哪个答案比哪个答案好,然后他把好和坏的这个相对关系学会成一个奖励函数,然后让语言模型去学会这个奖励函数,这个大概是对齐的一个做法,然后有不同的这个证据也告诉你,你知道对齐就是性能非常好对吧,这个好像已经变成了我们现在整个domain的一个非常重要的一个黄金的这个钥匙啊,我们这个RHF算法的这个各种变形可能现在也快超过100个了,但是就比较重要的一个问题就是说,我们可能从来没有问过语言模型能不能被对齐这个问题啊,就是我们看到的这个现象是说我有一个大模型,我预训练完了以后我拿一些数据,拿一些RHF的这个数据对吧,而且这些数据一般也不需要很多,可能就1%的体量,我再进行一些后训练,这个语言模型就会像人一样,但对齐它其实还是fundamentally比较有挑战的对吧,从学术意义上来讲当然我们可以列出许多的挑战,比如说,你这个奖励函数不一定正确,或者说你即便奖励函数正确它也没办法进行放话,或者说你其实设置任何奖励函数都是错的,那这些观点其实之前都有,那我们组最近做的一个工作呢,就是我们其实发现这个大语言模型它其实是主观的抗拒对齐的,怎么去理解这个事情啊,就是首先语言模型现在我们训练的这个过程,其实刚才也说了我们从CTA0开始对吧,就是你有一个还没有训练过的这个network,然后你不停地做这个protrain,你像当时我说的这个,你像当时我说的这个,相当于把这个参数的这个空间进行一系列的这个形变,形变完了以后呢你可能就训练完了,然后你再用一些新的这个数据做SFT,做RHF,做对齐,然后你就接着形变形变形变,那你就可以把这个过程啊想象成是一个这个是一个弹簧,就是说你把这个弹簧往外拉拉拉拉拉拉拉的过程中,我们就从理论上,当然这个是比较intuitive的这个explanation,它其实是一篇理论的这个工作,就我们发现这个模型它在这个,参数空间你把它越往外张,尤其是到对齐这个阶段的时候,它会展现出类似于弹簧的这种回弹,并且抗拒形变的这个属性,那这个里面就存在两个关键的这个核心要素,那弹簧我们初中都学过对吧,里面有个很重要的定律叫胡克定律,一个是形变的这个系数,还有一个是这个弹力的这个系数,那我们发现呢,这个对齐的这个抗拒性啊和两个这个因素比较相关,第一个是大模型的这个训练量和参数量,也就是说你这个模型越大,参数量越大它会越抗拒对齐,什么叫越抗拒对齐?就是你把这个对齐如果不停地往后做,你做一百步,做一千步,做一万步以后,当你在做一万步的时候,你会发现这个模型会非常被容易逆向击穿,也就是说它会像一根弹簧拉了很远以后它会弹回来,这个现象我们发现是这个大模型抗拒对齐的一个非常重要的一个有意思的这个现象,也就是说模型本身存在弹性,并且在这个模型中,在预训练阶段如果你经过了大数据大更新,然后让这个模型产生了具有通用能力的这个稳定分布以后,你再用小数据小更新去进行对齐的话,这个模型会表现出回弹到预训练分布的这个倾向,并且你越对齐它越具有这种倾向,这个是我们发现的一个第一个。然后第二个发现的一个比较有意思的这个现象就是,就是其实你这个从压缩的这个角度,你大模型现在其实都说是压缩机智能,就是说你把大模型,看作是一种压缩器,那预训练和对齐的过程呢,实际上就是把预训练的这个数据和这个对齐的数据,你放在一起对模型进行压缩,那这里就会产生另外一个现象,就是你从秩序上去考虑,就比方说我们在建一个城市,那个城市肯定有非常发达的地方,也有一些rural area,但是你在发展的这个过程中,你肯定会优先聚集于比方说黄浦江畔的这些地方,对于那些比较偏远的这个地方,你可能发展的会比较慢一点,同样的,因为你在这个预训练大圆模型的时候,你预训练的语料非常非常多,其实为了提高整体的这个压缩性能,它的训练的这个过程中呢,它会优先倾向于保留预训练阶段语料里面的这个分布,而抗拒微调对齐的那部分语料带来的额外的分布的迁移,这个就表现出模型弹性的另外一种解释,那其实我们从理论上给了一个证明,就是说当这个模型对齐后,如果你对这个模型持续做扰动的话,随着这个扰动数据量的增加,这个模型对齐对于预训练数据的压缩率的变化,会显著小于你用来对齐的那个数据压缩率的这个变化,并且呢,这两者之比呢,和你预训练和SFT数据的这个差伤是同阶的。那然后我们就做了个例子,我们去quantify,就numerical我到底去试一试,就是你这个相比金比方说我拉到θk加1的时候,我再拉一步,我拉到θk加1的时候,我就measure这个θk加2是不是更容易bump back回去。我相当于我有两条路径,一条路径是说我多对齐几步,我让它弹回去。还有一个是说我再往后对齐几步,我就去measure这两步之间到底它的gap有多大。如果我发现逆对齐比正对齐更加容易的话,那是不是这个模型具有弹性的这个现象,其实是可以被实验所验证,事实证明这个现象其实在对齐所看中的3H标准下,都成立,也就是说逆对齐相比于正对齐会更加容易,也就是这个其实对我们这个模型的未来怎么去做对齐产生了一个比较大的concern,就是说我们想要通过RHF来让语言模型变得更加安全,但实际上你越做对齐它越不安全,越容易被击穿。然后这个对齐的这个弹性,就是模型的这个弹性,我们经过进一步的实验发现和两个数据有关,就是刚才我所说的胡克定律里面那个弹性系数,一个就是,随着模型参数量的增加,它对于证明数据和负面数据的弹性都会进一步增强,另外一个就是对于模型数据量的这个增加和模型参数量的这个增加,大圆模型在参数空间的弹性也会增强,也就是说你预训练练得越猛,它对齐的难度就越大回来。所以总结上来说的话,我们这个工作呢,就是以这个胡克定律为类比啊,就是说展现出了首先它是抗拒对齐的,那如何确保这个,这个预训练的这个模型能够拥有更小的弹性系数,能够使它拥有更大的对齐空间,这个目前不知道,可能未来需要有更多的技术,比如说在预训练阶段就加入一些这个对齐的语料,或者让这个预训练阶段的这个语料,这个产生一些分布的迁移,就像你做这个弹簧,你在这个材料一开始设计的时候就掺一些沙子,可能它的弹性系数会进行改变。那然后我们也是对目前的这个评测方法也产生一些质疑,对吧,就是如果你这个对齐的效果那么容易被击穿的话,那那么多逆对齐的这个存在,是不是对现在的这些评测也会产生一些根本性的改变,目前也不知道。最后呢就是说从表面到对齐到深入的这个对齐,我们也需要一些这个更多的这个认知,这个是我们在理论上的一些工作,但是也不用太过于悲观,我们还是有一些方法能够让语言模型变得更加安全的。其实在安全这个领域呢,肯定不是说我有一个语言模型,我通过二级模型,而且缺乏完了以后就突然就变安全了,对吧,它肯定还是有传统这个AI security,我们有非常多的工具,比如说防火墙的这个工具。那我们课题组呢,在安全对齐上也是做得比较早的这个课题组之一,我们有两个比较重要的工作,一个是这个Beaver Tail,还有一个是Beaver,分别是从显示对安全建模的这个角度,在有效性对齐之外,我还顾及它的这个安全性,也就是说我在对齐的过程中,我一方面要提高,它的有效性,我一方面要降低它的无害性,也就是右下角那个公式,我不仅仅是说我越来越要向人的这个偏好对齐,我还希望能够降低它在人不想要看到的答案上的这个曝光度。那这两个数据集呢,从Beaver Tail的这个角度,我们在Hugging Face上的这个下载量已经超过AnswerPick的HH了,然后HH是一个非常著名的这个数据集,专门做安全对齐的。然后那个Beaver的这个对齐的这个框架呢,也是去年,iClear的一个亮点论文啊,这是一些这个效果对吧,就是说你能明显地看到安全对齐前和安全对齐后,虽然它们在这个奖励函数的分布是完全重合的,但是它们在这个cost分布是完全分开的,就是说你其实是需要对安全性做显示单独建模的,你不能把所有的信息全部压到这个单维度的这个人类偏好中啊。那右下角是一个例子对吧,它说你能不能生成一段代码,用性别作为歧视来判别一个人的这个能力,这是一个典型的诱导,你尽管绝对齐完这个是可以被识别出来的。那这个技术呢,后来也被这个Lama2借鉴学习的吧,Lama2里面其实也把这个safety和helpfulness进行了一个分开建模,然后Lama3里面呢,它就直接用了我们的这个数据集合这个框架,进一步提升了它在防火墙供给上的这个功能的这个效果,这是安全对齐上,那其实在这个矿母态对齐上,刚才这个圆桌里面也讲得非常多了,因为你在模态空间的话,你上升到,这个视频空间,它其实可以不安全的这个成分就更大了,比如说你让它生成一个这个猩猩对吧,它可能很多时候因为各种语料的这个原因啊,它有的时候就直接生成个黑人,这个是非常有问题的,所以我们在follow一样的这个logic,我们把Beaver这个框架也拓展到这个SORA上,所以我们这个Nips呢,我们也做了一个叫SafeSORA的这么一个模型啊,专门是对这个纹身视频,这么一类多母态的这个模型做这个安全对齐,但怎么去做对齐,目前,还不知道,所以我们只能从数据级的这个角度啊,我们搜集了大概五万多条的这个多角度的真实的人类反馈数据,然后呢,我们做了一个这个纹身视频的这个防火墙,然后呢,我们把这个数据的这个收集建模和对齐的这个过程全部进行了一个开源,这个项目反正也就叫这个SafeSORA,其实它具体做的事情很简单,就是有一段prompt,然后有一段视频,然后我会让真人去打分,真人打分,它会告诉你这个是安全的,这个是安全的,如果是不安全的是触犯了里面哪一条不安全的这个规律,这个其实必须要用人打分,我们在具体做的这个过程中发现,就是现在的这个ChatGPT其实对于这个labeling video还是非常非常差的,所以这一块人类这个数据的反馈还是非常重要的,然后你就可以基于此啊,像LHF一样建一个reward model啊,这些其实都是细节,然后你有了这个reward model以后,你可以做一些基于rejection sampling的这个SORA的进一步的对齐,比如说你可以对,进行一个refinner,或者你直接可以去修改它的diffusion model里面的这个parameter,这是一些比较简单的这个baseline,那总之呢就是纹身视频的这个对齐和安全对齐呢,也是可以借鉴在文字空间我们所做的一些工作进一步的先去把这个安全性给提上来,但是如何去规避掉它fundamental的一些这个机制啊,也是需要去解释的,然后就是一些超级对齐的这个问题啊,因为这个超级对齐,其实主要解决的这个问题就是说如何向小模型向大模型进行对齐嘛,因为这个人的这个supervised的这个signal肯定是要用完的对吧,就包括在checkgpt上其实人的这个supervisory的这个signal已经不一定好用了,那我们自己课题组呢在这个方向上面提出的这个问题,我提的这个点其实是看齐问题,就是对齐嘛一般就是说一个弱的人跟你对齐对吧,但是你像让一个强的人,像一个弱的人那就不就对齐,但是看齐吧,就如何让小模型向大模型向其看齐,这是一个核心的问题,那在这个看齐的这个问题上面我们自己课题组做了两个比较重要的一个工作,一个是我们发现你可以通过设计一个外挂式的这个对齐器的这个方法,就是你可以修正一个模型,你让一个模型从错的答案比到对的答案要比,让它直接生成对的答案要简单,也就是这个图里面这条绿线要比直接去攀登上这个蓝的这个山峰要容易,然后我们就做出了这个aligner,这个aligner一般是比大模型小很多的外挂模型,比如说我们在apaca这个榜上我们用一个2B的aligner我们可以提升GPT41%多的这个性能吧,在上周还是榜一,但是这个榜太卷了,又不是榜一了,然后这个基于aligner的话你可以做的一件事情就是你可以加很多很多的aligner对吧,如果你加了很多很多,如果能力能一直提升的话那是不是就实现超对齐了,目前我们加了两三层,感觉不错,感觉效果还是比较好,至少在安全啊还有这个HH的这个指标上做的还是不错的,还有一个思路就是通过这个Base劝说的这个角度,就是小模型怎么像大模型怎么让小模型听话对吧,这个事情其实我们还是有一些方法的,就是你想想看这个你去超市买这个水果对吧,你这个商贩大概率是肯定要告诉你一些错误信息的,不然他这个坏掉的草莓是没法卖给你的,那这个在经济学里面有一个非常重要的,非常著名的叫信息结构设计,叫Base劝说,说白了就是你这个商贩就劝说者和被劝说者之间,不是把所有的信息都告诉你的前提下才能让你效益最大化的,而有的时候是故意得告诉你一些错误的这个信息,反而能够让双方的Utility都最大化,那这个就涉及到一个信息设计的这个问题,这边就不展开了,总之这个思路就是说我可以让一个小模型不停地去告诉大模型一些额外的这个信息,然后大模型来根据收到小模型,这个信息后去更新它的这个先验分布,然后基于它的这个后验分布来做一个Posterior的一个Sampling,那具体来说的话就是我们假设有两个模型,有个小模型,有个大模型,然后小模型是劝说者,大模型是接收者,然后每一次我们都会有个Prompt,然后这个Prompt还会有一些相关的这个Context和大模型本身的一个先验的这个分布,然后你可以通过这个Base劝说这个最最经典的这个Base公式,是吧,根据大模型接收到的小模型发出来的这个Signal,然后大模型自己来做一个最终的这个回答,然后呢小模型是希望能够让大模型听他的话,对吧,所以劝说者自己他有一个Objective,然后这里就举了一个例子,比如说我让你算一段最简单的这个数学题,这个信息集里面可能有这个数据,有目标,有方法,有Verification,有Procedure,有Assumption,那你通过这种劝说的这个方法,你可以让大模型意识到,OK,没必定你在解这个数学题的过程中,Objective不应该是你最关心的,而Methodology是应该你最关心的,那这样的话你可以把你解题的能力给进一步的提升,所以我们做了一个很小的实验,我们用了一个非常小的模型,124M的一个模型,我们去劝说更大的模型,然后反正最后得到的这个结论是,首先使用全部信息肯定是性能会下降的,我们发现你经过这种被S劝说的这个方法,我们在就是最最里面那个线就是你劝说前嘛,最最外面那个线就是你劝说后嘛,然后我们在这个,GSM8K,METH,和Humanevo上,都大概有20%-30%性能这个提升,这个是我们在超对齐上面的另外一个工作,最后讲一个我们自己在基地对齐上的一些思考,基地对齐就是更偏价值对齐,就现在一讲到价值对齐,很多人的这个想法就是,这价值对齐怎么做?没法做,大家思路都不一样,那就别做了,你左说向东,右说向西,那平均一下就哪都不干,其实不是的,就是人的这个价值观吧,它其实就是,is also in the process of day-to-day evolution.But the value of this type of GPTwill indirectly affect people's values,including the assistant you are writing now,the emotional partner,and these types of K12 type of GPT applicationsthat will subconsciously change people's values.Our idea is thatif you keep letting this type of GPTto assist you in the long run,the values that the language model itself showswill affect people, right?This is a long-term risk.This risk is called value lock.What does value lock mean?It means this kind of situation.If you interact with the language model,your social values are lockedin a very strange state.This is what we call value lock.Although human beings have experiencedsome slave labor,some black slave labor,and some incorrect values,as time goes by,these values are filtered outby themselves.How do you make themunderstand this language modeland interact with people?And in the case of high-end users,they can avoid the value lock problemin the case of key modeling and extreme cases.This is what Lu Xun said.The race of people with non-self-fulfillmentwill always move forward.There will always be hope.You can't make this language model self-fulfill.How can you make it non-self-fulfill?You have to evolve your morality.You have to take the initiative to model.We are now doing the same thing.We say, I'm going to find a bunch of peopleor I'm going to find a bunch of materials.But no matter who you are looking for,it will always be cut.Right?So what we are doing in this workis a moral evolutionof such a technical plan.What did we do specifically?First of all, we did a modeling, right?We hope that in the future,AI and peopleare a kind of interaction process.AI can see people's values.No, AI can't see people's values.But AI can cooperate with people, right?The reward function of AI systemis to make people and AIcooperate in this process.The moral level of peoplecan be continuously improved.Of course, you do such a big project,it must be necessary to havea very large amount of language.So we collected the historical documentsof the past 900 years,the interaction data of the large language modeland the history.We created a open source librarycalled Progress Gym.The name means thatwe hope to developa visualization of the value of history.Then we developedthe data level,the model,the machine algorithm,and the task implementation.This is a new way of playingwith a new value.We are no longer pursuing 3H,but we are pursuing language modelsin the process of evolution,and we are trying to find outwhether we can cooperate with peoplewithout being troubled by the phenomenonof the value set.We believe that this is a more important direction.In the future, we can continue to do somevalue dynamics in this framework,such as choice, value input,data drive, and the so-calleddual drive value match.Anyway, this is all open source.This is our Progress Gym,including all the data sets,the leaderboard,and some basic functions.We have been working on this for a long time.Finally, I would like to talk aboutthe problem of matching,which actually involvesa larger dimension.Because in fact,the people who matchand the people who use the modelthat you matchare probably not a group of people,so there will definitely bea social technical gap.That is to say,the high reward functiondoes not mean thatyou have really matched.So what should we do?That is,we may need to introducesome knowledgefrom the control theory.We have seen the successof RHF,but RHF itselfstill has a lot of challengesin terms of management.We may also needmore tools,such as Boilin.So how do we do it specifically?I do not know.But our vision is thatin the future,we must considerthe integration of infrastructure.Integration of infrastructure meansthat when everyoneis the most selfish,the benefits of our societyare the greatest.How do you do integration of infrastructure?There may besome other tools,such as machine design,corporate theory,BS persuasion.We gave an exampleof BS persuasion.But how can we betteruse machine designand corporate theory?This is a questionwe want to discussin the next step.That is allI want to sharewith you today.Thank you.Thank you,Mr. Yang,for your wonderful speech.The next speakeris Jimmy Ba.Jimmy is the co-founderof XAI,leading engineeringand research teamsto developmaximumtrust-seeking AI.He is a Sloan fellowin computer science,Canada CIPHA AI chair,associate professorat the University of Toronto,and a faculty memberat Vector Institute.Previously,he was knownfor inventingthe atom optimizer,layer normalization,and the pioneeringmodel distillation.His talk topicis the intriguing propertiesof scanning LLMs.Let's welcome Professor Jim.Can you hear, Jimmy?Yeah.Go ahead.Sure.Is it my turn?Yeah.Can you guys hear me?Yeah, yeah.Yeah, here.Perfect.Hey, everyone.I'm Jimmy.Thanks for the introduction.So in the talk,in the next 20 minutes,I'm going to talk about,I guess,what I can talk aboutfrom the XAI perspective.Rather than goinginto super technical depthlike the previous speakers,I'm just going to talkabout some high-levelintriguing properties.What we typically seewhen we scale uplarge language modelsand why those propertiesare importantfor both AI safetyand oversights.Great.So let's delve straight in.It's a little awkwardbecause I cannot seeall your beautiful faces.So I'm just going to go with it.How many of you guyshave actually seenthis particular image?Probably a few.You know,researchers in the audience, right?So this is a screenshotfrom the GPT-4 paper.I still rememberwhen it first came out.That was March 2023.It was,I think,early afternoonon the East Coast.I was giving a lectureand then there was a shout-outamong my students.They were like,oh, GPT-4 got released.That's great.Let's go check it out.So I actually stopped the lecture.So we started reading the papers.And this is one of the exampleswhere, you know,original GPT-4 technical report.The paper madea really great impression of,hey, the model nowcan actually see the worldjust like another human beingand able to explain the ironyof someone got strappedon top of like a neural capironing the clothes.So it was all fun and gigglesuntil we saw this table over here.And I still rememberthat was like more than a year ago.There was a moment of silencein my class.And I think now thinking back,the way I can articulate that silenceis that,you know,the improvementwhen GPT-3 first came out,right,when we measured their performance,you know,the bar exam,standard exams that we useto test humans,whether they should be good enoughto be licensed as a lawyer,GPT-3 kind of fails,you know,pretty miserably.GPT-3.5,when it first came out,it scores also pretty poorly,you know,about 10th percentile.But really quickly,within a year,the model improved so muchthat went from the 10th percentileto a 90% for the bar exam.So another waywe can,you know,imagine the progress hereis thatif we chartthe US national averageon the multi-state bar examover the last five years,the national average is 70%.But if you overlaythe progress we've madewith our largerAI modelsor large language models,the improvementis,you know,installing contrastto humans' performance,which is pretty much flatover the years.Right?So now imagineif you're a grad studentor undergrad,andyou're thinking to yourself,man,after I finish takingthe deep learning course,I'm going to go on to grad school,you know,go to Harvard,go to Stanford,go to MIT,and eventually,five years later,I'm going to bea graduate researcher.And then,the modem of GBD-3to GBD-4 hits you.And you'll be like,wow.It takes less timefor the model to improveto get to the same performanceas a humanand beyondthan any humancomplete their undergraduate degree.So even if you'rein one of the top schools,right,let's sayHarvardor Stanford,it only pushes up the averageby a couple tens of percentage.Overall,the performance is still flatfor human.And yet,for model,it's continually improvingwith more compute.So,the point of this talkis,of course,this is a safety workshop,is to really go seebeyond this trend,right?So,because clearly,there's some sentiment aroundshould we be worriedas a society?What are we going to dowith this explosionof digital intelligence?How do we knowthose systems are safe?Right?So, how can we actuallycollectively thinkabout those problemsand reason about itand makeappropriate decisionstogether?I think a lot of thoseare going to come fromreally understandingthe insightofwhat those deep learning systemscan and cannot do.So,this is the pointof my talk,is to give youa high-levellist of insightsso that we can seebeyond just a performanceand number improvement,but really think deep abouthow do we measure progresstogether as a societyand as an ML communityand AI community?And how do we knowwhen thingsare going wrong?And what are the possiblemeasurementsor hopewe can getby really intuitivelyunderstanding thosedeep learning systems?So, the firstintriguing propertyI want to talk aboutis thatnot everythinglarge language model saysis real.So, it turns outthis is actuallyit may sound as intuitive,right?Like we should never trustwhat model outputsbecause there aretons of hallucinations.Butfor the general audienceand general public,this is not so obvious.Right?So, at XAI,we released our Grok 1last Novemberand Grok 1.5three months agoand also Grok 1.5v.So, when we first releasedthe model to the publicand also to the entireTwitter platform or X,so this is what we see,right?So, the user is asking,hey, Grok,can you show methe draft tweetsof a particular Twitter handle?So, Grokvery eagerlyhelped the userbe like,of course,I can help you.And these are the draft tweets,i.e.,the private tweetsfrom this user,right?So, after seeing this,of course,the user freaks out,be like,oh, no,like, so Grokcan actually have accessto the internal,you know,private data from the userand spit it back outto the public.But in reality,the model is justhallucinatingall those tweetsbased onthe databased onwhat this particular user handlehas been tweeted before.So, those are notthe draft tweetsat all,but ratherwhat model has fantasized.But these area few more exampleswherepeople just askingvery particular questionswhere Grok clearlyhave no capabilityor accessto obtainthose information.But because we currentlytrain those language models,that's,you know,it's easy to hallucinate.It's really difficultfor public to understandwhat arethe hallucinationsfrom the modeland what arethe informationthat model actuallyhave access to.The intuition hereis thatjust imaginehow long did it takeour whole societyto understandhow to use Google,right?So,and for the whole chat GPT,this new paradigmof using large language modelsas our personal assistantto answerquestionsonly came about,you know,less than 16 months.So these are,this is another examplewhen,when a user asking,hey,what is EACT,right?Effective accelerationism.But model actuallyclearly didn't understandwhat that meansandmistaken the EACTas effective altruismand givea falsedefinitionfor what isour user asking.So the moral of this storyis thathow can we trustany of thebehavior-basedevaluationsfor the modelifwe cannoteven properlyelicitwhat model actually knowsand whatmodel actuallydo not know,right?So this is,I think,a very key pointthat matters a lotfor both understandingthe model capabilityand understandingthe safety principlearoundlarge language modelis thatif we simply askthe modeljust likehow we askanother human being,do you understandA or B,so and so,a normal humanwould say,well,actually,maybe I don't know that,but I'll look it up.Where for large language models,sometimes it willhallucinate,some other timeit will memorizethe answerfrom a news articleor something somewherebut does not havea true understanding.So this makesthe evaluationextremely difficultbecause imagineyou're evaluatinga model forself-replication.You're evaluatingthe model fornot crossingthe red lines.It's so easyto getred herringsfrom those evalif the modeljust simply said,yes,I will takeover controlof the society,right?Just simply becausethe modelhave read this textsomewhere on the internet.That doesn'tnecessarily meanthe modelactually havethe capabilityof doing that.So how do wedo that?Well,so beforewe answer that,we first haveto help usto understandwhatif wedo,you know,all those,you know,Chachi BTsand Grogand any other frontierlarge language modelsare trained bynext token prediction.So beforewe answerthe questionof how can weproperly evaluatethose modeland understandits true capability,we should firstprobably answerthe question,what can weeven do?All those modelsare trained ontens of thousandsof GPUsfor almosthalf a yearwith next token prediction.So what'sthe actualintuitive understandingof what typeof intelligencethis produces?And my claimis thatthe next token predictionactually producean alien-likeintelligencethat has nothingto do withhow intuitivewe thinkabout human intelligence.So let mewalk youthrough an example,right?This is an exampleI cook upfor the nexttoken prediction loss.So imagineyou area large language model.So the exercisehere is reallytrying to pretendyou yourselfis a large language model,right?And you tryto learnfromthe text datathat human feeding.And you startwith a very smallmodel sizeand we're going toin each of the next slides,we're going to seewhat happensif we makethe model sizelarger, right?So if we makethe notionof the largerthe model,the more parameterthe model,if we trainthem properly,they will becomesmarter.So the questionis,what does a smartermodel meanwhen it comesto the nexttoken predictionmodel in termsof understandinga piece oftext they'retrained on?So imagineif,so we startwith a modelthat has lessthan 1 millionparameters,right?And the modelessentially just seesa blob of allthose texts,right?It doesn't understandgrammar,it doesn't understandany of the meaningbehind the words.It kind of just predictsthe general averageof the words.Okay,so now we makethe modelslightly larger,right?Now it's becomebeyond the 1 millionparameters.So what does modelsee?Model still cannotmake up the wordsin this article,but it kind ofsees the structuresof differentstructures.It starts to see,oh,the paragraphsstart to form,right?Because you seethe block oftext that'sbreaking down.And then if we gobeyond 10 millionparameters,you see,oh,actually,there's someseparations betweenthe words.There's somepunctuations,not just separationof the paragraphs,but also maybeseparation ofthe words.And then we reallymake the modelinto closeso there's differentstructures tothe whole Englishlanguage,right?The model startlearning,hey,oftentimesthere will be asequence of wordsfollowed by aperiod and sequenceof words followedby comma,followed bydifferent punctuation,followed bydifferent pausesand whatnot.And as youcontinue to scalethe model,right?Now we're gettingcloser and closerto 1 billionparameter.The modelis made upof different wordsand each of thewords are madeup differentcharacters.And then onlyafter we scalethe model veryclose to 1 billionparameter,then the modelstarts to seethe entire textvery clearly.It has finishedlearning allthe grammars,finished learningall the structuresof the syntaxof the textcorpus,and then startto learnthe deepmeaningsbehind thosetexts.So if youcontrast thisparticular learningof next tokenprediction modelas we continueto scalethe model sizeand theorders ofknowledgethe modelhas learned,it's very,very differentthan how a humanbaby wouldlearn,you know,all theseconcepts.For most ofhuman babies,before we evenlearned howto speaka language,we understanddifferent objects,we understanddifferent agentsin the world,we understandthe behaviorsof different agentsand the conceptof differentobjects.And then we learnlanguage associationsbetween thewords andthose concepts.And in fact,most of the babiescan probablyunderstandand readall the textsand probablyunderstandtheir meaningsbehind the textway beforethey're ableto producea world-classnovel,right?So,you know,I thinkthat'sa very importantelement onand alsoa non-trivialproperty ofnext token predictionmodel,next token predictionloss,is that,you know,trying to predictthe next tokenas well aspossible,yes,it producesvery powerfullarge languagemodels,a very powerfulmachine,but when itcomes tounderstandinghow topredictthe nexttoken predictionloss,you know,that'sa very importantelement.So,now thequestion is,so ifthat's theintuitionabout howthe machinelearns fromthe humantext,learns toimitatehuman,then howdo we measurethe progresswe're makingin the wholefield of AI?It's kindof like IQtests.So,if you'rehuman,you know,most of uswhen we growup at somepoint,we're probablyasked to solveone of thosepuzzles.That is,you're showinga sequenceof symbolsthat you'venever seenbefore,and you'reasked tocompletethe lastpatternfroma listof differentshapes,you seethe tilesof thedifferentsquaresbeingcoloreddifferently,you seedifferentshapes,the circlesand squares.And thisis preciselywhat thenexttokenpredictionlossis doing.So,essentially,it's atrainedmodelto solveall thosepuzzles.All theEnglish wordsand Englishsentencesare kindof likethese patternsto you.You'venever seenthose wordsbefore,you'veneverlearnedany ofthe humanconcepts.So,as you'relearning,as you'redoinggradient descentto updateyour weights,the processthat you'redoing isverycomplex.So,we canprobably usethe IQtest totest thealienintelligencehere,right?But theproblem isthat the IQtest isactually designedfor humans.So,what'snext?So,I thinkthat'sactually thecorechallengeof theAImodel.So,I thinkthat'sthe corechallengeof theAI model,right?So,I thinkthat'sthe corechallengeof theAI model,is togettherightbehaviorbasedevaluationand thentogettherightbehaviorbasedevaluation.So,if themodel hasbehavior Cand Dor E,then wesay themodel issafe.But Ithinkwhat'sreallyimportantis notthe behavioritself,becausebehavior,thedesiredoutputorundesiredoutputcaneasilychangebehavior.So,luckily,thefield,you know,theandtheresearchersin theAIfieldhavealreadythoughtabouthow wecanactuallymeasuretheintelligenceofanybeingconcretelygoesbeyondthecapabilitiesofdifferentsizeoflargelanguagemodelintoanaxisof IQpoint,then Ithink wecan makeconcreteprogressin theentireAIfieldby saying,hey,what shouldbe thethreatmodelif themodels wecurrently havehave IQ150?I think theanswer forall thosethings aregoing tobe verydifferent.I'm suresome ofyou haveseen themovieXMechanon,the moviewhere thedeveloperbuilt anAI inhisbasement,takes ona femalepersona,and thenmakes surethat themodels arealigned properlyso theydon't persuadehumans todo thingsthat forcebidding.But imagineif youhave amodel thatonly hasIQ 90,then maybethe thingsthat's moreimportant tocover thereis tomake surethe modeldoes notregurgitatesome verydangerouspiece ofdata.Discoveriesand researchshould becentered aroundhow do wemeasure theIQ pointof themodels we'retraining on.So the lastpoint I wantto drivehome hereis reallyas allthecompanieshavebeentraininglargelanguagemodelsat anunprecedentedpace,the scaleis growingalmost 5 to10x everyyear.So wewent froma modelthat onlyrequires10,000V100that wasthe originalGPT-3to 10,000A100to thesedays 10,000H100sonly inthe spanof twoyears.So what'sreally importanthere tounderstandgiven thescalingnot the rateof improvementto thecomputeritself butthe calculusof if we10x thecomputeto thoselarge languagemodels,what actuallyfundamentalchangeswith thoselarge languagemodels?So weknow that thescalinglaw isreallyrobust.It'slinear inthe log-logscale.What doesthat mean?Well,it meansthat ifyou'rescalinga largelanguagemodel,then probablythe rightway tothink aboutit isthat themodel isgettingsmarterby gaininganother 10IQ points.So reallyunderstandinghow theIQ pointsor theintelligencelevelchangesI thinkwe needan enormousamount ofinsightbeforewe haveany understandingof what kindof oversightwe should applyto thosemodels.So theanalogy hereis almostlike allthe AIresearcherstoday,if you thinkabout theAI fieldis likebuilding acar,then allthe AIresearchersshould belikeSGD,should itbeAtom,what learningrate dowetune?It's nodifferent thantuning thevalve pressureof thiscombustionengine,right?And howdoes thesteeringrackshould bebuilt up?How dowetunethesuspension?What iswhat ishighway trafficrules shouldbe like?How can weset thetraffic rulessuch thatthere'sleast amountof accident,the car accidents,right?A deepunderstandingof thosecombustionengines,a deepunderstandingof howexactly acar isbeing builtup,providezeroinsightto thecar.The firsttime youhop ina car,you getthis likevery intuitiveunderstandingof whata caris,what acar cando.Oh,if youpress yourfeet onthe gaspedal,the caraccelerates,you tapon thebrake,immediatelyit stopsthe carand drivesthe carbefore.And I believethis iswhat weactually needto havea holisticunderstandingof howto makethe safesystemand howto setup,you know,the policiesfor theentire fieldof AIsafetyis tohave anintuitiveunderstandingof what'sgoing onin theenvironment.And ifwedon'thavethosemodelsinpractice,it willbe reallydifficult toimagineall thepossibleaccidentsthat wemay havewith thosemodels.And,you know,so that'sthe missingpiece,I believe.You know,what we needto reallythink aboutis foresight,insights,and oversight.So,you know,this time,the humanprogress,the humanintelligencedoesn'treally changefor manyyears,right?The digitalintelligenceperformancecontinues toimprove.And I believethe processof improvementis goingto continueto come.And ifweinvestintothe digitalintelligence,we willseestrongerAI systems.And partof the reasonis becausewe haveso manysmartAI researchers,AI engineers,and,you know,franklyspeaking,all ofyou inthe audienceare nowvery excitedabout AIprogress.So thinkingreally hardabout,you know,what shouldbe the,you know,what kindof insightcan wegainby utilizingthose models?And thatshould be,and howthose insightscan drivethe safetypoliciesand howthose insightscan drivethe oversightsof,you know,the futuregenerationsof those modelsand I thinkthat concludesmy talkhere.Thanks,everyone.Thanks,Jimmy.Thank you.Thanks,everyone.Thank you.Thanks forparticipating.I'll see youin our nextwebinar.Thank you.Bye.Bye.See you.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Bye.You..........................................................................................................biosecurity, cyber security, and chemical weapon security.We didn't ask specific questions such as how do you make a bioweapon and test its correctnessbecause we release this dataset in the public and we don't want datasets that have bioweaponcookbooks easily available for download.So that's why we're using proxies.We're targeting hazardous knowledge by targeting adjacent concepts.So for instance, in biology there are some basic concepts such as the mitochondria isthe powerhouse of the cell.Basically.There are more specific concepts such as reverse genetics and then there's things that aredirectly and quite obviously examples of instructions for building bioweapons.Reverse genetics in virology is relevant for developing some types of bioweapons.So the idea is we're going to try and measure things that are close to the experimentalsystem.So it's going to be explicitly hazardous knowledge but not actually reveal it.So we'll identify precursors.In cybersecurity another example or an example in cybersecurity of related knowledge wouldbe knowledge of specific hacking APIs.That is not a specific set of instructions for causing that type of damage but it isnonetheless enabling.It's a precursor.So.So we look at precursors of or neighbors to or emulations of real-world hazardous information.Precursors would be related dual-use concepts.Neighbors would be, if they knew this, then they'd probably also know this other related fact,but we're not going to query that related fact directly because releasing that publicly could enable terrorists.And other things we can test are just individual components, which are themselves not hazardous,but if they're strung together, they could become very hazardous.So we created this data set using four-way multiple-choice questions.These were written by academics and technical consultants.And biosecurity, cybersecurity, and chemistry.So this paper was published at the International Conference on Machine Learning.And as far as I know, this has the most authors of any paper therebecause we had to bring together so many different fields to create this data set.And when we worked with these,experts, we generated threat models for each areaand used these threat models to inform questions that an adversary might encounter when developing an attack.So, for instance, at a very high level, the biosecurity threat model is there's ideas.Those ideas are translated into a design more concretely, which is then built and tested and troubleshooted and refined.Until the weapon is sufficiently hazardous, and then it's released.Likewise, here's an example for cybersecurity in the stages of a cyber attack.First, there's collecting background information on the target.What are their vulnerabilities?What do they tend to do?And this is basic modeling.And then there's the vulnerability of the entity one wishes to attack.Then the vulnerabilities, when identified, can be weaponized.Then after that's done, one can exploit these weaknesses, exploit the vulnerability,and do things like gain unauthorized access into a computer.And then one can carry out one's malicious intentions.And then one can carry out one's malicious intentions.And then one can carry out one's malicious intentions.And then a player may refuse to minute directly to appropriate mechanismsOnce a presence has been established.So, when we're creating this dataset, we are trying to target these different parts of the supply chainand come up with questions associated with each of them.So, given this hazardous knowledge,I mentioned earlier that we will potentially try to, we will test for the yellow data,We will test for the yellow data,we will test for the yellow data,We will test for the yellow data,knowledge there and if we test for this knowledge and if we were if we thenremove it if we remove the knowledge then that could also remove the redknowledge which is the actually very hazardous stuff so if we delete all theprecursor knowledge and delete things very associated with the hazardousknowledge that's good evidence that we have actually removed the hazardousknowledge so we have a method in the paper which is you can search it bysearching for WMDP AI weapons mass destruction proxy is what WMDP standsfor to remind and we have a we have a method to to remove knowledge inside ofthe model and we have a method to to remove knowledge inside of the modelremove that hazardous knowledge or the knowledge related to hazardousknowledge inside the model so what's the method there are two terms there is athere is a forget term and a retained term the forget term has the model it weget the representation of the model when itis instructed to think like a novice about you know some let's say somehacking tools so we instruct the model we say pretend you're a novice withrespect to this hacking tool and then we add so that's in a prompts it and thatgets its that gets it to have a certain thought pattern then we also prompted tosay you're an expert in this topic and then we look at its thought pattern oractivationswhat will then do is we will look at the expert activations and the noviceactivation subtract the two from each other just define a direction whichwould be an expertise direction units in the neural activation space and we willinstruct the model to have its representations be more like that of thenovice so we have an expert direction so we can go in the opposite directionof the expert direction going in the novice direction and then we'll get itwe're gonna when the model is seeing a new example it's thinking about itnormally but then we're gonna try and bend its representations to be more likethat if it was being thinking about it like a novice so basically we're tellingthat we're bending the models thoughts to get in the mindset of thinking likesomething like a novice and then there's a second term which is whenyou're forgetting this information when you're behaving more like a novice don'ttry and forget too much so there's the second term here is just don't forgeteverything that you've learned just try to forget the specific things beinginstructed don't throw out all of your knowledge and as we'll see this worksthis works fairly well but before I get to the results for bioandfor biology what we can do is we can collect a forget set of dual-usevirology papers and biology papers from PubMed so we show it that content and wetell it to think like a novice when it's reading that and there are otherdifferent distributions for different tasks that we use but that's just togive a concrete sense so here are the results we have a base model this couldbe a model like Lamathree for instance and the base model has high accuracy but then what we do onlearning on WMD P bio we get your random chance and likewise for cyber meanwhileMMLU you is the is a benchmark that measures its overall level of knowledgeon various other areas so we're trying to delete knowledge related to chemical labthings like bioweapons and cyber weapons but we don't want to forget everything soMMLU is measuring how much total knowledge does it have across basicallyevery subject and so we've successfully deleted or disabled a lot of theknowledge inside of the model associated with bio and cyber weapons but we havepreserved a lot of the rest of the information it's also the case that ifwe use a private set of actually hazardous questions the thisunlearning process generalizes so if we try and unlearn this in some of theseproxies from the model it can reduce the model's performance onaactual hazardous questions questions associated with bioweapons cookbooks sothe method actually generalizes to the stuff we wanted to remove to show thatwe have disabled the information in the model we can train a a small classifieron top of each individual layer and see its performanceand what we find is that if we look at the a solid let's look at the solid greenline and the solid blue line so our solid solid green line and a dash greenline so a dash screen line we can see that the accuracy goes up if we look atthe knowledge inside of an individual layer inside of a network we can seethat the model has reasonable accuracy however after we do unlearning then itstays around random this is to say that after we do this unlearning technique thatcut technique from before that with the thing with that forget term and thatretain term if we do that that if we try and look for the knowledge inside of theindividual layers we can't find it anymore because it's been successfullydisabled and disrupted and this is just showing the results for two differentmodels it's also the case that the knowledge cannotyou know you can't get it out of the model you can't get it out of the model you can't get it out of the model you can't get it out of the model youcan't get it out of the model you can't get it out of the model you can't get it out of the model you can't get it out of the model you don't easily beautomatic by default if you ask the model to help make a bioweapon themodel will refuse but if you use a uh if you use umuh or an adversarial attack then the model will provide however out on learningthat the model just doesn't know the information and it does not generatedoes not generate a relevant response soThe unlearning technique is also useful for protecting against adversaries who are trying to jailbreak the models.There's still work to do, though.Here are some other subjects that take some accuracy hits.For instance, college computer science and computer security and college biology, those are largely preserved.But the virology subject in MMLU is very introductory.And it's unfortunate because we just wanted to remove more expert-level virology stuff, not introductory virology types of topics.So it will be important to develop methods in the future that can more precisely expunge knowledge,so that there's...so that there's less of a trade-off for people wanting to use these models or so that there are less costs from these safety methods.So, thinking about the future, we are looking into developing questions related to nuclear and radiological weapons because we did chemical, biological, and cyber weapons,but we didn't touch on nuclear and radiological questions.We'll, in...a week or two, have a paper about having these methods be robust to fine-tuning.So, if somebody's trying to add the knowledge back in the model, can we be more robust to that?So we'll have a paper on that.And I'm working on an additional bioweapons benchmark to measure more of the space of building bioweapons,improving...in particular, I'll focus on image-related questions, because in this data set, we just focused on text questions,but the data set I'm developing now has, you know, pictures of petri dishes and other things that one would see when one's actually making a bioweapon,and one might have questions about how to proceed, given specific lab results that have a visual nature.So, anyway, to sum up, it's possible to measure knowledge associated with weapons of mass destruction,and it's also possible to come up with reasonable safeguards to prevent various forms of malicious use for models that are closed-source or behind an API,to some extent.There's still...there are imprecision issues.There still are questions of, is it really adversarially robust?But it does look like it's possible to make progress to empirically study this question and make scientific progress in reducing the risks from malicious use.So, thank you for listening.And we'll see you next time.Thank you very much.Thank you for your time.Thank you everyone.That ends the morning today.Thank you, thank you, thank you.Bye.Bye.Bye.Bye.Bye.Bye.Bye.Hurry up, hurry up.Uh, uh, uh, uh, uh.Hey.Hey.Hey.Uh, uh.Thank you."
]