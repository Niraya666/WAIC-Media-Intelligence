[
    "请不吝点赞 订阅 转发 打赏支持明镜与点点栏目欢迎大家来临2024世界人工智能大会智能计算与强化学习论坛的活动现场很荣幸能够和大家共同见证智能计算与人工智能技术的新进展本次论坛由世界人工智能大会组委会上海交通大学智能计算研究院山树科技联合举办我是主持人曹依然请允许我代表本次论坛的全体组委会对各位的到来表示热烈的欢迎和衷心的感谢欢迎大家当前智能计算正在成为时代刚需电网 交通 金融 先进制造 供应链 工业软件等重要行业对智能计算软件的需求剧增本次论坛活动旨在深入探讨基于强化学习的智能算法研发历史与进展展望智能计算的未来如何与优化技术 GPU计算 强化学习 统计等方法相结合以及在世界范围内的领先应用下面请允许我为大家隆重介绍出席本次活动的嘉宾他们分别是斯坦福大学李国鼎奖习教授上海交通大学访问奖习教授叶英宇欢迎您上海交通大学智能计算研究院院长山树科技联合创始人首席科学家葛东东欢迎您纽约大学斯特仁商学院中生教授计算机科学和数据科学中心教授陈希欢迎您普林斯顿大学中生教授AI创新中心教授陈希机器学习研究中心主任王梦迪欢迎您宾夕法尼亚大学沃顿商学院中生教授机器学习研究中心主任苏伟杰欢迎您让我们用掌声再次欢迎所有到场的嘉宾和观众朋友们接下来让我们进入论坛的主题分享环节首先",
    "有请斯坦福大学李国鼎奖旗教授弗诺依曼理论获奖者优化运筹领域领袖级学者叶英宇先生他将为我们带来主题演讲AI与数学优化求解超大规模数学规划模型的新进展好掌声欢迎谢谢大家我跟大家汇报一些我最近在AI和优化之间关系问题上的一些心得体会和一些研究的成果跟大家分享感谢这次大会给我这个机会OK这是介绍我自己对上面提到弗诺依曼我不知道大家知不知道弗诺依曼这是计算机的顶尖是吧创造了计算机其实弗诺依曼也是对AI也有贡献的特别对数字经济对均衡理论说怎么样的这样的方法上其实我们引用的都还是他的那一套思路OK我跟大家介绍一下什么叫做数学模型和规划或者说规划有时候也叫优化OK提起这个数学规划我们必须提到这几位人物这个叫Coopman这个叫Kantorovich这两个人因为在30年代40年代上世纪发明了线性规划或者叫线性优化由此而获得了1975年的诺贝尔奖经济学奖OK那么这两位得奖的时候他们就邀请了中间的这个人中间这个人叫谁呢就叫George Danzig这个人对线性规划的贡献是什么呢他提到的就是他提到的就是他提出了找出线性规划或者现在叫训练的一个算法叫单纯型法所以当这两个人创造了这个数学模型的时候他们深感如果没有算法这个都是一纸白文都是纸上谈兵的事情所以这样大家也知道一个数学模型或者一个大模型或者一个语言模型需要说明的需要数据需要模型而且还需要算法所以George Danzig提出这个算法的时候还没有计算机他的理念是你哪怕用笔用纸你也要征询一定的逻辑把那个最优的方案找到最好的解啊找出来所以他们就得到他们当时就把George Danzig也邀请到那个Pottings上他说You deserve the priceas much as we do他认为他们也是资格的那么一个数学规划呢主要有三个要素第一个就是数据第二个就是变量你能够做些什么您能够改造那些变量的一些值第三个就是目标我们通常叫做目标函数目标函数呢可能有些人说可能一需要最大化也可能有时候需要最小化那么第四个叫做约束我不能为所欲为我的决策一定要满足一定的物理或者经济规律而有时候还需要有些政策的约束等等这样叫做约束这个就是线性规划X就是变量A C B这都是数据这个约束通过数学的表达式表达出来比如说X必须在0和1之间有时候X要么取0要么取1这个application everywhere最近也把X的变量推广到矩阵上面这个矩阵需要是一个半正定的对称的矩阵通常把它叫半定线性规划也是有很多很多用这里我就不仔细讲了我跟大家举个简单例子这在我们优化的问题中叫做背包问题给了你一个可以承载5公斤的背包给了你5个产品每一个产品有数据比如说第一个物品2公斤重可以值得18块美元我一共有5个那么我们的数学问题来了我们的数学优化问题来了你应该把哪个物件放在背包里满足5公斤的重量约束使得你总共的背包里面的价值最大听懂了没有再来决定放哪个你就可以猜你可以重启法案每一个物品有一个变量0不放1放进去是不是每一个物品都有两种可行性呢那么5个产品有多少种可能性呢是不是2都5次方啊那么大家再想想如果我现在有几万个背包像一个生产线一样的我有几百个物件需要在生前的背包里面放进去如果我现在有几万个生产线需要放在背包里放在背包里就是在那个生产线上进行生产这可能性有多少啊你解决不出来的这必须要用数学的优化的方法靠人类已经来不及这个是海量的是天文数字当时我们也我们一些山树科技的小伙伴也比较迷信那个大模型当时去年就问了大模型他说你看这个就是把每个物件的属性都说出来了背包一共存五件针当时插了GPT-4也回答了他说这是一个经典的背包问题可以用数学优化的问题来解决然后怎么定义变量怎么定义目标函数怎么定义约束最后他得出的结论是求解结果表明其实他没有求解他说第3、第1、第3、第5个放进去最好听懂了没有1、3、5放进去2和4不放大家发现问题没有这三件中六个问题了这三件中六公斤违背了五公斤的背包约束当然插了GPT很会改正错误的你稍微跟他提示一下他会把它改过来我在这里想说明一个什么问题呢就是插了GPT这样的模型它是给你一个近视的结它不是给你一个科学的结或者精美的结或者100%正确的结这是本身就是大模型它不是基于一些规律基于一些数学原理的它是基于大量的精炼这些东西的所以这个错误是错误的当然这个是很早产生的就是想说明这一个问题那么其实数学规划就需要这种解需要用一种精确的需要基于逻辑的基于规律的基于数学的这样一个解要不然我们就出错了我们再回答一两个问题产生一两个图片出错可以但是如果这是一个非常精准的需要完全100%正确的结它可能会造成人命可能造成其他的巨大损失的所以我们必须要依赖数学规划依赖刚才George Danzig的这个方法说到George Danzig就是刚才中间那个人我是82年到美国去读书的那个时候能工智能很火叫专家系统我们搞了很久我当时就跟我们斯坦福大学的一个华人教授叫谢德森搞专家系统那位专家呢他想当时在82年的时候比如SimonCognitive Psychology他想搞一个中医的专家系统结果我们到去但是那个时候没有数据他说没有数据你可以走出去到那个什么三藩市的那些中医学堂去找那些中医们去让他给我们提供一些这样的数据提供一些他的那个知识经验结果我还果然去了去问老中医结果老中医没有出来他儿子出来了对不起祖传秘方天机不可外露我不能告诉我父亲的这些机密所以当时专家系统搞不下去没有数据了所以在毛泽东以上现在之所以能工智能发展得很快实际是产生了大量的数据这个需要感觉有很多数据都出来了我没办法当时没有数据专家系统搞不下去转头就搞了优化跟那George Danzig就去搞线性规划而且当时搞线性规划呢特别火当时出来一个内典法什么东西所以就跟大家讲一讲所以优化是依靠数据AI需要数据当然还需要善力其实还有一个主要的方法就是算法其实本身大模型的训练就是在解一个优化的囚犬它也有变量也有目标函数也有各种各样的一些约束条件等等这样的东西大家也知道神经网络神经网络它主要就是要调参每一个参上的参数怎么调到最佳使我的一个损失函数的值最小通过大量的数据放在一起所以在矛盾意义上大模型的训练本身就是一个优化问题它的变量就是神经网络的各种参数它的目标就是让它使得它的答案最佳OK我们最近我不知道大家可能大家只关注大模型的功能没有功能大模型是怎么训练的它解的优化问题现在用的最多的是一个算法叫ADAM比如说这是一个喇嘛7个billion的工作它通常要12个2乘以8 16个billion的数据所以它每一个数据有4个bytes就28个GB加起来总共需要86个GB才能把数据乘下去为什么现在GPU解不了必须用GPU因为它存储空间大一些精度不需要那么高这是在800上面我不知道大家知不知道像训练一样像训练一个大模型现在需要的时间是month按月来计算的需要用到大量的电力才能做一个预训练这个和我们当时解一个大规模的线性规划所需要的时间差不多等一下我会讲一讲现在我们解同样规模的线性规划已经从月提高为秒所以我的感觉是虽然大模型很火但它目前的训练方法是不可持续的需要大量的人力物力财力而且特别是能源的消费这里面我就举了GPU support training现在大家都想怎么提高这个ADAM呢想了很多很多年了想了很多方法怎么把时间手挡但是有些方法现在都没有成功最近我们做了一个工作因为我们懂优化我们就把它的目标函数拿出来就发现它有些规律这个搞优化的人都知道一个目标函数它有梯度还有那个hashing证它的hashing呢几乎是一个block diagonal这个可能和它的层一层一层的有关系的就基于这个思想方法因为ADAM为什么用得好啊因为它就是用了它有个scanning你可以把scanning看成是一个preconditional diagonal那么既然我们找到了规矩我们就能够找到更好的preconditional用更好的scanning而且这个scanning不需要那么dense那么一个block而且每个block之间的东西都非常均匀的它只是block block之间不同那么我们不需要把block中间的每一个东西都要scanning我们只要把那整个block矩阵加一个scanning就可以了就这么加上我们建立了自己叫ADAM mini已经可以降低了三分之一的时间也就是说原来需要三个月的一个月现在够了这个已经被其他的individually verified我们自己文章也挂出去了你可以试试save memory by 45-50%降了一半为什么原来要乘两个大向量的现在是需要乘一个就可以了另一个只需要成立几个数就可以了and achieve 50% higher throughput让ADAM现在大家都在用的W在喇嘛的那个7B的那个pretraining上我想表达什么意思优化是可以为AI起作用的而且AI也需要依赖于优化特别是一种比较efficient的优化那么优化还用到哪里呀航空特别是载能飞船的回收技术在最后一分钟的时间要是能够安全的回答原来一样mask的star X总是失败后来成功了用了什么技术啊用了优化就是数学规划的技术特别和Stanford这边是有一定关系的我们都碰到做了一个远应结合的一个求解器是解一个二阶规划的问题不单的调节速度不单的调节而且是实时的优化是要微秒集中间要做去决策的这个时候你不可能去问大模型的你说我现在要把角度调到多少啊来不及的模型就已经产生了人工对话的这个控制系统你想想以后要打仗了指挥官还问问大模型我现在应该把这个问题A部队布置到哪里我的那个无人机现在要飞到哪里全部都要自动化的都要通过数学模型进行调配非常高兴听说在国内三数科技也把它用在国内这个非常长这个葛老师到时候会给大家讲一讲还可以用什么东西啊电网的调度我不知道大家知不知道电网是怎么平衡的经济中最大的一个规律或者物理中的最大规律是什么你发的电和需要的电要平衡怎么达到平衡呢我要通过调度开哪些 关哪些风电那边送多少火电送多少水电送多少闸门开多大全部都要调配的多少的变量啊上亿的变量达到那个平衡微网 各种各样的东西同样的你说完全靠大模型行吗你问一答明天上海气温达到39度你觉得我们要哪台机器开哪台机器关它可能会给你个粗略的但是达不到实时的控制平衡非常高兴南方电网现在用上了优化球解器而这些优化球解器呀原来在各个行业中已经在欧美已经是广泛使用的OK比如说电网我们国家电网现在用的都是国外的球解器不是说不用确实用排产华为原来就是用的IBM的球解器或者调配系统这个各种各样的系统吧我不知道轨迹控制刚才我提到了生产计划化工生产计划大家用的最多的软件叫什么叫AspenAspen中间的核心是什么求解一个非线性的优化问题大家还知道我们芯片比较落后大家只觉得我们工艺落后其实我们在设计上也落后而设计中的关键是什么优化球解各种各样的几何尺寸各种各样的参数包括路径怎么走电板怎么排都需要球解器不是不用不是我们不知道我们原来完全的依赖国外的但现在问题来了2019华为的球解器不让用了整个就熄火了这时候找到了我们因为当时我跟华为的几位高中人都还挺熟的郭平啊包括任正非啊他们经常去斯坦福其实他们很崇洋的经常去我们那里特别是当时跟思科打官司我不知道大家听说过这个事情当时因为所以经常去结果他知道我搞现行规划就找到我们能不能赶快把这个球解器顶上去花了半年时间果然我们搞出了自己的球解器而且帮助华为度过了难关当时发现不光顶上去了而且用的比原来还好为什么我们给它给的是原程序IBM给你的都是一个黑箱你要适应它的水土而不是让球解器适应你生产的水土或者生态环境这个之间时间差别就非常非常大了光数据传输啊原来是要通过文件传输的现在通过代码就可以直接进来了这也是为什么在2019年我们推出了当然有各种各样的应用这个跟老师到时候自己讲一讲还有包括陈希老师啊苏老师啊等等都会讲所以我跟大家总结一下数学建模或者数学优化的计算和大模型它能够起的应用的之间的一些区别我们认为是基于科学和逻辑而那边更多的是基于知识和经验我们是要有一套物理或者经济或者数学的规律原理的你说自由落体是吧那个二分之一几什么平方的这个是牛顿观察了好久发现出来的那已经形成一个定理了那个物理规律必须征询的OK但这边的更多是观察和行为应该说这个还是比较客观的那个是比较主观的现在实际上很多主观的最后是human feedback是要靠能耐决定哪个好哪个坏的我们是比较确定的那个是基于概念的刚才几位老师下面还会讲因为我们是依照物理和经济规律的它的得出来结果是可以解释的而那边多半是黑箱的我们可以用到在线决策那个多半是离线训练几个月我们需要高精度超速度那边可以慢节奏低精度也可以是吧不是人命关天的事情谁好谁坏没有谁好谁坏我的观点是互补要结合有些也确实不是有些本身就不是善事的东西确实需要人的经验去建模的但是一旦有了一定的有了规律的话我们就尽量用数学和物理这是人家几千年几千年总结出的一些规律的东西了是放置四海而皆准的东西你尽量用还是要用然后你再去重新训练重新找这里我就结果斯坦福最近做了很多工作就是在这两个方面怎么结合这个郭老师会进一步介绍的我就不详细讲了特别最近有几篇文章在怎么用数学因为我们后来在实践中发现有很多问题他对数学模型不是很懂这些不懂的东西正好我们可以通过大模型帮助帮你一个实际问题怎么建立成数学模型的问题大家也知道我全国有多少中学生参加数学建模比赛这些建模比赛说不定以后都可以通过大模型把它代替了所以这点我不是说不重要是非常必要的所以就是怎么结合起来这里比如说刚才的背包问题我们演示一下吧现在三数做了一个通过大模型建模的对比PPT中简单的装箱问题使用ChatGPT3.5得出的答案并不理想现在我们向小欧抛出一个更为复杂的同类型问题这里小欧智能助手正在给出详细的模型建议不仅如此小欧还输出了直接可运行的代码使用配套COPT球解器进行高效球解通过这样一个简单的例子可以看出当与GPT面对同样的情况小欧不仅可以写对模型还能够调用更加专业化的软件处理约束限制更多规模更大的复杂问题大家想想一个生产线几万个生产线几百万个物件怎么在上面拍怎么去装这个不是靠人可以达成的也不是大模型可以凭经验制它本身是有数学规律的我可以用线性规划刚才要把它球解出来球解器我们追求的是什么准确速度大家也想想南方电网要排产或者排生产线有的地方是一周排一次为什么球解器跟不上来我们要是能够把一周的时间放在一天的时间就把它排成完了我每天都可以重排我能够处理那些不确定状况的前景效率是不是大得提高了因为计划永远赶不上变化你只有靠计划的快速性来比较环境的动态性和不确定性所以我们在球解器上一直追求速度OK那么这个球解器实际上刚才说了39年人家发明了线性规划79年芝加哥大学首次提出了球解器就变成一个商业软件或者通用软件来帮你球解这大家上课都知道如果大家上线性规划可能都用过Lingo然后83年爱登堡808年2011年我们国内有了开源的数学球解器就是把公式啊什么放进去拔上9让它自动算出来19年从此我们有了自己的商业这是国内第一次有了自己的商业球解器为什么2019年呢就是因为我们帮助华为解决了问题以后我们觉得国内有这个需求就在下半年自己推出了自己的商业球解器这个是这里面的规划球解器的各种各样的东西只是说非常sad的这个方面呢跟芯片一样95%原来都是控制在欧美的那么经过这么几年的放大国内的球解器到底是个什么水准呢我跟大家讲一讲这是一个有个创因为这是一个客观的计算问题美国建立了上千个数据库每个月各种各样的球解器在那里进行一个测试解的速度是多少比如说Copt是我们国产的球解器一是什么意思你可以理解这些问题平均解球解的时间是一秒钟那么第二个最好的是什么1.29什么意思啊它比第一位的慢30%听懂了没有所以这都是一个答榜表我们数学问题球解问题有各种各样不同的问题有的是线性规划的有的是Facebook的问题各种各样的东西这里面还有很多但是我必须说了有一类问题现在国内的球解器是排名第二位是1.44第一位的还是Globe是美国的一个球解器但是用的很多但是现在也是不断地在进行追赶在其他一些问题上现在国内的球解器几乎都是世界第一混合忍受这个方面也是第二几乎是要么第一要么第二用他们的说呢就是拿了十几个金牌还有好多银牌那么现实中的问题负担性还在迅速增长有些问题我们现在还解决不了目前的计算体系还解决不了你比如说这是一个典型的欧洲物流问题十几年提出来的我们一直解不动它太大了还有谷歌的page rank问题我们现在你比如说这个问题解了59400秒才解出来而且是最近解出来的其他这些问题都解不了这个是蓝网的就是我刚才说电力调度的优化问题这个需要1951秒什么意思需要20分钟才能够把它调度出来要是我们能够把速度舒展到一分钟以内那我们可以重新调度因为情况总在不断地变化需求总在不断地变化我们现在做了一个什么尝试啊既然大模型的训练可以用GPU我们优化是不是也可以用GPU啊把我们的优化在GPU上实现这个在国外认为是不可能的国外认为是不可能的因为优化的计算需要高性能高准确性而我们的GPU通常那个floating point都比较低整个16位啊或者什么32位就已经到底了但是我们提出了一些远应结合的思想非常高兴大家59400秒的这个问题我们现在916秒就可以解出来所有这些原来解不下来的问题都可以解出来难完的问题需要千秒的百秒就可以解出来我们还在不断地耕耘不断地改进这些都是实打实的可以节省经济节省效率的去理一些环境的所以OK大家刚才说的是依靠GPU那么不依靠硬件的发展软件还有什么算法还有没有新的突破呢刚才那个结果啊确实是我们是10月份去年10月份公布的迅速地引起了世界的一些瞩目2030年英伟达GTC他们也开发了自己依赖于GPU的线性球件我们的工作是和一个芝加哥大学当前在MIT跟卢教授一起合作的我们都是开源地放到网上的结果有的学生看了一下他们的那个基本上和我们是一样的所以这在我来说是第一次黄老板要学我们的东西他这个人我在斯坦福尔我的那个building就是接生黄娟的五年前还不是怎么样现在很伟大不得了但是他们现在也像借这个GPU的那个仕途尽量用一些软件开发在上面做一些延伸的应用这是他们讲的这个也报道了这个斯坦福尔当时也报道了那么现在我们不光是在在现行规划上在其他方法有些算力是提高了60倍百倍甚至提高了万倍我跟大家对我跟大家举个例子就是刚才那个欧洲的最大物流问题3000万个变量2000万个约束变量很多很多这个问题20那个提出来的时候提出来先改不了后来到了2009年的时候终于解出来的139天花了4个多月就是我刚才说的10年以后通过远应的结合16个小时可以解出来在我们刚刚发布的那个软件上只用了27分钟就解出来了我刚才讲的每一个时间的提高就意味着经济的节省能源的节省环境的改变如果放在大的范围上不光是经济效益的话社会效益也是很大的我再跟大家讲一个SDP的问题这个SDP从来认为不可能解到上万维的原来解一个千维的时候需要5分钟我们后来开发了一个软件可以解到两千维这是我的一个学生在Argon Lab解的用了4分钟那么如果你要解到5000以上的非常非常难8分钟30到50个小时解一个解一个一千万乘以一千万维数的矩阵那么现在我们需要动作早我们是需要两分两秒钟就可以解出来这是对我来说是amazing我没有想到这个半点规划是我也属于开创者之一发明这个模型的之一我从来没有想到半点规划可以解到这个规划我认为在我的lifetime中间解不到这么高位的现在解出来了而这个半点规划实际上在很多量子计算的包括很多很多这种物理计算的中间都是非常有用的特别是一些高性能的规划那么大家可能说这都是依赖硬件的改进而提速的纯优化纯理论纯算法的通过人的智慧把算法改进的而不应该的而不依赖于硬件它的中用在哪里它的作用在多少啊我现在就用他们参数科技的从第一版到现在的第七版是不是啊每年的速度从第一版到第七版现在已经提高了速度提高了3.5倍原来需要3.5个小时的现在一个小时就可以了这是完全不依赖于硬件的我们是同样的版本的算法放在同一个机上进行限制我讲这个是讲什么硬件不够软件是可以补的通过人的智慧通过算法的改进和设计人家不给你芯片不给你H100不给你A100你可以在算法上去就像我刚才说的那个大模型我们就轻轻地用很传统的优化方法改了一下速度减少了三分之一不依赖于硬件的完全是愚人的智慧所以在这个SDP推出两年速度已经提高到两倍OK啊我知道大家知道在芯片设计中有个什么MORE LAW每次都可以降低一半的我是希望全世界在优化算法的投入上能够继续使得我们优化算法不赖于硬件的改进每年也可以提高一倍那些就好了我们就可以解决很多问题了总的来说吧对我来说数学优化对我来说是非常重要的我很热爱这个学术科目所以我做一说线性规划万岁它是上世纪三十年代提出来的直到今天还用在各种各样的东西而且用在非常重要的东西像我们斯坦福专门有一个叫钢移植配对的钢移植怎么配对呀线性规划现在都用这个线性规划来进行配对了所以不光是经济效应不光是那个图片看得精美一些不过是看得更生动一些它确实影响人的生命影响人的生活所以最后谢谢大家",
    "谢谢感谢叶教授的精彩分享为我们展示了在球解处理复杂数学规划问题上的最新技术突破为该领域的研究和应用提供了宝贵的思路放眼全球COPD球解器团队已经基于新的GPU架构开展了一系列开发与建设工作并将球解器与大模型决策平台结合形成了一套全新的运筹学教学与工具应用系统接下来让我们用热烈的掌声欢迎上海交通大学智能计算研究院院长山树科技联合创始人首席科学家葛东东先生为我们带来主题演讲智能计算与大模型决策理论与实践",
    "掌声有请葛院长好 谢谢大家这个非常荣幸啊有这个机会来跟大家分享我这个其实就跟叶老师这个等于是接着讲接着往下讲就是说我们这考虑的这个就是我们主要考虑的就两件事情因为刚才叶老师也提到实际上我们做这个人工智能嘛人工智能最重要的一件事情就是做到现在从十年前大模型兴起到现在嘛最重要一件事情落地场景在哪里对吧落地场景在哪里就十年前就是有这个大数据AI这些东西第一次兴起的时候大家考虑的一个问题就是说它能解哪一些问题到底在现实中有什么用处是吧十年过去了嘛就是说大家看到的可能用的最多的还是比如说像视觉识别啊就这些是吧就相对来说就是不是特别多的行业把这个用上去了现在像大模型兴起以后也有同样的问题是吧今天早上我还看到说很多大模型公司这个下一轮又融不到钱了嘛是吧原因就是没有一个好的适用场景但刚才也说到叶老师也介绍了就是在现实中其实我们有很多行业呢它是需要大量的这种基于数据的去进行这种复杂决策的能力的对吧那这些行业呢就是说很难程度上就是说我们就是直接的去用一些最直接的比方去做一个回归啊做一个预测啊这些事情呢它不能马上直接解决问题是吧原因就是这些问题过于复杂对吧它是一个现实中的问题叶老师举的比方说这种物流问题啊华为的生产问题啊就这些问题为什么就是说AI帮不上忙呢或者说能帮上忙的幅度很小一个很大的原因就是说这些问题它很多的系统很复杂对吧就你把数据分析完以后它变成一个各种约束互相制约比方说有几千万条约束互相制约的一个问题了本质上就是这些问题呢就是说你去做任何一个决策的时候单纯的去靠比方说AI方法去做一个预测是不够的就是预测这些东西呢往往会破坏这个比方说这些约束性破坏这个问题的feasibility所以这种情况啊就是说你还是要就是说多种方法结合嘛就是特别是就是说我们有很多这种优化的方法怎么跟AI的方法去结合包括这个刚才提到说算法的结合另外就是比方说优化这些年就是说芯片这个像GPU这个算力这个发展也很快你怎么能够优化这些方法怎么能用上这些硬件真正把这些硬件用起来这是我们最近在考虑的一些问题嘛然后就大模型因为就刚才说的本身大模型目前这个处理这种非常复杂的这种决策问题它能帮上多大忙这也是我们在思考一些问题所以在我们今天我主要就是汇报这两件事情嘛就是说这是我本人就汇报两件事情就是说一件就是说就当我们解决一个问题首先我们要建模对吧就是建模这个事情就是我待会会谈到就是说我大模型能不能帮助我去建模另外一件事情就是计算当我建完模型以后这个复杂的系统问题我怎么去求解它就这两件事情我们今天就简单讲一下我们就是交代这个我们这个智能计算研究院在做的就是说我们本身呢就是说这也提到是我们在上海交通大学有一个叫智能计算的研究院我们这个研究院主要就是解决这两件事情的这我们就是刚才叶老师也讲到就是说我们开发过一个或者正在开发一个叫做COPT的一个求解器这个求解器目前的从水准上讲嘛基本上你想线性目前是第一此前一直是美国第一嘛然后非线性是第二就是这个还是比较成熟的一个相对来说已经比较成熟的软件了但这个软件它有个什么问题呢就是就是说所有的计算架构在其实一直在去年之前吧去年之前二三年底之前所有的这个这个叫做数学规划数学优化我们做这个事情这个叫三数科技来开发的开发了这么七年七年中无数投资人问我一个问题说你这个东西为啥不能用GPU为什么不能用GPU为什么就在CPU上跑而且这种计算软件传统上不光是这个数学规划有很多高精度的计算的这个软件像流体力学啊就里头有很多问题呢如果你不用AI的话就是说有这些软件呢专用的一些计算软件你会发现它都是用CPU架构的都是用CPU架构的就是说这种高精度高复杂度的这种科学计算呢很多时候呢就是说GPU帮不上忙就是GPU帮不上忙帮不上忙的原因就是我这也列了就CPU和GPU之间这个天然它们物理性质带来它们能力上的一些差别这大家想必下面听众很多也比较清楚了所以就是我这列了几点我是说这个我当时跟这些几百号投资人我讲同样的话我讲了很多遍就是说GPU这个事呢对这种像数学优化算法这些比较复杂的数学运算呢它不友好都不友好我这写了比方说我们最经常用到的一阶算法二阶算法和这个整数规划属于离散优化是吧这几个范畴它都不友好是吧当然大家说那不是用的都是一阶算法吗现实中是吧现实中是用了一阶算法但大多数这个机器学习的问题嘛它本身就是一个预测或者说它本身一个近似的在这种情况下你只要找一个近似级而且对那个最后的收敛精度要求不高的情况下那一阶算法是很好的是吧但在我刚才讲到我们做很多高科技就科学运算我们做的是科学运算你比如说线性规划刚才也是讲的是吧最后我们的收敛标准要达到负6一般要达到10的负6负7才算收敛甚至要达到10的负9次方是吧那GPU呢它物理特性决定了它本身它计算精度只能达到10的负4次方所以就无论如何它在这个事上是做不好的以前我们一直是这么想的是吧一直是这么想的但是去年底以后呢就是刚才叶老师也提到了就是说它出现了一些变化是吧就是说这是就是说它本来是做不好的是吧现在我们发现说这个去年底的时候做了就是说第一个这个世界上的第一个就是用GPU架构去做这个线性规划的计算的然后就以前解不出来的问题解出来了然后以前几万秒的变成几百秒能解了是吧刚才叶老师也提到说一个五百秒五万九千秒了吧是吧最后变成了916秒把它算出来了对所以就是说这个就是说就CPU加GPU这个议构体系下我们去做这个事情是吧然后刚才叶老师也提到就是说有另外两类问题一类是这个半征定规划是吧就原来是解了几万是吧现在能解了1.4亿乘1.4亿了是吧这个是非常一个结果嘛是吧然后还有QCP就是用二次图规划二次图规划你用这个CPU解的话就是说这我们有一个比较是吧就是说你看这个当问题比较大了十万维的时候就基本上就time limit就是7200秒两个小时它算不出来了它用我们传统的求解器然后到了100万维的时候它直接就内存溢出了是吧那这些问题呢你现在发现说在GPU上去跑的话我们就1.7秒还是8秒钟就能跑出来了是吧所以都是非常惊人的一些进展是吧这些进展呢其实就是说它发生在这个就发生在最近嘛是吧就发生在最近10月份就是说我们第一件就刚才提到第一个PDRP嘛这个有一个一阶算法是吧用这个用CUDA来和这个GPU和CUDA来实现是吧最开始是我们那个芝加哥大学一个姓鲁的是吧罗海浩老师做出来的是吧就是他是当然罗老师做了以后呢他因为他不太擅长工程实现所以就找了我们把这个就是用专业软件把它重写了是吧写了以后发现这个效果是非常好的是吧拜托于罗老师也是我们以前的学生是吧也是我们教大的学生是吧对然后就这些东西呢就是大家还是比较关注的不管英伟达还是这个斯坦福都做了一些报导是吧所以我想说你看到就是这个事情去年十一月份开始到现在半年功夫是吧发生了非常多的进展是吧这些进展就是爆发式的增长所以这个爆发式的增长就意味着什么事情就是说你会发现说这个这种高精度高性能的计算呢它正在发生一个科学范式的转变从一个CPU为主的为主要的这个算力基础一定会发生的事情对高精度高性能计算来说是吧这样我也写了英伟达做了很多事情是吧英伟达他发布了这个QOPT这个就是贝斯在我们那个秋节期上刚才写了这个上写的时候然后英伟达还做了两件事情一个是他发布了一个酷函数叫QDAS这QDAS很重要为什么很重要呢就QDAS是能够实现矩阵分解就是我刚才讲到就是所有的我们这个数学优化里面有两大类算法是用的最多的一类是只用T度信息的是吧在机器学习里最常见的算法第二类就是说我们用的比较多的就二阶算法它要用到矩阵的这个问题往往会用到矩阵的问题的比方说那个海森矩阵海星metrics二阶导数是吧所以呢这些呢就是最后往往要要对应牛顿法内点法这些都要解一个现行系统那往往对应的要解 做一个矩阵求逆矩阵求逆这个事情呢是所有二阶算法的base但这个事情呢GPU上始终实现不了高速并行十几年前我跟叶老师最开始我们就在当时最第一个想法要做求解期的时候我们就第一个当时第一个想法其实就想到说为什么不用GPU对吧实现一个弯道超车所以我们当时就跟英伟达联系过十三年前我记得是还是十二年前但是英伟达就跟我们说说我们也不知道这个事情怎么办就是GPU怎么在矩阵求逆这个事情怎么在或者矩阵分解这个事情怎么在这个GPU上实现一个高速并行对所以我们这个事情其实十来年前现在我们发现英伟达就是今年四月份他终于做了叫你做了一个叫Kudas的函数能够实现矩阵分解我们现在去测了以后发现说他这个性能不是特别好能做一些问题在一些问题上他跑得比明显比CPU快但是还有很多问题跑得不如CPU包括他这个性能不稳定就每次输出的结果不是比较随机就每次分解的结果比较随机然后他也没有实现这种多卡并联只能在一个单卡上实现所以目前就英伟达也在找我们讨论说这个事情接着怎么做也希望我们帮助他们一起合作继续做这个事情就是所以就这个事情也是英伟达非常关注当然大家也知道就是他发布了这个像B200这种新的架构就是我刚才说我们未来这个计算趋势的发展是从CPU向这个CPU加GPU这个混合架构发展这个混合架构并不是一个固定的而是这个CPU和GPU因为这个东西它以后会怎么发展你也不知道从A100到B200我看到就是说CPU和GPU这个数据传输速度好像从15GB变成了450GB一秒所以它提升了30倍这就意味着以前我们发现我们当时跟卢海浩去做这个第一个现行规划的时候发现最大的一个问题就是CPU和GPU之间交换占了86%的运行时间所以假如你这个东西假如快了30倍那这个Bottleneck就不存在了其实我们当时就不用大费周折去写很多特殊的算法了那接下来就是还有一个就是大家提的不管是我们这儿还是英伟达上海市也好称说在做一个弯道超车的硬件就是我听下来可能跟英伟达的一些想法比较像就是我让这个CPU和GPU呢共享一个计算显存谁用谁去取这样就完全不存在一个数据传输的这个Bottleneck这个设计不存在了所以在将来如果发生这种事情你现在设计的算法到时候又要重新去重新设计所以就是说这是一个动态的过程不停在演进不停在你要跟着这个硬件走去设计更高效的这个做法的所以这是很有意思的一个问题很有意思的一个问题目前这块就是关注的人不是特别多就是我们跟芝加哥还有MIT还有英伟达几家在这儿做目前我们做的东西应该是最多的应该是最多的但现在就是有一个问题就是说所有的东西都是在英伟达的芯片上实现的没有在国产芯片上做对吧就是依存在英伟达那个库达架构所以这个问题就是说有两件事情机遇有了机遇就是说它正在发生这个范式转变而这个东西呢对应的就是真正的是有很多实际场景叶老师刚才讲了几个场景国防这个航空航天交通物流能源电网供应链智能制造包括工业软件设计就是我们做过200大概做过280多家企业中国所有的头部企业我们基本上都打过交道都做过生意很多然后从这里头讲我们这个东西呢其实是用户范围是非常非常广的不是一个简单的视觉识别那么狭窄的但是就是说就是说这个东西呢就是它非常依赖这个计算然后这个计算我说了它发生一个范式转变这个是必须把握住的一个机会但同时呢我们比较受制于硬件硬件呢就是说它有一些特殊性你比如说它一定要FP64双精度那国内现在连华为都只有单精度国内目前只有一家公司有自主的产权的这个双精度芯片然后呢就是说性能比较差相当于A6000的70%然后真正跑起来呢酷寒数你比如说比较成熟是矩阵计算对吧是很成熟的然后国产的这个他们对应的酷寒数呢我们测了两家这个具有双精度能力的芯片自主知识产权那家它那个酷寒数大概比英伟达同样的事卖了18倍另外一家买的是那个AMD技术的它卖了160倍所以就是说这种本来应该很成熟的酷寒数国内的这些国内这些GPU厂商都做得还是很差的然后像这种英伟达做的第一个矩阵分解这种酷寒数呢国内是完全没有的也完全没有这个能力的我们做的我们团队正在做也做了一个但是还是比酷寒数卖很多我们正在尝试把它加速这是我们最近的很重要的一个工作所以就是说这个我们我在交大嘛交大这边我们跟计算机这边有很多老师做像Alex良有很多很有名的专家做计算机体系架构的大家跟我讨论大家一个共同的观点就是说其实我们在GPU这个硬件上的差距没有大家想象的那么大就基本上是绝大部分是够用的其实最大的一个问题就是这个酷寒生态跟中国的这个生态问题就说白了就是说这块的像数字底层的这些数字函数计算这些算子完全是非常空白的一个状态这个差距是更大的就软件就是底层的这个酷寒数建设这是其实最致命的一个差距反而不是硬件反而不是硬件这是我们交大老师们的一致共识所以从这点上讲这些事情是非常需要去做的这是我们智能计算研究院上海市分配给我们的一个任务就是说主要是承担去建设这些底层的最基本的这些数字计算的算子对所以这就是说我们就是说这个东西就是我们目前要去解决的一个问题因为我们这个计算院是今年今年年初刚成立的就是也是借这个机会去汇报一下我们做些什么事情我们主要就是说在目前这个计算范式发生这个一个大的变化的情况下那我们是不是应该去做这一整套新的理论该怎么去设计这是我们主要的一个使命也是希望说市里头也是说国家也是希望说我们去把这个这方面的这个软件做好这样的话我们在硬件因为未来很长一段时间我们肯定只能依赖国产的GPU硬件那我们怎么在依赖国产硬件的基础上来弥补这个它硬件这个短板是吧就是说能够让这个软硬一体化以后别比人家英伟达那套差太远是吧不能指望说做的像人家那么好吧是吧就不要被人家拉下先咬住是吧等待这个硬件这个差距上来是吧对再我就简单说一下刚才你像叶老师说的就是那几个case下就是它那个加速效果怎么实现的我这儿列了一些比方说这是我们develop的几个你像这个是线性这是S2这是这个半征定是吧每一个它都采用了一些新的这个一些方法或者说以前CPU的一些方法我们把它重构了一遍是吧做了很多理论上的工作是吧所以它不是一个简单的这个测算的工作而是要重新设计很多里头比较复杂的一些数学步骤是吧这反正都有这几个关键词大家可以上网搜我们都写了paper也都放到iCloud在CAP上开源了给大家参考是吧给大家参考对 这儿也简单写了一些比方说一阶算法正常情况下解一个大的计算问题大概是这么一个流程是吧然后我们现在发现说在我们新的这套体系上它有要加很多新东西是吧要加很多新东西这些东西就是解决比如说它有几个大问题一个是一阶算法它那个假如你要去做一个混合架构的时候它们之间的协调机制CPU和GPU是吧有些工作CPU做有些工作GPU做是吧它们之间有延迟是吧有这个通讯问题是吧有延迟问题是吧就这些异步问题这些你怎么去构建是吧第二点就是比方说它那个一阶算法的很大一个问题就是数值复杂的问题它做不好是吧就是数值困难的问题呢它往往就不收敛了所以这种情况下你对这些数值困难的问题你怎么去处理它是吧这是另外一个对一阶算法必须要克服的所以你这个时候问题进来以后你进行这种欲求解是吧中间去做这个收敛这个分析步骤这些事情都需要重构是吧跟那个CPU实在是完全不一样的一套体系了对对包括整数规划我刚才叶老师也提到其实就现实中大家遇到了就真实的我刚才说200多家企业其中这些你会发现80%的问题是来自于整数规划的这是非常困难的一类问题是吧这类问题就是说你需要去想很多办法对这类问题来说AI怎么帮助它是吧其实有三个途径是吧第一个就是说用GPU来加速是吧刚才我也讲了就是它整数规划天生这个对GPU不友好所以GPU加速它不太好加速但是它也有一些方法你比如说整数规划每一步要先解一个线性规划就作为子程序反复调用是吧那你这个线性规划你又可以用GPU来解是吧但是GPU解完以后呢它那个解呢就GPU你用一阶算法解出来那个线性规划最后解了那个解了它虽然是最后但是其他性质很不好这种情况你怎么把它变成一个好的性质去解以便于它产生一个整数解这是我们目前在政界研究的一个课题是吧我们跟MIT那边应该跟刚才提到卢老师在合作也跟MIT那边一个博士生在合作那博士生也是我们以前的学生是吧就所以中国人在干这些事的人还是挺多的是吧对 而且跟我们好像都有些千丝万缕的关系是吧对然后就第二个方法就是有很多子步骤呢就是你可以通过一些这种AI的方法来加速尤其是现在很多问题呢就像电网刚才叶老师提到的电网问题它有很多历史数据那你怎么就是通过对这些历史数据的分析找到一些比方说它那个算法中的某些子步骤对这些镜像加速这是另外一个方法第三个办法就是你在CPU这上你能不能设计一个好的分布式对吧CPU这个分布式呢以前只有德国做过德国有个ZIP研究院他们做过他们把那个八万核的CPU上做一个加速能够解一些巨大的问题历史上解不出来的问题我们现在在跟那个德国那个研究院叫ZIP正在合作就是考虑说它那个解放我们研究里头有很多步骤呢其实也一些调度啊什么这些方法其实也可以通过AI来加速他们原来做了很多初操所以我们现在跟他们有一个项目做了有一年多了在帮他们说吧这个东西也重用AI方法重新写一遍所以就是用AI分布式和这个GPU硬件三个途径我们都在尝试对所以这就是我们主要做的一些工作当然我举了一些例子就是举你比如说这种很大的一个公联网络这个网络很复杂它是一个飞突结构的然后这个网络之间那种拓扑关系就是这些生产的比方一些原材料到部件到整机就是这中间有一个有很多拓扑关系一些原材料比如很多工厂很多部件共享就是这个关系是有上百万条编的那这类问题就是说你比如说疫情包括现在这个战争它会带来一个很明显的问题就是你你在这个大规模的制造网络或者公联网络你要找到一些关键点疫情的时候我们把这个东西叫做读点就是block了整个公联生产效率这些关键点然后你怎么去设置这个安全库存对吧怎么去提升整个公联的效率这些问题其实不太好做的以前就是就是解个两千个节点的问题都要一千两百个小时现在你像国内的话你像疫情是我们分析整个整个这个上海周边的这些网络大概就要一百万条编所以怎么把这些效率给它比方说几分钟内把它解出来这是我们目前我只是举一个例子举一个例子像叶老师刚才举了航天电网这些问题对吧对就是这是我们就是说要做的这些事情我这也列了就是中心要做的这些主要还是说我们希望说在做这套新的基础计算架构体系的时候我们是一个国际的引领者就是我们去定义这些标准定义这些架构怎么去做对这是第一件事情计算第二件事情是建模对吧就是做这个刚才也讲了我们有很多复杂的问题你刚才想电网这种电网它有两百万条约束华为的生产问题有六千万条约束我们做过俄罗斯铁路那个有一亿多条约束这些约束就是你看华为的生产约束它有六十类对应六千万条那这些约束就是说所以这个问题它很复杂就从一开始你看电网的问题就是华为的问题我们最开始做了两年才把这个问题从车间里提取出来提取出来所以就是这里头就是说当你用大模型的时候大模型做这一类问题就是特别复杂的一个建模流程其实它做得不太好到目前为止要是刚才举那个3.5时代GPT3.5时代它做一个很简单背包问题它都做不太清楚当然这里头我也列了就是大家也知道就是对这种这种这种通用大模型对应这种比较针对性的行业性的或者锤类的这些模型训练的时候它会带来一大堆这个长尾包括这个幻觉这些问题大家也知道是非常常见的一些问题我也就不仔细去解释了就是说所以导致它很难做到一个很好的这个结果到现在就是你用GPT4还是用4o1你会发现说它还是做不好你比如说我这也还是举了一个很简单的问题这里头就是说最后你发现说你用4去建模的时候其实4已经是我们我们测过130多个大模型测了130多个大模型签了GPT4包括还有几个像那个换方的DeepSeq那几个就是能力最强的大概有那么GPT4也在前三名里头了但即使这样的话你一定给它一个稍微麻烦的问题但还是说不清楚你看这儿就是说像这种生产排斥约束就是本来我们可以写很简单线性约束把它排除掉但是它会去引入一些非线性的像这就导致说这个模型这个问题就一下子就变成很难解了本来是一个比较简单的线性规划它建对了但是它建对了建模是对了但是解不动了本来是很简单能解了它解不动了再有就是它比方说它可能把一些文字理解偏差以后它还是会把一些模型写错的所以就你发现它还是不太够的所以就是说这个东西就是说使用怎么用大模型来对这种优化这个决策这些问题进行建模这是我们一直在孜孜不倦研究的一个事因为我们不光要求解我们还得建模对吧所以这儿就是有几个我举了几个例子你比方说这个有个NRFOPT这是一个尼泊斯办的比赛就是说基于自然语言的这种建模竞赛再比如说这个最早的时候它有一些你像去年那个iClear上有一篇文章这是华为写的华为跟浙大一些人写的就是说一个用这个就是它这个在GPT下面去做train搞了一个建模的这主要就是用这个一个链式建模的思维使用这个GPT软件使用API让大模型去建模和写这个代码是吧然后再有就是一个比方说斯坦福也搞了一个去年底的时候斯坦福也搞了一个叫这个OptimusOptimus就是说这个模型呢就是说它就是说它的做法是它用多角色分工就是Multiagent这种是吧然后把这个几个几个agent就是分别有这个主管建模手是吧编码手和debug是吧就几个角色把这个任务拆解是吧然后用API让大模型来写就是说这个作者里面你看这三个作者这有一个高文字这也是我们以前的学生是吧我以前在财大的是一个学生后来到斯坦福去读博士了跟他导师一起做的是吧所以我们都一直在这里头干各种事情是吧这是微软做的微软做的就是它就是还是用这个GPT是吧然后它就是说它主要像刚才提到有很多供应链的问题它去分析供应链这些问题用这个也是一个agent的模型是吧去分析这供应链就是输入以后它告诉你说该怎么去做相应的这个模型调整是吧对然后再有就是我们做的我们最近做了一个叫ORRM这么一个大模型是吧这是我们的一个工作我们山树跟交大还有港中深是吧就是几家还有斯坦福大家联合做的是吧这个就是说我们这个是一个我们也把它开源了是一个开源的可以有什么优点呢它可以私有化部组是吧然后可以采用任何基座是吧任何对然后它走的方法就是说用那个合成数据是吧用Skinning Law的Skinning Law加合成数据是吧让大模型自己建保然后用我们的COPT求解器来求解这些问题是吧对这是一个就是说我们讲的就是运筹学不光是运筹学就是我们刚才说的智能决策它有些痛点就是说我刚才讲的时候我们这个东西它适用性比较广就是所有刚才提了那么多行业它都能用能用的就是为什么能用呢就是说因为它有各种各样的困难实际上有各种各样的困难它都能克服这克服我指的是当你是一个非常有经验的专家在对运筹学这些东西建模这些事情非常了解的时候这些问题可能对你来说都不是问题但是对大模型来说这些都是问题就是这个稀缺性就数据它本身是很稀缺的对吧问题本身难度又很大是吧范围广什么行业都有可能我们做过大概20多个行业就质量数据质量低然后就是说另外一个事情很重要的一个事情就是因为这个大家也知道这个数据基本上都没用完了是吧所以你现在training得它是很少的是吧所以现在大家做法基本上做法都是一些合成数据是吧但是这种问题你很难合成数据因为它问题过于复杂你比如说你让它合成了数据你会发现你给它很复杂的一些数据最后它给你合成新生成数据都是很简单的没有用的甚至合成一些错的所以就是说这里头有很多瓶颈是吧新的这个模型好处就是尽量去克服这些瓶颈是吧然后很快地在各个行业能用上是吧对这就是也列了就是说我们这个框架里头就是说对我们的种子数据我们到现在大概做了289个企业了所以有这点好处是吧我们还是有别的人没有的这个刚才说像京东顺丰滴滴华为就各种各样的企业是吧富士康小米是吧南网就各种各样的数据我们都有是吧我们把它脱明以后就是说在内部做一些训练训练的时候主要用了两个技术一个是in-contactin-contact learning是吧对一个是这个这个中文怎么说是吧一个是叫做上下文是吧学习是吧对另外一个叫叫练试的我也不太记得中文怎么说了是吧反正那个就是说你可以把这个复杂任务进行拆解是吧然后把这个数据给它合成出来一个比较高质量不稳定性比较好的一个数据对再就是说经过这些以后我们做了一些这个实验是吧就是说再比方说就是因为这个NR4OPT这是Nips搞的一个相当于一个比较公开的数据集你在上面测起来比较容易是吧这是这个就我们几个做的方法就是TingoExport这个应该是华为的Optimus这个斯坦福后面是我们几个根据这个问题就是训练模型大小是吧我们做了几个模型你可以看到我们还是明显好一些的是吧明显好一些的就在这些问题上的表现是吧然后这里头也有几类别的你像Industry OR那就是我们现实中我们提了70个就是从我们的业务中找了70个比较复杂的业务问题是吧让它去建模去测试看能不能跑起来是吧就目前当然你看到这个比率也都不高是吧虽然目前我们还是最好的但是说这个事还有很大提升空间是吧就是这个事情这儿也裂了比方我们跟这个GPT去比是吧GPT-4去比是吧这个还是有明显优势的是吧就还是有明显优势特别是这儿也裂了你这些问题就是说不管是简单还是难的是吧对应的就是在比方线性问题这整数规划是吧就这些问题上我们的建模和求解成功率是吧NRP这个比较这个主要是你看这个都是0的这个是因为我们只有一个例子都没解出来是吧所以不代表大家都很差是吧所以这目前这个东西我们还在跟斯坦福合作是吧还在跟斯坦福那边合作继续在弄是吧继续在弄大家有兴趣的话可以跟我们一起讨论是吧对总的来讲就是从这个商业上讲的话这个东西有什么价值吧这我也写了就是说像大量的这种我们像我们这种做这种复杂建模的问题就是说不管是我们作为乙方是吧去服务甲方还有很多比方是甲方的单位就像刚才说的电网 航空就这些公司是吧做这些任务的人呢都是一个所谓的水平比较高的一些这种技术专家是吧技术专家或者叫算法专家那用这些人去做这些事情呢它有很多问题比方说这个我在列了几个从管理学的角度讲嘛我在商学院嘛说我们讨论从管理学角度讲你比如说知识传承与管理是吧一个牛人走了你这个事可能就做不下去了是吧对吧就人工依赖性也非常高是吧就然后就是历史经验你到底怎么传承新来人怎么去学习这些东西对吧所以这些其实都是很麻烦的一个事这也是很多时候为什么Consulting这些公司它必须得去顾成本很高的一个原因是吧但这些东西我们后来发现在大模型里你都可以解决是吧你比方说走了一个人就是新来一个人那你历史上比方你做的所有项目经验大模型它很大一个好处就是它不光是说我把数据拿过来你这个你写了比方说这个为了解决这个问题你写了石板代码这个代码每一版的演进过程它都能看得到大模型都看得到然后你在这个做这个项目你花了八个月跟这个甲方做这个项目这个项目中你所有积累的document每一次的meeting notes什么的这个项目的所有的东西你全部都可以输入大模型它都能学习到它都能学习到是吧然后它去给你分析然后比方说产生一个预测的模型这个预测模型里有哪些特征因子它甚至会告诉你这些特征因子少了这个会造成什么影响它都能干是吧所以我们发现这还是很对我们来说对我们公司来说很神奇的一件事情或者说很省心的一件事情是吧你比如我举一些例子你比如说这个东西这个有一个某个某个这个这个珠宝厂商是吧就是很有名力讲不能说哪一家是吧我们商场公司做了两个月两个商场公司做了两个月给它做到百分之八十五是吧做到百分之八十五就是一上来大概有百分之六十嘛是吧你调了两个月调了百分之八十五在我们把同类的模型就跑了一会儿就跑出来个百分之八十六八十六点七那就意味着我们那些算法公司对吧花大家钱雇的当时做这个事应该还是两个小海龟是吧都名校毕业了发现他们好像都没啥用了是吧所以这个就是当然我不能说因为这个我们就裁员了是吧确实裁员了是吧对因为确实这个事呢确实很你去想这很吓人的是吧比如说这个东西跑了百分之七十五我那个大模型跑了百分之七十二正常情况我是两个月干了活是吧干了百分之七十五是吧大模型的百分之七十二说白了我把前面就八周的事情我前六周都不用干了前六周都不用干了我只需要干两周那就意味着我还是有四分之三的人是不需要了所以从这点上讲我发现这个最先淘汰的是高技术工种是吧高技术工程以前我们说的或者这两年大家很多去做这种数据分析师是吧实实在在的是吧所以这个东西是很吓人的很吓人的是吧这也是非常这就是说我把所有的这个历史就是我们每一个项目你比如说我们做了快销我们做得特别多是吧做了三十家那所有快销的东西我全灌进去以后我们做快销就发现就特别准是吧特别准只要三十家就够了当然我们自己刚才说了我们用了很多合成数据是吧就是合成数据你一定要想一些我们刚才写的那些论文的最大的一个创举非常稀缺的数据是吧所以你要设计一些新的算法所以这就是我们在建模上在大模型上一些探索工作是吧就是建模和球解是吧两年事情是吧我今天基本上就讲到这里是吧就谢谢大家",
    "当然这个动画演示叶老师演示了个片段我就不演示了是吧好谢谢大家Thank you感谢葛院长的精彩分享让我们了解到领先团队为复杂决策过程提供的智能化支持如果说人工智能可以应用于解决各种任务和问题那么生成式人工智能作为其中的一个子集专注于生成新内容和模仿创造力接下来",
    "让我们有请纽约大学斯特恩商学院中生教授计算机科学和数学科学中心教授陈希先生为我们带来主题演讲生成式人工智能机遇和挑战及在量化交易中的应用掌声有请陈教授好谢谢大家",
    "感谢大家这么热的天过来我给大家讲一点希望比较轻松一点的这个题目就是我们讲一下生成式人工智能的我觉得一些挑战和我最近做的一些工作然后后面我给大家讲一下我觉得人工智能在量化交易中的一些应用当然这是我的一个背景然后一会我会讲到我会结合我在亚马逊广告组做首席科学家的一些经历这个会讲到当然在讲之前我给大家做一个小小的自我宣传这两本书第一本书是讲这都是相当于给MBA读的比较科普性质第一本是Beyond AI然后我们讲了一些关于AI在不同商业场景落地下的一些想法这些想法会在今天的讲座中讲这是由Splinger出版的这个书正在翻译成中文的过程中另外一本书就是还没有上市希望今年下半年就能够出版是讲一个区块链web3相关的这个书希望能成为一本web3相关的一个MBA的教材因为我看到这个市面上很多书但是并没有一本商学院是和MBA读的理论与实践结合的一本教材所以我们就和哈佛大学商学院的朱峰教授还有耶鲁大学计算机系的他是做blockchain的专家张帆教授然后一起写了一本书希望大家希望今年下半年能出版当然我不用多说2023年3月14号我觉得是一个值得纪念的日子因为我们出了ChaiGPT-4这个GPT-4在各种性能上都吊打了以前的ChaiGPT并且它实现了多模态的一个问答比如说这边就给一幅图片然后你就问它这幅图片有什么不对的地方它告诉你你说你把一个电脑的接口接到一个iPhone的手机上它就能够对图片进行识别从这个GPT-4诞生之后我们就产生了整个的声称是人工智能的浪潮就开始掀开了包括语言的 文本的多模态的等等等等我们今天就不赘述这个我们想一想我觉得人工智能尤其是声称是人工智能现在还有哪些挑战我先讲一下它的局限和挑战当然对在座的小伙伴来说对在座的小伙伴如果有感兴趣创业的我觉得其中也会有些创业的点可以跟大家分享第一个我就讲一下情感沟通和隐私保护第二点我讲一下幻觉第三点讲一下我最近的工作主要是一个去中心化的验证技术我们就讲一讲当然我们给一些具体的例子比如说大家可以看到我这个比较胖是吧是一个不爱锻炼的人我就经常想问GPT说你能不能劝服我多锻炼锻炼吗GPT说可以你多锻炼有什么好呢可以增强体力控制体重提高心理健康增进免疫系统但我想问大家你看了这样的回答它能让你多锻炼吗不可能啊就是说这些回答你们都无数遍我不希望听到这样说教的这个回答那我们比如说我们就会想一个就是说你可以跟他进行一个更好的一些对话就有一些软件我们尝试了你就问他那为什么阻止你去锻炼呢你就回答你就问GPT可能会问你说那什么东西阻止你去锻炼呢你就说我的健康伙伴都被搬走了然后他就说这个社会支持非常重要你可以加入一个跑步的俱乐部然后你就说没有时间我确实没有时间给自己是很难的然后就说总而言之就说即便是对话式的这样的一些工具它也很难的产生一个情感的沟通那我理想中间的一个GPT可能会是这样的对吧比如说他会说你今天完成目标了没有你说算了我今天已经下班了他说运动呢我就提高你的能量水平帮助你消除疲劳然后你就会回答说不要不要明天再说吧然后这时候GPT如果跟你说那你先去走五分钟怎么样你说这个我可能可以做到所以我觉得下一代的GPT应该更多的是一个基于情感沟通和意图沟通的这样一个东西而不简简单单是一个说教式的一个沟通所以我觉得这块其实是有机会的当然了这话我简单提一下就是说这个GPT通过这样的对话它可能会泄露一些隐私的问题就比如说他问你锻炼了吗你说我经常去一个24小时的健身房然后如果他再问你那他就知道这个健身房是24小时去的并且你可能经常去这里头有一些隐私的问题当然这不是我们今天重点讨论的问题总而言之我的一个第一个想分享就是说我觉得情感沟通可能是未来的这个GPT的一个方面第二个方面我想大家都比较了解就是GPT会非常自信的发明很多东西我们把它自信的发明的东西是不真实的我们把它自信的发明的不真实的东西叫做幻觉这里这个字比较小但是就是说它可以产生一些文献比如说你用GPT写论文然后你让它产生一些文献它给你产生了很多文献然后你在网上一搜这些文献根本就是不存在的就这头是个推特就是说它说一个教授说你知道他2018年在发表上的文章吗他说我说了很多次了他完全是在编造文献听起来很普通很不错但是我可以保证我从来没有写过这些文章这个是一个所以大家如果要用GPT生成帮你进行文章写作一定要小心很容易将来会就是你发表了之后可能大家会发现这个文章你引用的文章压根就不存在那么这个现在有一些方法来解决这个问题其实我想跟大家分享的一个就叫做检索增强生成我不知道大家有多少在座的小伙伴可能在这个大会上也都听说过了它就是把搜索引擎跟这个这个GPT结合起来就比如说我举个例子这是GPT-4你问他What did the president say about justice prayer这是美国的一个大法官他就说我不知道因为我这个最多只能到2022年我的信息只能到2022年的1月我没有办法提供更多的信息了那这个这个搜索这个新一代的这个检索增强生成其实是这样做的一件事情他把这个用户的这个问题转变成了一个这个项链然后呢他有个项链的数据库在项链数据库中进行对比这个项链数据库是随时联网并且更新的他把这个项链数据库中进行对比找到项链数据库中的匹配的这个比较相关的这些网上的信息他把这个网上的信息和你的问题呢一起汇总成一个长的一个文本这个长的文本呢他一起丢到这个提示模板中然后呢让这个提示就当做prompt然后把这个prompt丢给像GPT啊丢给这样的大模型然后大模型呢提示到这个之后呢他就会给你产生一个答案这样呢我给大家举一个例子啊就是说这个是一个NYU的一个在美国挺火的一个NYU的一个就是我们的这个一个这个央乐控的一个学生他做的一家公司叫Perplexity AI他这个就问你同样的问题他说What did the president sayabout justice player他就给了这样的答案并且给了答案之后他给了不断的引用这可能看不出来看不太清楚啊这有123上面就把这个你搜索的这个网页123全部都找到了你可以通过这个引用呢你就看一看这个网页他甚至会帮你标注哪里的这一段话导致了这样的这个文本的产生这样的话呢就可以从一定程度上呢减少这个幻觉的发生啊当然如何解决幻觉啊这我觉得是一个非常重要的问题包括刚刚葛老师讲的在运筹学中间呢这个叶老师讲的如果出现幻觉呢可能会导致很严重的后果对吧那些问题你就不能通过搜索隐形的方式因为它是一个建模的问题你也不可能在网上搜到所以怎么样减少那些问题的幻觉呢我觉得当然是更困难的但是呢这个通过这个这个Perplexity AI呢我觉得我意识到一件事情啊就说我觉得这个东西会带来一个这个数字广告的革命我自己在2021年到2023年呢在亚马逊广告组呢作为首席科学家其实工作了两年啊就是说这个那就说大家看到现在这个谷歌的搜索当然百度也是一样的对吧它这个主要它这个赚钱的方式啊是来自于上面的我们叫Sponsor Search就是说你这个一些商家可以在这贴广告是吧那现在问题你想过没有如果你用了这样的大模型的这种检索的方式是吧那你就没有办法就说我也不看网页了对吧那我就根本就看不到这些Sponsor Search的结果我当然不会点开这些网页那直接在这些网页上投告投放的这个这个广告商啊他也就不能挣到相应的这个钱了所以我觉得这个是对广告商是一个很很严重的一个影响那这个那甚至我在想谷歌为什么在这个就是在这种搜索引擎上这种RAG上做的没有那么全力的去推是吧那他可能就说再往下推可能会自己革了自己的命就说因为他这样如果推的话那么这个东西就说那这样的话这个广告商就不在谷歌上投放了因为我直接看答案就好了对吧所以我就没有这样的机会那相反来说可能未来的广告的主战场就会变到比如说抖音像TikTok还是YouTube像这样的方式可能会变成将来广告上的一个主战场所以我觉得整个这套RAG的模式可能会对整个数字广告包括数字广告投放的方式数字广告的分析会带来一个万亿美元以上的革命那这头肯定也会有很多创业的机会就给大家分享一下好这是讲了一个第一个是情感沟通第二是幻觉第三个我给大家讲一讲是去中心化的人工智能那这个呢就说我们训练大模型这个代价成本是非常非常高的大家可以看到这头OpenAI在GPT-4的训练中就已经花了7800万美元然后Google在Gemini的这个计算生成中花了将近1.91亿美元这是一个海量的一个天文数字那么这个是一幅就说我们说能不能小公司也能完成这样的训练呢这是有希望的比如说这是一个分布式的计算这是由Stanford非常著名的篇文章他就说如何训练大模型在一个分布式的环境中然后他做了一个小的实验有不同的美国欧洲 亚洲的地方把这些计算中心的结合起来做这样一个训练但是我想讲的是即便你能够在小这种在不同的通过分布式的训练的方式来降低成本但这头有一个核心的问题就是说一个像这个就是说一个分布式训练的市场这是一个Carweave是美国一家公司这家公司已经估值10.5亿美元了应该是Fidelity像这个以10.5亿美元以10.5亿美元的估值来支持来买就是来投资这家公司他就是说能够在能够利用这些网络上闲散的算力把这些闲散的算力拼接起来来完成一个大模型的训练如果不能做训练至少可以做微调我们叫Find Tuning这样的工作就是说他可以来做这样一件事情但是如果你用小公司去做训练这头有很多的问题其中一个很严重的问题就是可验证性你怎么知道他真正完成了这样的工作我来举几个可验证性的问题我来举几个可验证性的一个几个实例第一个实例就是说比如说客户说你帮我用一个175别令的参数的大模型来产生图片那这个厂商他接到你这个任务他用了一些闲散的算力来完成这件事情但他实际上可能只用了7别令的大模型来给你做推理就给你产生了一张图片你可能一时半会你也分辨不出来对于没有经验的你也不能分辨出来这张图片是否是由7别令产生还是由1别令产生还是由175别令产生的对于他就可以在这时候造假对吧就是一个造假的问题然后第二个就是数据的可验证性对吧我们刚刚说了这个数据到最后都要做微调甚至做reinforcement learning就是做强化学习到最后比如说这里头哪座山是最高的山是吧然后如果你回答这是一个好问题你就会达到一个difference因为并没有直接回答这个问题但如果你的回答是珠穆朗玛尔峰你就会达到一个高分这有个type你会达到一个高分这样的话你就可能会去形成一个正反馈但是总而言之你这个AI模型比如说你把一堆数据你是一个金融公司或者你一个法律公司你把一堆数据交给了AI模型那AI模型如果擅自篡改了这些数据你可能也很难就是说抓到甚至比如说这个数据中比如说我们说有一些是违法的有些是不违法的然后你把这个数据交给AI然后交给一家公司结果这个公司如果有意的作恶他把这个标签给改了把违法的改成不违法不违法的改成违法的这个是一个而且你很难去验证这件事这是一个问题那第三个是整个训练过程的可验证性这个是我做的一个研究我们想象一个这样的商业场景假设你是一个律所你有很多法律文件你想训练一个大模型帮你提起诉讼写这个诉讼文书这个很正常有很多创创公司都在做这样的研究但是呢你是一个律所你都是一帮律师你并没有这样的AI的专家于是你找到了一个AI模型的训练商你说我给你提供这些数据你训练一个大模型然后呢你把这个大模型返还给我我可能拿一个开源的模型给你希望你在上面进行一些微调然后给我一个新的大模型这时候就有个问题了他训练完大模型他给比如说你给他一个7个别人的参数他返回你7个别人的数然后给你一个API调用的接口他就向你说了说我这个训练成本要100万美元你说这100万美元从哪里来的呢你为什么要收我100万美元呢他给你一个账单说你看我一共用了10万个GPU小时做的训练所以我们按照律师的收费一小时收费多少钱所以我用了10万个GPU小时然后做这个训练所以我就应该收你比如说100万美元这个问题是你如何验证这件事你说你用了10万个GPU小时你就用了10万个GPU小时吗同样的你也没有办法去验证这件事所以但如果大模型分散在各个小厂商中间进行分布式的训练其实呢我们如何建立这种信任的机制这个是非常非常困难的就是我们的数据的提供方和这个服务的使用方和这个模型训练的提供商之间如何建立这种trust建立这种信任机制我觉得这个呢其实是一个将来大模型走向就是一些各个传统领域中间也会面临的一个挑战当然了我这都举一个例子就举一个例子是我一个工作当然我这个工作就给大家简单的讲一讲那其中呢有一种方法呢就是可以用我们这个区块链的技术用区块链的技术我给大家简单的介绍一下区块链区块链其实非常简单它就是一种这个分布式的不断增长的数据库其中每个区块呢都包含数据当然大家常听的我们所谓的比特币就是说每个区块中都包含了交易的数据比如说我给你传了这个这样的币当然它其中可以包含各种各样的数据甚至包含图片包含文本包含各种这样的数据然后从经济上呢那怎么能完成这条链呢就有一个东西叫矿工矿工呢就去为了这个上链这个事情呢进行比较难的计算问题当你解决了这个计算问题的时候你就可以认为我就有这个上链的权利了那下一个是个把这个下面的一个内容存储到这个链上呢就由我来写这么一个block对那其中呢这个工作量证明呢就是说那怎么来设计这个问题呢其实呢这个一直从比特币开始呢其实大家就用了一个用一个叫解哈希问题啊其实是一个相当于一个密码学上的一个问题一会儿会简单的讲一讲但是呢大家知道哈希问题呢实际上是一种计算上完全的浪费因为这个问题呢算哈希但算了哈希对这个人类啊没有任何的帮助其实它消耗了大量的能源每年要消耗120太瓦相当于一个中等规模的国家的全部的能耗都是用来干什么就挖比特币用挖比特币那么那为什么当时的比特币的提出者叫钟本聪他会提出用哈希难题为这个工作呢当然我们到现在也不知道钟本聪是谁啊他只是当时写了一个白皮书那大家要满足几个问题什么叫这个工作量证明呢它有这个问题第一个是这个函数相当于一个方程啊是你很容易验证就是告诉你答案你很容易去去验证这个是不是对的但是呢你要反解这个函数啊就是我告诉你这个方程你要反解这个答案这个是很难的但是第三点呢是要可控制难度就是这个哈希函数呢是很容易控制难度他就说你前面有几个零就你通过一个random的东西通过这个哈希函数前面有几个零零越多呢难度越大所以我们听到什么比特币就是其实前面多一个零这样计算难度呢就直接乘以二自从这个工作量证明提出了之后呢其实大家都在想怎么能让这件事情有用其实刚刚跟这个这个葛老师耶利杰老师谈的呢其实最简单的一个想法是我们为什么不用这个东西来做一些解方程或者是举针求逆因为你看呢如果是解方程的话是不是它首先一个大的线性方程它难以反解但是易于验证我告诉你答案你就知道ax是不是等于b我告诉你x你很容易知道ax等于b但是呢如果你要告诉你ax等于b你要解这个x是非常困难的但这头有一个问题解方程做不到就是控制难度因为呢你把这个矩阵ax等于b呀你把这个a的行数增加并不能保证它这个计算的时间是一个比如说线性增加一倍因为它这个东西很多时候跟这个a是这个结构是有很大关系的比如这个是系数特定的结构这都会导致呢你这个难度控制非常复杂所以其实从比特币提出2010年开始呢就有不断的人去想我怎么能够去把这个工作量证明变成个有用的这个工作量证明但是都没有方案就都没有一个合适的答案但是呢我最近就发现了这个AI计算其实是可以做到的这为什么呢首先AI计算呢比如你训练一个大模型是非常非常困难的如果你把一个训练大模型看作解方程的话训练大模型的确是非常困难的但是难度是可以控制的为什么呢我们训练大模型中间用的是这个随机梯度下降的算法我们这个每次呢取一个batch of data就是取一部分数据当这个数据中呢我们这个过数据多少遍这个难度基本上是随着你取多少个batch线性增长的就基本上是这样所以它的难度是可以控制的最后呢就是说是否易于验证这个就需要呢我们做一些科研的工作了就是是否去要验证那我们通过理论的模型呢尤其通过这个我们都用了一些game theory一些博弈论的方法呢去证明它其实可以验证的而验证起来并不困难那这头呢我再总结一下我们通过AI的这个验证呢去代替这个就是以前比特币的这个工作量证明呢就这要满足三件事我们叫做一个三角形第一个是要安全性第二是要计算效率第三个呢是要可难度可以控制如果同时能满足这个三个东西我们就等于说我们可能提出一种新的用AI的方式来代替这个比特币这个分布式这个上链的过程但是呢它还能为人类产生价值为人类产生价值当然了这块我们就没有时间给大家细讲这篇论文了我们其实这个想法非常的简单我们这个想法是什么意思呢就在你训练过程中加入一些旗帜加入一些flag就是加入一些相当于水印或者说一些寻宝游戏然后呢让这个验证在训练的过程中加入这些旗帜然后让验证者呢去找到这些旗帜没找到一个呢我就给你一些奖励当你把这些旗帜都找到了就算你的这个验证成功了但是我们证明了这件事情的计算量是非常的小的假设你要训练训练的时间是t就你训练的这个时间是ot你要经过t个epoch或者t个minibash那我们这个验证的成本呢只是oe或者ologt就可以了当然了这头我们也能证明呢它的传输的我们也有通过算法导致这个传输的这个这个communication cost呢也会变小这样呢我们通过这个方式呢这个论文在这里我们今天当然就做一个科普就不给大家细讲了但是呢通过这样一个寻宝游戏的这么一个验证方式我们就能够解决我们刚才的问题我们就能够告诉大家说哎这个这个你是否就是说是这样一个问题啊就能解决这样一个商业场景中的应用就是说我们能够控制你去验证你整个的这个这个训练过程啊你到底用了多少个小时你有没有用合适的参数你有没有篡改数据啊这三点呢都可以在这里头完成这样的话呢以后比如说我让AI模型训练商去训练他们告诉我他有多少个小时能够提出自己的定价因为这时候为什么是很难呢就是说大家知道这AI中间啊我现在还是不能够通过这个你回答的答案的质量定价比如说我给你一个法律文件对吧你给我个大模型我不能说因为你这个我让你生成合同你出了错误然后就不给你钱了因为呢我也不知道你这个生成的合同好不好这个需要专家去评判而且并且如果你产生出来的合同不好对吧可能是我数据的问题不一定是你训练的问题所以呢这个现在的定合理的定价模式呢第一的就是按照计算的这个过程你是否去完成了整个计算过程去定价而我们通过这个区块链的这个一个想法呢就给大家这个这个这个介绍了一种方式啊这种方式呢就是说我们可以对整个的这个训练过程进行验证而这个验证的计算成本呢是比较低的它呢从另外一角度来说呢它可以把原来的这种工作量证明区块链的工作量证明变成了一种有用的区块链的工作量证明好那这时候我分享了这个第一部分给大家分享了这个一些我觉得这个这个声称是人工智能中一些痛点啊就包括这个情感沟通包括这个幻觉和包括这个可以验证的技术那下面呢我再最后再占用大家十分钟的时间给大家讲一讲这个我觉得这个人工智能呢在量化金融中的一些应用我觉得这个也是一个我觉得比较让我觉得这个比较exciting的一个领域吧大家知道这个量化投资的这个兴起啊就是前一段时间有一位著名的这个数学家这个我相信可能叶老师也认识这个叫Simon他给Berkeley也捐了一个这个这个很好的研究中心他以前是石西大学的一个数学教授并且呢他呢跟我们这个中国的著名的数学家陈醒申教授他们一起合作了很多论文后来呢他就进入了量化领域他就相信这个数学能找到这个股市上的规律通过这个东西去赚钱然后成立一家公司叫文艺复兴也是世界上最成功的这个这个量化公司他这个就是说呢在这个这个复杂的资本市场环境中他需要投资能产生稳定的回报那基本上呢当时他就提出了他最早就提出了我们通过海量的数据通过数据挖掘和AI呢执行交易并且这个交易呢就会获得收益他这样呢不受人性的这个弱点的这个影响对吧就是你不会看到股市跌了你就非常恐慌因为他这个完全由这个计算机来执行这样的交易那么最早呢其实这个在理论上呢就是说Fama呢就提出了一个这个著名的三因子的模型就是说他说这个这个收益呢能够通过这个证券一个证券组合的收益呢能通过一个线性的方程呢来表达这件事情就是说他把这个上市公司的市值账面市值比和市盈率来去做一个线性回归的方式来表达这个股市的这么一个一个组合的收益这是一个简单的线性模型当然人工智能的发生呢就是说我们可以把这个模型呢有两个方面第一个是一个线性的我们可以把它变成非线性的第二点呢这都其中呢这个公司的市值账面市值比和市盈率我们这个叫因子就是叫factor那我们这个factor它是人为定好的是吧那我们可以去通过自动的方式去找寻这个市场中间那些关键的factor通过这些factor来解释和预测股票的价格那那这个人工智能的兴起呢就导致了很多这个量化投资的应用这当然是美国的这个很多公司还有一些这个国内的可以进行多因子的一些量化选股包括深度学习但另外呢就是大语言模型的产生让我们微星摇杆和图像数据都有了更加深刻的理解而这些理解呢会直接产生出我们更好的一些策略可以给大家介绍一下其中呢比如说这又导致了2015年开始这家公司大家一定要对量化感兴趣的朋友们记住叫xts market现在最火的公司这家公司2015年建立的它也是由俄罗斯的当时一个纯数学家做代码的代数几何的教授叫Alex他这个开个玩笑说在俄罗斯做教授很穷后来他就去了德意志银行他就发现他通过人工智能的能够做量化交易2015年成立这家公司我给大家讲一讲这家公司有多么恐怖首先Alex去年是全部英国纳税人第一名他这个资产第二点这家做外汇他们是从高频外汇起家的他们的外汇交易交易量超过了剩下9家大银行的总和他们是世界第一名但为什么说人工智能他们刚开始的这个理念就是说我们想通过这个暴力的方法用算力去平推这个一开始我们如果对高频交易感兴趣的同学可能知道高频的理念就是说我们一般讲大家都认为高频要在微秒级到毫秒级做出交易决策必须要用简单的模型比如线性模型他们的理念就是说我们可以上非常复杂的人工智能的深度学习来做高频交易这个在2015年他们XCX Market之前是没有人敢想的他们对于开辟了这个新的范式他们这个数字大家可能读不清我就给大家说一件事情他们现在即使去年为止他们是世界上拥有显卡排名第四的公司前面三名是应该是谷歌OpenAI和Facebook甚至他们拥有卡的数量比微软还要多他们A100的卡是全世界第二名他们用了这么多的卡去做这样的高频交易导致了整个量化交易新的范式的一个变革当然这个公司非常的神秘所以他们的人也很少我们并不知道他们是怎么做的这个并不知道怎么做我给大家分享一点我自己对量化交易也是比较感兴趣曾经这是以前在一篇和清华大学李健教授我们做过一些通过集生学习的方式就是把不同的强化学习的方式结合起来做一个量化交易的算法当时我们做了一些工作这个是发表在paper大家感兴趣的可以看一下当时也是在A股上做了一些测试取得了不错的效果我们后来把AI的算法也用在一些大宗商品的CTA策略上为什么做大宗商品大家知道国内的A股是不能做空的但是大宗商品是可以做空的我们对大宗商品做了一些比如说高收益的策略也可以做一些能够回撤能够把回撤能控制的一些策略就做了一些这样的研究工作大家感兴趣的可以具体我们可以下面再聊但是除了AI直接用来预测股价格和做交易另外的AI产生了更大的一个影响是对于另类数据的分析和研究AI会产生很重要的作用什么叫另类数据我给大家简单的介绍一下我们把以前凡是你们做交易中间的量价或者作为基本面的财报或者价格这些东西我们都叫传统金融数据另类数据就是说它的来源不同它的格式也不一样大家可以理解比如说新闻就是最常见的另类数据但是我们还介绍一些别的另类数据比如说首先大模型很多公司都在尝试用大模型去提取新闻的上下文的文本表示看大家对什么样的公司的评论因为这个评论可能代表了大家的名义通过这个东西来预测收益率然后通过上下文的捕捉文法的句法和语义来更好的理解大家这个评论因为有些人可能说反话在以前的AI以前的机器学习可能是没有捕捉到的但通过现在的Transformer和整个大模型做了理解这头举一个例子这是芝加哥大学他们发表的一篇文章和美国AQR也是一个金融企业他们发表了一篇文章他们就说我可以让这个像以前把新闻分为正面和负面现在他们分的就很细了可以叫同意的表达非常激动的感激的Enjoy的他可以把情感分得非常细负面新闻同时他也把负面的新闻分成了很多种不同的情感的品类他们说在小股票和负面新闻大模型可以在预测小股票和负面新闻之后他们做了一些实验结果更加的明显他们说新闻报道对股票收益率是有很好的预测能力的尤其是对于这种中小型的股票这是芝加哥大学他们最新的研究成果大模型在最文本的理解的过程中把它运用到金融市场另外想跟大家讲一讲除了文本之外大模型对于图片其实还有各种方面都有很重要的作用比如说这个例子很有意思大家知道2020年的时候就是说发生了疫情所以你知道公司每年我们每年对于公司股价最重要的时刻我们有开个玩笑说叫赌财报什么叫赌财报呢比如说特斯拉或者英伟达他要发布一个他今年的收益他在之前华尔街的分析师就会对他的收益进行一个预测往往他这个收益如果比华尔街预测的好他的股价就会大涨他如果比预测的差即便他盈利了他的股价也可能会大跌所以一般讲在财报之前华尔街的分析师就各大分析师就会对这个今年的收益做一个预测但是到疫情的时候就发现有很困难的一个问题就比如说沃尔玛我没有办法去预测因为以前的历史经验都不工作了就说我们不知道疫情的时候大家对沃尔玛买东西的行为会产生多大的影响就多大的变化当时这个另类数据和AI就产生了巨大的作用怎么办呢他就通过卫星摇杆的数据直接去拍24小时的监控这个沃尔玛门口停车场的车的数量然后通过车的数量来判断大家有多少人去沃尔玛进行消费然后通过这个消费对当年沃尔玛的营收的状况做了一个准确的估计这就是一个另类数据通过AI的方式通过这个大模型对于这个图片的识别来去进行一个预测这还有一个给大家讲一讲很好玩的东西在美国有一个债券这个债券叫Catastrophic Bond什么意思呢叫聚灾债券什么意思呢你买了这个债券如果比如说今年佛罗里达没有发大水你可以稳定地享受到将近年化15%以上的收益就等于你买了一个债券如果佛罗里达没有发水没有发洪水大家知道佛罗里达是一个比较爱发洪水的国家你就买了一个债券你就稳稳地收到了15%的收益但是你如果佛罗里达真的发了大水你可能你的收益甚至是负的就是说你要赔钱它是通过这种方式政府是通过这种方式干了什么呢就是说如果佛罗里达没有发大水我通过政府的钱我给你一些回馈一些分洪但如果发了大水这是你的责任你买了这个债券你就要承担相应的风险这时候你的钱就会用来进行灾后的重建这个时候大家就想知道如果你要不要买这个债券很多时候你要对天气做一个非常准确的评估然后这个是当时它的一个收益图这个是当时债券的一个收益图大家可以看到蓝色是这个债券红色好像是一个指数这个债券就收益很高有意思的是通过这个债券大大推进了什么呢推进了气象学的发展就是因为以前气象学的教授可能工资也不是很高就做研究但是通过这个方式各大金融机构就请了很多气象专家成立了小组去研究到底佛罗里达会不会发大水然后它的研究成果他们也乐意跟当地的政府去分享最后特别神奇的是通过这个金融的一种机制的方式市场的一种机制的方式就导致可能佛罗里达或者美国那一些州对灾害的预测能力大大提高了因为以前没有人做这件事但自从发了这个债券金融机构可能花了大价钱请了一些科学家请了AI的专家来做这件事就反而导致了它整个带动了就是因为为民做了好事最后气象学的预测变得越来越准了这是很有意思的一个事情总而言之我想说当然了AI用在金融学上和我觉得用在运车学上也是有一定的关系其中最大的问题可能还是一个可解释性就是说我们假设你有损失你如何去解释性能和可解释是一定要权衡今天的苏老师在最后会给大家讲讲AI对整个经济学的一个影响我这边只讲了个量化算是经济学中很小的一块这给大家做一个抛砖引玉的功能好最后我就说AIGC生成式人工智能的新时代即将到来这也切合大会的主题谢谢大家",
    "感谢陈教授的精彩分享当前生成式人工智能的挑战体现在情感沟通与隐私保护的局限性甚至会带来幻觉的产生在演讲中我们特别了解到利用区块链技术来实现新AI时代的去中心化的算力市场以及现代机器学习和AI模型对量化教育行业产生的革命性影响接下来让我们继续探索生成式人工智能领域有请普林斯顿大学终身教授AI创新中心主任王梦迪女士为我们带来主题演讲生成式人工智能的运用面向生成优化的引导扩散模型",
    "掌声有请王教授大家好今天我能做一个更多还是从大模型本身的技术层面来讨论现在的一些比较重要的问题然后我想稍微的往几十年前回溯一下历史智能或者说怎么样去让一个机器完成一个有智能的任务这件事情不是从大模型也不是从生理学习开始的这件事情是从控制论开始的控制论是诺贝尔·威纳是美国的一个数学家他提出的在这个过程中他其实和香农在一起讨论信息的传输如何从信息到决策基本上控制论和信息论差不多是同一个年代的最重要的理论和方法论基础他们也后面直接带来了信息时代互联网时代和人工智能时代控制论出版于1948年一个非常基本的概念就是其实控制论是要讨论智能生物 人 动物是智能机器也是智能他们的共同点是什么呢他们的共同点是这样的系统都是一个控制系统不管是一个生命体还是一个机械他们都需要在一个复杂的环境里面能够根据环境收集到的信息做出反馈这里面的计算可能是通过物理和机械元件直接造成的完全是物理上的所以计算也可以是数字的这是控制论更现代控制论就变成了强化学习强化学习第一次就是让家家户户都知道这是2014年的AlphaGo它是一个对抗式的博弈但是它的解决方式是让每一个博弈的单方来自动地学习如何去最大化他们的利益所以它是一个对抗式的强化学习算法那么它的核心还是来自于最早的控制论强化学习还可以用于多个智能体的实时决策优化还可以用于机器人我们看到了很多很多巨神智能的然后还正在用于自动驾驶也是马上就会出现在我们生活中的然后另外一个强化学习非常有意思的应用这个是发表在Nature上2022年大家知道2022年有哪些AI技术产生了革命吗对XGPT就是2022年但是2022年的AI技术里面对人类产生最深刻的影响有可能不是XGPT有可能是这个用强化学习去控制核聚变reactor里面的等离子场可控核聚变是控制领域最难的问题因为这个核聚变的这个系统本身是一个混沌的chaotic system那么传统的控制理论经过复杂的模拟计算依然不能解决控制问题但是强化学习算法基本上特别精准的控制在任何一个我们想要的形态上面那么这些都是强化学习然而强化学习有非常强的去在高维的系统中去找到最优策略能力但强化学习本身有很重要的弱点就是强化学习不是特别善于利用数据那么我们今天要讨论的不是强化学习我们要讨论的是大模型大模型是否可控就是做互联网做技术的朋友可能要了解凯文·凯利是一个非常著名的作者然后他有一本书可能大家都读过是凯文·凯利的一本书叫《失控》这本书其实是一本整整30年前的书他这本书里他又预测了从控制论开始来讲生命体和机器在变得越来越像机器越来越智能生命体也越来越被工程化这本书里预测了大量的零散的小的元件可以整合在一起通过离散式的大规模计算他们可能会产生更新的智能这本书预测了深度学习大规模神经网络这本书还预测了涌现他说每一个神经元是无意识的但是它们整合在一起的时候这个意识会自然而然地涌现出来就是EmergenceEmergence这个词现在大家已经比较熟悉当我们说大模型的时候这个意识已经涌现了但涌现这个词是来自于这本书然后这本书的名字叫失控为什么叫失控呢因为这本书它成功地预言了从1994年到2023年基本上所有人工智能的发展但是最后它预言了当智能体超过人类的时候这时候会发生什么会发生失控这本书没有预言后面再会发生什么所以就是说当智能体超越人类的时候我们面临的这个已经变成现实了那么这个时候在去年的7月份OpenAI提出的一个问题就是完全一样这是我们现在的现实超智能很有可能在未来几年内或者我们如果悲观一点未来十几年几十年内实现但是当这件事情发生的时候AI系统比每一个人都聪明甚至是比我们全屋的人加起来都聪明那么我们如何保证它还能够按照为人类的意志服务那么这个问题就叫alignment或者叫superalignment超对齐那么超对齐是个非常大的问题现在所有的尝试都是很浅的那么现在最一个比较初等的肯定未来会有很多很多新的发展但现在一种超对齐的方式是做用人工反馈的强化学习它大概意思是这样当我们有一个已经预训了好的大模型的时候你可以想象相当于我们来了一个从来没有接触过世界的一个天才儿童他知道很多信息但他从来没有把这个信息灵活地梳理起来也没有真正解决过任何问题如果你跟这样一个没有跟人说过话的天才儿童聊天是有点痛苦的所以最简单一件事情就是你先要教这个天才儿童怎么好好说话怎么好好跟人相处之后再教他解决更复杂的问题那么这一集在这里做到的作用就是说它要不断地让大语言模型和人类用户进行交互收集人类的反馈并且这个反馈是个很简单的反馈就是说人对这段回答是满意还是不满意或者说我给你两段不同的回答那么你更喜欢哪一段然后用这样的数据去训练奖励函数再用这样的奖励函数把大语言模型的生成过程当作一个控制策略再用强化学习的方法进行微调这是一个基本的基础思路和各种不同的场景下不同的应用然后呢也就是说为什么我们要从强化学习角度来理解这件事情因为token的生成是一个控制过程如果我们仅仅是模仿那么它学不会推理而是我们要把如何生成变成一个策略当我们自己在说话的时候我们是有策略的我们不是仅仅去像鹦鹉学者一样所以这是为什么要用控制和强化学习但是强化学习是有很多很多不足的我们先不说大语言模型我们光说机器人这是我们组去年一个工作就是说当我用很多离线的用户反馈数据试图去对齐机器人的时候这时候有很多问题因为离线的数据和我们理想中最优策略对应的数据的分布不一样这时候会造成一个distribution shift也就是说数据不完全覆盖的时候这里会有很多很多问题比如现实中叫reward hacking或者over optimization我们看到最理想的一条红线是理想中的performance但实际能达到的和理想中有个很大的gap对然后为了解决这个问题我们提出了一种双层强化学习算法尼克利也是我们在对齐策略的时候同时试图在对齐策略然后用这种双层优化的思路我们就可以大大的逼近这个理论上最优并且和这个没有做双层对齐的算法实现了一个很大的效果上的一个接约然后这是一个计算机器人然后那个最近我们组在我们在这个大语言模型的对齐上做了很多工作这里面有很多具体的场景和很多可以深入的方向比如一个很具体的问题就是说我们可能有很多不同的不同的人类需求有一些人喜欢这种风格有的人喜欢那种风格那么我想做一个更精细的对齐而不是把少数人的需求直接淹没在大量的数据里面我们可以用一个最高目标的强化学习的对齐方法尤其用于帮助让大语言模型能够注意到少数人的少数需求然后同时我们最近还发布了这种基于自我对抗的更深度的离线对齐算法以及实时收集在线数据的在线对齐算法我们在线对齐算法比这个现在的SOTA应该提高了我也读不出来数这个30%到50%的样子然后这是对齐就是控制对大语言模型不仅仅是微调我们另外可以做一件事情是我们可以做把一个小的大语言模型和一个大的大语言模型结合起来为什么要做这件事情呢因为如果我们要用大的大语言模型做生成的时候这个大模型的成本是很高的我们现在这个方法是2023年有谷歌D1的第一次提出的不是 我是我们第一次提出的当我把小模型和大模型组合在一起会发生什么呢相当于你把小模型当成学生然后大模型当成导师然后这个学生去做生成大模型只要去监督就可以了但我们希望大模型就要保证生成的结果跟用大模型做是一样的那么怎么去协调小模型和大模型之间怎么让它们互相协调达到一个最好的加速这个也可以用强化学习来做然后我们刚刚发布的这个spec decoding++就能够实现大模型推理2.26倍的加速然后更进一步就是说当我们讨论对齐的时候我们甚至可以把对齐的奖励函数变成数学证明的对错或者说自动编程的对错从对齐的角度来讲我们可以从一个已经对齐的模型出发当我们有一个新的奖励函数的时候我们永远可以再迁移到一个新的奖励函数上然后在这种迁移任务里面我们得到的最优模型它的效果能够提升25%甚至到超过100%所以这里讲的都是怎么让大语言模型可控并且更好的为我们所用然后接下来我们来换一个话题我要讲的是可控大模型大模型不光有大语言模型生成式AI也是一种大模型当我们想到生成式AI的时候我们想到的都是这个图片 小视频抖音 快手但是生成式AI同样也可以定点的针对任何一个靶点去生成相应能够结合的这种蛋白质这些看上去是不同的问题但它们底层技术是一样的那么先回顾一下生成式AI的一个非常非常短的历史生成式AI其实是一个副产品最开始人们是在用variational autoencoder试图把高维数据压缩到低维数据他们发现这个系统里面的decoder可以把噪声变成像模型一样的图片那么这个副产品就非常有意思了然后另外一个曾经非常火的模型叫GAN对抗式生成网络它最早也不是为了生成它是为了做预测它是为了做classification和discrimination但是这里面也有一个因为为了对抗嘛有一个生成器结果这个生成器就特别有意思所以呢就是科学家发现生成和压缩还要好玩所以生成变成了一个单独的任务所以当我们现在说生成的时候就是说我们希望有一个模型或者一个方法能够把噪音变成我们想要的设计我们想要的图像甚至是其他的设计然后如果我们去看这个VAE和GAN这两个生成模型它有共同点共同点在于它的这个生成器decoder或者generator都是一个神经网络相当于它一步就把噪音变成一个新的数据点但是扩散模型完全改变了生成是AI的逻辑扩散模型本身就是在控制一个过程扩散模型是从一个噪音开始解码经过一系列非常多的timesteps控制一个从噪音作为出发点终点是一张新的图片这是一个控制过程然后这里面每一步每一步的迭代它都是由一个神经网络来引导完成的所以本质上我们现在训练的这个神经网络不管它是什么architecture这个神经网络最终起到的作用是一个控制器然后多说两句扩散模型是一个非常非常优美的一个大模型因为这个模型它的出发点就是要找到一个随机控制过程我希望从噪音生成图片非常难但是如果给我一张图片让我把它变成噪音这个太简单了所以这个过程是它有一个前向过程是从图片变成噪音我只要加噪行了我可以设计一个特别简单的随机过程我可以把它的VN方程写出来那么我想做的事情是把这个过程取逆然后概率论里面有这个非常优美的理论可以告诉我们这个过程的逆过程这个drift function应该长成什么样然后就是基于这样的随机过程的理论扩散模型就是说从一个加噪声的过程里面进行试炼最终学到了这个去噪声的控制过程学到了去噪声的控制器然后我们组这个年份错了我们组去年的一个理论工作就是从这个统计理论上解释了为什么这样一个加噪声去噪声的过程能够去逼近一个有着latent就是有一个低微流行结构的复杂数据点然后更进一步就是说扩散模型本来就是在求解一个控制过程但是呢我们是不是有可能能够用它做更多的事情比如我想求解一个优化问题如果我们只做传统的优化的话我可以我们会想象我们有一个叫求解的一个优化函数的函数我有一个landscape比如我可以用我可以做模拟退火我可以做梯度法它可能是非凸的也是非线性的然后我们要在这个空间做搜索我可能要设计一个很复杂的算法如果我的算法恰好符合这个问题的几何结构的话我可能就会效率高但是呢我没有利用数据的能力所以我们想说的是如果我们想优化一个蛋白质优化一个电路设计尤其是当它们特别复杂的时候我们很有可能利用生成式AI比如扩散模型作为一个底座然后在上面再把求解器搭上去这样我们球可以又利用数据又利用求解器去直接生成更复杂的任意的新目标函数的解然后这就是我们去年到今年的一系列工作然后这一系列工作就是是和这个业余老师一起合作并且我们还在不断地推进这个方向比如我们做了一系列工作来找到了一种轻量级的把一个已经预训练好的扩散模型进行自适应的方式使得它在我们不需要改它的甚至可能不需要改它的权重我们只需要在这个解码的过程解码的过程其实是一个温文方程我们只需要在解码的过程中加一些额外的控制量然后我们可以去设计这个控制量让这个解码的过程最有效最平滑能够生成任何一个我们想要的目标函数的解这说明什么呢就是我们想做一种能指哪儿打哪儿的设计然后最后一页就是说当我们想用生成的方式去升级优化算法的时候我们除了做视频和美图我们还能做什么比如啊生成式模型应该大量用于机器人的控制生成的是从以往的机器人轨迹数据里面生成新场景里面新的机器人的数据然后我生成新的数据的时候新的轨迹的时候还可以自动做优化然后我们可以生成蛋白质结构然后我们可以生成DNA和RNA的分子我们组今年年初有一个自刊的论文就是说如何用生成式的语言模型来生成新的mRNA疫苗设计然后同时还有很多新的方向比如去年还是谷歌D1的有一篇paper是用生成式模型做材料开发它在一篇论文里面生成了超过220万种新的晶体结构晶体结构是很复杂的需要满足一些数学上的拓扑结构然后这220个新的晶体结构都是可以合成的然后这篇论文里面还估算了一下如果是人来搜索这些晶体结构所有人不是一个人按照全人类原来的搜索速度他们需要800年才能找到这一篇论文生成的晶体结构所以就是说当我们用扩散模型我们讨论生成式AI的时候我们能够更可控我们能够越精确地控制扩散模型我们就能够生成越复杂的设计然后它的应用远远不止于小视频和美图它可以生成新的材料它可以生成新的电路设计新的基因序列还可以做机器人的控制最后就是说当我们已经有大模型的时候如何让它可控为我们所用这应该是通往AGI的一个必经之路谢谢大家",
    "感谢王教授的精彩分享在演讲中我们深入了解到了扩散模型的统计特性以及它们在优化问题中的潜在应用扩散模型代表着生成式人工智能的重大突破而在当今人工智能迅速崛起的时代如何拥抱变革把握机遇至关重要接下来我们有请宾夕法尼亚大学沃顿商学院中生教授机器学习研究中心主任苏韦杰先生为我们带来主题演讲人工智能和数据科学的经济展望",
    "掌声有请苏教授谢谢以往我给报告一般都是先做slide piece做好以后然后再起个标题这个标题只要符合内容就行这一次是我先起了个标题然后再想怎么样做一个slide然后这样给我增加了非常大的难度我是今天上午才做完然后特别大的难度在于什么呢这个展望什么叫展望因为这是预测未来发生的事情这个是最难的因为我设计到人工智能数据科学 经济其实特别是经济我也不是特别懂经济虽然我在商学但不是特别懂所以今天这个报告更多是比较偏文科就是更多是启发性的我更多是一种抛砖钻隐域的一个期望对 这是我本人对这个前面几位老师已经讲过很多铺垫了去年开始是生成是人工智能的一个可以说是起点吧然后突然它之前有很多年的积累但是去年开始就突然大的爆发了那么这个它已经成为了一种军备竞赛从一开始开始需要几千块GPU所以门槛慢慢慢慢增加到几万块甚至最近看到新闻是XAIElon Musk XAI他们准备购买十万块H100然后来训练他们的模型就是已经这个是这个需要的能量可能是需要以国家级别的能力才能够驱动这样的这样的训练了那么然后它的影响力是什么样的它是什么样的从学术角度它让我们以往比较公认的图灵测试可能没有那么重要了因为图灵测试是计算机的一个非常重要的一个假想实验就是如果我们能够闭上眼睛跟一个东西对话发现它跟我们人自己跟人直接说话没什么区别那么我们觉得它已经达到了人工智能现在Language Model大模型对话确实有种感人的感觉是差不多但是我们还不能说它达到了通用人工智能包括叶老师提到一个例子就是一个简单的问题它都会出错看起来很合理但是错误总是在这跟我们人还是不一样的那说明图灵测试这个图灵在可能70年前提出了一个假想实验现在已经慢慢失去了它的客观性吧它比较擅长于Coding写作和知识的一些整合更多还是一种整合吧它并不是一种完全新的知识的产生然后它产生了巨大的商业价值当然商业价值很多大家都在抱怨它这种价值都归给NVIDIA了很多公司AI本身创造的价值是在创造但是怎么样把它能够在一个垂直众生的领域中产生这需要精根细做还需要一段时间的积累所以暂时只是让NVIDIA赚了这个钱那么从经济的角度我们标题中很重要的一个关键词就是经济那么经济上这个大模型跟经济什么关系比较早期它就已经有一个论文这是会亮的吗就GBT第二个GBT并不是另外一个缩写这篇文章在GBT4刚出来的时候就做了一个分析然后分析它在就业市场上对每个不同的工种它会有什么影响然后得出来的结论是对一些比较高精尖的工作的影响可能会更大对于一些比较相对我们可能看起来低端可能对体力要求高的工作可能影响反而比较小这个跟葛老师的观察完全一致确实他首先替代的其实是我们这些人所以我当时看到GBT出来的时候我的第一个反应就是我以前比较觉得自己有点小小骄傲的一些技能我觉得其实我可能高估自己了确实比如今年年初年初有一个Alpha Geometry是吧就是一个Alpha GoldDeepMind推出了一个工作可以解决IMO就是国际数学奥林匹克几何几何的那种题目这是以往我自己觉得我是比较擅长的我觉得我是偶尔觉得还有一点点小自豪的一些技能但发现这些东西现在已经完全被自动化了越是觉得相对可能以前觉得抽象高端的能力其实它解决得更快一点反而那些体力那些要真的动手去做的那些能力其实是反而现在替代可能是比较慢的那么那么它其实现在从经济角度而言它现在产生了一种计算机跟经济的一种交融那么从经济的角度那么我们怎么样数据成为了一种新的资源怎么样给数据定价那么就是一个问题这个陈老师也提到了怎么定价是一个非常重要的问题因为根据完全根据推理的那个时间可以很好的反映它本身的这个价值它价值来自于它它是否解决了问题还不在于它什么训练的时间有多长那么怎么样能保证它的公平性公平性在经济中也是个非常重要的问题因为我们不能只关注51%的那个大多数少数的49%甚至更少的1%的那些目标也是需要我们关注的这也是陈老师提到过的那个reg那个信息增强这个也是从经济角度这是一个online advertisement一个非常大的一个价值那么从更大的角度从宏观的角度这个可能更微观吧微观经济从宏观经济的角度来讲那么AI的发展使得它怎么样慢慢扩散到我们的就业市场那么它的速度有多快这是一个不确定的问题是一个非常值得研究的问题然后怎么样它慢慢可能会替代我们人类这个也取决于我们人类的工作的这个复杂度然后是不是跟环境的这个动交互密不密切还涉及到两极化因为现在有一种理论是AI如果再继续发展下去中产阶级可能会消亡因为中产阶级其实就是最容易被替代的那些人就是我们底层的靠体力其实暂时还不会受到威胁然后资本更不会受到威胁因为AI是不能替代资本的所以这样的话它把中产阶级给消灭了那我们的人类的社会最近工业革命以后工业社会就不稳定了可能是一个巨大改变所以从这个角度而言它比第二次第三次工业革命肯定影响力还要更大还有是美国的巨大优势因为AI的它对如果通用人工智能实现它的影响力是远远超过前面两三次因为它是本质上是替代人而不是光增强人所以某些国家可能在这方面会有极大的优势它会让其他一些国家在根本就没法竞争那么这是我对AI和经济的比较粗浅的理解有两边的理解第一边理解就是AI怎么影响经济在上面我们怎么样根据我们的理解能够更好的设计AI的模型使得它更好的有些经济的考量那么我们先考虑上面这个上面这个就怎么样AI会影响经济我再次声明开放的一些讨论现在没有定论那么它对工作的影响呢刚刚我也提到它对工作的影响与传统的理解稍微有些不太一样那么我们可以引进这么一个概念叫做information complete信息是不是完备什么是信息完备呢信息完备的意思是这个工作它是不是像计算机有个输出有个输入它中间这个过程它跟环境互动是比较少的它是可以说是在一个封闭空间中就可以完成的工作越是这样的工作呢首先这类工作有哪些呢比如像程序员会计啊包括作家其实也是相对比较容易替代因为它输入输出是比较确定然后但是第二类呢其实我觉得也是相对可能会比较容易取代的我相信是这样就是最聪明的人就图令奖得主诺贝尔奖得主费尔斯奖得主我觉得是最容易工作的所以它对我们人类的替代这个程度是跟我们普通我们以往的想象可能是有点不太一致的那么我们人类有很多很多种能力啊我这个只是随便写了几种啊就是信息处理的能力语言理解还有推荐还有复杂问题的决策还有像我们做research的也是一种能力对然后那么它AI什么时候会取代哪些能力那么现在这是一个非常讨论非常激烈的问题而且这个讨论过程中我感受到一些比较震撼的一些一个方面是原来最头顾的那些人物他们自己都相互并不同意对方这个是比较少见的因为以往大家的分歧以往对历史上大量的问题的不确定往往是来自于两个不同的群体他们是来自不同的背景啊或者来自不同的区域什么的各种但是现在是什么这三个图灵对图灵讲得主啊就是图灵讲得主深度学习的三巨头啊三巨头想必他们是应该现在是我们人类中对AI理解最深刻的这三个人至少是三个人之一吧但是他们之间其实对AI怎么样会影响人类从长远长远的宏观的角度是理解很不一致的耶勒昆觉得AI还很遥远从即时实现它对人还是一个更多是一个工具啊但是Hinton包括Benjo已经现在开始已经投入很大的时间去研究AI的risk从长远长远的角度它对我们人类社会的破坏性就那个失控包括王老师刚刚提到的那个失控那本书就是很危险就是后面就没有故事了那失控以后就是人类的文明终结吗所以这个是Hinton和Benjo现在比较关心的关心的课题所以他跟耶勒昆是完全是是相同的那么从他们的讨论他们相互还在激烈的讨论那么我们能看出来是这个是一个公开的公开的问题现在还没有定论但是我们我们不知道这个问题并不等于我们什么都不知道其实我们还知道一些的这是我自己的总结我的总结是这不是我的这并不是我的research是我的个人体会添加了这个这个人类的这个人类的这个人类的2010年一个人称那个人称一个人称是里面的有一个人称Skilling Law当然我们接下来稍微会更多讲一点Skilling Law本质上是说随着你计算的资源数据增加你的模型的能力是完全可预测的是服从一个简单函数一个密词函数那么这种情况是我觉得我个人觉得是一个比较如果是有个上帝的话它是一个比较nice的上帝是我们可以也许一定程度上我们是能把握自己的命运的另外一种是一个不可预测的世界就是新的技能会不停地涌现然后我们不知道什么时候会存在也许就再努力一把我们就可以有一种新的技能但是也许因为我们不知道在哪里我们就永远地失去这个机会了所以这个一个代表就是这个涌现就是这个涌现能力这个也是在大约模型中现在观测到的一个现象就是没法预测前一秒觉得这个能力好像还遥遥无期下一秒再努力一把就直接就达到完美了就是有这种有个突变那么有突变的话那么我们就其实我们人类的命运未来很大程度上是要靠运气我们不能完全自己把握自己的命运所以这有两种可能性我不知道是哪种希望是第一种对Scaling Law和Emerging Ability那么介绍一下吧这个Scaling Law就是指我们有很多的计算资源然后函数它的选择函数是服从了一个非常可以说是曲线一条直线取了log以后就是一条直线了这个有点像物理因为物理中我们经常能看到这样的完美的预测在计算机在FG学习中几乎从来没见到过这样的预测我心中就出现了相当于如果有这个Scaling LawScaling Law能够继续下去那么相当于我们有个水晶球我们能够把握自己的命运那么另外一种角度是它是一个涌现那个涌现的话那么就是一个不确定的未来什么时候出现我们是不知道的这个就是诺贝尔奖物理得主这个量子力学的Founding Father之一那个波尔就提到的预测未来永远是最困难的这也许也是量子力学的本质吧量子力学就是不可预测当然从理论上其实这两者也是有联系的这个也有工作说Emerging Ability和Scaling Law并不矛盾就是并不矛盾但是它给我们的世界观是两种完全不一样的世界观一种是非常可预测另外一种听天由命两种非常极端的情况那么这个我讲完了这是第一部分这是我个人体会就是AI对经济的影响简单的说有两种可能性一种是可预测另外一种不可预测但是具体哪一种没法预测那么现在我们考虑第二种就是从经济的角度能不能我们对AI的设计做一些事情呢那么这张图好像在之前好几位老师slide中出现了特别是王梦迪老师刚刚也在他的报告中出现了然后他也做了非常详细的介绍那么也许我就可以过了简单的说就是这个是大圆模型训练中微调的那一部分怎么样让它更好的能够跟人交流符合我们人类的预期然后知道我们人类想要什么不想要什么那么从经济的角度我们其实不光想知道51%的人想做什么我们永远要关心那些49%甚至1%的人想要什么我们需要知道整个世界的它的分布而不是只知道最大的那部分比如我们想知道现在有多少人喜欢咖啡有多少人喜欢茶这个关键是知道它这个比例而不是只知道大多数人喜欢什么那么这个不光是公平性其实这个对经济非常有影响因为我们做经济决策的时候需要知道准确的比例这个准确的比例有助于我们做经济上最完美的选择那么比如我想抽象一个例子我想现在开一个披萨店还是一个寿司店那么我需要知道我这个区域中喜欢披萨的人和寿司的人他们的比例如果比例是80%对20%那么也许我应该是披萨但如果是10%跟90%那么应该完全就不一样那么我们怎么样能够模拟这种事情呢我们就需要做一些简单的一个模型通过这个BTL模型来通过来概率上来确定多少人喜欢Apple多少人喜欢苹果多少人喜欢这个橙子好那么但是在这个过程中我们一定需要保证少数人他们的利益能够他们的我们要得到他们的有关注点这个在在OpenAI训练的数据中也有体现他们的训练数据中公开的instructivity中他们有雇用了大量的人工标注数据人工标注数据中也有百分之大概百分之二三十是不一致的那说明那说明什么那说明这种不一致是一种天然的这种天然它不光光是它不是错误它是体现了一种自由度像茶和咖啡本身一部分人就喜欢一部分人就不喜欢它并没有对错之分这种没有对错之分的这种分布它其实适应了是一种经济的一种考量经济上我们要要最大化我们的收益我们就需要考虑它的整体分布而不是只知道它最大的那个mode好那么异地链小小的数学我们近期的一个工作是这样这个是强化学习对这个这个是强化学习中最后有一个reward model以后用强化学习然后希望这个大模型能够更好地符合人类的预期但是呢我们先提出加一个正则项这个正则项呢就是是一个函数这个函数什么函数呢是它这个分布的函数这个probability大概多少人喜欢它有多少概率喜欢这个苹果有多少概率喜欢橘子就这么一个函数我们想确定这个r这个函数的形式是它能够完美地符合我们它跟人类的这个语言人类这个偏好然后我们简单地简单地一个计算然后数学上证明了它当且仅当当且仅当这个r这个这个损失这个正则函数它符合它一定是这个形式这个形式那比较重要的重点呢是这个log那这个log呢本质上就是说什么呢它本质上是说我们是要对它加一个相容的一个商的一个惩罚项商要大一些商不能太小什么是商商就是一种不确定性就是零一之间我不要都是零也不要都是一我希望在中间 是吧特别是二分之一的时候商是最大的所以这个商是entropy它希望我们能够保持能够保证它原来的分布而不是把49%就设为零了对那么我们建议在这个范式下我们建议前面都是一样就是先训练做预训练然后通过rlhf满足人类偏好但最后最后一步我们通过加这么一个preference matching加一个log这么一个项做一个微调使得它更好地符合我们人类的偏好对然后这个是我们的第一部分经济上那么总结一下这部分是通过经济上的偏好的概率分布然后我们有选择的使得大元模型的损失函数能够符合我们的这个预期那么第二部分呢我们讲一下这个数据那么数据呢是21世纪的石油石油这也是Elon Musk他当时想买下Twitter的一个动力现在看来他这个决策是挺正确的因为他现在同时拥有Twitter现在叫X和一个AI公司XAI这两个公司其实是缺一不可因为Twitter提供大量的数据而且是实时的数据海量的数据然后XAI现在它就可以直接用Twitter的数据来训练然后它Twitter现在已经不允许外面的人爬虫了所以Twitter成了它的私有数据这方面是XAI它那个公司现在以比较小的投入但市值已经达到了好像有个估计是预期一个估计达到OpenAI的三分之一所以OpenAI果然是有这个很有眼光它很早就是可能布局了那么但是这个数据重要的过程中大家发现就是它引起了一个专利的问题专业问题因为它可以有些有专利保护的数据也在用于训练AI的模型比如一个早期的例子对这个扩散模型大家直接通过说输入一个提示词是一个人的名字然后它就直接生成一个原始训练数据中的一张图片这张图片基本上完全是一样的只是这个稍微像素点稍微低一点但其他就没什么区别那如果这张图片是它不光它的影响是非常深远的它破坏了隐私我们不希望有些数据我个人的照片直接就是让别人看到了还有一点更严重它如果是受专利保护的话那么就是这个它会引起法律上的一些纠纷那么现在像纽约时报已经正式起诉了OpenAI和微软然后起诉他们在侵权他们训练他们的单量数据像索尼已经禁止AI公司训练他们的音乐训练音乐然后有各种现在很多的起诉这方面那么各个国家各个国家的反应是不一样的像日本日本基本就放了放弃了数据的保护所以这也是OpenAI最近它在除了三藩之外它最近在东京开了一个分部好像我不记得应该因为OpenAI在其他地方都没有分部吧它在东京有一个分部主要也是看中了日本的这么一个政策它在日本训练AI模型数据不受任何保护也能理解因为日本基本上就错失了前两代的信息技术所以它可能想在这一波生成技术上能够跟上那要跟上的话就是必须要非常手段那非常手段就是说随便训练没有任何保护好的但是这个问题是如果我们不保护专利短期而言可能是痛快了我们想训练是什么随便训练但长期以往它会伤害我们人类的作为一个智慧的物种创新能力会影响我们的创新能力这个原理很简单因为我们创新我们都是需要有激励的我们创新是因为我们会能得到更大的回报但是如果我们的作品最后被AI学到了然后它能够生成跟我们差不多的风格的作品那艺术家我们人类中最优秀的那些创造型人物有创造力的人群他们怎么样能够获得一个他们怎么样生存下去这就是问题那如果这样的人群消失了那么未来我们在互联网上的那些数据就充斥的本身就是由AI生成的这些数据所以这个就变成了那个就是garbage in garbage out现在一个信仰就是model collapse已经有实验表明如果AI模型用自己的输出的数据来训练它的效用就会逐渐下降最终就是慢慢慢慢慢慢变得越来越差好的那么我们这里想提出一个方案就是我们希望引进一个经济上的一个角度把copyright呢用经济的手段把它来解决我们也不是完全保护copyright但是专利但是让有专利的让有专利数据的公司或者个人能够得到相应的回报那么经济用经济或者金融的手段解决这种问题本质上其实还是overall其实我们想达到最高的整体效用整体的效用我们想是最大的所以我们希望AI模型还是用最好的数据来训练它不受任何限制因为我们希望它能达到最好的性能但是同时我们希望人群中我们贡献了这些数据贡献了这些专利数据的那些人群它能够得到相应的回报那这个问题是什么回报这个回报有多大这就是一个重要的问题怎么样我们平衡平衡这个回报那么一个例子呢就是一个著名的音乐分享网站分享平台Spotify现在很多音乐家就是就是在Spotify然后售卖他们的音乐作品在这个过程中平台会收取一部分费用但是如果音乐家这个歌手他的作品非常非常非常流行很多人听那么他的回报也会比较多所以跟音乐家本身的贡献也是成造成正比不是线性的正比但是有这么一个趋势那我们也想设计这么一个机制然后能够达到这么一个效果就本身上AI公司需要make an offer就是要reasonable至少reasonable对那么我们的这个想法呢本质上是用了博弈论的这种想法在博弈论中有一个sharply value叫sharply这个sharply它首先要明确一点什么呢明确一点现在考虑若干个人S每一个人都有他们的数据然后我现在只用他们的数据来训练一个AI模型那么我想知道这个模型呢它的效用有多大那么我们提出来这么一个东西Log分子除以分母那分子是什么呢分子是我生成这个图片只用S的数据的这个概率分母呢就是我什么都不用我只用比如免费的数据公开的数据来训练这个模型然后生成那张图片的概率那么如果这个分子大于分母那说明S的数据也非常有价值非常有价值那么去log以后也比较大那么这个回报就会比较高那么这就是他们这个S的这个coalition这个组合的它应该应该应用的价值应用的价值那么一个例子就是我们现在有四个艺术家比如梵高 莫奈等等毕加索 伦伯朗那么我们可以用他们之间的组合比如我只用梵高和莫奈的数据来训练那么训练一个模型然后看看这个模型跟不用他们的数据的模型它使得效用提高了多少这就是我们认为梵高和莫奈这个组合他们组合应该拿到了回报那么现在我们只是讨论了每一个组合我们并没有落实到单独一个梵高或者一个莫奈那么怎么样从整体的这个组合的这个收益然后退到每一个个人艺术家个体艺术家的这个回报那么这个就涉及到了一个诺贝尔经济学奖的一个贡献就是这个叫做Shapley值Shapley是为他跟纳斯非常像他们背景都是数学家但是都获得了诺贝尔经济学奖这个Shapley value是他的诺贝尔奖的贡献之一这个贡献怎么说呢他就是把每一个组合就是比如梵高和莫奈他们应该得到了回报然后能够推导出每一个具体的单个的艺术家他应该得到的回报而且他这个值以他自己名字秘密的这个值呢可以在某种意义下是唯一的因为他保证了这个有效性对称性等等他是唯一的他是唯一的那么我们就是用这个Shapley value然后就得出每一个艺术家应该分享的这个比例比如在这里梵高他的比例是最高的38然后其他稍微低一点这个比例当然实际情况是整体他应该是需要有一些沟通有一些negotiations就是在艺术家整体艺术家和那个AI公司比如AI公司保证他把20%的利润分配给这些艺术家然后这些20%的利润呢然后这些20%的利润呢总量下以这个比例以这个38.5%啊等这些比例然后再一次分配到这些单独艺术家身上对然后这个过程中对这个过程中每个艺术家他当然也可以选择退出这个机制他存在这么一个退出机制在这里面这个就是我们的一个实验简单地说比如我们能够看到这些非对角线上的一些有些比较大比如相对这个比较大一点这个就说明什么呢这说明莫奈和梵高它们之间风格有一定的相似性就是用梵高的作品相对可以更容易地生成莫奈的作品好的那最后我结束了时间应该差不多是吧然后现在可能正在形成一种新的经济学我不是经济学的这方面的专家但是我能够体会到因为生成是AI或者说更大来讲AI它是一种真的是革命性的影响它不光像以往第一次第二次工业革命前面几次工业革命更多是让我们人跑得更快飞得更高是增强人但是现在这一次AI的的,的涌现其实某种程度上是替代我们人类或至少替代至少让一部分人类能够替代其他人类所以在这个过程中经济学的原来的假设原来的一些机制可能会有颠覆性的颠覆性的一些变化所以有志于做对经济学和经济学和AI感兴趣的朋友这是我非常建议的一个领域因为AI其实如果你不是工科背景想直接去设计它们这种架构难得但是把AI作为一个工具应用到应用到经济学中那么一下子它的自由度就会非常大这是一个非常相当于是个蓝海是有很多很多机会的那么最后这个是我的一张这么一张图AI呢是这个霸王龙它非常强大但是我相信我们人类有机会能够掌控它应该不会失控的我相信是谢谢大家",
    "感谢苏教授的精彩分享数字经济浪潮席卷而来催生了各行业技术与应用的不断迭代今天我们和大家相聚于此共同见证了智能计算技术推动数字经济增长的新征程也探索了人工智能技术不断发展促进数字经济产业创新的美好未来尊敬的各位来宾下面我宣布AI加领航智慧未来2024人工智能大会智能计算与强化学习论坛到此圆满结束再次感谢出席本次分会的所有嘉宾和观众朋友们最后祝大家工作顺利万事如意期待与您下次再会谢谢大家"
]