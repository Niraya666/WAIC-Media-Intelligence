请不吝点赞 订阅 转发 打赏支持明镜与点点栏目
订阅 转发 打赏支持明镜与点点栏目
订阅 转发 打赏支持明镜与点点栏目
订阅 转发 打赏支持明镜与点点栏目
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
掌声有请主持人
我们在每位观众的座位上,都配备了同声传译的耳机。
interpretation earphone is set on the table, so you can use it and to have a very fun communication with each other, okay?
在演讲进行时,您可以佩戴这些耳机,以便更好地理解演讲的内容。
在会议结束后,请各位将耳机归还会场出口的李小姐。
您的配合将使我们的活动更加顺利,我们对此表示衷心的感谢。
每位嘉宾演讲结束后,我们还安排了互动的环节。
届时,您将有机会向我们的专家和演讲学者提问,进入深入的交流。
最后,再次感谢各位与会的热情参与和支持,让我们共同期待今天论坛的思想碰撞和火花。
现在,我宣布。
2024年华园计算认知世界智创未来主题论坛正式开始。
首先,让我们掌声有请上海市经济和信息化委员会信息基础设施管理处及人工智能发展处处长潘燕为本次论坛进行开幕致辞,大家欢迎。
各位来宾,各位专家,女士们,先生们,大家上午好。
非常荣幸今天能受邀来参加我们认知世界智创未来的这样一个论坛活动。
首先,我请代表上海市经济和信息化委员会对长期以来支持我们上海市信息化工作的各界人士表示衷心的感谢。
当前,在全球竞争的环境下,
大模型为代表的生成式人工智能等前沿技术加速迭代和演进,与社会生产的各个领域融合,成为全球经济发展的强劲动力。
昨天,总理在开幕时期做了主旨演讲,指出了上海人工智能围绕AA发展与治理率先进行了许多有益的探索,
并希望上海继续巩固优势,打造人工智能的发展高地。
上海将把人工智能的发展高地,
作为三大先导产业之一,加快打造世界级的产业集群。
上海已经取得了显著的成果,我们在产业规模上持续扩大,创新成果上持续涌现,产业生态上日趋完善,
我们在全国首创的创新型生态社区模塑空间已经落地,吸引了80余家大模型企业入驻,形成了开放、活跃、合作的气氛。
上海加大算力资源的统筹供给,以市场化机制加快高质量的鱼料级建设,加速企业的技术创新与应用落地。
面向未来,上海将与海内外企业一起共同合作培育发展上海的人工智能产业。
智创未来这个标题我们认为非常好,只有现在人工智能逐步引领了未来。
我们也很希望大家能够和各位专家、产业界的朋友们一起推进上海的产业。
我们将加快核心技术的公关,突破神经形态的计算,生成式人工智能等前沿关键技术,强化关键的硬件产品公关,系统提升认知智能核心的技术的支撑力。
我们将进一步激发市场活力,发挥政府资金、基金市场基金的引导作用。
和引导社会资本,加大对人工智能的投入,形成多方参与、携手共赢的局面。
我们将进一步强化跨学科的研究与合作,鼓励高校、科研院所加强跨学科的研究与协同创新,促进计算机科学、数据科学、认知科学、工程学等多个学科的交叉融合,共同推进认知智能的基础理论研究和应用实践。
各位来宾,今天认知世界智创未来。
汇聚了众多海内外顶尖的专家,我们期待各位专家学者、行业大咖发表深刻的洞见,畅谈创新观念,并以此为契机,增进共识,加强合作。
我们也希望华苑计算继续能发挥骨干型科技企业的移另作用,抓住产业变革的机遇,担当时代责任,加快壮大,为上海将提供更加开放的平台,更加丰富的场景,
更加优良的环境,与各位专家、产业界朋友们一起共同创造人工智能的美好未来。
最后预祝本次活动圆满成功。谢谢大家。
非常感谢潘处长的致辞。上海一直致力于打造人工智能技术和应用的高地,在这个人工智能数字赋能迅猛发展的时代,
我们对未来充满着无限的期待。相信它将为我们的世界带来根本性的、革命性的变化。
下面我们有请华苑计算股份有限公司董事长、创始人宣小华博士致辞。大家欢迎。
华苑计算股份有限公司董事长、创始人宣小华博士致辞。大家欢迎。
华苑计算股份有限公司董事长、创始人宣小华博士致辞。大家欢迎。
贡献创始人宣小华博士致辞
尊敬的各位嘉宾领导女士们、先生们。大家上午好。
首先请容许我对代表华苑计算向远道而来的各位嘉宾朋友
表示最热烈的欢迎和衷心地感谢。
感谢大家在百忙之中,
 receive本次论坛,
共同探讨并推动认知智能领域的创新发展
即至今年
华愿计算已经多年举办了
以认知智能为主题的论坛
这一值得纪念的历程
不仅是世界人工智能大会主委会
对我们的信任和认可
更是各位嘉宾一路以来的支持
在这六年里
我们见证了人工智能技术的飞速发展
也目睹了它在各个领域的广泛应用和深远影响
目前人工智能处在一个全所未有的新阶段
并且已经成为推动经济社会高速发展的核心引擎
作为一家深耕在认知智能这一领域的企业
我们的团队秉承着
创新驱动技术引领的发展理念
在过去的一年里
华研院计算驱动了更多突破性进展
例如法律大模型
以及今天将在现场发布的钢铁领域大模型等
除了大模型的发展之外
我们还通过深入研究小样本学习
鲁棒学习和知识推理等核心技术
力求让人更多的知识推理和核心技术
让认知智能为科技和社会的发展
贡献自己的更多力量
此次我们深感荣幸地邀请到
学术界享有盛名的布隆教授
多年来我们与布隆教授在线上
进入了深入的学术交流
而今天我们将有机会在现场
聆听他对人工智能
尤其是意识
读灵机的见解
为我们带来新的启发
同时我们也迎来了
来自牛津大学的Bronston教授
他将带来一场精彩的演讲
让我们对人工智能有更全面的认知
此外我们也非常高兴地邀请到
在多模态学习领域
有着深刻见解和集中贡献的青年学者
高胜华教授和
王小梅博士
他们的加入为我们的学术探讨
注入了青春活力
最后特别感谢从全球各个地区
国家和地区远道而来的各位来宾
他们分别来自亚洲
欧洲
美洲
非洲
包括了阿联酋
卡达尔
多维特
南非
埃塞俄比亚等国的一些朋友
他们不远万里来这里参加科技盛会
让我们的交流更加国际化多元化
最后我再次向各位的到来
表示衷心的感谢
预祝各位在接下来的会议中有所收获
我相信在大家的共同努力下
本次大会还将去的元术会议中
我相信在大家的共同努力下本次大会还将去的元术会议中
我相信在大家的共同努力下本次大会还将去的元术会议中
我希望这个成果让我们有一个完美的结果
也让我们的这个社会圆满成功
让我们共同期待这次盛会
将成为我们交流学习的平台
并让我们收获丰富的成果和珍贵的友谊
谢谢大家
非常感谢宣董的精彩发言
宣董呢 也是我们数学
宣布是多年来在数学
特别是应用数学领域的基础学科的研究
以及在具体产业领域的应用的坚持和执着
以及带领华愿计算
在算法技术上的滋滋追求
表示最深的敬意
在产学研结合的道路上
华愿计算不断探索与突破
致力于算法的自主创新和产业应用的落地应用
我们期待华愿计算的辉煌未来
相信其在推动科技进步和社会发展中
能够发挥更为重要的积极的作用
下面我们进入主题演讲阶段
首先我们有请牛津大学DMINE的人工智能教授
Michael Bronsted
他将为我们带来几何深度学习
从欧基里德到药物设计
他会通过对称性揭示
深度学习的几何统一
从而引入几何深度学习
这一通用的数学框架
并为我们讲解如何应用
到生物学 医学 药学设计等新的领域
大家掌声欢迎
Ladies and gentlemen
Michael Bronsted
Thank you
Thank you very much
Great pleasure and honor to be here
So in the next 30 minutes
I would like to talk about some research
that we've been doing in the past years
and a topic that we call
geometric in learning
and I hope to show you a little bit
the historic roots of this research
and how it can be used in different applications
from patients from computer vision
drug design and drug discovery
So allow me to start a little bit from far away
so from historic roots of this field
and I would like to start with a quote
So this one is from Herman Weil
who as you know
probably was one of the greatest mathematicians
of the 20th century
and he spoke a little bit poetically
about a very important concept
which is symmetry
according to him as wide or as narrow
as you may find its meaning
it's one idea by which men
through the ages has tried to comprehend
tried to comprehend and create order beauty and perfection
so the term itself actually comes from Greek
and ancient Greeks believed that
symmetry plays a very important role in the universe
so Plato for example considered that
symmetric polyhedra
that nowadays we call platonic solids
are so fundamental that
the entire world should be made out of them
and if you think of it
it's not a very crazy idea
because this is how crystals are organized
so Plato also according to a legend
had an inscription at the entrance of his
academy that
no one
not skilled in geometry
was allowed to enter
and this also gives you an idea
that geometry was
attributed a very important role
in Greek mathematics
and geometry as we still teach it
it's called the geometry
goes back to Euclid
and his famous treatise
The Elements
which is based on five axioms
or postulates
that as you know
the fifth one about parallel lines
somehow stood apart
and generations of illustrious mathematicians
tried to prove it to no avail
until came in the 19th century
early 19th century
the realization that you can actually
construct self-consistent geometries
without resorting to the Euclidean
assumption about parallel lines
and again here the credit is probably disputed
so Gauss apparently was one of the early
mathematicians to work on this topic
but he never dared to publish
because it was very controversial
this
the foundations of geometry
stood unshaken for more than 2000 years
the first one to publish
was the Russian Lobachevsky
and he almost sacrificed his career
because he was heavily criticized
for publishing these ideas
but somehow with time
towards the middle of the 19th century
these ideas became more common
and it became apparent
that other geometries
outside of the Euclidean world exist
and it created a somewhat unhealthy situation
a zoo of different geometries
that departed from these assumptions
such as fine geometries
projective geometries
hyperbolic geometries
and so on and so forth
and it required some kind of new approach
or a new paradigm
or a new mindset
that would make order in the zoo
and will permit to define
even what we mean when we say
that we have a geometry
and this new insight came from a young
German mathematician called Felix Klein
so he was only 23 years old
when he was appointed as a professor
at the University of Erlangen
in Germany
and as it was customary
and probably still customary
at that time
he was asked to deliver
research prospectus
which entered the history of mathematics
as the Erlangen program
in which he suggested
the radically new treatment of geometry
basically considering it as a space
with some group of transformations
so essentially you study
how objects change
when they are transformed by some group
and he showed that for example
Euclidean geometry rises
from the group of rigid motions
what we call the Euclidean group
so a lot of properties are preserved
or remain invariant
when we take an object
and rotate or translate or reflect it
so we preserve areas
parallelism of lines intersection
and so on and so forth
but of course it allows you
use the same framework
to consider other groups
such as fine groups
or projective groups
and in fact
he was one of the first to
also reconcile different
non-Euclidean geometries
that existed at that time
using this approach
and this created a huge shift
in geometry
in mathematics more broadly
so some of these ideas
changed geometry forever
and broadly mathematics
so more abstract ways of thinking
like category theory
or shift theory
algebraic topology
actually take their roots
from the Erlang and program
but probably more importantly
it also created a revolution in physics
where also in the beginning of the
20th century came the
the first realization that
you can derive the very laws of the universe
from these
foundational considerations of symmetry
probably the most famous result here
is by Emi Neuter
who showed that
you can derive conservation laws
in physical systems
from considerations of symmetry
which is
if you think of it
it's quite a big deal
because before that
conservation laws were
mostly empirical
so you conducted an experiment
for example
you measure energy
and you see that it's preserved
but now you could derive it
mathematically
from considerations of
the symmetry of time
and certain evolutionists
of this concept
to which while that
I quoted in the beginning
that's contributed
resulted in
a unified theory
which we nowadays call
the
interactions between particles
as well as the structure
what's called Minkowski geometry
of the space-time right?
and I think it was summarized
laconically and beautifully
by Philip Anderson
and Nobel laureate in physics
who stated that it's only
slightly overstating
to say that physics
is the study of symmetry
now you may ask
what does it have to do
with machine learning
and artificial intelligence
so I think
we have no doubt that
deep learning in the last ten years
has created the evolution
data science
and made certain problems
that remained out of reach
possible from computer vision
to a language
to protein folding
but at the same time
we now also have
this situation
when we have different
neural network architectures
that historically were derived
from different considerations
for different types of problems
such as convolutional neural networks
that originated in
image analysis
recurrent neural networks
that were derived for
time series analysis
and so on and so forth
so again as we've seen
in the later half of the 19th century
so we have this
zoo of different objects
that apparently have no connection
to each other
and no underlying mathematical principles
so people often think
that deep learning
is a kind of
collection of tricks and hacks
and not a real science
so we would like to challenge
this statement
and what we call
geometric deep learning
is a common denominator
basically a mathematical framework
that is inspired by
Kleinser-Langen program
that tries to derive
the foundations
of all these methods
from the same principles
which are invariance and symmetry
so in this case
if we consider the
problem of
supervised machine learning
so you can cynically say
that this is just
glorified curve fitting
classical example is
image classifier
so I give you
images of cats and dogs
and you want to tell
whether it's a cat or a dog
and of course this is a
two-example mathematical
obstruction of
more complicated problems
that you see for example
in self-driving cars
and in general
in AI-based perception
so basically
what you try to do here is
what is called function
function approximation
so you have some unknown function
that you sample at a few points
so these are examples of
dogs and cats in this
in this slide
so these are samples of
your function at a few points
and you try to predict
how the values of this function
at examples or at points
that you have never seen before
and it's a good question
what actually
what kind of class of function
you put in this black box
so how you choose
to approximate your function
so from late 50s already
we know that
a good candidate
are neural networks
so the first thing we know is that
the first such architectures
were devised by
Frank Rosenblatt in 1957
called them perceptrons
and this is one of the
stuffles of deep learning
when you
basically you connect
many of such layers
into very complicated systems
with sometimes billions
or even hundreds of billions
of primitives
now what you know
mathematically about these systems
that just
a perceptron with just two layers
is what is called
the universal approximation
so if I give you
a continuous function
you can approximate it
to any desired accuracy
with such a system
now this is
not a constructive result
so it's kind of
results in mathematics
that tell you that
something exists
but it doesn't tell you
how to achieve it
right
so it doesn't tell you
the number of neurons
it also doesn't tell you
how many points
you need to sample
of this function
and this actually
is quite a problem
because it appears
that
these
the standard results
that you have in mathematics
about the behavior
of continuous functions
right or some
standard regularity classes
that we use in approximation theory
they don't scale well
with dimensionality
so we run into problems
that are called
collectively
the curse of dimensionality
and in our example
of cats and dogs
the number of samples
that you will
need to show
to a neural network
to train it
to recognize cats and dogs
at least in theory
will be not only larger
than the number of cats and dogs
on earth
but probably larger
than the number of atoms
in the universe
so it's computationally
totally intractable
so we need some kind of
different type of
regularity
this is what we call
geometric priors
and historically
some of the first
neural network architectures
of this kind
maybe without
explicitly realizing it
were actually derived
from works in neuroscience
so the famous
noble winning work
of Hubel and Wiesel
who analyzed the structure
and the function
of the visual cortex
of the human brain
this brought them
the Nobel Prize in Medicine
in 1981
and they showed
that the cells
of the brain
the neurons
have local connectivity
and they share
their
connectivity patterns
so the
inspiration
from this work
was initially implemented
by Kunihiko Fukushima
in a new
neural network architecture
that he called
the neocognitron in 1980
and then
in a simplified form
on the first digital processors
by Yan Lekhan
the famous
convolutional neural network architectures
that achieved their breakthrough
in 2012
with the famous AlexNet
and this is now
one of the staples
of deep learning
that are also broadly used
in science and technology.
in the perception
and vision applications
anywhere from
object recognition
QR codes
to self-driving cars
now if you look
mathematically
what happens
inside convolutional neural network
so the main
idea was
to use
local connectivity
with shared parameters
right so instead of
allowing any
neuron in the input
connect to any neuron
in the output
and then they have
n squared parameters
inside the connectivity
here only a few
local neurons
are connected to one output
so we
hold the receptive field
drawing from terminology
from neuroscience
and the same parameters
are shared
so if I move
to a different location
in the image
I get
I apply the same
transformation
to different inputs
this also means that
you can very efficiently
map these architectures
to hardware
that has
the structure of
single instruction
multiple data
and the early DSPs
and modern GPUs are
a good example
basically
what they know
to do is
to apply the same operation
to different data
extremely efficient
and in parallel
and this also endows
your architecture
with this property
that we call
translational environments
which is very important
in object recognition
so if I try
to recognize a cat
in an image
it doesn't really matter
where the cat is located
right
I just want to say
that this is a cat
so we call this property
translation environments
or shift environments
so it means that
the architecture
has this property
built in rather than
needing to learn it
from multiple examples
this is what set apart
convolutional neural networks
from the early
models of perceptrons
and this also
allows
to break
the discursive dimensionality
and make the training
more efficient
and
attractable
now this principle
again
is just
a single
manifestation
of what we call
more broadly
the geometric
deep learning blueprint
so basically
you can choose
a different
geometric domain
on which your data leaves
you can choose
a different transformation
so in case of
convolutional neural networks
the domain is agreed
and the group of
transformations
are translations
but we can consider
also other things
so we can consider
for example
three dimensional objects
that are modeled
as many folds
or meshes
right
surfaces
in mathematical terms
and there
the transformations
are more exotic groups
of local transformations
that we call
special orthogonal groups
so this is related
so it's called
gauge symmetries
in physics
and they will
basically they describe
how to
the ambiguity
in the choice of
local coordinate frames
but probably
most interesting
symmetries
associated with graphs
and in this case
we have permutation symmetries
and graphs are
very important objects
because they allow you
to model
systems of relations
and interactions
between arbitrary things
right
so you can describe
knowledge as graph
right
as relations between entities
you can describe
for example
social networks
as very large graphs
when you describe
how different people
connect and interact
with other people
computer networks
are also described
as graphs
and also
in biological systems
you can use graphs
to describe
different interaction networks
such as gene interaction
protein interaction
or the very molecules
themselves
so what is shown
actually here
is a molecule of caffeine
that some of you
are now drinking
the atoms are
the nodes of the graph
and the chemical bonds
are the edges of the graph
and a very typical
application
that we see
in biology
right
drug discovery programs
is I give you
a molecule
that is represented
as a graph
and they want to
predict its properties
it can be a bunch
of different things
it can be properties
chemical or physical properties
such as toxicity
or binding energy
to some
from a theoretical target
the problem here is
how you actually
feed this kind of data
into a computer
right
a graph is a
topological object
we need to somehow
represent it as
an array of numbers
because this is what
a computer is able
to digest
and the first problem
that you encounter
is we don't have
a canonical way
of ordering
the nodes of the graph
right
so they can come
in arbitrary order
and it means that
we have some kind of
permitational ambiguity
that is built into
any problems
of burning on graphs
so what we can do here
is basically
we need to impose
a property that
we call
permitation and variance
and in this case
it has to say that
I don't care
how I order
the atoms in the molecule
the properties of the molecule
as a whole
are not supposed to change
right
so this is just
our convenience
our notation
how we order
these nodes of the graph
so here as well
there was an entire
class of architectures
that evolved
historically probably
in the late 90s
but more recently
probably about 15 years ago
which are called
graph neural networks
that are again
an instantiation
of this broad
geometric deep learning
blueprint
which operate by
what is called
message passing
so every node in the graph
communicates with
its neighbors
so it's received
what is called messages
and the way that
these messages are aggregated
is by a function
that is
oblivious to the order
of the neighbors
so it's permutation invariant
and if you
apply this function
at every node of the graph
then what you get
is
is permutation
equivariance property
which means that
if I permute the nodes
in the graph
then the output will be
permuted the same way
and we can apply
multiple layers
so basically
they have a deep
neural network of this
of this kind
and then the
the output of these layers
is aggregated
with permutation
invariant functions
such as sum or maximum
this provides us with
an invariant readout
now another property
that we see
in particular in molecules
that these are not just
abstract graphs
so we don't just have
connectivity between the nodes
that are abstract entities
these are actually
what we call
geometric graphs
so they have a
geometric realization
so these are graphs
that live in
three-dimensional space
so each node has
a set of categorical features
like the type of the atom
for example in a molecule
but also a set of
continuous euclidean features
so the coordinate of the atom
in the three dimensional space
and here we want
an additional property
if we subject this graph
to some transformation
such as rotation or translation
we want the output
to remain invariant
and this is to say that
no matter how I position
my molecule in space
I want its properties
to remain the same
right
so we call it
SE3 invariance
SE3 stands for
special euclidean group
so it accounts for
transformations
that preserve orientations
so it's rotations and translations
reflections are
importantly ruled out
because reflections
actually do change
chemical properties
of molecules
what is called chirality
now as I mentioned
so this is widely used
already in drug discovery pipelines
and one of the issues
when you discover new drugs
the space of potential
molecule candidates
is very large
so the estimate
for small molecules
is something like
10 to the power of 60
so it's extremely large
combinatorial complexity
you cannot really
synthesize and test all the molecules
so it's extremely large
what you try to do
is what is called
virtual screening
so you predict computationally
the properties of these molecules
and then only among
the promising candidates
you send them to the lab
and you test them
initially in vitro
and then in vivo
in clinical trials
and graph neural networks
have actually been used
very successfully
for screening
potential drug candidates
and one of the important results
was the work from MIT
by the group of Jim Collins
one of the luminaries
of synthetic biology
where they showed
that you can use
graph neural networks
for virtual screening
of new antibiotics
and as you probably know
antibiotics
was probably one of the most
important discoveries
in the 20th century
Sir Alexander Fleming
so faculty at Imperial College
where I used to work once
this probably saved
if not tens
maybe even hundreds
of millions of lives
across the world
across the century
and as we know
we are now running out of power
because we have new pathogens
new microorganisms
that are resistant to antibiotics
so a pandemic
that will claim millions of lives
that is caused by
an antibiotic resistant organism
is probably not a question of if
it's a question of when
so we need to develop
new antibiotics
and these methods
that I described before
have been used successfully
so it was the first time
I think in the last 40 years
that a new class of antibiotics
has been discovered
computationally
and then experimentally
now another
important application
of these methods
of geometric deep learning
was in structural biology
so as you know
proteins play a very important role
in practically any living system
so we in fact are
not familiar with any form of life
that is not based on proteins
and proteins are polymers
so they consist of
tiny building blocks
called amino acids
that are organized in a chain
and then this long polymer is folded
into complicated
three dimensional structure
by influence of electrostatic forces
so in the early 70s
it was conjectured by
by Anfins and Nobel laureates in physics
that you can determine
the structure of the protein
entirely from its amino acid sequence
and it's been a very challenging
polygrail problem
with structural biology
what is called protein folding
and in 2019 and then in 2020
there was a breakthrough in this field
that came actually from a very unexpected
source
which was deep mind
an artificial intelligence lab
that was never notorious
for work on biology
but they developed a program
a deep learning program
that is called alpha fold
that used these principles
of geometric deep learning
so they used in particular
equivariant transformers
to predict very accurately
at the level of experimental technology
such as x-ray crystallography
the three dimensional structure of proteins
and this completely changed the field
the way that we work because
these experiments
that were used before
that are extremely expensive
and now we can replace them
by computational tools
so I've also been working on
protein design actually
in a sense an inverse problem
with my collaborators in Switzerland
the group of Bruno Correa
where we use geometric architectures
probably some of the first
in this field
to predict the function of proteins
basically how proteins will interact
with other proteins
or other small molecules
and to use these architectures
to design your proteins from scratch
what is called
nova so these are
proteins that have never existed in nature
so evolution has never discovered them
but we can design them
in a way that
have certain functions
and these functions for example
allow them to bind
to important therapeutic targets
so we actually build these proteins
in the lab
and what you see here
one example is a protein
that binds PDL1
so it's called
program death ligand
it's important target
that is used in cancer immunotherapy
so it's
blocks a mechanism
that cancer cells develop
that allow them to
evade the
normal functioning
the immune system
that would otherwise kill
malignant cells
and another
design that is shown here
is a binder for the spike
protein of the SARS-CoV-2 virus
that famously
was responsible for the COVID-19 pandemic
but we actually tested it in the lab
also on
pseudovirus neutralization
and it worked similarly to the clinical
that was
that was
commercialized by AstraZeneca
so
we are nowadays also talking about
what is called generative AI
so these models that allow us
to generate these
beautifully looking images
right like
me journey or DALI
pont to prompt with an image
that looks very realistically
and sometimes even can win
first places in artistic competitions
so mathematically
these methods are based on
what is called denoising data
and what is denoising diffusion
so essentially it boils down
to your possibility of sampling
from high dimensional
probability distributions
that describe how the data
the data works like
so again these
kind of problems
that suffer from
the curse of dimensionality
but what you can do
you can design
a diffusion process
and learn
incremental step
of removing noise from your
by doing it
you can basically
you can reverse diffusion process
and sample data
from your distribution
and this allows us
with additional
geometric constraints
to do problems
for example in drug design
so you can generate
not images but molecules
that are conditioned
by certain properties
for example the
three dimensional structure
of your target
that is shown here
protein pocket
in your therapeutic target
and there are many extensions
of these methods
so we've been working on
extending generative models
to non-Euclidean architectures
such as
Vimanyan flow matching
metric flow matching
and it allows us
to generate new proteins
with generative models
we can model protein
as a collection of rotations
elements of special
euclidean group
and we can generate
new structures
from scratch
as you can see here
and this is an example
that shows
that we actually perform
significantly better
in terms of the novelty
basically how different you are
from the training set
to the protein
data base
and design ability
basically how
the protein refolds
when you
apply something like alpha fold
and the baseline here
is a ref diffusion
so this is
an algorithm that
recently received funding
of
in excess of billion dollars
so the company
David Baker
from the University of Washington
so you can see that
more advanced
theoretic
significantly better performance
so this can be used
already for drug design
so you can
do what is called
motive scaffolding
so you can
pick some part of the protein
that you know that binds
to your target
and then build
a bigger protein around it
so this can be used
for the development of
biological drugs
proteins that bind
the therapeutic target
and we can also do
all sorts of things
so we can do
zero shot molecular dynamics
the molecular dynamics
is these very heavy
simulations that are run on
super computers
that describe how
molecules
change over time
right so
complex quantum
mechanical systems
so we can replace
these simulations by
by genetic models
so to summarize
these genetic principles
that underlie
neural network architectures
that go back to
ancient Greece
have found
incarnation in modern
neural network architectures
and they are
really what powers
some of the
most exciting developments
in the field of AI
in the last years
so I will stop here
thank you very much for your attention
question and a comment
yeah
it's mind-boggling
so it's totally
intractable
hi
hi
professor
bronstate
your talk is actually
is very inspiring
and actually
I work on
3D generation
I also work on
AI for drug discovery
and in your talk
you mention that
you use geometry
measure to
do the virtual
molecular
virtual drug screening
and currently
most measures are
only based on
molecule
but for
for
in real applications
for different person
the same drug
for different person
may have different effect
in
in this sense
so the effect of
molecule
may be related to the human
so currently
we work on
the phenotype
related
phenotype based
virtual drug
disco
virtual drug
screening
do you think
it is
possible to
generalize to
in applications
yeah so well
it's a very good question
right so if I
basically to reformulate the question
so we
basically we have different
levels right
or biological scale
so we
we have the very basic level of
of molecules right
where we understand everything
right we can simulate
at least in principle
everything right
so we
this is
driven by
by quantum mechanics
of course you cannot
simulate on
classical
hardware
so you need to
resort approximations
like molecular dynamics
on the other hand
we have
cellular scale
right so that's where
where
basically you know
how the cell
leaves or dies
whether it's
when it's
exposed to
some
to some compound
so the gap
between the
scales is
something like
ten orders of
negative
so a cell
contains
I don't know
how many
quadrillions of
atoms
so this is
not something
that can be
tractably simulated
so we need to
breach this gap
so we
probably have
some kind of
emergent
laws or
emergent properties
in
in these systems
but we don't
really know
how to
describe them
and capture them
so I think it's
methods that
use
to predict
phenotypes
we'll probably
need to
combine both
some kind of
generic
machine learning
as well as
physics based
machine learning
physics based
meaning that
you're building
some meaningful
symmetries or
conservation
like what we
do but
it's
on itself
it's not enough
for example
using
genomic information
right or
using
cellar
staining
techniques like
companies like
recursion or
etc.
but i
you
you
my name is
daniel
marking
from
south africa
steel ambush
university
i
enjoy
to talk
but i
realize
that
you
use
graph
neural
network
with
geometr
ick
deep learning
now the
question is
how do
you
obtain
that
us
for
在深入学习的过程中
要确保系统能够给予您想要的出口
您是如何得到这些数据的数据
对于细胞的关系
我们具体设备了一些化学结构
我们知道它们的实用性
也能够使用它们
也能够使用它们的技术
例如DFT
也许是化学结构
所以它们通常是
试验的混合
好的 谢谢
有问题吗
如果没有问题
我们再邀请主席
下面我们进入青年学者的专题
首先是请出我们香港大学的副教授
高胜华博士
他今天主讲的这个内容是多模态学习
多模态学习是认知智能领域的关键技术
关于多模态机器学习的技术与应用
大家对于其在实际场景中的
到底发挥什么样的作用以及价值
一定会非常好奇也有兴趣
那么事实上
我刚刚已经说了
今天事实上是两位青年学者
首先请出香港大学的副教授高胜华
以reconstruct a world generate a new world
为主题来进行演讲
大家欢迎
请出场
各位早上好
我叫孙华高
来自香港大学
今天想要向大家介绍
我们的工作
是在3D创造
和3D制造
我的主题是
reconstruct a world
 generate a new world
这就是我对这个课程的主题
世界的知识
还有未来的创造设施
其實我們住在3D世界中,每當我們醒來,我們都看到這個世界,
而我們對這個世界的角度感受到,例如我們抱著這個標準,
而我們也感受到這個標準的角度,
而當我們走在路上時,我們也感受到這條路的角度,
所以角度在我們的世界中是重要的一部分。
所以除了在這條世界的語言中,我們也想將這條世界的3D形式描述出來,
所以我們在3D感受中研究。
另一項研究是關於年代。
最多的研究是關於2D年代的研究,最有名的研究是關於2D溶解。
而2D溶解的模式是關於GaN模式。
所以我們看到一些GaN基礎的結果。
例如,
這張是一張雞皮疙瘩,
這張是一張雞皮疙瘩,
我們可以看到,
2D畫面來看,它們的溶解不容易。
從不同的角度來看,
當我們看同一個人時,
他們的溶解不容易。
例如,
他們的嘴唇、頭髮、髮型。
例如,
這張畫面來看,
我們可以看到,
它們的手臂的溶解不準確。
所以,
2D溶解的模式不準確。
因此,
為了達到這個目標,
我們想用3D模式來溶解。
因此,
我們認為,
3D溶解其實是一個可行的解決方案,
可以溶解一個正確的世界。
所以,
我們目前的工作主要集中在3D建築和3D溶解。
接下來,
我們將展示一些這個方向的工作。
所以,
我們展示了一些房間設計、
物體溶解、
人物的模式和滑鼠。
以這文字為例,
我們想要重新建造一個高質素的世界。
以文字來講,
我們想要用3D模式來溶解一個單位的畫面。
以人物為例,
我們想要用Movies來溶解這個人物。
這就是我們所使用的應用程式。
今天我們會討論三件事。
第一件事是3D建築。
第二件事是3D形狀建築。
第三件事是畫面剪輯。
所以,
3D建築中,
最著名的一件事是NURF。
而NURF的方法,
其實它代表每個畫面是一個讀取領域。
而在這項領域中,
我們將一張圖片放在一張圖片上。
然後,
我們在這張圖片上,
計算出這個圖片的厚度。
然後,
我們在這張圖片上,
計算出這個圖片的顏色。
我們從讀取領域中的圖片表現出來。
我們將它們に計算成一張圖片的圖片,
如果你們是試用紙面,
或是點電源,
你可以分辨它的圖片是怎樣的。
我們使用MLP來代表畫面的圖片。
這就是NURF的方法。
從跟NURF的方法,
我們只能記錄建築中的圖片,
並不是這個圖片的幅度。
因此,
這張圖片的圖片的角度是不正確的。
因此,
我們為了使用這項建築,
我們也會加上一個圖示
除了計算幅度
我們還會計算SDF
然後我們會使用SDF
來計算幅度
我們還會計算
輸出的畫面
這樣我們可以
得到正確的畫面
所以這是SDF
或者SDF基礎的方法
但當我們看見SDF的幅度
我們也看到
這房間的幅度
並不是正確的
樓層的層層
我們可以看到
它們非常霧狀
原因是
我們只在比較
輸出的畫面
和輸出的畫面
而這些畫面
例如這房間
和這個會議室
有很多不同的色彩
如果我們無法
計算正確的幅度
但它們也相同的顏色
我們就無法
計算正確的SDF
因此
我們希望
在這些
幅度的
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或
或或
或者
或者
或或或
或或或
或者
或或或或
或或或
或或或或
或或或 or
或者
或 opin
��
尊
約
 режим
至
子
我也
Express
Essou
我
所以我們可以把SDF改變成幅度
這樣我們可以測試畫面
所以這是標準的圖案
其實我們提到的
是這個畫面的位置
所以我們可以使用
預測幅度的測試測試
然後我們可以測試
預測幅度和測試幅度
所以我們在這裡
介紹了幅度測試和
平均表面測試
這樣我們可以使用
預測幅度或平均表面測試
然後我們可以看到
它是比较好的表現
尤其是這個
幅度測試的位置
例如樓層、桌子
但是我們可以看到
有些細節不明显
例如椅子的層面
為了解決這個問題
可能有些原因
這些位置的細節
在測試測試時
可能性和測試測試的
可能性相關的
非常低
所以我們用了
一個器官
測試測試的
證明
我們可以看到
這就是自然的測試測試
我們可以看到
這一個器官
如果您測試測試
您可以看到
它是正常的測試測試
比如我們
一般的測試測試
是最高的
證明大約是
如果我們比較測測範圍和預測範圍
我們可以說是這樣的
例如 雷通過物體去測測範圍
而測測範圍就在這裡
例如 這裡
黑色的標準
而範圍的範圍是藍色的標準
而測測範圍不符合範圍的範圍
所以我們可以預測範圍的範圍
而範圍的範圍是由A物體影響的
這意味著我們必須要提供
範圍範圍的範圍的範圍
以解決這問題
所以我們會提供範圍範圍的範圍
以解決範圍範圍的範圍
而這裡是範圍
我們提供範圍和範圍的範圍
然後我們提供範圍
來範圍的範圍
這意味著
為了測出範圍的範圍
我們使用了延長範圍
而在範圍範圍的範圍
我們提供確認範圍的範圍
並且那著重範圍的範圍
來範圍範圍的範圍
以解決範圍的範圍
因此我們能夠達成
辨知範圍的範圍
而這範圍的範圍
通常會為了範圍的範圍
而我們還想要
以適正的範圍
來詳細展示
適正的範圍
這意味著每個空間的項目
我們都要計算對方的物體
每個項目都要計算對方的重量
然後才能達到最明顯的重量
然後才能達到最明顯的重量
我們看到了結果
這是基礎選擇
當我們介紹解析器的形容詞
我們可以更好地計算細節
例如桌子的長度
當我們介紹解析器的重量
我們可以更好地計算桌子的長度
例如桌子的擺盤
這是另一個數據
我們現在看到的基礎選擇
其實基礎選擇是由RGBD數據獲得的
而桌子的長度並非正確
所以我們可以看到很多黑色的圖片
但我們可以很明顯地重測影片
所以我們的計算
即使桌子的長度並非正確
但我們仍然可以很明顯地重測畫面
這是我們在智能設計上的結果
我們可以看到
最後一項是輸入畫面
我們的計算很明顯地與桌子的長度相似
這裡我們可以看到更多的結果
我們可以與其他方式比較
這是MonoSDF
我們先介紹一下
桌子的長度和平均度
我們的方式可以很明顯地重測畫面
以及保留畫面的細節
這是最近公開的PIPAMI工作
這裡我們可以看到更多的結果
而之前的工作其實是關於3D建築
除了建築3D
我們也想創造新的3D畫面
所以我們看到3D建築的結果
第一項是Dream3D
我們使用了3D圖片
作為3D建築的代理
在Dream Fusion模式中
第二項是房間設計
我們想保留不同的房間的穩定性
當我們創造3D建築
第三項是麥可蘭吉羅
這是上一年紐瑞夫的工作
我們想創造一個3D模式
我們想創造一個3D模式
我們想創造一個3D模式
如果我們想創造3D形狀
例如寫字或一個畫面
我們想要尋找3D形狀
所以首先
我們要用3D形狀
另外我們要ズa
而這部面是寫字的
我們所以想想想
我們想想
我們想想
其實我們想想
我們想想
我們想想
我們想想
我們想想
我們想想
我們想想
 quienes會喜歡我們
我們想想想
大家想想
這個解析器的解析器的解析是一個圖案
我們可以用一個圖案來解析一個圖案
然後我們把解析器的解析器輸入3D解析器
然後我們就可以創造3D的物體
所以這是一個非常簡單的想法
但是當我們做了這個解析器的解析
我們發現其實形狀並沒有與輸入合
有可能的原因是
我們用3D的解析器輸入3D的物體
但是這裡我們直接使用2D畫面的功能
或是寫字的功能來輸入
這兩個功能的空間不太好
所以為了解決這個問題
我們必須先把功能調整好
所以我們介紹了調整基礎的輸入
這就是一般的解析器
我們想把滑鼠從滑鼠中輸入
然後輸入滑鼠中的滑鼠
所以我們可以達到這個程度
在滑鼠中
以及達到多種模式的程度
例如我們把畫面和文字輸入
我們也把畫面和文字輸入
例如用Clip輸入
然後我們想強迫畫面和文字輸入
從滑鼠中輸入
在這所程度上
所以我們介紹了一個這樣的綜合研究
我們也想強迫
滑鼠中的交流跟用後工序的配合
我們建立了一樣的綜合研究
讓我們能夠達到不同的空間
而綜合研究的 Algun性
之後我們可以直接輸入
相片、文字輸入的列印模式
以為我們想足夠的延長
然後再輸入運輸機裡的延長
這就是3D形狀
這就是我們所作出的結果
例如,我們使用的輸入畫面
可以保持細節
例如,這張圖片
實際上,輸入形狀的質素非常好
我們也使用了寫字的結果
例如,我們使用了兩隻手
我們也使用了3D形狀的質素
這就是我們使用的3D形狀
使用了一隻輸入畫面
可以保持多個3D形狀
我們可以看到
其實所有3D形狀都正確
這就是我們使用的輸入畫面
我們也使用了細節
我們只是從網上上載了一些畫面
我們可以看到我們的方法可以很容易發動輸入畫面
我們看到了車和玩具
接下來,我們要講的是3D編輯
我們提出了一個3D的軌道和車輛
軌道和車輛的意思是
現在的自動駕駛非常受歡迎
我們可以拍攝影片
例如,這是在車上拍攝的影片
我們可以看到有很多的車輛
或玩具在這段影片中
我們想將影片創造成3D的畫面
沒有車輛或任何人在裡面
所以這是輸入畫面
所以根據輸入畫面
即使是影片中有車輛或玩具
或人在裡面
我們可以創造出3D的畫面
我們可以在任何不同的畫面中
創造出這個畫面
所以這是輸入畫面的輸入畫面
然而,這項任務
其實是在這項任務中的第一步
這項任務非常困難
我們不太了解車輛的軌道
我們沒有車輛
我們不知道車輛在哪裡
有些情況下
車輛在這項任務中有很多原因
我們看到3D和軌道前
我們可以看到軌道很糟糕
但以我們的方法來看
我們使用了2D的駕駛方法
這個就是我們幾年來做的
我們可以比較好地創造景觀
我們也可以比較好地 gesure 中國的路形
所以這就是避開手輪的挑戰
例如來看,車輛的尺寸
還有車輛的範圍是很長的
所以這是一種傳統的
例如影片的尺寸
in painting of 2d image based in painting cannot well solve these issues
because we don't have ground truth
and we just use
actually here we use a 2d Gaussian based solution
for example for this scene
we can calculate the semantics of this scene
and we can find the semantics with the 2d Gaussians
and we can identify which Gaussians correspond to the cast to be removed
and then we remove the Gaussians
2d Gaussians correspond to this area
and we also render the image
and optimize the rendered image
in this way we can achieve a very good solution
here we compare our method with other 2d in painting solution
we can see that this video is inconsistent
and this is our results
and this is video based in painting
you can also see that
it's not easy to achieve material consistency
this is result
so our method can simultaneouslyachieve good geometry and good visualization results
here we also show the geometry of our method
so that's all for all my work
actually let me summarize our work
actually we want to build the 3d world by varying the 3d geometry
and we also use this geometry to regularize the 3d generation
so
next
i would also like to thank all my collaborators
that's all thank you
so also we have time for questions and comments
with the new method you worked out
what area you are applying
one possible solution is autonomous driving
the last work
and the second application is
we are game
we can design
we can generate a scenario where people
we can use it as a scene for gamers
for game players
okay
yeah okay
thank you very interesting
can you say something about
the hardware requirements
for your method
so is this
I presume it works on a standard GPU
but can you have something specialized
currently most of our methods are run on
regular GPU for example
NVIDIA 40
okay
so any question
further question and comment
yeah please
ok
ok
ok
i think
i'm going to ask
a question
of the 3d video
what are the limitations
for the production time
and then
what are the limitations
in the production time
and then
what are the possible
deficiencies
in the production process
ok
in the production process
there is only one reasoning time
so reasoning
the modeling training is good
so the reasoning time will be very fast
okay
so
the challenge is this
that is
in the current work
像那个Dream Fusion这样
那么通过优化一个3D的3D厂
来实现一个3D的生成
那还有一种呢
是基于大量的3D的数据去生成
那现在尽管有大量的3D的数据集
已经被发布
但是它们的质量并不是很高
那么在3D生成领域
是不是有ImgNet这样一个大的数据集
这也是一个非常
如果有的话呢
将极大的促进这个生成
但是很有意思的一件事情
其实我们人在从小到大的过程中
我们并没有看到
完全看到海量的一个物体3D的数据
其实只有小的时候
我们可能玩魔方的时候
乐高的时候
我们清确地观察到了
这个魔方和乐高的这个3D的数据
但是我们其他感知场景的时候
我们只是感知了双目的数据
所以一个可能的场景是
我们具大量的双目的数据
一部分的3D的数据
去做一些3D的生成
那这个可能更符合我们人的认知
从数据的处理上
也将极大的降低对于数据的门槛
OK
If there is no further question
let's thank the speakers again
Thank you
Dr. 王小梅
I'm from the Fudan University
and Department of Computer Science
So let's welcome
尊敬的各位来宾
大家好
今天我报告的主题是多模态大模型
在行业当中的应用
首先我们讲就是通常提到的多模态大模型时
其中的模态可以是我们常见的视频
图像 文本这些
也可以是传感器信号
红外信号 雷达信号等模态
当然还有更多
多模态大模型可以从功能上划分为三个模型
可以从功能上划分为机座大模型
和特定任务的大模型
其中机座大模型更多是面向
any to any的场景
也就是说
输入可以是文本 语音 视频 图像等单一模态
或多个模态的数据
在给定指令的条件下
输出对应的模态结果
这里的输出同样可以是一种模态
或者是多种模态的组合
或者是多种模态的组合
而特定任务的大模型
是面向固定任务训练的模型
比如图中所示
为一个分割任务的大模型
给定一张图像和一个指示
比如说将图中的黑色狗的区域分割出来
多模态大模型通过对指令和图像的理解
就能够分割出对应的区域
通常引起大家注意的大模型
通常引起大家注意的大模型
通常引起大家注意的大模型
通常引起大家注意的大模型
大部分是基座大模型
它也是从最初的以Bolt GPT系列
它也是从最初的以Bolt GPT系列
Lama等为代表的大规模语言模型发展起来的
Lama等为代表的大规模语言模型发展起来的
Lama等为代表的大规模语言模型发展起来的
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
之后很快就出现了以Clip, Bleep, Flamingo, VideoChat和Lava为代表的视觉语言多模态大模型
VideoChat和Lava为代表的视觉语言多模态大模型
近期又进一步出现了以GPT-4O、AnyGPT等为代表的视觉、文本、语音大模型
而融入了如传感器信号、雷达信号等更多模态的大模型
则更多赋能于机器人和自动驾驶等领域
多模态大模型的应用非常广泛
以大模型视觉语言模型为例
以大规模视觉语言模型为例
它主要可以做理解类任务
比如常见的VQA、图文匹配、或者说图文检索、还有广告推荐等
也可以做文本、图像、视频、代码等生成任务
此外,还能赋能巨声智能的场景
比如视觉语言导航、多目标巨声问答、巨声交互等等
接下来重点讲一下多模态大模型在工业中的应用
其中最重要的任务之一就是表面曲线检测
传统的表面曲线检测模型一般是
先人工对采集的数据进行比较
然后进行训练和推理的过程
通过不断增加人工标注数据来微调模型
多步提升性能
而基于多模态大模型的表面曲线检测
可以采用大模型的功能来设计自动标注框架
进行自动标注
以及对少样本标签的数据进行增强
基于这个过程得到的大量样本
可以训练出具有更优效果的模型
实际节约了人工标注的成本
提高了标注效率
也缓解了常委分部对模型的负面影响
首先在自动标注方面
我们在实际标注中
会先对样本进行粗分类
比如将表面的图像分成全图曲线
无曲线部分曲线三个类别
在这个任务设定下
抽取三种不同类别的图像
分别输入到多模态大模型当中
利用图生文能力
得到每张图对应的文本描述
然后利用多模态大模型的in context learning能力
将三张图像
三个对应的语句
三个对应的缺陷
外加我们一个需要区分的
类别的图像以及文本描述
送到多模态大模型当中
来得到分类结果
在整个的这个过程当中
用到的多模态大模型
一般都采用流行的预训练
通用大模型
无需自己训练
然后在数据增强方面
需要对少量样本的缺陷进行增强
首先随机采取一张图像
可以是带缺陷
也可以是无缺陷
将图像
一步步进行diffusion processing
得到噪声图像
之后引入关于增强的
目标缺陷信息作为条件
和噪声图像一起
送到denodding unit中
生成带有目标缺陷的图像
这里的条件
包括目标缺陷图像
目标缺陷形态的文本描述
以及想要将目标缺陷放置的位置等一些信息
在工业场景当中除了表面缺陷检测之外
还有一个很重要的任务就是生产安全管控
以违规操作
监管为例
可以基于现有的多模态大模型算法
来设计一个
面向违规检测这个垂直场景的
大模型框架
比如图中所示
将图像送入到
视觉编码器中进行编码
同时对设计的指令语句
做tokenizer操作
这两个模块的输出
然后送入到大规模语言模型当中
生成多模态编码序列
然后这个序列与视觉编码器输出的纯视觉的编码
一起送到设计的
视觉语言对齐模块来生成
Hitmap图
再用一个轻量级的box decoder
把Hitmap解码成
对应的框
在整个模型框架中
一般情况下
视觉编码器是固定的
可以采用
Clipboard
Vimage encoder或者是VIT等
去训练模型
Tokenizer模块通常也是固定的
在训练时可以
选择采用Laura微调的方式
来更新大语言模型的
部分参数
而视觉语言对齐模块
和box解码器一个模块
可以是自己设计的
这两个需要在训练时
来更新参数
除了上述提到的多模态大模型
可以赋能工业领域中的缺陷检测
和生产安全管控场景之外
它也可以用到
智能配料管理
产品质量预判
生产闭环控制
等其他场景中
华愿以认知智能引擎为核心
构建了钢铁行业大模型平台
通过模型的训练和推理过程
来赋能工业领域的智能化
今天介绍了多模态大模型
而数据知识融合
在工业场景当中
也起着非常重要的作用
尤其在指导算法训练
提升系统性能方面
甚至有比多模态大模型更重要的作用
除此之外
还有智能人机交互和AI Agent任务调度
这些能力
构成了我们华愿的认知智能引擎
近年来
华愿与浙江大学
上海高等研究院
等新型研发机构
成立了算法联合实验室
来发展工业大模型
除了工业方面
华愿的认知智能引擎
还可以
以基座多模态大模型框架为基底
采用预训练和指令微调的方式
同样赋能法律
生命科学
文旅
材料
等其他行业的智能化
最后我们做一下总结
首先就是多模态大模型
可以显著提高数据标注的效率
特别是在样本少
和样本分布不均衡的情况下
可以通过
自动生成多模态数据
来提高模型的准确性
其次
在工业领域中
专家支持至关重要
它能够帮助优化检测方法和参数设置
指导算法训练
同而提升自动化系统的性能
确保产品质量
减少生产成本和风险
第三
华愿基于自主研发的认知智能引擎
和多年在工业领域的行业经验
开发了面向工业领域的
钢铁行业大模型平台
可以助力于表面区间检测
生产安全监控
产品质量评估等场景
我的分享就到这里
谢谢大家
小梅
你可以用那个脱稿
你再讲个三到五分钟
把你今天要讲的内容重新再概括地讲一下
录出你的笑容
她是很爱笑的一个人
所以你现在就可以脱稿了
你重新讲个
你还可以讲个三到五分钟
因为你现在时间有过多的时间
所以我觉得你不要那么紧张地去念那个你准备的稿子
好不好
你重新可以讲个三到五分钟
总结一下你今天
想讲的事情
好的
那我快速地过一下
麻烦再放一下我的PPT
其实我们主要从几个模块
首先我们要知道
模态
我们要做多模态大模型
我们要知道模态它包括什么
有什么样的模态
除了我们常见的这些
语音文本
制品之外
其实还有一些传感器信号
红外信号和雷达信号
当然这些更多的是
面向的是巨神智能的一些场景
此外当然还有一些更多的模态
我们其实也可以去采用
在我们自己的领域当中
然后就是说
我们去讲多模态大模型
那么我们就要看它的功能是什么
就是
现在大家可能印象比较深刻的基本上都是
我的基座多模型
基座多模态大模型
因为基座多模态大模型它可以做很多事情
它是一个end to end的场景
就是我可以输入
任何我想要的一些模态
然后输出
输出我需要的模态
还有一种就是说
我基于特定任务的
多模态大模型
其实这个更多的是对于一些
比方说我
我专门我这个任务我就是做检测的
或者我就是做分割的
它能够有一个很好的
有一个在这个任务的设定下的一个好的效果
但是即便如
我们仍然需要多模态的引入
如果我要去分割一个东西
我可能不是说我把所有的场景都分割
需要根据我的
特定的我想要的一些东西来进行分割
我想要让它分割成什么
我需要的哪一块的内容
这就是两种
然后我们看整个多模态的就是这个发展过程
它也是
从
大规模语言模型爆发
爆发出来之后紧跟着就是
视觉文本
因为视觉文本我们在
呃
研究领域也好
或者现实场景当中很容易拿到一些视频的
视频的信息图像的信息
大规模的
这个文本信息
就当有这些信息的时候很
就是我们的模型一旦算力提升出来
所以发展就会很快
然后再一个就是
接下来就是语音
开始加入了
就我这现在三种模态的
这个大模型
已经有很多了
随着更多的就像在刚才提到的
像传感器信号
或者是说雷达信号这些
它更多或是我机器人
场景当中
然后我们看多模态大模型能干什么我们这里是以一个
大规模的视觉语言模型为例去讲它能做一些什么事情
首先就是理解类的
理解类的就是我们常说的就是问答
问答图文问答
图文去然后或者是图文理解
或者是我的图文匹配或者是我的视觉文本的匹配
再一个就是我可以做一些生成任务
现在就是大家也都能看到各种的推出来的就是我的
文本生成我的视频生成我的图像生成以及代码生成的一些效果
再有就是巨神智能
包括就是
视觉
视觉语言导航
是机器人常用的
然后就是
对以及巨神智能
巨神智能和巨神交互
接下来讲一下就是我们华愿在这个工业场景当中我们是怎么去赋能
我们现在的这个
这个领域的
当然传统的大家可能就是我首先构建一个模型
然后我找一些人去标数据
当有了这些数据之后
我用这个模型去训练一下有一个初始的效果
然后我基于这个初始的效果初始的模型我再去标数据
然后再不断地去训练这个模型
去优化这个模型
来达到一个比较好的效果
当大模型
当基于大模型出现的时候
它就能缓解很多问题因为我们传统的方法会存在两个
比较
比较大的问题一个问题就是人工标注
然后人工标注特别是在特定场景当中
像我们工业当中一些缺陷
我们就是作为一个就是就是没有接触过他专业领域他是不他是不知道怎么去标的
他也不知道这个缺陷属于就是是有什么差异的
他是有什么产生的
所以他的标注成本特别大
然后就是再一个就是一个样本不均衡
就我们很容易面到面面对一些问题就是我们的缺陷他
本身
很重要
但是他出现的次数很少
可能一个月一次可能一年一次
但是他特别重要
如果他放在统一的我们所和其他的数据进行一起训练的时候
很容易就是由于常委分布导致的
这个
模型
模型过年核问题或者是模型在我这
我这个少样本的任务上的识别效果不好
或是
或者是检测效果不好
这样我们怎么去赋能呢
赋能我们从两块去做
然后一块就是说
我用大模型来进行这个标注
再一个就是我用大模型来进行数据增强
然后在这个就是一个大模型进行标注的其中一个场景之一就是
呃
然后在这个这个场景当中其实我们用到大模型的两个能力
这个过程当中虽然是
比较
呃模型可能
我们选
比较新的一些模型
但是我们不需要训练
可以完全自动化
然后我们再看这个数据生成
我们数据生成其实就是基于我们现有的因这个框架也是基于现有的
比较流行的
呃这个数据生成的
框架
呃
在赋能我们
我们工业领域
因为现有的毕竟是就是
open source的
所以
我们去需要把我们工业的一些图像
先去先去给他呃得费用的process
然后得到一个造成图像
然后造成图像
在基于这个造成图像
我们想要扩展的目标的图像的一些信息
包括他的文本信息
以他的位置信息
或是其他的我觉得可以
可以去增强的一些信息
包括
呃传统的当然翻转折叠也可以
也可以增加进来
和和这个造成图像
然后呃
通过这个点到人用耐的当然这是这只是其中一个
网络可以替换成其他的网络
来生成来生成带有
带有我们的目标想要想要扩充的这个缺陷的图像
但是上面一个是缺陷检测场景然后接下来这个就是说
呃是一个安全管控的一个场景
安全管控的场景当中我们这里设计的就相当于是我们面向安全管控这个特定领域的大模型
然后这个大模型去呃怎么设计呢就是
呃当然就是基于
其实呃也基于现有的框架我首先进行这个文人引扣的
文人引扣的就是开采用一些
啊啊就是现有的预训练的大规模的模型
来去做
然后然后再送到这个
把这个因斯扎克人就是我们的想想让他干什么事儿
比方说我就想让他去检测出来这里有哪里违规来告诉他
来进行编码
送到这个大规模语言模型当中与我的视觉编码先进行一个融合
在这个融合过程当中
呃
因为现有的大模型他可能已经不适用了
就是就是我如果直接用可能不行了所以我需要微调一下其中的一些参数
但Laura是其中的一种微调方式
然后得到了融合的
融合的这个信息之后
再把
我的视觉
视觉的模型视觉编码的模块送到这个
视觉语言对齐模块生成一个hatemap然后解码
从这个
解码出来这个框
其实在这一块如果是我们的
当然这个对齐模块
可能也需要一个
也可以是我们自己设计了
可以在某些场景当中甚至都可以去掉它
当然这都是这是一个基础的框架
这个就是我们的就是认知智能
引擎
平台
就是我们除了赋能我们刚才所说的工业场景当中的
表面宣言检测
和安全
安全安全管控这两个之外
还可以
还可以做
比如说
生产闭环
控制
还有
产品质量评估等其他的
然后我们这个智能认知引擎
我刚才讲到的只是就是
多模态模型的一方面
其实在智能认知引擎平台
还有非常重要的一点其实就是
数据知识融合
很多我们专家的我们的专家知识
它更能体现一些本质的信息
当然还有其他还有就是
智能人机交互和AI agent这个
任务调度
等等
然后就是大规模就是多模态大模型除了说
基于我们这个认知智能平台
它除了可以做
这个工业领域工业场景当中
我们现在其实还有
法律的
生命科学的
还有文旅的
以及材料方面的其实也在
也在
一个探索过程当中
然后接下来就是
总结总结
总结主要是三点
一点就是多模态大模型它可以赋能
很多领域
包括这个我就像刚才
我们已经介绍的这个数据标注
来提升性能
来降本增效
再就是在工业领域当中我们认为
专家知识也非常重要
我们不能因为
多模态大模型现在很火
我们可以把它用起来而忽略了最本质的这个专家知识的这个
使用
再就是
我们华苑的
自主研发的
认知智能引擎
在在工业领域
可以赋能
就是
可以赋能缺陷表面
缺陷表面检测
的场景
这个生产安全管控场景
还有
产品质量评估场景等等
来实现一个降本增效的这个
效果
郑老师
我问一个问题
我问一个问题就是
你刚刚特别强调了和专家知识的
这个融合
那一般来说在你们这个平台上怎么样把专家知识的那些内容能融合到你的模型里面
就最重要的
或者最
基本的那些方法或者算法是怎么设计的
其实有几种嘛
然后
当然一种最常见的就是规则
但是我们会有一个专家知识的
这个表征
将它的首先我们讲
就是可以将专家知识
以文本的形式来描述出来
然后表征融入到我们的
多模态模型当中
这是其中的一个思路
然后还有一些就是看专家知识它是以什么样的形式去表现出来
还有一个问题就是你现在特别强调
多模态的大模型
这边的大一般来说是怎么
显现的
它的特征是什么
或者参数的这个量是到什么样的一个级别
我们现在大基本上都是在上
就是十亿
十亿
十亿左右的一个级别
或十亿以上
如果大家没有问题的话
我们再次对王博士的精彩演讲
表示感谢
也是同时感谢两位青年学者的精彩报告和分享
下面我们进入到主题演讲的环节
我们今年还是非常高兴的
请来了美国卡内基梅隆大学集团机科学学院的名誉教授
也是95年图灵奖的获得者
美国三院的院士
Manuel Bloom教授
那么事实上在过去的五年之中
Manuel Bloom教授
一直是我们华园计算
认知智能主题论坛的重磅的演讲嘉宾
为我们的学术探讨和技术创新
始终提供了非常
宝贵的
经验和启示
今年呢我们也非常的荣幸
再次邀请到
Bloom教授
为我们带来最新的研究成果
和深刻的见解
那么下面用我们热烈的掌声
来欢迎Bloom教授
登台演讲
大家欢迎
Bloom教授
Hi Michael
I'd like you to think of the model I'm going to present to you as a model for AGI, for Artificial General Intelligence.
您知道我們的人工智能技術非常好,它們很適合各種特定的東西,
但是我們對於AGI來說,基本上,我們想要有一個機器可以解決任何問題,
它可以打球,它可以做數學,它可以做一大堆不同的事情。
因此,
我認為,
我認為,
我認為,
我對知識有興趣,
但是當我看了這個模式時,
我認為這是一個可能的模式對於AGI的模式。
所以當我談論知識時,
你可能想要思考什麼是AGI需要的。
讓我告訴你,
這其實是一場家庭討論,
Lenore Blum,
她有一個學位,
她在數學上做了很棒的工作,
和史蒂夫·史梅爾,
她和我做了一個相同的合作,
她現在也不在這裡,
但是她和我做了一個相同的合作。
我還想說,
我們在我們在我們模式上工作的時候,
我們有一個問題,
我們有一個問題,
我們想要解決,
如何,
這個模式,
是的,
這個模式有很多的過程,
每個過程中的過程中有不同的資訊,
如何讓一個過程中的資訊的重要性被決定?
這就是一個問題,
我們也提過這個問題給我們的兒子Avram,
他應該也在那裡,
他說,
爸爸,
我已經有你需要的東西了,
這叫做睡眠專家的問題,
你會看到這些專家,
他解決的問題是,
如何讓這些不同的專家,
解決他們的特定的資訊的重要性?
好的,
我們開始吧,
讓我告訴你,
為什麼我對知識有興趣,
我曾經在第二年級的時候,
我的老師告訴我媽媽,
我曾經在第二年級的時候,
我的老師告訴我媽媽,
我曾經在第二年級的時候,
我的老師告訴我媽媽,
我曾經在第二年級的時候,
我曾經在第二年級的時候,
我之後 그럴時代可能 doin高中 Shanghai,
我 warming that I might be able to get through high school,
但但vez o災難北方,
但不會想我能第二年級到上大學,
你讓對手感覺很不滿意,
這麼陽脆,
哦,謝謝,很好的,
你讓對手感覺很不滿意,
你讓對手感覺很不滿意,
不想在我在學習而更輕鬆 slices yourảng賴 Inter threatens her very manage to mess me.
所以我再問了爸爸,
我能在說話時的咩可以 checks your'änn 呢?
我探討,
 sailing throught drei處跑 ettric,
我一直對我,
我一直lect on slug,
他告诉我,你知道吗,如果你明白你头上的事情,你可能会更聪明。
我认为这是一个很棒的想法。
这是一个很棒的想法。
我记得我四中学时,我走进森林里,尝试了解我头上的事情。
你知道吗,尝试不太好。
我感觉我头上有个小人物,看着我眼睛,
我想要解释我头上的事情,我必须要解释我头上的事情。
所以我认为这样的事情不会有效。
我说了所有的……是的,你会看到我说了所有的事情的原因。
所以……
所以我实际上能够通过高中,甚至到高中。
我非常幸运,我能够去MIT。
当我在那里时,我举了一个课程。
当我是中学生时,我举了一个课程。
当我是年轻人时,我读了一本博物馆的文章,
实际上我可以解释到这个问题。
我读了一本博物馆的文章。
我可以填上了他们教的。
中学生时,我读了一本博物馆的文章,
实际上我可以填上他们教的。
在中学生的时候,我可以显示我自己的罪名,
通过罪名,我可以认为我自己是高中的高中生。
所以我和高中生的高中生,
我可以读到最后几句幻想,
有趣的是,正方形智能有了正方形智能和正方形智能的輸出,
而當時的科學家說,我們沒有看到正方形智能。
所以,賈雷克和彼茲說,它必須存在。
這就是一個有助於數學的例子。
事實上,他們終於發現了正方形智能的輸出。
我想提出,賈雷克是我最重要的導師。
他是個很好的人。
任何事情我都想做,他都支持。
他讓我做我所想做的事。
他鼓勵我做。
我做得很好。
只有一次,賈雷克對我說不。
我幾個月後開始跟賈雷克合作。
我告訴他,你知道,我真的想做的是明白意識。
首先,
賈雷克在他第一次和唯一一次的生活中,
他對我說,你不會研究意識。
所以,就是這樣。
意識研究只是不允許的。
直到15、20年前。
現在已經允許了。
意識研究有很多講座。
意識研究做得很好。
我非常幸運地可以說,
我認為我可以給4th grader
他想要的知識。
好,讓我看看,
看來這不太合適。
對,對,可能會合適。
你看,我有幾個圖片。
我還不確定我用哪個來開始。
AI模式。
對,好的,很好。
對不起,我只是學習用它。
好的。
那現在呢?
我會,實際上,
給你解釋什麼是模式。
我非常滿意模式。
讓我開始,
我會告訴你,
4th grader
想要的知識。
所以,15年前,
Lenore告訴我,
你知道嗎?
你一直想要研究意識。
你現在可以做到。
終於,你可以做到。
她給我一本書,
Bernie Barr的書,
在Berkeley的科學學院,
在Berkeley的科學學院,
給了一個非常好的解釋。
所以,這個解釋,
我會給4th grader,
是Bernie Barr的解釋,
關於我們頭上的事情。
它叫做
Global Theatre Model。
他看著你頭上的事情,
作為一個戲劇。
一個戲劇,
有很多觀眾。
而觀眾們,
正在看著
這個戲劇的說話。
如果這個戲劇有問題,
觀眾們,
有一個很重要的問題,
觀眾們,
會嘗試解決它。
也許,
其中一個觀眾,
會來幫助解決它。
所以,
這個戲劇的模式,
就是你現在看到的,
觀眾們,
正在看著
這個戲劇的說話。
所以,
這個模式,
非常好。
呃,
我對這個模式的
唯一問題是,
Bernie Barr
沒有解釋
觀眾們,
該如何決定
觀眾們要
上台。
你看到這些觀眾,
你知道,
這種問題,
可能是
你,
你知道,
你去過
一個,
一個,
一個派對,
呃,
你見到一個,
你知道,
那些人,
呃,
啊,
你,
呃,
你不記得他們的名字,
對吧?
不可思議,
這常常發生,
也有的,
所以,
呃,
這個模式,
呃,
某個觀眾的成員,
非常尷尬,
想要這個名字,
在台上,
說,
她的名字?
呃,
這是傳播,
給大家,
但是,
呃,
這就是,
呃,
這就是,
呃,
她的名字是什么
也许他们没有找到
也许有人会记得
我不知道她的名字
但我记得我认识她在上海
那是大家的播放
然后别人现在可以来说
我记得了别的事
所以她的名字开始 with S
这就是你可能会有的
你知道这个很糟糕的事情是
在我的经验上
这些成员会来说话
但你知道你不懂
你回家的时候
太晚了
你会听到她的名字
她的名字是什么
啊,Tina
好
但是
她的名字是Tina
Tina
太晚了
还是一个很美好的事情
知道你的处理员
观众们都在研究这个问题
你甚至没有意识到
它只是从不知不觉地出现
你不知道谁得到的
你得到答案
因为处理员在寻找它
对不对
我确实是处于心理罢默
对
我看到一个人
他某种意义都拥有了
他的眼睛是谨慎的
这是我的眼睛
我的眼睛和我的眼睛
他对多少有关系
对于自己的眼睛
他 download 的就是
这个人的眼睛
这个是他的眼睛
对
听了吗
听了吗
对了
对了
這個討論,我不確定我會有多長時間,我將討論作為一系列小討論,每個討論大約有五分鐘,
五分鐘後我會問一些問題,然後我會進入下一個討論,你隨時可以離開,
你會知道你可以離開一個討論與另一個討論。
這是第一個討論,這個討論是關於為什麼要研究意識?
因為它是一個重要的開放問題,
意識是今天不被解釋出來的,它是紐頓時代的能量的源頭。
在紐頓時代,人們以為可能有森林燒焦,但這不可能,這不可能發生。
然後還有生命的源頭問題,
這些是很重要的問題。
這是一個很重要的問題,
我認為意識問題會被解釋出來。
所以我們,Linor和我,
相信意識有一個解釋,
從經典的定義物理來解釋。
你不需要,
你不需要任何光芒來解釋它的源頭。
現在的一些理論,
你不需要任何光芒來解釋它的源頭。
所以我告訴你,
你知道康威爾的《生命遊戲》嗎?
這不是一個遊戲,
它是一個智能電腦,
具有特定的定義規則。
這些定義規則,
這是一個定義規則。
每一刻,
一個核心可以知道
是否活著還是死亡,
根據它的鄰居是否活著還是死亡。
這是一個美麗的遊戲,
是一個美麗的例子,
是一個定義性的組織。
所以在這個《生命遊戲》中,
我們可以建造一個智能電腦。
你可以建造一個智能電腦,
你可以建造一個自動重生的智能電腦。
你可以實際地建造一個世界的組織,
就像我們自己一樣。
我們看著它,
就像是神一樣,
看著這個世界,
我們現在可以知道
世界將會什麼樣的,
未來將會什麼樣的。
但是在這個世界中的組織,
它們無法告訴我們發生什麼事,
因為它們需要進行計算。
因此,
我們使用這個,
例如,
為了能夠解釋自由意志。
這是一個舊的問題。
你知道自由意志的問題嗎?
Samuel Johnson說,
所有理論都是對自由意志的問題。
他知道紐頓法,
所有科學都是對自由意志的問題。
所有經驗都是為了自由意志的問題。
這真是一個美麗的奇蹟。
你知道,
這個問題已經存在很久了。
我們在爭論,
我們可以解決它,
但哲學家並沒有解決它。
他們沒有考慮到
解決自由意志的問題。
所以,
我可以給你們解釋
自由意志的問題的解決方案。
但主要是
解決自由意志的問題的時間。
我們認為,
一個完全決定性的世界,
就像遊戲中的生活,
可以支撐出奇蹟的意識。
我們認為,
理解意識會
幫助我們從AI到AGI的解決方案。
我希望你們會明白我的意思。
所以,
什麼是意識呢?
當然,
我問了 chatGPT,
什麼是意識呢?
但忘記了。
意識就是
你所看到的,
你所聽到的,
你所感覺的。
你知道,
你晚上睡覺時,
你是意識你的夢想。
你是意識你的夢想。
而當你不是夢想的時候,
你並沒有意識到任何事情。
所以,
那就是意識。
而問題是,
什麼是TCS,
理智電腦科技,
要求理解意識呢?
所以,
理智電腦科技
採取資源限制。
而這讓
理智電腦科技
解決了
很多基礎邏輯的問題。
即使它沒有任何大規則,
它也能解決
一些精彩的邏輯。
我提到
我已經提到
自由自由的問題,
這個問題
已經有幾千年了。
你知道,
佛洛伯已經提到
自由自由的問題。
而這是另一個問題。
佛洛伯
提出了一種方法
來製造
幾乎無數的數字。
它叫做
中間平均的方法。
他說,
以無數的數字為例,
把數字平均,
然後,
當你平均,
你會得到2個無數的數字。
把中間平均的數字拖出來,
它就會變成無數的數字。
你想要,
你把數字平均,
把中間平均的數字拖出來,
然後繼續這樣。
後來,
它就會停止
製造無數的數字。
後來,
它會滾動。
但在大多數的情況下,
你肯定可以
一直用它。
所以,
佛洛伯提到,
任何人
認為
有無數的數字製造的
無數的數字,
當然,
是一種罪的狀態。
這是一種罪,
因為,
你知道,
它是一種機器製造的。
它不可能是無數的。
就像,
你知道,
你可能會決定
用雞蛋的數字
作為無數的數字。
種子可以告訴你
在無數的數字製造的地方。
然後,
從那一刻起,
你開始製造。
你知道,
如果種子是
相當大的數字,
那一段會看起來
是無數的數字。
以我們所知,
雞蛋看起來是無數的數字。
但是,
那是無數的數字嗎?
這是一個亂象。
你怎麼,
你怎麼可以
告訴佛洛伯
如何解決它呢?
而那是
卡爾摩戈爾夫。
卡爾摩戈爾夫
提出了一個方法
嘗試脫離這個地獄。
他說,
他提出了一個解決方案。
他的解決方案是,
他說,
一個數字,
一個無數的數字
是無數的數字。
現在,
我們稱之為
卡爾摩戈爾夫無數的數字。
如果任何一個計劃
製造出這個數字,
它是無數的數字。
所以,
基本上,
一個數字,
一個無數的數字
就是卡爾摩戈爾夫無數的數字。
如果一個計劃製造出來,
它必須要,
必須要記錄這個數字。
那是一個很棒的解決方案。
但是,
它也很棒的,
卡爾摩戈爾夫無數的數字
是無數的數字。
實際上,
很難證明
這個數字是
卡爾摩戈爾夫無數的數字。
我可以證明
卡爾摩戈爾夫無數的數字
是卡爾摩戈爾夫無數的數字。
我可以證明
這個數字
不是卡爾摩戈爾夫無數的數字。
因為我只能找一個短的計劃
來製造一個大數字。
但是,
證明這個數字
是卡爾摩戈爾夫無數的數字,
其實是不可決定的。
你不可以這樣做。
所以,
這是一個很棒的解決方案,
但這不是我們想要的。
所以,
這就是這麼努力的做法……
是的,
這個策略alling
有福祉
的方面。
是,
這個策略alling
也是負責
 heraus
滴
供
 Feeling
感
情
你也有设计计算数
我还认识一个中国的数学家
华洛根
他也有设计计算数
但是这设计计算数是如何设定的
对于意识上的设定
所以我没理解
是的 很好
我仍然在试图给你们展示
这些设计计算数能够解决
现在我想和你们解释一下
这些设计计算数的解决方法
安迪耀
您认识的秦华
非常好
因为他对我们解释了
如何解决问题
这是实际的
我有一个很好的设计计算数
但是这是安迪耀
非常重要的设计计算数
他的解释是这样的
我们会说
设计计算数
它是分辨
什么是有效的
有效的设计计算数
和没有有效的设计计算数
你认识的
托宁分辨了
有效的设计计算数
和没有有效的设计计算数
但设计计计算数
使用了
有效的设计计算数
和没有有效的设计计算数
所以安迪耀的解决方案
是这样的
我们会说
设计计算数的设计计算数
是适当的设计计算数
是真的适当的设计计算数
如果你能够
能够用适当的设计计算数
从适当的设计计算数
你能够用适当的设计计算数
设计计计算数
但没有有效的设计计算数
能够分辨
适当的设计计算数
从适当的设计计算数
适当的设计计算数
没有有效的设计计算数
没有有效的设计计算数
如果有适当的设计计算数
和适当的设计计算数
能够说
哪个是哪个
这就是适当设计计算数的定义
这是一个很好的例子
因为它显示了
如何有效的设计计算数
为了这样的设计计算数
这是一个很强大的设计计算数
如果你可以自动生成
就用适当的设计计算数
但没有什么能够说明
适当的设计计算数
是否适当的设计计算数
是否适当的设计计算数
在这里
这是一个
所以
我只是给你
那么这是两个例子
适当设计计算数
和适当设计计算数
在解决这些问题中,数学学家都在尝试解决这些问题。
有很多类似这些例子,所以我不会去解决它们。
但是,我对知识有兴趣时,
我认为,也许这种方式可以使用来帮助解决知识。
我认为它是有用的。
我们可以使用它来理解知识,
并在过程中建立一种方法来解决AGI。
我们的知识模式是知识启动机器,
以及CTM。
CTM是知识启动机器。
这是一个设计的知识模式。
所以,这个知识模式,CTM,来自于两个东西。
它来自于启动机器。
你知道,启动机器有一个非常不同,
非常简单的设计机器,
来解决数学问题。
非常简单。
你知道启动机器是什么吗?
非常简单。
它是一个非常简单的设计机器。
它让你明白。
邱凌导师表示,
有些功能不可计算。
整个问题,不可计算。
那是一个非常简单的设计机器。
所以,我们想要这样的设计。
我们想要这样的设计。
我们将...
我们想要...
我们想要
一种非常简单的知识模式。
它必须是简单。
你知道,第四期生想明白知识。
它必须是简单的,
要让他明白知识。
其他问题是。
其他很多关键的事情是,
CTM来自于...
它来自于,
它来自于,
而是根据脑子的认识
所以这是根据脑子的认识
但是也是一个非常简单的模式
认识学家不喜欢我的模式
太简单了
脑子里面有各种各样的东西
你会看到有短暂的记忆
但是认识学家告诉我
我们有很多不同的记忆
所以你必须认识这是一个非常简单的模式
试图理解意识
这是一个正式的模式
让我们来看看
Turing的模式
这不是一个Turing设计
CTM不是一个Turing设计
我们只是把Turing设计放进去
因为我们是根据Turing的建议
所以要保持它简单
保持它简单
CTM是一个巨大的系统
的连接连接设计
观众在这里
想想这些观众
其中一个是Google
另一个是ChatGPT
其实我会建议
这个模式是复杂的
我们可以建造
观众应该有很多很多Google
很多很多Google
很多ChatGPT
因为你知道
一个Google复杂设计
每次你有一个问题
你可以问这个问题
另一个Google复杂设计
可能将问题放在前面
让它成为学生的结果
另一个
让它成为
现在出现的
2024年
所以你可以有很多Google
很多ChatGPT
全部都是
非常有能力的
观众
所以
所以
我也要说
这个模式
是
这个模式的重要性
是它是复杂的
是一个复杂的
数学模式
而复杂的开始
是时间
时间在线
是100秒钟
是10秒钟
是10秒钟
这是脑的旋律
所以我们将把它
成为脑的旋律
而
而
而模式有一个很短的生命时间
你知道
Turing机器
它们可以永远持续生命
但这种模式有一个很短的生命时间
而这很重要
因为复杂的数量
是相等于
模式的数量
它不能太少
它不能太少
不必太多
不必太多
但是
我们只要让事情简单
我们将它
我们将它
数量的剂数
将它的剂数
等于计算器的数量
当你做这些
那
它会变成
CTM
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
那
在这次的中心车车工会活着的时间上是33年
所以我超过了生命的时间
好
我看看能不能继续
所以我们如何检查我们的模型
我们如何检查我们的模型是正确的
这是对我来说非常重要的
检查我们是正确的
我们的数学家很幸运
因为我们知道证据
我们可以作出一个说明
然后你无论能否证明
还是不能证明
证据是非常重要的方法
证据是非常幸运的
因为它可以做试验
实际上是试验和证据
所以它可以检查答案
但我们如何检查
我们如何检查
我们没有一个正确的
知识的定义
您知道您是知识
我知道您是知识
因为我知道您的脑子
像我一样
所以您必须是知识的
但是我们如何证明
即使定义不是很明显
因此,知识游行机的目的
将是实际上
证明知识
我们将专门证明
正确的
您可以与我辩论
是否正确的定义
那么我们如何证明
我们将要确认
我们将要确认
定义是非常非常简单的
是非常简单的
应该是非常简单的定义
要使定义是非常简单的
我想提醒您
我们试过很多个定义
但它们没有成功
很多个定义失败
失败的意思是
他们没有做足够
所以我们要改变定义
我们要改变定义
我们要改变定义
我们要继续改变定义
变得更复杂
然后我们把它都搞砸了
然后我们试了另一个型的定义
我们试了另一个型的定义
意识游行机
是一个明明的定义
我认为
它要有决定性的性格
我真是想
它要有决定性的性格
因为我想示范
自由在决定性世界上
是有可能的
但它不能
CTM的定義是失敗
雖然看起來比CTM的定義更簡單
但它是失敗的
所以失敗的意思是什麼呢
當你沒有意識的定義
你會如何
失敗的意思是什麼呢
所以我們非常幸運
有一位教授
當時他是一位
哲學教授
是哲學教授和數學教授
他說
任何一個意識模式
應該能夠回答
這十六個問題
所以他發佈了
十六個問題
他希望任何一個模式能夠滿足
所以這就是我們的標準
我非常開心地說
CTM的定義
不像定義的
非常簡單
我們認為
它能否回答所有他的問題
我們必須讓他同意
但我們看看他會否答答
我相信它會回答
所以
任何時候
是的
任何時候有人建議
把機器複雜
好的
所以
我提供了一個CMU的課程
在意識模式上
然後我提供了幾個課程
在Peking U
哦
那裡的孩子很棒
我喜歡
這些
這些年輕人
在Peking University
所以
我們給他們工作問題
他們要解決工作問題
這顯示了他們明白
CTM的運作方式
然後我們挑戰他們
我們挑戰他們
要找到
CTM應該可以做的東西
而它需要改變
才能做到
在每個情況下
當他們提出建議
他們都提出建議
在每個情況下
我們都能顯示
你們不需要
對CTM做任何改變
讓它做到他們想要的
好的
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃
呃呃呃
我们认为意识就是那种鸟类
CTM是唯一的
好,所以这里有些东西关于CTM
首先,有很多模型
意识
全球演艺模型
每个模型都有一个主要行政人员
一个在上面的行政人员
你知道,国家有总统
乐团也有行政人员
总统,总统,领导,领袖
我会提出有趣的事情
我告诉你,默克勒是我的训练
非常重要的训练
每年,美军研究院
支持他的工作
总统会来跟他谈谈
然后,沃尔恩会来跟我们谈谈
我记得很奇怪的事情
总统说,在军队里
你可能会认为
军队的总统
决定军队要去哪里
不,军队是军船
那船船就是他军队
军船自然在军船上
这方面我
我今天刚刚分析过
如此多的事情说来这里,军队是军队
我时间有多少?
好,我们继续
我们继续
好,我们继续
第二个
首先,有什么问题?
请立即站起来
是的
您说,每个模式的解决方式有16个问题
那么,您的模式解决了多少问题?
我们认为,有个问题
每个问题
并有许多的关系
一个是我们不明白的问题
又一个是我们不明白的问题
我们不明白的问题
我们认为我们已经解释了所有问题
我们认为我们已经解释了所有问题
我们还在等着凯文·米歇尔
给我们他的选举
好的
我已经提及了
谢谢
CTM採用了,如我所說的,
由Turing的思維方式去計算。
CTM並不是Turing的機器,
但它是一個非常簡單的模式。
它採用了巴爾斯的戲劇模式,
採用了意識方式,
你會看到它看起來有點複雜,
但這就是科學科學家所採用的模式。
我所展示的模式
主要來自於數學和智能科學。
這就是巴爾斯的戲劇模式。
我可以告訴你什麼嗎?
很複雜吧?
那是短暫記憶體。
這是舞台。
輸入從左邊進來,
它們走到舞台上。
在舞台上的任何東西
都被播放到下方的處理器。
最後,
在舞台上的任何東西
都會產生出現出現的反應。
你可以看到,
在舞台上的主管主管
正在舉行表演。
好,
這就是智能Turing的機器。
看起來不太簡單吧?
嗯。
好,
但無論如何,
它也有一個舞台。
但是這裡有重要的
和重要的差別。
輸入不在舞台上,
而是在一些專業的處理器上。
你知道,
你有一個視覺處理器
把東西從你的眼睛裡
帶進來,
而產生了
我們稱之為
什麼樣的
噪音。
它們是
處理器的
輸入處理器,
輸入處理器
傳送到
手臂和腿部。
那裡有一個上方的樹,
就是輸入處理器的方式。
輸入處理器
在比賽中
會比賽,
他們會比賽對方。
他們會比賽對方的比賽,
而那裡有一個非常具體的
正式的方式,
讓輸入處理器升級,
然後任何輸入處理器
都會被播放到
所有其他輸入處理器。
那麼,
CTM的定義。
那麼,
我不會寫出
正式的定義,
但我會告訴你
最初的方式。
那裡有時間,
在TICS,
0, 1, 2,
到T,
10到7的TICS
是CTM的10到7的輸入處理器。
這裡是CTM的定義,
1, 2, 3, 4, 5, 6, 7,
7個元素。
這就是
數學家的定義,
對嗎?
這就是7個元素。
短期記憶,
這必須被解釋,
對嗎?
短期記憶,
長期記憶,
10萬個輸入處理器。
這裡是短期記憶,
以獲取資訊。
這裡是傳播的下降點。
這裡是左邊的輸入,
右邊的輸入。
與傳播器連接的連接。
這智能轉換機工作方式,
一開始它沒有連接任何東西。
沒有連接.
你知道,
我們在使它很簡單的地面上需要的,
我們不知道哪個連接需要。
而且我們不能連接所有的傳播器,
因為它們都是10到7的傳播器,
如果我們連接它們全部的話,
它們將會有10到14的連接,
而它們只能有10到10,
在脑中的Neurons只有10到10的Axons
所以Neurons不能有Neurons
只有1000分的Processors
2000分的Processors可以联系
好,那么这里是CTM的定义
然后这里是均衡的定义
我提过了,内部是从眼睛或耳朵 或什么的
到专业的Processor
这里有几个专业的Processor
它们把你所看到或听到的东西转成一块
所以所有的Processors 都在我们称为Brainish的语言中
英语、西班牙语、土耳其语、Brainish
这就是Processors用来说的语言
他们自己用语言来说
并不是建立的
这个部分是有趣的
因为这个部分已经存在很久了
你认识的George Miller 认识这个部分
第一次,他说
你可以记得,有七个部分
他说
实际上,电话的数字应该是七个数字长
因为如果有人告诉你
你的电话号码
你可以保留在你的头上
你能够在你的头上录制
直到你能够找到纸纸和纸纸
来写下来
所以有一个特别的
在Barr的模式中
有一个特别的Processor
来录制
在这里
语言录制
你可以取得一个七个数字的数字
你能够记得它长足够长
才能找到纸纸纸纸
那就是Visual Spatial Sketchpad
所以有一个非常好的理由
相信这些在脑子里存在
当然,它们不在CTM
因为那是我们不需要的复杂
我们可以用
我们可以用普通的复杂
来做它
让我给你们看
两个模式之间的区别
这是什么
哦,对,这里是短期记忆
这里是长期记忆
这里是Barr的模式的
意识性事实
就是当入口进入阶段
它们过去
你会意识到它们过去的
在我们的模式
意识性事实
所以我们的模式是相当不同的
在我们的模式
我们说CTM是意识性
就是意识性
所以这是一个正式的
数学定义的定义
意识性
你意识到
所有的复杂
所播放的东西
我们认为
这个解释
至少是一部分的解释
意识性
每个复杂
都知道它所播放的东西
如果任何复杂
负责意识性
或任何一系列复杂
它都知道
它所播放的东西
这不是完全的定义
这就是意识性的定义
注意力
意识性的感觉
这是另一个问题
这叫意识意识
我可以告诉你
这就是世界模式的定义
让我试试这样做
这里有点有趣的东西
当婴儿生长
我想想
当婴儿生长的CTM脑
当婴儿生长的CTM脑
有什么事
这些很
这些复杂计划
并没有联系
他们从没有知道
他们的复杂计划
他们的复杂计划
我提到复杂计划了吗
不 我没有
是的
对
对
对
对
复杂是很重要的
复杂是很重要的
因为
因为
George Miller定义了复杂
但是这是一个很不公平的定义
这是复杂的定义
它是复杂的定义
它是复杂的定义
它是复杂的定义
它是复杂的定义
它是复杂的定义
它是复杂的定义
它必须是
60个字的词
最重要的
的是那个
会让你确定出来的
重量
就是
进度
在结束的时候
回向最高的一部分
这个重量
就是最高的到位的錄像
它有一个调味
就是适当度的数
这就是适当度的数
那个
老师们不喜欢
他们说
那不是调味
这是一个例子的情绪
但他们不明白
我们想要谈谈
绝对价值的数量
好语言是
压力
我们想要谈谈
绝对价值的数量
好语言是情绪
这就是一块
所以
公正的定义是知识
公正的定义是一块
并且
当一块的定义
并没有任何
压力和情绪
并没有任何压力和情绪
这是绝对重要的
这是绝对重要的
你知道
你也知道
你也知道
你也知道
你也知道
你也知道
你也知道
你也知道
在一个压力的定义中
有机会
你也知道
你也知道
这些至于 emphasized
我给你还是个例子
我给你还是个例子
首先
首先
我们开始
这就是儿子
儿子生长
儿子生长
处理器
不连接
不连接
他们不知道
没有什么
没有什么
不知何处
不知何处
凡是不知何处
但每个瓊子
大约是1或-1
这就是正常的
重量是非正常的
正常的数字是1或-1
所以没有太多发生
除了在婴儿时
某个过程中
有一个计算器
有很多计算器
特别计算器
像是食物的计算器
还有一个是饿的计算器
另一个计算器
有关痛的计算器
你知道吗
越多痛
越大重量
另一个计算器
是吸气的计算器
在婴儿生长时
计算器认识
婴儿需要吸气
开始提升重量
提升重量
提升重量
提升重量
提升重量
直到它到台上
然后叫出
我需要吸气
有趣
有10亿的计算器
每个计算器的压力
每个计算器的压力
每个计算器的压力
每个计算器的压力
在吹响
为了夺取
10到7的1
要比10到7大
要比10到7大
或至少
10到7大
或10到7大
或10到7大
是一个非常大的压力
你必须必须
一切都要到台上
是一个非常大的压力
而这是我们需要的
压力的原因
而这是我们需要的压力
就是认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
认识
所以每个处理器只能做任何它能做的事
它也不知道它能做什么
有一个处理器是用来处理腕部和腿部的
所以它是用来处理腕部和腿部的
我看了一下
是的,当婴儿出生的时候,他们的腕部和腿部是处理的
其他呢?
我们预测过它可能会呼吸和吐
这些是它能够做的
我们可以在谷歌上搜索一下
是的,当婴儿出生的时候,他们会呼吸和吐
它可以做任何事情
其中一个能够做的事是叫声或哭
这就行
婴儿可以做到
婴儿哭了,他会吸收气泡
这是婴儿的第一个学习经验
哭了,你会吸收气泡
后来,它会发现每次它有问题
如果它哭了,它会解决问题
如果它哭了,它会解决问题
这很好
这有几次发生,婴儿就知道该做什么
它的狀态会警醒去了
婴儿就知道该做什么
它 weed
婴儿怀念
婴儿言语
婴儿负责
婴儿要求
以前,我想 DAY comple-
婴儿是经常起乩的
定于 land
说婴儿否然
在右边,知识事件是播放的结果,播放的接收。
在左边,没有具体的方法把信息送到舞台上。
巴尔说的就是,制作人们联合来决定要送到舞台上的重要性。
他不具体的说明。
这对我们有很大的担忧。
所以,在定义的定义中,我们给出一个具体的定义方法来决定这个决定。
所以,这就是一个很明确的竞争,让我们在短时间内进入短时间记忆中的部分。
巴尔的模式是,没有世界的模式。
世界的模式是非常重要的,
因为你真的没有看到世界。
你的眼睛看到了世界,但这并不是你认为的世界。
你的疾病只是看到了一些很明显的东西。
你正在看到的主要的东西是褪色。
而孩子,孩子只看到了一个灰色的褪色。
这是他的世界,这是他知道的世界。
它将开始把这个世界做得更具体。
因为你知道,它会做出预测。
世界模式是很重要的。
整个世界都在继续预测下来的东西。
孩子看到了一个褪色的褪色的褪色。
所以,自然的预测是,
在下一个褪色的褪色时,
你会看到同样的褪色的褪色。
你看到了一些东西,但你不知道下一个是什么。
所以,它会是一样的东西。
但是,你发现的东西……
我的时间到了,是的。
你发现的东西是……
我已经完成了。
你发现的东西是……
所以,孩子发现的东西是……
它不太……
它预测了同样的褪色的褪色。
但其实,那褪色的褪色似乎移到不同的位置。
它会发现它的头部的移动移动了那种褪色的褪色。
而这就是世平的开始。
慢慢地,
你开始了,
发现世界的观点是一个好的观点。
你知道,目前,
这个观点,
这个世界的模型,
已经发展得那么好,
你感觉到你看到了世界。
好……
那是……
世平的无形模型。
它必须在那里。
世平的模型。
中央行政部在Barr的模式中,没有中央行政部,没有中央行政部,所以这是非常有趣的,因为我们所知道的所有组织有些什么在头上,所有的意识模式有些什么在头上,
但是这并不合理,因为如果有一个任务要做,没有中央行政部能够知道这10亿人中,谁最好能够解决这个任务,谁最好能够,谁有时间,谁有兴趣,谁需要知识,没有中央行政部能够这样做。
你必须要有一个不同的决定方式,而决定方法就是这个中央行政部,所以我浪费时间了,所以我只是说说中央行政部是一个非常好的方法来把正确的信息拿到头上。
但是,中央行政部的好处是,这个竞选中,这个竞选中有一个可能性,就是一个特别的部分,从一个处理器上移动,是比它的重量相对的。
这并不是对于球赛和球赛的事实,而是对于中央行政部的事实。
还有一个很简单的事情,
你能做到的事情是,这是一个决定决定的竞选中,你做的事情是,每个处理器都有自己的重量,
而当两个处理器对着对方玩耍时,决定者得到所有敌人的重量。
如果你这样做到头上,那么在头上你会看到,可能性是他们应该的。
这是一个很好的情况,因为并不是无论处理器在哪里这种情况,这一个处理器的重量完全向着后手。
无论处理器在哪里在,无论处理器的重量都都在选择但它完全确定,处理器的处理不在用或不在处理的地方。
这很棒吧?
因为,后面的答案是,你一定不能将处理器移动。
你可能能移动球赛 игр手。
但不是用過程器
而以此而言,這就行了
無論如何,這就是了
好的,非常感謝
時間限定了
什麼?
可能是因為時間限定了
當然
我們沒有問題
沒有問題,請進來
因為博士的討論時間快到了
如果有興趣的話,可以打電話給我們
接下來,我們要進行服務檢查
我先用中文
下面我们进行华愿钢铁行业大模型的发布仪式
华愿计算一直致力于
焦化和冶金行业的全流程工艺优化和智能化升级
通过全站自研的认知智能引擎算法平台
不仅能解决生产过程中的复杂问题
更能推动行业生产力的质的飞跃
2023年中国粗钢产量达到10亿吨
稳居世界第一
然而高产量背后隐藏着质量挑战
当质量问题的产品流入市场
不仅会引发客户的质量异议
还会给钢铁行业带来严重的经济损失
因此提升质量检测水平
一直是钢铁企业关注的焦点问题
针对这一行业痛点
华严计算发挥了自身在感知智能
和认知智能领域的双重优势
结合常年深耕于钢铁冶金行业的丰富的经验
创新地开发了一种融合图像识别技术
和专家经验的缺陷检测算法
该算法通过深度学习技术训练
能够捕捉到产品表面的细微
缺陷大幅提高检测的效率和准确性
今天我们自豪地向大家展示
华严计算的最新成果
华严钢铁行业大模型
它不仅减少人工检测的依赖
降低了成本
还提高了检测的一致性和可靠性
这款大模型的应用
预示着钢铁冶金行业的智能化
和高质量发展的新篇章
与行业的转型升级
提供了强有力的技术支撑
让我们有请华严计算创始人董事长
宣小华博士
中国科学院院士
浙江大学数学高等研究院教授
阮永斌院士
浙江大学上海高等研究院常务副院长
浙江大学人工智能研究所所长
吴飞教授
张江集团董事长
周泽 prá
iatricardi そう
先生啦
小 高先生上台为我们共同开启这一激动人心的时刻
为老师站至立体的发光柱前
大家一起按下手印
5 4 3 2 1
让我们为华源钢铁行业大模型的正式发布
送上热烈的掌声
好 谢谢
至此今天华源计算的主题论坛圆满落幕
今天我们进行了思想的碰撞
对认知智能的探索之旅充满期待
正如路漫漫其修远兮
不将上下而求索
愿我们携手在人工智能的道路上
不断前行
感谢各位嘉宾和媒体朋友的支持
你们的到来为论坛增添了无限的光彩
现场照片已经通过千道桌
以及大屏幕名相册的二维码
供大家可以来获取
大家方便的时候也可以回顾和分享
在此祝愿各位老师
各位嘉宾工作顺利
心情愉快
2024年论坛已划上
圆满的句号
谢谢大家的参与
也期待明年我们能够再次相聚
再次极致
再次推进人工智能领域的发展
谢谢大家
明年再见
谢谢大家
请不吝点赞 订阅 转发 打赏支持明镜与点点栏目
字幕志愿者 杨茜茜
优优独播剧场——YoYo Television Series Exclusive
优优独播剧场——YoYo Television Series Exclusive
优优独播剧场——YoYo Television Series Exclusive
优优独播剧场——YoYo Television Series Exclusive
优优独播剧场——YoYo Television Series Exclusive