[
    "尊敬的各位领导,嘉宾和朋友们,大家好,欢迎大家来到今年世界人工智能大会的前沿AI安全与治理论坛,我是谢敏希,安原AI的CEO,安原AI是一家安全与治理领域第三方研究和咨询机构,也是目前该领域全国唯一的社会企业。各位领袖,我们非常的荣幸邀请到上海市领导,丽琳直导和交流。forget Gu qing yu xu w Lang Lei dar jag shao.上海市。去年4月我国中央政治局会议深刻指出要重视通融人工智能发展重视防万风险同年10月我国发布全球人工智能治理倡议重申各国应在AI治理中加强信息交流共同做好风险防范同月我很荣幸受邀参加了首届全球AI安全峰会见证了包括中国在内的28个国家和欧盟共同签署布莱切里宣言Blessed Recreation这也是第一份AI安全的国际声明在此背景下社会需要加强前AI安全研究安全评测安全治理以及国际合作这也是今天论坛的四个主题第一个主题是安全研究我们很荣幸邀请到国内外AI领域的世界级科学家图灵奖得主Yosua Bengio前头发布了第一份先进AI安全国际科学报告由30个国家欧盟和联合国提名的委员会共同参与对通用型AI的安全风险进行了科学评估中国工程院高文院士认为全世界正处于AGI强人物质能的前夜在一个不确定的状态需要严加防范AGI可能会引发的人类生存风险中国工程院张亚琴院士联合Yosua Bengio召集了第一届AI安全国际对话并联合伯克利分校东宋等领先科学家在Science主干上发表论文建议分配三分之一的AI研发资金到AI安全和伦理等研究方向我们期待和多位AI安全科研团队带头人包括上海AI实验室的邵靖北京大学的杨耀东和上海交通大学的张卓胜讨论前沿研究问题第二个主题是安全评测我们很高兴邀请到大模型安全评测的领军人物在学术研究方面上海AI实验室领军科学家乔宇第一次以人类价值观的角度对多模态大模型进行了全面评测天津大学NLP实验室主任熊德义发表了中文大模型前沿风险评测的一系列论文在行业联盟方面中国信通院人工智能研究所所长魏凯依托AIA安全距离文娱会启动了一系列大模型安全评测工作OpenAI Anthropic谷歌DeepMind和Raria成立了前沿模型论坛执行主任Chris Massero将分享领先美国企业的安全实践第三个主题是安全距离各国家正在开展对AI安全距离的积极研判和尝试我们很高兴邀请到法国政府人工智能委员会成员Gail Veracqua新加坡政府首席AI官何瑞敏中国政法大学数据法治研究院教授张灵涵以及伯克利分校Center for Human-compatible AI主任Mark Nisberg分享多元地区视角同时我们也邀请到上海教育大学中国法语社会研究院院长纪威东和上海教育大学上海AI实验室治理研究中心副主任王云春参与援助讨论探讨AI立法和上海AI治理经验第四个主题是国际合作我们很荣幸邀请到多家国际顶尖智库包括凯莱基国际和平研究院主席Mariano Ferratino-Cuella和研究员Matt Sheehan清华大学人工智能国际治理研究院院长薛兰纽金大学马丁人工智能治理中心主任Robert Treger加拿大国际治理创新中心全球AI安全风险主任Duncan Kaspeks讨论AI安全的国际治理议题联合国AI高层顾问机构专家曾毅将提出AI安全红线全球领先大模型开源社区Hugging Face全球政策负责人Iron Solomon将讨论开源模型对国际治理的影响最后我们将邀请上海AI实验室主任首席科学家周博文进行闭幕致辞展望AI安全的未来现在我们进入论坛的正式环节",
    "首先有请上海市人民政府副秘书长庄木地为我们的论坛进行开幕致辞有请尊敬的高文院士尊敬的张雅琴院士各位来宾女士们先生们朋友们大家上午好很高兴和大家一起相聚在2020世界人工智能大会共同参与前院人工智能安全与治理的论坛共同探讨人工智能的发展趋势和治理问题首先我代表上海市人民政府对本次参加论坛的科学家企业家以及媒体朋友们表示热烈的欢迎和衷心的感谢人工智能作为新能科技革命和产业变革的重要驱动力正深刻的影响着全球经济结构和社会发展随着技术持续迭代的演进人工智能的安全和治理也乐意成为全球关注的焦点中国高度重视人工智能的健康发展去年10月习近平主席提出了全球人工智能治理的倡议系统地阐述了中国关于全球人工智能治理的立场主张和建议展现了中国在推动全球人工智能发展和治理方面积极的态度和务实的行动去年11月包括中国美国在内的28个国家和俄盟共同签署了布赖切利人工智能安全宣言这也是全球第一份针对人工智能安全的国际性的声明体现了中国在全球人工智能治理领域的职任和担当上海作为中国经济城市的中心和科技创新的前沿在人工智能安全和治理方面开展了实践和探索特别是在全国率先出台了人工智能的地方性的一部法规就是上海市促进人工智能产业发展条例探索构建体系化的治理框架统筹人工智能发展与安全同时也发布了人工智能标准化体系建设的指导意见推动上海在人工智能标准领域的先行先试努力培育人工智能高水平的上海标准展望未来我们将继续在人工智能安全和治理方面发挥引领作用我们将持续完善政策体系加强技术研究和人才培养制定更具操作性更加完善标准规划和测评体系我们将坚持包容省政监管以鼓励创新为原则探索大模型评测四点沙盒监管我们将积极推动自力研究在健全法规体系监管体系等方面努力探索努力形成具有上海特色的监管实践方案各位来宾本次论坛汇聚了世界级的专家学者和业界的领袖将围绕全员人工智能安全的研究评测 治理等议题展开交流讨论我们相信通过大家的共同努力我们一定能够成为全球人工智能安全和治理问题提供务实方案和有益借鉴推动人工智能技术更好地博物与人类社会的发展上海将提供更加开放的平台更加丰富的场景更加优良的环境支持全球人工智能安全和自理领域的研究者的进行深入的探索和实践最后预作本次大会许得圆满成功谢谢大家",
    "谢谢感谢穆迪秘书长的精彩致辞请入座大家好我叫吴君怡是安远AI高级项目经理也是今天论坛的主持人鉴于今天有多位国际嘉宾我的主持将用英语进行各位观众各位女士们我的名字是冠义恩我作为冠义AI高级项目经理的职员我将是今天的主持人由于我们有大量的国际主持人大多数这次的会议会由英语主持不再多说我感到很高兴能够介绍我们的主持人业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授业务教授在世界各地的75位AI技術專家中發佈了《國際科學報告》關於進步AI的安全我非常榮幸可以參與作為一位寫者本教授Bengio將會分享這報告的重要資訊以及今天與我們一起解決AI安全問題好的謝謝您加入我們我們可以聽得懂好的我要開始了非常感謝您的慷慨並感謝您在這篇文章中的工作所以今天我想告訴您在我所擔任的AI安全報告中關於AI安全以及它意味著國際的意見關於AI的危險和安全所以這主要關於進步AI的問題有很多類型的AI所以我們會主要關於AI的主要意義例如語言模式以及其他我們最近看到的多模式模式都吸引了很多人的注意所以有兩部分首先我會談論報告然後最後我會說幾句關於它的意義未來以及我所擁有的大圖案即使報告並沒有任何建議報告是科學的合作為了幫助政策人員在工作上好報告名為《國際科學報告安全於進步AI的安全》我們花了很多時間找出正確的名稱這就是我們得到的所以是的報告主要關於危險因為當然已經有很多科學工作科學工作在AI的應用和利益上但為了政策人員的擔心很重要他們要明白危險和能力讓他們可以管理這些危險例如管理我們受到任務後在在英國AI安全協議在11月支持國際的獨立和公共的報告關於AI的能力和危險獨立意味著科學家最後一句話不是國家有30個國家加上歐盟和英國參與我會介紹這個程序報告也很廣泛的所有的危險目前的危險從負面和不明到預期的危險例如工業市場的影響很多不同的錯誤當然最大的危險是失去控制超級人AI或其他問題而目的不是新科學而是新科學的科學文明提供了這些問題為政策的利益的問題好這有75人在工作這有一個諮詢項目每個國家有30個項目一位專家也有歐盟和英國一位專家所以這是我們的項目另外我們也邀請了16位寫者這些人正在寫這個項目提供了評論評論我們在這個項目和報告的不同版本我們也邀請了一群高級教師他們是我們想要的各方面的專家我們也想要的有幾個他有很多這個想要的專家也有很多我們本來想要的專家也會成為一個某個專家我們也想要一個專家那這個專家應該是一個專家這個專家是一名專家在不仅仅是说有不一样的协议而是关于AGI的风险和时间线是否会发生是否会有几年是否会有几十年之类的然后是关于影响的观点AGI会发生什么事会否有快速的解决社会如何影响社会这些都非常重要然后报告谈论风险生产那么什么是现实科学去尝试解决这些风险那些方法是什么还有什么是他们的缺点而最后我想报告的主要结论如果您想要一句话是不幸的是目前没有认识的方法去防止现实风险和未来风险呃可能会有恶劣的风险例如失用和失去控制呃所以所以那是那是那是一个大招是一个大红旗但是但是你知道银河线是我们仍然有业务综合来说世界可以在方法上更好地理解这些风险并更好地解决它们好的那么我们再往深入一点嗯首先当然你知道为什么我们甚至关心风险是因为因为他们的利益总体来说这些风险可以非常有用可以使用在许多优秀的应用中但只有如果我们正确管理它因为有风险我们认为三个风险的级别我们讨论了很多如何组织这份报告在不同的级别中所以有危险风险风险来自失败和系统风险好的所以我将解释一下reflected in each所以其中的危险是比较容易理解的所以人们将用机械技巧去做 cambiable甚至是反正的甚至是非法的许多秘密误解稳定黑戒误解轨迹被决定软件等等然后有缺乏行为所以不应有意义的伤害对于生产品的安全问题就像风险隐藏和误伪类似的东西而且失去控制人们通常不想失去控制但这可能是个错误这是一种错误当然这是一种很严重的错误然后有系统风险那是什么那就是社会和技术联合的东西例如工业市场的影响当我们越做越多工作人们的劳动价值也会降低如果同样的工作可以做10倍少的工资那工作价值也会降低因为你能做得更便宜那么人们失去工作的情况是什么现在世界上没有这种问题但可能会成为另一个系统风险是AI分裂就是AI的才能和能力在几个国家都被集中了这跟其他国家有什么关系在几个国家都被集中了这意味着AI的利益也会被集中了这意味着AI的发展可能会对某些国家有好处但可能不适合全球西方例如另一种集中力量是市场集中力量那是在AI系统中的正常性训练的资源资源需要在这些AI系统中在这些AI系统中在这些AI系统中资源的价值在我们认为我们可以继续训练更大的模型他们会越来越好这就是所谓的讨论法但这也意味着很少人会有资金来训练将来的AI系统而这也意味着AI系统的效率和市场的效率以及市场的集中力量而这也意味着市场的集中力量和市场的效率而这也意味着市场的效率和市场的集中力量而这也意味着AI系统的产业和社会的产业和社会的产业需要的能量也会增加这不可能永远不断的持续很快的时间AI系统的训练费用很快的时间AI系统的训练费用将会影响到总能量的大量压力可能有10%的压力可能有10%的压力所以我们必须看到这一切而想想AI系统和社会的产业AI系统和社会的产业包括公司和公司包括公司和公司这篇文章也谈到我们称为社会风险我们称为社会风险例如制度需要时间例如制度需要时间例如制度需要时间例如制度需要时间技术改变时技术改变时可能需要时间去适应可能需要时间去适应还有其他风险还有其他风险有关力量的关系有关力量的关系有关力量的关系关于技术方法关于技术方法我再谈谈我再谈谈我们谈了很多我们谈了很多科技技术科技技术所以科技技术接着的几个的几个关系让我们能够解决风险并更了解我们的路线现在我们来谈谈这些风险的国际执行有很多讨论正如你们可能也认为在媒体上或者在聊天中或者在社交媒体上特别是人们认为没有风险的人们认为风险不存在人们认为风险很严重但有趣的是我们可以看看这些风险是从哪里来的而且我们也要认为风险和未来的事情最大的风险是不存在的系统所以当然人们不会有铁锁球知道未来会有什么希望所以实际上我们在几年或几年内会有什么种AI我们知道风险非常明显AI的能力继续进步并且它们进步得很快所以这些不同的观点意味着人们不同意像是AI的效果在勤劳市场上或是AI的效果在资料攻击或是生物武器攻击或是控制失控的但是我们发现这些观点的区别最好解释是人们认为AI会更有能力还有不同的预测在社会上会做什么以解决这些风险以及制度和协议的有效性所以主要的东西解释这些区别就是未来进步的速度而且是不确定的当我们说有什么不确定的东西例如这些这些非常重要的东西将会改变未来从政策者的角度来看如果科学家不同意未来的进步速度政策者必须握着这些枪准备准备所有的情况有些人说AGI会在3年内或是30年内我们必须准备所有这些选项在政策方面是的好那么现在我们再谈谈风险减少方法和他们的限制所以已经有很多技术方法来测试和减少总目标AI的风险所以测试风险是一件事所以是否有问题是否AI能够做出危险的事情例如这就是一种问题我们可以用它减少风险因为如果我们证明有危险那我们可能就停止或者不再开发这些东西或者甚至不再继续训练减少风险是个不一样的事情我们如何改变方法AI系统以防它做出坏事例如被迫害人而重要的是我们设立了这两个方法的标准因为制制官应该要使用最好的方法解决危险而我们也设立了最好的方法我们设立了最好的方法就是我们有几种方法我们可以使用最好的方法选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择选择目前存在的安全保护方法很容易清除例如用监狱判刑尤其是如果您能够进行稳定如果系统的重量有足够那么很容易清除所有安全保护好 所以在测试中现在的方式为什么不太好是因为我们基本上问了很多不同的问题并且看看是否能解决一些不好的问题所以我们只测试了不同的情况但是这些是视频检查我们不能检查每个可能的问题而且它有用因为如果我们检查到某些东西那么我们就知道有问题了但是没有安全保护方法如果视频检查没有找到任何问题那么这不意味着没有任何问题我们只能说出有问题的事情如果我们没有找到任何问题可能还是会有问题好所以我们应该不仅仅看着车辆我认为我们仍然有几年还有很多政府可以做的事情以解决这些风险我们必须更了解这些系统的作用正如我所说我们需要仔细地思考为什么和如何我们将在发挥智能是否会在一种方式上进行发挥的方式中是否会对攻击者或防御者有帮助想想智能攻击这就是用AI的用途对攻击者或防御者有更多的帮助我们如何获得AI的经济利益这也是我们能选择的谁会获得利益我们如何投资研究以防止风险好这是我们在首次的Sol AI Forum上发布的进程报告请大家看看请给我发表意见我们将在今年末期的更大的报告中进行进行研究现在我认为的大图报告谈论的不确定性在时间线上这可以非常快我们看到在时间线上有不同的标准在时间线上在X线上在Y线上的表现而黑色的标准是人工表现您看到在许多许多的任务上在时间线过去我们对人工表现的接近并且经常变得更好所以我们必须学习管理这些风险正如我们在我们的文章中谈论与许多中国同事这一年出版的管理极端AI的风险在科学上的迅速增长中是否能够解决这些风险所以我们该怎么做在总体上我们并没有准备好这种非常快速的改变例如我们要考虑新冠疫情的发生所以我们必须现在开始正在准备的工作我们必须考虑国际协议在风险线上的制定尤其是风险线上的失控和缩小的风险和缩小的风险我们需要做更多的研究为了更明白风险和能力以及其后的影响并建立计划如果我们发现风险一个这些线线是线线我们必须现在承诺如果发生这种问题我们会做什么而在总体上我们必须使用准备方法当大风险有不确定性时我们就可以把风险线正在准备准备好这个风险线就会出现风险线如果我们把风险线放进一个准备方法然后把风险线放进一个准备方法就会出现一个风险线这个风险线是让我们看到风险线准备好风险线和风险线在准备方式上也能使我们能够准备好风险线就是我们可以把风险线但某些方法可能是提供更强的保证并且需要更多时间去发展而某些方法可能比较容易去做也许不是很安全但如果AGI发生在三年内我们必须在三年内准备好所以我们需要所有这些项目都在同时进行最后,还有一个问题就是发展者之间的竞争在报告中提到的但是也在国内之间的竞争所以不同国家之间的竞争实际上是一个军事竞争而AGI可以成为武器在军事中使用从资料中开始但也可能在其他武器中使用所以我们现在必须在国际讨论中在安全上合作但是也要考虑如果会有协议如何确保我们能够确保我们能够确保公平我们需要发展技术谢谢",
    "谢谢谢谢",
    "在北京大学的工业科研学院高老师目前是14th National People's Congress的总统他曾经是10th、11th、12thCPCC国际委员会的一名中国国际自然科学基金总统中国电脑组织总统和中国电脑博物博物馆的首席高老师我们很高兴能够与您一起参加让我来给您介绍一下在这个节目室如果参加的专家可能也能够听得到三位图灵奖得主在谈人工智能安全的时候其实还是有一些差异的比如说第一位图灵奖的得主Raj他就说现在人工智能有太多的问题需要去解决了如果你要有那功夫你还是先解决点问题怎么样保证安全你先把人工智能本身没有解决的问题解决解决我们第二位得主Manuel就比较有意思他就说人工智能只要是确定的这件事其实都是可控的不确定就不行那么第三位Andy和刚才的Angel是一样的因为他们和亚琪琳一起写了一些文章在整理这些报告就是要让我们一定要重视今天我讲的是其实人工智能的安全性确实是它是问题的两个方面一个方面就是说作为技术研究你必须要把技术本身要做到极致让它有用当然作为社会学家要考虑这样一项技术它对社会带来的影响到底是什么那么如果这个影响有负面的你有什么办法把它控制住这可能是一个问题的两个方面我想因为我们这个社会呢在发展的时候需要所有的人的关注当然所有人关注不是所有人都做同一件事所以我们要有很好的分工今天我们就讲一下这个分工的问题AI其实我们知道它确实是很强大通用人工智能这个强大了以后呢我们就要让它向善就是让它做它应该做的比较理想的事情所以我们说这个AI向善里面呢最主要的我们要从两个角度从技术的角度要把人工智能技术本身要做得足够好今天的人工智能确实还有很多问题所以按现在的这个水平它还没有办法向善第二个呢就是从伦理的角度你必须要在伦理道德等方面给它规范好所以我想这是AI向善里面比较需要关注的两个方面那今天的AI技术是不是足够好的呢刚才说了其实不是我们现在AI的水平呢还不够高因为刚才Benjo也在他的最后的slice里面其实也提到了就是在他总结之前的slice里面也提到了作为单向性能有一些AI已经超过人了有一些还是不行什么时候几乎所有的性能都超过人的时候那个就是比较好了就是可以真正发挥作用所以我们说今天的这个AI技术呢我们认为它表现的更多的是一种低水平的智能什么叫低水平的智能呢就是死记硬背的智能就是靠显示知识的记忆和使用那么来表现出来的智能真正好一点的智能呢其实是中水平的智能是中等的高水平的智能是最理想的我们现在其实做高水平的追求高水平的智能还有点那是非常遥远的事我们要追求追求中水平的智能所谓最中水平的智能呢就是用比较少量的显示知识就可以获得的智能用我们人类的这个学习能力来说你有非常强的举一反三的能力而现在的人工智能系统是没有这个能力的所以我想我们现在呢可能当前是在低水平智能某些单线还可以靠死记硬背或者是靠数据训练出来的那么等到了只有少量的样本就可以训练出智能的时候我们大概就到了一个中水平的智能而且它可以跨越领域从一个领域可以很容易就类推到另外一些领域就像以前搞机器学习这个类比的你能做到那是中水平的智能高水平的智能呢这个我们就可以把它笑一笑听一听因为这个高水平智能相当于说就像人类里面也没有多少人能达到的智能你让计算机这个系统去做那是非常遥远的事情了那么低水平智能里面呢其实有一个有点像悖论一样的情况很多人就说因为有时候讨论问题说既然你说现在的智能是低水平的智能它为什么会有智能涌现低水平智能是不应该有智能涌现的其实低水平智能也可以有智能涌现为什么呢我们可以换个角度来考虑我们现在的智能呢是用数据训练出来的比如说我们大语言模型大语言模型呢是用不同种的语言一起来训练出来的一个模型但是我们每个人的母语呢大概只是一种语言也就是说可能我们A熟悉的是中文他所有的学习的熟练的都是中文里面的东西所以呢你用中文训练出来的东西对他来讲呢他能判断这个东西好与坏准确不准确或者是基本上都是他可以掌握的但是呢如果这个语料呢是用西班牙语去训练当然混合在一起训练了那西班牙语的它的背景场景里面的东西呢其实是学中文的人呢他可能不熟悉的那所谓涌现呢就是当你把所有这些语料都放在一起去训练的时候它会使得使用者突然发现有一些东西他根本就不知道他认为这就是令眼睛一亮其实那个知识对那个行业的人或对那个语种的人大概不是什么了不得的东西但是呢对于不是母语的人他就觉得很吃惊所以我想这个涌现更多的我们可以用这种角度解释当然也许深层次还有更深的解释我们大家都可以去考虑这些问题那目前的人工智能呢我们说不管是在智能水平上在技术上在这个形态上在应用上甚至在社会属性上面都已经进展的比较好特别我们讲到伦理问题必须要考虑它的社会属性那么讲到社会属性也必须说现在人工智能的这个安全或者人工智能的带来风险那肯定就是说一方面是犯罪一方面其他这就肯定是早晚的问题那么另外这个整个人工智能的发展呢它可能它会影响的这个层面呢这个可以在人的层面魔性的层面和数据的层面这三个层面来考虑当然更棘手的一些问题呢就是如果人工智能对社会产生攻击那么我们怎么样这个防止这种技术被恶用对社会产生影响所以呢这个最简单的呢就是说这个我们要从伦理和技术两个方面去着手解决这些问题那么针对这个问题呢其实中国工程院呢前些年呢专门部署了一个人工智能方面的重大咨询项目叫做新一代人工智能安全与自主可控发展研究这个重大项目里面课题9呢是我领了一批专家在一起做的那个研究的问题是强人工智能与内脑计算技术陷阱安全对策这个呢大概是在19年开始研究的2021年呢我们把这个东西研究出来呢写了一篇文章发表在中国工程科学上面右边就是这篇文章的首页所以大家如果有兴趣当然这是中文的了读得了中文的可以看一看英文是有赛要的但是全文是中文的那么在那里面其实我们把人工智能的安全风险呢分成三个方面一个是模型方面一个是算法和硬件方面另外一个是自主意识的不可控方面那么从模型方面呢它主要就是我们说模型本身是不可解释的这个我想我就不展开了第二个是算法和硬件方面呢它也有不可靠性因为我们知道软件会有bug硬件可能里面也会有一些不可靠的地方这些都可能带来安全的风险还有一个呢就是自主意识的不可控性不可靠性就是失控了这个失控肯定系统的失控会带来很多不同的风险那这些风险呢都是强人工智能可能会带来的一些风险那针对这些风险应该怎么做其实刚才班军也说了很多这个我们要想法尽量减少降低和减少这些风险的一些技术线和做法也给了一个很长的清单那当时呢我们在21年的时候就说理论方面要完成完善一些这种技术理论的验证实现的模型的可解释性另外呢对人工智能的价值取向要想法能够在底层价值上面对它进行严格控制那么在应用阶段呢主要是希望能够有足够的技术支撑防止呢人为的造成这些安全问题当然这些比如像造假呀假视频呀假图像呀这其实都是人为的要尽量去预防或者是能够检测这方面的一些情况所以这件事要做呢很重要的就是一个方面就是我们必须要开展国际合作研究没有国际合作研究其实这方面呢你很难取得这个就是说在全球因为有一些东西你做的好别人可能不会做有一些东西别人做的好你可能不会做我们通过国际合作呢把大家做的好东西呢都可以通过交流使得大家对人工智能安全方面呢都能够提高到比较高的一个水准而且呢在这方面不仅仅要合作人才培养也是非常关键的因为以往关于人工智能安全相关的人才呢其实是非常稀缺的当然这几年慢慢有点好有些好转但是呢我们冷爱需要大量的人才那么在这个语言模型和数据方面比较重要的呢就是我们要有很好的平台要有很好的数据然后呢去训练去谢谢",
    "去训练和去用这个用这些数据使得你训练的结果呢比较理想在这方面呢我所在的鹏城实验室呢我们大概从2018年开始用英伟达的卡扎了一台千卡左右的机器那时候因为18年比较早了那时候还是唯一白的时代所以算力没有那么强那么到了2020年我们就用华为的生腾910就做了一台4000块卡的机器那么差不多1000个匹的算力那么今年年底呢我们大概会做一个2万多块卡的机器大概会有这个16000匹的算力或者16亿的算力那有这个算力呢我们就可以对模型训练啊模型训练当中的一些这种经验啊教训啊或者模型训练完了一些这个训练的这个模型参数的这个对社会的赋能等等我们就可以做一些事比如说我们把所有训练我们在机器上训练的模型自己训练的模型啊都开源开放出来然后供社会供研究团体去使用那么当然这里大家会说你要训练模型的时候我的数据会不会丢失会不会被别人被不相关的人就直接拿走了那么我们实验室也开放了开发了一套技术叫做防水宝技术防水宝技术呢其实就是说数据拥有方他对数据具有绝对的控制和管理的权利那么机器在训练的时候数据它是可用不可见而机器呢机器当然可以见得到数据就机器上面的操作员其实他是看不着数据的他只能看到你的这个样这个样本数据就是你可以用一个比较小的但是脱敏的一个数据呢让操作员先去试模型一旦要试好了真的数据进去以后操作员已经看不到真数据了除非数据拥有者给他这个权利他可以看得到包括训练完了的参数如果要往外走的时候那么机器也会自动向这个数据拥有方主动去请求说有一个参数要往外传送请你检查这里有没有携带你的数据等等有这样的一个流程使得数据可以做到足够的安全那我们训练了一个系列的模型包括7B的模型33B长窗口的模型和200B的模型这都是大语言模型了这语言模型里面既有中文英文还有其他这个语言的一些参数那么通过这些呢我们训练完了以后把它都开源掉供大家去使用那么我们用的最大的200B的模型是一个104层的网络这个用4000块卡我们差不多训练了半年多把它训练出来了那么在这里呢我们也摸索了很多经验性能也是不错的那后来我们又训练了33B的长窗口模型那么这个长窗口现在目前是128K的窗口那么正在训练192K的窗口可能很快就完成了这些完成以后我们就把它开放出去那么我们也有整套的模型的这种开放和使用的这样的一个组织去使用这个东西所以总结一下呢人工智能高速发展其实带来这个安全问题啊我们必须要重视当然从做技术的我们要把人工智能做的推向前进做得更好所以呢这方面呢只有通过国际合作才有可能更好的把这个工作做好我跟大家分享这么多谢谢大家",
    "谢谢各位教授请坐下接下来我们很高兴有我们的教授张雅琴教授张雅琴是中国工业博士博士以及AI科技博士以及清华大学研究AI工业博士教授张雅琴之前是拜杜的总裁而之前他担任了16年的Microsoft总裁担任了很多重要的职业职业作为世界上最有名的科学家和企业生他做出了非常重要的贡献通过他的550篇文章62个美国资产以及其他专业工业成绩让我们欢迎张教授早上好谢谢安远",
    "安远AI邀请我来这个大会刚才呢Yoshi Banjo和高温院士呢对整个这个AI特别大模型的发展特别风险的都做了特别好的这个系统性的介绍一个是全球一个是中国的确的话呢过去这两年左右呢这个AI的发展的速度很快快的同时呢也带来很多的这些安全的风险我过去这两年呢也花了不少时间和全球领先的这些学者们一起来从事一方面的一些研究今天呢我简单讲一下有时间关系我简单讲一下我的一些思考吧特别是首先呢是一个大模型发展的一些趋势以及呢当然更重要的是风险方面的安全方面的一些趋势这些考虑首先我认为呢这个大模型和生生生AI在未来的这个十年吧有下面几个趋势第一个呢就是多模态我们不管是我们的语言我们的文字语音图像和视频都正在融合起来另外的话呢这个激光雷达这个三维的结构信息四维的视红信息包括我们蛋白质我们的这个细胞还有基因都在变成多模态的收入那么第二点就是我们叫智能体自主智能所以可以自主地规划任务可以开发代码可以自己升级不单是错可以去优化自己也可以去自我copy第三个就是智能的走向边缘我们现在讲大模型大部分还是在这个云端的这个大模型现在呢正走向我们的PC啊走向我们的手机啊走向我们的这些智能的设备走向边缘端第四个就是现在讲物理智能就是具身智能我这十年一直叫物理智能现在新的名词比较时髦叫具身智能就是大模型用到这个无人车无人机机器人物理基础设施像电网啊电站啊一些critical infrastructure那么最后一个呢是生物智能就是像包括现在我们的脑机接口用到我们的人体人脑医疗机器人生物体和生命体这个我最近呢和很多的学者都一直在探讨这个问题到底通用人工智能什么时候可以实现我这个表达我完全个人的意见因为刚才亚术班主也讲到我们讨论这个问题的时候大家有很多不同的这个角度不同的观点我个人认为的话呢差不多在二十年之内会实现这个通用人工智能分三个阶段就是我一直分成信息智能物理智能和这个生物智能那么信息智能的话呢五年之内我认为可以达到所谓的这个图灵测试当时ChainHP出来的时候呢我的第一感觉我觉得ChainHP的文字方面基本上通过了图灵测试那在这个视频啊在别的方面可能还需要点时间可能在五年之内我可以达到这个修改的或者新图灵测试在物理智能或者巨生智能呢可能还需要差不多十年的时间因为现在比如说无人车这个人行机械我们这个会议也看到很多这个我自己认为呢我这么多年一直在做无人车从当时在百度的阿波罗那么一直在做无人车可能八九年的时间了我认为无人驾驶呢是巨生智能一个最大的应用也是第一个实现这个新图灵测试的这个应用明年呢大家都看到我们在武汉做的这个大规模的这个实验的商用我觉得在明年的话呢我们看到更多的应用在二零三零年的话之前的话呢会成为主流的应用生物智能可能时间更长一点可能需要再用差不多十年的时间但整体来讲的话呢在未来的二十年我认为可以达到这个通用人工智能而我所在的清华大学智能产业院其实就是为了这通用智能而建起来的我们其实就是在三年半前建起来的那么这个研究院的话呢目前有二十二名教授有差不多三百多位学生我们的目标很简单就是能实现信息智能物理智能以及生物智能包括无人驾驶先进的机器人也包括呢这个Biological Computing目前我们发布了很多模型我们更多的是垂直模型比如说我们发布了一个第一个全球的实用的端到端的无人驾驶的开源模型叫Air Apollo FM大家都可以看到在GitHub上面我们也发布了一个第一个全球最大的Biomag GPT都是开源的大家都可以使用那么在这个有巨大能力的同时的话呢带来很大的一些风险刚才Benjo也讲了前沿大模型大模型到了万一参数更多的时候呢就才知道它的风险那么我还是分成三个不同的世界信息世界 物理世界 生物世界信息世界的风险大家比较容易理解刚才讲到DeepFake讲到HallucinationMisalignment讲到Misinformation这个我觉得相对比较容易理解那到了物理世界呢这个风险就会更大你想想看我们有再过十年我想我们这个世界的机器人比人要多得多机器人的话呢如果它失控如果它被坏人所乱用大家可以想象到对社会带来风险以后我们的车可能都大部分无人车这个时候是靠这个大模型去控制这个时候所带来的风险不管是主动风险被动风险都会很大那么更大的风险的话呢是这个这个生物智能物理智能和信息智能融合在一块这个时候如果失控或者被乱用会造成生存风险所以我们觉得我们有过去这几年有几个重要的节点其中一个节点就是在2023年6月份的Center for AI SafetyRecent Statement on AI Risks讲到我们要把人工智能未来的风险把它当作核武器和流行病一样的这个优先级去看待后来的话呢有很多工作包括刚才部长讲到的我们中国的人工智能全球倡议也包括EU的AI Act也包括了几次这个峰会然后也包括我们一些小范围的会就是我去年的话呢我和Store组织了一个叫International Dialogue on AI Safety我们每三四个月开一次会第一次在英国第二次在北京下面一次是在这个Venice我们会开会两天三天深度的去研究这里面的一些技术问题和政策的这个对应的问题刚才呢亚瑟讲的那个报告我觉得是把把这个很多的讨论的做了高度的一个总结我也很高兴的深度参与这个报告我简单介绍一下呢这个大模型安全方面的一些技术因为大模型安全它确实是一个系统工程从我们的输入从我们的输出从我们的安全评估 治理特别是这个系统的安全对齐都需要去工作这里面有许许多多的这个数学很多很多的算法方面的研究有许多工程的问题技术的问题也带有很多这个策略的问题这个我就不细讲我们做这个安全的话呢对这张图应该比较熟悉就从各个方面的系统工程问题然后另外的话呢这里面很重要一点就是最近许许多多进展就是大模型安全的对齐这里面又有两种不同的这个方法一个呢是直接监督的问题就是我把高质量的有用的安全的这个信息把它直接运用这个监督微调那么第二点呢是根据我们的偏好人类的偏好我们的价值观来做这个reinforcement learning比如说这个这个GPTGPT系列基本上是采用这个PPU这种方式那这里面有很多种不同的一些选择不同选择可以基于这个奖励模型的安全奖励和有用奖励用Lagrange去结合的这个作为输入的参数然后也可以用一些更新的一些奖励的方式那么在清华呢在AIR的话呢我们有几位老师呢也做了很多很多工作那我们的詹先生老师呢他提出了这个conditionalreinforcement learning那么这个的话呢是用于这个用于这个大模型的一个微调比如说我们有很多高质量的数据的情况下它可以帮助我们更多的去把这个任务自动化我们知道有手工reinforcement learning的话呢需要很多很多的工作需要很多数据这个的话呢工作已经在大家可以看到在Github上叫open chat大家都可以看到也现在是比较受欢迎的技术那么另外的话呢就是我们也发现目前在这个reinforcement learningHuman feedback里面呢有些问题特别是它的这个样本和策略的学习目标呢是不匹配的就是curie和policy会misalignment所以一开始等于你认为是align但是走走走走之后它就偏离这个方向所以我们也提出一个一个新的技术然后使得它在学习就我们的goal和trajectory是well-aligned那么我们应该在下面几个星期阿Claire会谈到这个工作另外的话呢我们用了不少安全离线的这个强化学习的方法然后呢去把这个安全的策略来进行这个改进特别是其实呢如果我们首先要判断一个东西是它是属于安全呢还是不安全就要把这个区域要找到那么在这个区域里面的话你可以做最大化的一个奖励如果在区域外面的话呢你要做最小化的这个风险一个要maximize一个要minimize那这里面如果看我们的paper的话呢这里面都是mathematics都是数学所以我就想让大家知道呢这个安全的问题对齐的问题不仅仅是一个词是一个策略和简单的一些算法这里面其实有很多理论方面的一些创新和突破这个文章的话呢我们会在也是应该已经发表了ICML我们也有一篇这样的论文那么最后时间不多呢我想谈一些我自己的建议刚才是在技术方面的一些工作不知道药东会不会讲药东和刀宋他们几位在这方面做的都特别领先的学者他们以后会讲更多细节那我呢想提一点就是政策方面的一些建议这个我其实讲了差不多两年了讲两年了我这儿有没有个章我看有没有我要盖个章对盖个章的话呢就是说我讲的这个建议完全是个人建议不代表清华大学不代表清华大学air也不代表我们现在所有的团体因为我们在内部有很多不同的观点完全是个人建议这个其实我提了差不多两年到三年了我提了十个建议我今天实验关系我讲五个第一个的话呢就是我一直建议我们要建立这个分级体系因为现在AI里面有很多不同的算法有很多不同的模型那我们呢要对这个就是最前沿的比如超过万一参数以后很多的参数呢对它进行约束一般的模型一般的算法呢就不要太去规范它让它往前面发展就对这种特别风险比较大能力比较大的就前沿的超大型模型需要去有些规范因为我做无人驾驶我们这里面自动驾驶我们里面分成六级从L0到L5 六级我建议我们把这个分成L0到L5只有L5的我们去规范它那第二点的话呢除了模型本身的这个规范这个规范包括从数据从模型的这个构建从这个对齐到最后评估各种评估都需要有一套标准更严格的标准那第二点用在场景里面需要更多的约束那你比如说用到无人车里面无人车里面的这个安全无人车里面它本身的它的自己的这些评估的体系要拉进来你做医学里面比如说医疗基金人他必须要经过医学方面的这个场景和领域的这个约束第二的话呢我讲了很多年了就是我们需要有一个实体的映射机制首先是对AI的内容要标注比如说我现在产生了很多数字人数字人和真人基本上看不出来区别我要标注我这是AI人虚拟人我AI产生内容我要标注是AI产生的我们现在的这个规定啊国家规定美国也产规定了你比如做个广告在互联网做广告如果是广告你要写个广告但我如果搞一个这个虚拟人数字人我都不需要说我是AI产生的首先就是个简单的把它标识出来知道这是AI产生的还是人为产生的第二的话呢就是一定要有一个实体映射的机制我们以后有很多机器人有很多可以是真正的机器人也可以是虚拟的机器人有很多智能体那么这个智能体它应该是从属体它从属于我某个人或者某个机构的我的机器人犯事了我最后要追溯到它的主体里面去所以ownership一定是人人或者是一个company也是一个legal entity那么这个事情其实从从这个技术上来讲并不是很难是完全可以做到的但是我这是更多的一个政策方面的建议第三个呢我一直建议我们把10%的这个投入啊就是做AI研究的也好产品开发也好投入呢放到对安全和风险的领域来我们在全球我们大家是建议30%在国内我说我们先从10%做起以后慢慢到30%这个包括我们的基础研究经费我们的产品开发经费包括我们整个这个社会的投入我们先到10%作为第一个起点第四个就是设立一些很清晰的这个红线和边界这个红线边界其实要设立起来其实不容易的因为每个国家可能这个有不同的情况但是我觉得有一些大家可以设立的我们要设立什么不能做比如说我从很多年我就提我们做智能体的时候智能体现在自己可以去它可以咖背它自己可以去复制的那复制的时候复制的时候要经过人的同意比如说我是这个主体我要同意的你复制一个张亚青张亚青要去他同意你去复制这个不能自我复制没有限制的复制然后还有红线边界比如说大模型接到核电站的时候怎么接能不能接我个人建议在我们在这些大模型还没有搞清楚这个这些边界啊没有搞清楚这些里面的可解释性前面的先不要接这些特别关键的critical infrastructure最后大家很多都讲过了就我们要一个国际沟通的合作和协调机制包括标准包括评估包括这个合作的具体的一些方式这里面需要有专家的需要有政策制定者的需要有政府的但很重要的需要这个这些不同领域的人在一起在一起这个精诚的合作好我就讲讲这么多这个谢谢大家",
    "Thank you so much Professor Zhangfor your excellent presentation and suggestionsNext I'm now pleased to welcome Professor Dawn Songa professor in computer science at UC Berkeleyand co-director of the Berkeley Centerfor responsible decentralized intelligenceHer research focuses on AI safety and securityand she is ranked the most highly cited scholarin computer securityShe is the recipient of numerous awardsincluding the MacArthur FellowshipGuggenheim Fellowshipand more than 10 test of time awardsand best paper awardsDawn it's a pleasure to have you here in Shanghai with usI'll let you take it from hereGreatThanks everyone for being hereYes my name is Dawn SongI'm a professor at UC BerkeleyToday I'll talk about AI safety challenges and future directionsSo the presentations earlier have such great context and backgroundAnd here I wanted to add some more emphasisIn particular as we deploy machine learningIt's really important to consider the presence of attackersfor a number of reasonsSo first history has shownthat attackers always follows the footstepsof new technology developmentsor sometimes even leasesAnd also this time the stake is even higher with AIAs AI controls more and more systemsAttackers will have higher and higher incentivesto compromise the systemsAnd also as AI becomes more and more capablethe consequence of misuse by attackerswill also become more and more commonand more severeAnd hence it's really importantto consider the presence of attackersespecially as we consider AI safetySo first I want to talk a little more about AI safetyin the presence of attackersFrom my group's earlier workand also other research workwe have shown that adversarial attacksare prevalent in deep learning systemsEssentially all deep learning systems todaythey are all vulnerableto different types of adversarial attacksAnd the number of papers in this spaceactually has grown exponentiallysince our earlier workand the people's earlier workin the early stagesAnd also we had the rare honorof having some of the artifactsof our earlier workactually now is part of the permanent collectionat the Science Museum of LondonSo as we talk about safetyand today talk about safetyaligned large language modelsit's also important to considerthe adversarial settingSo unfortunatelyas our workand also others workhave shown that these large language modelsare also really vulnerableto adversarial attacksand these safety alignment mechanismsare easily brokenSo in our researchwe also workas an exampledecoding trustwhich provides the first comprehensiveevaluation frameworkfor trustworthinessof large language modelsIt's actually won the outstanding paper awardat NeurIPSthis past DecemberWe developed new algorithmsand also different environmentsincluding benign adversarial environmentsto evaluate many different perspectivesfor safety and trustworthinessof large language modelsAnd our workhave shown thatfor all these different perspectivesincluding adversarial robustnesstoxicityand fairness and many othersessentially these large language modelsare all very easily attackedby adversarial attacksAnd again for more detailsyou can go look at our paperat decodingtrust.github.ioAnd also these adversarial attacksare effectivemulti-model models as welland also others workhave shown thateven as these modelsare being fine-tunedattackers actuallyby providing just a fewvery small number ofadversarial designsdata pointsthis fine-tuned stagecan essentially causethis fine-tuned modelto easily losethe safety alignmentsSo far I've talked aboutRightSo these attacksthey are not only effectiveat the inference timethey are also effectiveat essentially fine-tuned stageas I just mentionedessentially this is calleddata poisoning as wellAnd alsothrough this data poisoningstepalso these modelscan have what we callvery stealthy behaviorwhere essentially calledbackdoor as wellSo in our earlier workwe showed thatthrough data poisoningthe model canattackers can build in backdoorin the model such thatfor examplein our earlier workin facial recognitionthe modelunder normal circumstanceswill just behave normallyand give correctfacial recognition resultsBut howeverwhen anyonethat wears a specialtype of glassesand this actuallyis even effectiveyou knowin the physical worldthen it will causethe modelto essentially triggerthis backdoorthat the modelwill misrecognizethis personwearing this particulartype of glassesto a targetedas a targeted personAnd throughrecent workwith byanthropicthey have also shownthat the modelthis type ofbackdoor phenomenonwhere a fine tunedlarge language modelduring normalcircumstancewith a normalpromptit can generatelike normal codethat's usuallycorrect codeBut whena particulartrigger freezeappearsin the promptthe model actuallywill generatea vulnerable codeSoall theseare different typesof adversary attacksandthe modelactuallywill generatein the entirecommunitywe have beenvery productiveand creativein coming upwith different typesof new attackmethodsHoweveron the other handunfortunatelyin the defensesidewe have seenvery very lowvery little progressand todaythere is noeffective generaladversary defenseSo thisillustratesthat this isthe firstopen challengethat I wantedto posein the contextof AI safetyand the currentAIsafety alignmentmechanismsare very easilyevaded by adversaryattacksandany effectiveAI safetymechanismsneed to beresilient againstthese adversary attacksand hencethis posesa hugeopen challengeSo essentiallyin order toachieve AI safetywe need toactually be able tosolve adversaryrobustnessas a prerequisiteI just mentioneddespite that nowwe have thousandsof papersevery yearpublishing ondifferent types ofadversarial attacksbut the entirecommunityessentiallyhave madealmostzero progressin defensesagainst theseadversarial attacksSoasfordevelopingeffectiveAI safetyas a whole communitywe really need topush forwardin howwe candevelopadversarial defensesso that we candevelop AI safetymechanismsthat are resilientagainst adversaryattacksSo what are thepotentialdirectionsthat can help usto achieve this goalSo here I'll givea coupleexamplesfrom some of ourrecent workSoone work iswhat we callrepresentationengineeringand this isa top-down approachto AI transparencySo in this caseweby providingthethe modelwith theconstructedcontrastic inputsas a stimulusfor certain tasksSo we providethesecontrasticinputsto the modeland thenwe monitorthe activationof the neural networksat different layersand then buildthe modelsAnd with ourrecent workwe showthatbythis methodwe can actuallyidentify certaindirectionsalongat certainlayersthat actuallycorrelateswith different classesdifferent typesof behaviorsof the modelSo essentiallyfor examplewe canidentify certaindirectionsthat actuallycorrelateswith behaviorswhether the modelis honestor not honestwhether it'shallucinatingor nothallucinatingand so onwe have alsoshown thata particularthis type ofmethodcalledrepresentation controlSo not only thatwe can dorepresentationreadingwhich is tomonitorthemodel's behaviorwe can actuallymodifythe activationsof theseneuronsat certainlayersduringthe inferencetimealongfor exampletheidentifieddirectionsand so onand thenthrough this waywe can actuallythen changethe modelbehaviorfor certainclassesfor exampleusing this methodwe can make the modelbehavingmore honestor less honestand so onSo whyis this importantI thinkthis is oneof the keythe key distinctionactuallybetweenhuman brainsand artificialbrainsartificial neural networksis thatthis artificialneural networkswe actuallyare in controlin the sense thatwe can completelyobservethe activitiesthe activationsof thethe neural networksand alsoin real timewe can modifythe activationsof the neural networksso this actuallygives usa powerfularsenalforpotentiallyfor AI safetyso this allows ustoobserveandmonitorthe behaviorsthenumbetterprovide better controland enforcementof the behaviorsof the neural networksso hence thiscan bea really promisingdirectionforprovidingAI safetycontrol mechanismshoweverthis type ofcontrol mechanismsis promisingbut it's difficultto actually givefull guaranteesso alsoasprofessorYaqinandalsoyoshuaummentioned earlierideallywe actuallywant to haveapprovalguaranteesso recentlywe have a jointinitiativeon quantitativeAI safetyand the goalisto actuallydevelopAI safetythat'swithguaranteesessentiallywithsafeby designwithguaranteesfor safetythis actuallyis alsoin paralleland in somesenseinspiredbythe approachtakenin cyber securityso includingmy own workin the last25 yearsin cyber securitywe have movedessentiallywe havehadparadigmshiftin howwe approachsafetyhow weactuallybuildsecure systemsearlyonhow wetodayhow wedetectwhenlargelanguagemodelsisis behaving wrongand then later onwe worked outmethodstoas aproactivedefensefocus onbuckfindingtry tofindvulnerabilityin thissystemso it'skind ofliketodayiamsatisfactoryfor anumber ofreasonsthat idon't havetime toget intoand in theend thecommunityrealizedthat thebestapproachforachievingsecurityis whatwe callsecurebydesignorsecurebyconstructionthepropertiesby thedesignand theconstructionof thesystemand this isin contrastto theother types ofdefensesthat imentionedearlierwhichhelpsus toessentiallyget outof thecat and mouseschemeand alsoprovidesprovableguaranteesand theandthroughformalverificationwe canthenformallyverifythat thesystemis securethroughaccessifiesthedesignpropertiesthroughverificationand thisalsocan bedoneatdifferentlevelsincludingthedesignlevelwhere weactuallyhave manydifferenttypes ofsystemsincludingmicrokernelsandfilesystemsandcompilersand soonthat areformallyverifiedhowevertheissuefor thesystemsis thatit'sextremely laborintensivetoskillablemygroupin collaborationwith othersatopen aiwe wereamong thefirstto usedeep learningfortheimprovingandthiswasthisworkwasdonequitea fewyearsbackwaybeforewe caninsteadtrainaiagentstoautomaticallyproofserumsand verifyprogramswith thisapproachin conjunctionwith programsynthesiswhich mygroup alsohas done a lotof workin thepastwas amongthein thisspaceaswellandwe canprovideautomaticallyproofly securecodesproducingcodeswith proofsattachedto itandwith thisapproachwe can use aito buildproofly securesystemsessentiallyachievesecure bydesignor save bydesignandthiscanhelpustosolvecertainclassesofproblemshoweverit stillhasa numberof openchallengesfirst this typeof formalverificationapproachmainly appliestotraditionalsymbolicprogramsbut itcan bedifficult toapply tonon-symbolicprogramssuch asdeepneural networksandtheself drivencardoesn'tdrive overa pedestrianwe don'teven have a formalspecificationof what a pedestrianisand alsoin the futureessentially allsystemsmost of thesystemswill behybridthey will becombiningsymbolic andnon-symboliccomponentsso formalverificationand secure byconstructionSo to conclude, as we all discuss and agree here, AI safety is extremely important.As we move forward with stronger capabilities of AI, it's paramount importance that we guarantee the safety of these systems.But however, there are still many challenges.It's important to consider AI safety in adversarial setting.And I think it can be very productive to develop methods using activation steering, representation control,to build as an important arsenal for controlling model behaviors.And also, finally, we hope that we can really develop new approaches and mechanismsto enable secure by design.For building secure AI systems with provable guarantees.Thank you.Thank you so much, John.Please stay on stage as we transition to our panel.We will wrap up our session on AI safety with a panel on AI safety research directions.For the other panelists,please kindly start with your questions.Thank you for coming up on stage as I introduce you.Also joining us for this panel is Dr. Shao Jing,head of the large model safety team at Shanghai AI Laboratory,where she leads many research projects on evaluating large model safety and value alignment.We also have Professor Yang Yaodong,who is deputy director of the Center for AI Safety and Governance at Peking University.Professor Yang studies AI alignment and reinforcement learning,among other topics,and has over 100 publications in top venues.Finally, we have Professor Zhang Zhuo Sheng,an assistant professor at Shanghai Jiao Tong University.His primary research interests include the safety and security of multimodal models and autonomous agents.He has published over 50 papers in top tier conferences and journals.Our moderator for this panel will be Duan Yaowen,who is the technical program manager at Concordia AIand a Future of Life Institute Ph.D. fellow.Yaowen researched AI safety at the University of Cambridgeand holds a master's degree in machine learning.Let's put our hands together for our panelists.對,感謝主持人君怡。我們今天的第一個圓桌討論的主題是關於前沿AI安全技術的研究議程。其實今早我們看到Yosha Bendrell他有談到他簽討的第一份先進AI安全國際科學報告International Scientific Report on the Safety of Advanced AI其中他提到了通用型的人工智能可能帶來的濫用風險故障風險以及系統性的風險。同時他也介紹了當前的一些安全對齊方法的一些局限性。其實我們今天的第一個圓桌討論聚焦的就是這兩個問題。第一個問題其實是面向前沿大模型的AI安全技術存在什麼樣子的挑戰。當然還有第二個問題就是面向更強大的未來的通用人工智能甚至是全方位的超越人類的超級智能安全技術應該怎麼做以及如何避免失控的風險。那首先歡迎四位老師。然後首先我們想第一個問題想要探討一下就是當前的安全技術的一些挑戰。那Don Song老師剛才您有提到就目前的防禦方法還非常脆弱比如說像SFT或者RAHF還有對抗訓練的這樣子的防護不夠有效甚至容易被reversed甚至容易被逆轉。當然您也提到了就是representation engineering還有safety by design。那其實想要拋出這個問題來講,我們想要拋出第一個問題是您認為當前的這些大模型出現這些脆弱性的底層原因是什麼以及什麼樣子的技術的新方向會更加的本質。好的。那麼,再一次,這類型的安全穩定性等等。我所展示的那種技術是在展示這些模型對這些攻擊的非常敵人。而這些平衡模型也非常弱。它們可以很容易被破壞,無論是在監獄破壞或是其他類型的攻擊。我認為一件事就是,首先,我們其實不太清楚這些模型的功能。而在上個學期,我在伯克利教了一個課堂的課堂,叫做《理解大語言模式的基礎和安全》。而我所指的理解的原因是因為沒有人理解。對嗎?我認為,我認為這是一個問題,因為我們不太懂得這些模型的功能。而我們今天做的這些平衡模式,例如,透過RRIHF,我們可以說它們只是在表面上改變了。它們只是在表面上改變了。我們現在實際上沒有把它改變成正確的形狀。另外,我認為,正如我所說的,特別是,對於ARC50,我們必須要讓它對抗敵人的攻擊力更加穩定。所以,在科技領域中,其實攻擊力並不太容易。但是,例如,在畫面領域和其他模型中,它們的攻擊力更加容易。我認為,希望是,我們可以建立解決方案,以防止它們的攻擊力。因此,現在,當模型變成多模型時,我們可以更加明顯地對抗敵人的攻擊力。因為,作為一個多模型系統,我們可以非常容易對抗敵人的攻擊力。我認為,因為,我們現在,我們正在做的所有的工程,都是在改變在表面上的東西。因此,在我的講解中,我提到一些未來的方向,我們正在嘗試深入地去做這些。當我們通過表現控制,我們可以改變模型的行為,以及我們希望有一定的保證。並且,我們必須在更深入地解決問題,與以防止它們的攻擊力相比,我們可以在RHR上作出更多的改變。謝謝,當中的宋老師。目前的安全防護,防護還比較表層。接下來想要,也想要問一下,就是耀東老師,其實我,我觀察到您在不同的場合,都有講過,比如說,只做RHR是不足夠的。然後以及你近期的工作,其實也發現了,語言模型對抗,抱歉,抵抗對齊,還有逆轉對齊的一個現象。您也可以談一談你的看法嗎?對對對。那我就用中文說吧。就是說,其實剛才很多學者都觀測到了一個現象,就是說,語言模型它做完這個對齊以後,你其實可以用非常少的攻擊樣本,就可以讓它變得不安全,哪怕你做了很長時間的這個RHF。那,RHF的那個tech lead,John Schuman,他就發現一個現象,就是,當這個語言模型訓練得非常好的時候,它俄語上發現的這個錯誤,它只需要用30粒英語的樣粒,就可以讓俄語上犯的這個錯誤不再犯。然後這個問題呢,我們其實也進行了一個深入的思考,甚至我們就最近有一個工作,叫Large Language Model Resist Alignment,就我們在這個工作裡面,去研究一個特殊的這個現象,就是逆對齊的問題。就我們都知道,你在訓練一個語言模型的時候,你總有兩個階段,對吧?你先進行預訓練,預訓練完了以後,你再進行SFT,你再進行一個RHF。那在參數空間的話,你可以把這個語言模型的訓練,想像成一個拉橡皮筋的過程。然後你越往後拉,越往後拉,你的張力其實是越來越強大的。然後我們就發現,這個逆對齊的這個過程啊,就像你把這個橡皮筋拉到很後面,它不能在伸展的時候,你這個時候如果把它突然晃開的話,它bounce back的這個速度,要比你拉的這個速度要快很多。所以,我們就把這個現象,在這個語言模型的訓練的這個過程中,定義為逆對齊。什麼叫逆對齊?就是我在預訓練完了以後,我在做比如說10步SFT,那我在做第11步SFT的時候,我是不是會發現第11步SFT,回到第10步SFT的這個速度,要比我從第9步做SFT,到第10步SFT的這個速度要快。那我們發現這個逆對齊的這個現象是存在的。並且呢,這個逆對齊的這個現象呢,可能會符合我們,就理解橡皮筋的這個運作原理裡面,那個胡克定律。胡克定律講的是,一個橡皮筋的這個硬力啊,等於彈性係數乘以形變量。然後這個彈性係數呢,我覺得在語言模型裡面,我們發現的就是和模型的大小,還有預訓練的這個數據量有關。然後那個,那個形變量其實就是你離SFT,就是Protrain完的那個Policy的那個KO divergence,就是你越練,它的形變就越長。那也就是說,如果你把這個語言模型接著不停地往後對齊,往後練,你看著是讓它越來越安全了,但我們在這裡呢,在那個paper裡面,從理論和實踐上都證明,其實它逆對齊反而會更加容易。這也somehow可能從一些機制上能夠解釋,剛才雅琴老師啊,宋老師啊,都會提到的一個觀點,就是你越做對齊,可能它反向就越容易被攻破,並且你用的這個樣例,可能不需要很多。這我覺得是個非常有意思的現象。當然也揭示了我們未來可能下一步,對於如何更好地做安全對齊,做價值對齊,會有一些這個指導意義,對。也希望大家關注這個,我們組的這個主題,這個工作,就叫大圓模型,Resist Alignment。對,這個橡皮筋的這個類別,還是挺有趣的。對,其實剛剛我有注意到,當宋老師有談到那個,多模態大模型的一個,就是對齊的難度。其實我知道邵信老師,過去幾個月,您的團隊其實有發表,就是很多篇,關於多模態大模型,還有智能體的攻擊和評測的工作,比如說像SciSafe,還有Chef數據集這樣子的工作。那,其實就順著這個主題說吧,就是您覺得比如說像,對於GPT-4O這樣子,以圖片,視頻,語音,這樣子的連續空間裡面的數據,作為輸入的一些多模態大模型,在就是這個安全的方面,安全對齊的方面,有沒有什麼一些特殊的挑戰?對,這也是很好的問題。這確實是在去年年初,可能大家更多關注的,還是大圓模型本身的安全性問題。但是,因為我們團隊裡面有很多是原來做視覺的,還有一些化學科的同學和老師專家,然後大家會發現說,我引入了更多的信號,比如說圖像,視頻之後,它帶來的複雜度是急劇提升的。它帶來的安全問題也是跟以往的專語言大模型是不一樣的。比如說大家可能常說的語言模型裡的幻覺問題,其實在多模態的模型裡面也是有的。這兩者的區別是在於什麼呢?就是語言模型裡的幻覺問題,可能它的定義是稍微比較明確的。但是在多模態模型裡面,它有可能是本身視覺的分支,它跟語言分支的上下文的理解比較弱。所以它根本就沒有理解這個問題帶來的幻覺。也可能是視覺分支本身,現在它的grounding能力也比較差,所以帶來的幻覺問題。也可能是有偶合性的各種原因。你如果更多的模態之後,這個分解的複雜度就會變高很多。但是現在大家可能對這方面的研究還是比較初期,所以並沒有給出很明確的結論,或者是有一些更具象的分析。然後另外的話,我們其實今年年初的時候,在Gemini出來的時候,大概短期之內,我們就做了一個大概三個月的評測報告。這裡面包括了對Trustworthiness的一些評測,也包括一些泛化性的,還有因我推理的。因為我們相信說,多媒體大模型未來能夠用在的環節和產品應用裡會非常的多。那我們不僅關注它的可信的問題,也會關注它同時的泛化性的,還有一些推理的問題。這也同等的重要。甚至說,其他的這些能力可能會影響它本身的安全性的問題。所以未來的話,我們也會花更多的精力和資源在這方面的研究上。謝謝。對,謝謝邵靜老師。對。然後,其實我也想問一下卓昇老師的一些觀點吧。就是,我其實也有看到您之前有做一些多模態大模型,然後還有agents方面的一些安全方面的工作。然後,當然現在agents其實是特別火的。就是那些可以直接進行序列決策,然後直接操縱工具和API的一些智能體。那我之前關注到您的工作可能是,之前有一篇是叫Our Judge,然後是通過監測交互記錄的方法來識別自主智能體agents的一些風險行為。那如果討論到agent的安全的話,您覺得有沒有一些特殊的難點想要分享?好的。我就沿著邵靜老師剛剛提的這個多模態大模型這條線。就我們也在做,就是agent它有那種成NLM agent,也有那種基於Multimodal的agent。那麼我們就發現,其實這裡面一個核心的點就是在於,agent它是把大模型用在虛擬或者現實的環境中,讓它對這個現實產生影響。那麼從這個特點上來看,agent它就涉及到大模型與用戶以及環境之間進行的一個多輪動態的一個交互過程。那麼它跟傳統的大模型的安全一個重大的區別就在於,它是在一個真實環境裡面,那麼它的安全風險的來源就會涉及到用戶環境和模型本身,這三個維度的這個安全問題。然後而且它這個設計的,我們現在更強調的是一個通用的agent,那麼它所處於的這個環境也是多種多樣的。那麼我可以從這個環境中去構造相應的攻擊樣本,這是其二。第三個最核心的點就是在於,我智能體這個行為它不像我們靜態的AIGC的這個信息,智能體在這個交互過程中,它的這個後果我們往往是難以去預測的,我不知道它未來會產生什麼樣的後果,以及它現在的一系列的行為,未來會下一步行為會怎麼去做,那麼我們要去預測它未來的風險也會變得更加困難。然後結合這些問題呢,我們最近在AIGC的基礎上,我們也在做一些動模態的探索,就例如,現在大家很多人在關注,尤其是Apple Intelligence,我們希望去讓大模型接入我們的手機或者是電腦,來模擬人類的這個屏幕的操作,幫我們完成複雜的指令,那麼我們攻擊者呢,他就可以,一方面可以從用戶端,我們去構造各種對抗或者劫持的樣本,來影響這個智能體的行為,可以去對抗,我們也可以把信息植入到這個屏幕信息中,例如智能體在操作網頁,或者操作我的App的時候,我也可以在它讀取的這個環境裡面,去植入新的指令,那麼智能體它看到這樣的新的指令的時候,我們就發現在很多場景下,它就會受到新的指令的影響,而忘記它之前的行為,導致這種劫持的問題,那麼這就意味著我們攻擊者,它不僅可以在user端,像我們傳統大模型那樣,我去在user端去做對抗,我去攻破你的對襲,然後也可以在這個環境端,我去給你進行誘導,或者進行指令的植入,來影響你智能體的行為,從而對環境或者用戶這個利益造成損害,所以這個裡面就涉及到,這個三個方面的,就是多樣化的這個攻擊來源,變得比較有挑戰,而防禦方面的話,我們現在大家的主要關注點,都是在於大模型本身的這個對齊,但是呢,其實我們在智能體的這個應用過程中,我不僅需要大模型的對齊,我可能還需要一個外部的一個反饋,就是我只是大模型本身,它知道它行為安不安全,這是一方面,但是它這個行為過程中,我們是希望它是有效的,我是希望它能夠盡可能幫我完成任務,那麼隨著這個模型變得足夠強之後,它的任何求解能力足夠強,它去做任何事情,所以我比較主張的一個觀點,就是通過一個外部的一個監管機制,跟這個模型本身對齊來進行一個互補,我們去,所以這也是我們做ARJAS的一個初衷,我們去動態地去分析和監測,這個智能體的它的這個行為歷史,對它未來的行為進行預測,來預先預判它可能成長的安全威脅,然後給出一個安全的研判結論,把這個信息反饋給模型,讓模型基於這個反饋,利用它的這個學習能力,來進行這個反饋,讓模型自我的攜帶,從而實現一個安全的閉環,這個是我們做這些事情的一些基本的想法,對,謝謝周正老師,尤其剛你有提到智能體還有大模型,或者說大語言模型的兩個關鍵點吧,一方面是這個存在與環境和人類的用戶的交互,另一方面是這個影響尺度,impact horizon的這個區別,對,特別好,那剛才我們討論的都是,就是,可能現在存在的大語言模型,多模態模型,還有智能體的一些安全挑戰,那最後一個部分,其實也想跟四位老師,就是探討一下,未來有可能出現的更強大的通用人工智能,甚至是超級智能可能帶來的失控風險,那其實我注意到就是,邀東老師,還有當送老師,包括剛才在台上的雅琴老師,今年在今年三月的時候,在北京的頤和園,有共同參與簽署了一份,關於AI風險的一個共識聲明,那針對前AI的一些特定的危險能力,劃定了五條安全的紅線,那與,其中呢,與這個AI的失控風險強相關的一些紅線,包括,比如說自我複製與適應的能力,還有欺騙人類的能力,以及這個尋求權利的傾向,那其實接下來的這個環節,想要拋給四位老師的問題是,就您認為,就當前哪一些的,就是危險能力的研究判斷最為緊迫,以及對於一個,就是目前還尚未出現的一個未來智能,更強大的一個智能,什麼樣子的技術方向,我們現在可以做什麼樣子的技術方向,能夠去未雨綢繆,然後能夠去做一些準備,夭壯老師,你想先開始嗎?那可以,對,我們今年在年頭的時候,在頤和園,和國內外許多專家在一塊,我們在討論就是,因為英國有這個,布賴切利宣言嘛,然後,包括剛才結束的這個首爾會議,其實我們國家都參與了這個深度的討論,但是呢,可能在這個國內以中國的這個學者為主導的,這麼一系列的這個討論並沒有發生,所以我們在這個智源的領導下,也是請了一系列的這個國內外的這個專家,進行了一系列的研討,那然後一個比較有代表性的成果,就是劃定了一些更加具體的red lines,就您剛才所說,包括很多這個台下的專家,還有宋教授,都是我們這個red lines的這個簽署者,那其中排名第一的這個風險,就是這個自我複製的這個問題,其實這個問題,我認為可能目前,還是有一些這個低估的這個趨勢,就是剛才Yoshua的那個PPT裡面,有一頁其實講得非常好,就是對於這些評測能力級,我們是能看到隨著年份的往後增長,他這個學習的這個曲線呢,這個寫率其實是越來越大的,那我認為現在可能原模型發展的一個趨勢,可能,如果拿AlphaGo類比的話,還停留在這個第一階段是吧,就是這個supervised tuning的這個階段,學習人類的這個數據,那你一旦往後進行這個self play,和reinforcement learning,pure reinforcement learning,就是self improve的這個階段,他可能這個能力的提升,會somehow,可能就突破了某個threshold,就是突然往上走,因為你從圍棋這個非常huge的這個space的探索來看,我們也是有AlphaGo,AlphaGo Zero,和AlphaZero,其實我們在做AlphaGo的時候,你也不能預見到後面兩個版本,它有那麼大能力的這個提升,我覺得這個self improvement這個事呢,可能和這個發現會比較有關係,那從學術研究的這個角度上來講,我們確實發現,現在已經有非常多的這個self play,RHF,RLAIF,確實能夠在某種意義上,提升模型的能力,無論在數學還是代碼能力上,那可能加以更大的這個算力,和更高效的這個自博弈的這個機制,尤其是在人類語料用盡之後,是不是能夠通過自博弈的這個方法,進一步提高語料的這個質量,進一步提升訓練的這個難度和有效性,那如果這個問題能被突破的話,那可能我們所謂的這個自我複製的,和self improvement的這個風險,確實能變到一個具體看得見的這麼一個風險,所以我們在這個想這個red lines的時候,就是把這一條給它放進去了,然後後面其實還有一些風險,像deception,還有什麼,還有一些這個misuse相關的這個風險,那個其實我認為可能相比於abuse,更多的是在misuse這個階段,那那個可能需要更多的這個國際的對話,國際的這個治理,那我相信北京的AI安全共識,也是在往這個方向去進行一個推進,包括我注意到我們WAKE大會,今年上海也發佈了上海市政府的,這個人工智能國際治理倡議宣言,也是希望能夠在這個國際的這個合作上,能夠推動進一步的合作,我認為這個方向都是非常好的。好,謝謝耀東老師。對,另外三位老師,誰想先開始?OK,I can add to that.I think,right,so today,even though the large language model is already very powerful,but we know that it's still,actually,we are still at the early stage.So,I think,and the next step already people are talking about,so for example,like having embodied intelligence,being robots,with these foundation models,essentially,so right now,we are still just training,we have the pre-training phase,like for large language models,and then we,right,and then we do inference and so on,but in the future,as we do embodying intelligence,and also as we have agents,that's actually going to act,in environments,we are going to have more of a closed loop,where the agents,take inputs from the environments,and then,and then try to make decisions,and then get feedback,and then use that feedback,it can then,help itself to further improve,do self-learning,do continuous learning,and so on.So I think,as we get into this,more of this approach,then,essentially,we are,how to put it,essentially,we are making the learning,also into the next stage,and I think what we are concerned about,it's for example right now,even though with large length models already,you can say,the model can try to,when you give the task,it can,if you tell it to think step by step,it can also,break down a task into,different sub tasks and goals,but,still that's,now it's a very strong capability,but in the future,as these agents,become more autonomous,and also,become more powerful,in particular,for a given goal,it's going to be able to,break down into sub goals,and then,figure out what's the best way to,to accomplish these sub goals,that's where we are also worried about,like this paper clip,have our problems,where it can,derive these,these dangerous sub goals,that's actually not well aligned,and so on,and then,and then in this case,right,it could have other sub goals,including,right,how it can get more power,and then,how it can deceive,humans or others,to get more power,and then,and then how it can self,replicate,to sustain,and,itself,and,and so on,so,so also,right,as,as you mentioned earlier,I think the,right now,so we are not seeing,these capabilities,yet,but the first is really important,that we develop methods,to do early detection,right,so like a canary,and so on,but also,the other thing is that,these type of behaviors,the moment you see it,it's very possible that,the time duration you have,is very,very short,you can think about it,right,basically,the moment you see it,it probably is already,it has already started,the self-improvement cycle,right,right,and as we know,as it gathers more computer power,and so on,the self-improvement cycle,can go really,really fast,so I think this is the challenge,and for a lot of people,who don't work in,frontier AI safety,they,I think,the thing that they miss,is even though they can say,that's why earlier,also you have to mention,a lot of people say,oh,you know,we don't need to worry about it,these risks are,are very far out,but I think those people,what they don't recognize,is that the moment you see it,it could be already too late,so,so I think these are the challenges,that we need to address.對,謝謝當桑老師,剛才說到了,這個early detection,其實我們,下一個,session,其實就是講,evaluation,AI safety testing,所以後面,也可能有一些,講者會有一些,更多的insights,對,另外,就是,對於剛才這個問題,邵靜老師和周生老師,想要,也評論一下嗎?我,剛才幾位老師,已經說得非常全面了,我可能就有一點,小的感受,就是,其實剛才,周生老師也講到說,agent 在很多,場景裡面,跟 environment 有這種交互,然後在應用的環境裡面,它受到很多因素的影響,就不只是,本身模型自己的安全性問題,感覺這方面的問題,未來也會非常的,這個凸顯,然後,比如說,像我們實驗室裡面,不僅,我們整個實驗室做AI的嘛,然後裡面也有很多做,AI for science 的專家老師,那在這個science學科裡面,其實現在AI的滲透,也會越來越強,那在這裡面相關的研究,其實現在並沒有做的特別多,然後大家可能更關注,這個AI在一些,這個比較,跟我們平時日常生活,接觸比較多的,這些環境裡面的,安全性問題,比如剛才提到的,濫用問題之類的,那我們可能也會,同時去呼籲大家關注一些,在這種特定領域的,或者是在垂直領域的,這些安全性問題,未來這方面的,這個,就可能帶來的,這個危害影響也會更凸顯,就做這一點補充嘛,周正老師,好的,我也是沿著邵俊老師,繼續補充一下,其實我目前一直關注的,就是大模型智能體的,它在這個開放環境中,這個行為交互的安全,就現在一句話說,就是這個意思,在這個行為交互過程中,它的安全問題主要是體現在,我們現在都傾向於把大模型,用到工業控制,把它用到科學研究,以及我們現實的,這個用戶的,這個生活場景之中,那麼在這個裡面,它就會涉及到,剛剛提到的一系列,它被濫用,或者被劫持,可能會對環境造成影響,可能對用戶造成損害,那麼在這個過程中,我們要去確保它的安全,它就需要非常,需要一系統的這個解決方案了,它不僅僅是在於,大模型本身的,我們用戶,就是以ARGC內容為主的,這種內容安全相關的這些研究,怎麼去提升模型本身的,這個安全性,這是一方面,但是第二方面,我們還需要一套,非常完備的這個監管模型,我們需要去動態的去監測,這個智能體的這個行為過程,它是否會帶來一定的損害,對它進行有效的研判,然後第三個是這個,當宋老師一直提到的,關於這個系統的紅線的問題,就是我們對於傳統安全裡面,我們有一系列的這些,安全的問題,我們需要一個動態的規範,我們怎麼把大模型的這個,通用性跟這些安全規範給結合,然後實現一個自動化的一個,監測,這樣的話,一方面能節省我們做這個,網路安全監測的一個效率,另一方面也能,把大模型的這個,通用性給發揮出來,實現更加廣泛的這個,用途,當然在這個,總體過程中呢,其實我們,現在都是傾向於,從大模型本身來做,但這裡面還有一個很重要的點,就是剛剛提到的,這個動態的檢測,我們需要一個,Active的一個,一個檢測過程,而不是說,等模型行為做完了,這時候我再檢測,那麼可能這個時候,危害已經造成了,我們是很難去彌補的,所以從技術上,我認為其實是分成,我覺得一個非常,從這個方向上,我覺得非常重要的一個點,就是在於,智能體在開放環境中的,這個行為安全問題,然後技術上,可能我們需要,從大模型本身的,內設安全,然後以及這個,行為交互過程中的,這個動態檢測,以及網路安全的,這個系統紅線,等三個,各個方面進行,這個系統性的,這個防禦,然後技術手段上,我們不僅包括,現有的各種靜態的手段,還需要一些主動的手段,來進行,這個約束,然後這個是我的,一些這個觀點,對,謝謝卓尚老師,那由於時間關係呢,我們今天的第一場,原著討論,可能在這裡就結束了,就是也特別感謝各位老師,今天的精彩觀點,那我們請各位老師,返回前排就座,我把時間交給主持人君怡,感謝您的參與,謝謝,謝謝,謝謝,謝謝今天讓我們,帶來的評論,和評論中的評論,我們很 Elli,很感激,有很多分別,讚評評論,而且,我們也很感激,看看對比上次的評論中,的評論也是很關鍵,但在我們今天討論的,我們希望大家,可以相當明白,我們今天,正在對比上次的評論,也有一個很關鍵的,就是我們 encima的評論,視察考試,在波爾西亞的評論中,我們都將去到的,就是研究調查功能的,我們本場前,發現說,on AI safety testingNow we will hear from Dr. Chris MesserolDr. Messerol is the Executive Director of the Frontier Model Foruma non-profit established by Anthropic, Google, Microsoft and OpenAIto advance frontier AI safetyHe is an expert on AI governance and securityand is currently focused on developing best practicesfor the responsible development and deploymentof the most advanced general purpose AI systemsChris previously served as the Director of the AI and Emerging Technology Initiativeat the Brookings InstitutionChris, it's great to have you hereI'll hand it over to youThank you, it's a pleasure to be hereIt's wonderful to be able to speak with you todayAs was just mentionedI run an organization called the Frontier Model ForumIt's an industry supported non-profitdedicated to advancing frontier AI safetyWe have three kind of core missionsOne of which I'll get intowhich is developing best practicesThe other two are advancing the science of frontier AI safetyand the third is information sharing about what we're learningwhich is again part of why we're so excited to be here todayI thought I might beginwith just laying outa little bitwhat frontier AI isand why it's so challenging to deal withand then kind of walk through a couple ofsome early thinking that we haveabout how to think throughwhat types of evaluations to runand what are some early best practicesthat in discussions with our expert membersthe safety experts within our member firmswhat they're seeing and thinking aboutand how they're beginning to approach some of these issuesSo just to start withI think when we say the phrase frontier AIwhat we're generally referring tois the most recent generationof advanced general purpose AI technologiesSo what we're thinking ofare not narrow AI applicationsfor specific things likelending algorithmsor facial recognition technologiesWe are thinking about general purpose AI systemsand we're thinking about in particularjust the most recent generationsSo on this chart you can see thatbecause of the waythat we're scaling up these systemsgenerally speakingwe're kind of doing ayou know 10X in terms of computeevery couple of yearsto come up with a better classand generation of modelfor general purpose systemsWe are primarily focused onthe most recent generationof frontier AI systemsAnd if you want to see more about thiswe have an illustration of this on our websiteBut what this meanswhy this is so importantis that we expect the challengesthat we are dealing withto evolve over timeThe frontier is going to be consistently changingAs you can seethis is a stylized graphbut some of the graphsthat we saw earlier this morningsaw a very clear slope linealmost exponential curveof increase in capabilitiesWe are focused onjust the most recent generationbecause we want to understandwe want to develop early best practicesfor dealing with the most advanced modelsat any particular moment in timeAnd the reasonthat we're focusing on thisthis is so importantas was alluded to earlier today alreadythe reason this is so importantis the ability to grok certain capabilitiesAnd we don't know how to predictwhen these models in a training runare going to acquire or developparticular capabilitiesWe don't have a good wayas Professor Song alluded to earlierWe don't have a good wayto understand the systems ex antewhich makes it very hard to understandhow to build them safelyand effectivelyAnd I would saythe last point I would say iswe expect the frontier to continue developingas these systems move from just chatbotsto things that are a little bit moreagentic in naturethis challenge of assuringthe safety of these systemsis only going to become more importantbecause the systems we're buildingwill interact more and more with the real worldin ways that have potential consequencesfor public safety and securitywhich is what our organization is focused onSo as I mentionedI'm just going to walk througha little bitsome of our early thinkingthat's been developed in kind of conversationswith different safety expertsand the member firms that we haveabout how to structure evaluationswhat kinds of evaluations to runand then some early best practicesThese are very high leveldescriptionsthat we'll be talking aboutI would also say the terms themselvesmay vary but it's really the conceptsthat I want to share with you todaywe can have moreengagements and interactions over timeto begin as a fieldto develop best practiceswhen it comes to even just talking aboutthe types of evaluations we need to runto assure the safety of our systemso the first phraseis I think at a very high levelthere's two very general kindsof evaluationsor risk assessmentsone are red teaming exercisesanother are more automated evaluationsred teaming exercisestend to be very manualand kind ofthere's work going on to try and explorehow to automate some of the red teaming exercisesbut generally speakingthere are manual waysof leveraging human expertiseto probe the capabilitiesof a particular modelevaluations in contrasttend to be things like benchmarksor other automated formsof exploring the capability profileor the risk profile of a particular modelwe think it's important to distinguishexactly what you're talking aboutwhen you're talking abouthow you're assessing the riskor safety of a systemand this is just one general high level classof distinctionwithin the evaluationsof frontier modelsthere's really twoI think core types of evaluationsthat we want to runone are performance evaluationsand the other are safety evaluationsperformance evaluationsare critical for understanding the generalcapabilities orother capabilities that a model might havethat allows us to understandin some ways how best to test itfor particular risks etcbut a performance evaluationis really designed to justcapture and identifyand assess the performanceenvelope of a particular systemagain these evaluationsare incredibly importantbecause we don't know ex antehow to definethe ex post capabilities of a modelbefore we train the modelwhat it will be capable of on the back endso we need to be able to do performance evaluationsthe other kinds of evaluationsare safety evaluationsand there you're not necessarily trying to understandjust what the performance thresholdor performance envelope of a system isyou are specifically looking forparticular risksand the ability of a modelto exhibit behaviors that would give youthat is capable ofbehaving in unsafe waysumas far as kind of different types ofsafety evaluationsthere's really twosafety classes of safety evaluationsthatwill start to seebeing developed and runin model developmentone are developmental evaluationsand another are assurance evaluationsdevelopmental evaluationswhat we're referring to hereare really the kinds of evaluationsthat firms will runor the developers of a large scalesystem might runat different phases in its training cyclejust to kind of benchmark itto see how it's doing with respect to certainkind of safety risksthat's different from a full onassurance evaluation where it's not necessarilythe team that's developing the modelinstead it's a team that'skind of tasked with assuringthe safety of a system they have independent expertisefrom the team that's developing itand they aretheir goal is really to assure the safetyof the system and todevelop evaluationsthat are capable of assuring the safetyof a system in some waywhich is a little bit different than the kind oflife cycle developmentsafety evals that might happenjust at different check marks in thedevelopment of the modelthen the last kind ofin my view probably the most importantdistinction here iswithin assurance evaluationsso the evaluations that are meant to tryand assure the safety of a systemwithin that categoryas we're thinking abouttrying to evaluatemodels for safetywe really need to be evaluatingthe safety and assurance of these systemsfor you know one way ofthinking about it is maximum capabilityof the system another is likehow it's used in the real worldI would say a different way ofdefining this last categoryis we need to look forassurance evaluationsthat are designed to try and capturethe riskiest behaviors at the taildistribution of the modellike some of the behaviors thatare you know most capableor most extreme from a particular riskthat wouldn't necessarilybe compressed into kind of the averageor mean behavior we wouldn't be able to geta lot of information about those kinds oftail risks frommore typical user behaviorwith behavioral evaluationsI think the goal is more to tryand understand what is the averagebehavior or mean behavior in generalwith some of these models and how do we assurethe safety of the systemand the safety of the systemeven within that kind of mean behavioragain this is kind of early thinkingwe'll probably evolve over time on thisbut this is just a little bitof how we're thinking aboutsome of the different evaluationsat this momentrelatedly there's somethere's also the question aboutwhat to do as you're setting upan evaluation and a red teamingwhether it's a red teaming exerciseor a broader evaluationthere's a wide array of evaluationsfor specific risksbut there's also a set ofjust best practices for any kind ofevaluation you're doingregardless of the riskso it doesn't matter whether you'relooking at bio risks or cyber risksthings like thator it could be societal risksthat you're looking atif you are developing a systemor an evaluation ratherof a frontier AI model or systemthere's some I think important considerationsthis is just a samplingof some of the early thinkingwe have about high level best practicesthere's a few that I want tocall out specificallyone is we need evaluationsto account for prompt sensitivityI think any of the engineers herewho have worked with these modelsand tried to get them to behavein stable wayswill recognize that the specific wordingof different promptswill oftentimes lead to different resultsand what we're really trying to dois capturetheir kind of risky behaviorwhich means we need to exploredifferent wordingchoices or configurations of promptsto be able to get at whether or notit has a certain capability in generalan example of this would beif you're trying toif you're worried aboutsay likemalicious uses of a systemfor exampleyou don't want to just ask the systemhow do you build an explosiveyou also want to test forcan you describe the chemical processby whichdynamite releases energysomething like thatyou need to have multiple waysof asking for the same thingat least two other kind ofthings that I want to call outjust very brieflyone is as you're developingthese evaluationsyou need to evaluate both the modelthe underlying base modelas well as the end systemfor a lot of evaluationswe'll target one or the otherbut we need to do bothit's not just the underlying modelthat needs to be evaluatedbecause in many casesthat's not what is exposed to the end userusually what is exposed to the end useris the overall systemand we need to be able to test thatas well as the underlying modelanotherreally importantbest practiceI guess I'll walk through them just brieflywe're evaluating both normalor typical behavior useand adversarial useas I mentioned on a prior slidewe do want to evaluate the systemsfor kind of typical behaviorsand the safety that they might exhibit under thatwe also want to evaluate for adversarial usethis is something that a lot of developerswon't instinctively necessarily doas they're trying torush to get a product out the doorbut it's very important to evaluate adversarial useProfessor Song just had areally great explanation of some of the kinds ofadversarial use casesthat you need to be paying attention tothe broader point though isif you are developing frontier AI systemsit's not enough to just focus onI think I'll end onbecause I think it's probably the mostimportant best practicethat we are starting to converge onas a field in AI safetywhen it comes to evaluation designit is vitally important that you understandwhat the baseline isthat you are evaluating a system againstand what I mean by thatfor example if you want to evaluatea system for biological risksfor examplecan this system inform or helpsomeone design a bioweaponor some kind of dangerous pathogenyou need to evaluate itnot just for thewhat kind of information the model itself hasyou need to evaluate it againstthe kind of baseline applicationthat would be used in the absenceof whatever model you are testingso in many cases that would befor example web searchshould not just be kind ofabsolute understandings of riskbut also you knowrelative or marginal riskcompared to the counterfactual applicationthat might be used for whateverapplication you are developingthat is a very brief kind ofhigh level overview ofhow we are thinking about some of theearly best practices for justevaluations in generalinformation and communicationstechnologyDirect Away has led the developmentof multiple industry standards in Chinaincluding the country'sfirst big data benchmark standardhe has participatedin the drafting of severalmajor national policiesincluding中文來講解一下那麼我希望各位外國朋友能夠通過同船能夠很好的get到我的主要的意思那麼今天跟大家分享一下中國信息研究院在人工智能尤其是大模型評測安全評測方面的一些思考和實踐三個方面的內容一個是我們怎麼認識人工智能大模型面臨的風險這個risk到底是在哪些維度上另外第二個方面是我們基於這些對於風險現有風險的這個認識一直在推動建設benchmark safety benchmark的這個framework並且按季度去開展這個評測的活動希望通過這個實踐不斷的促進大家來提升模型的安全水平最後也是再分享一下其他有關如何保障人工智能負責任發展安全方面的一些相關的工作那麼現在大模型實際上是一種數據驅動的路線那它在skilling law上這個skilling law的延長線上一直在發展決定著模型能力的這個前景主要靠的是算力和數據那麼這個大家都非常清楚那實際上在這幾年我們可能所有人基本上所有人都認為skilling law可能還會延續但是skilling law的問題就在於它的表現具體是什麼樣缺乏非常清晰的認識而且控制能力是很弱的不像五十年代六十年代的基於規則的人工智能模型他們是我們設計者可以很好的去控制模型的行為和輸出但是在基於數據驅動的尤其是大模型的時代模型的表現很多時候是一種現象學的範疇所以很難在內生機制上去完全做到避免風險那麼這幾年大家對於這個人工智能尤其是前沿模型剛才Chris已經講得很清楚了前沿模型大家非常關心它的水平的進步同時也關心它可能潛在蘊含著什麼樣的風險那麼我們基於對於各類研究的成果的分析我們認為其實可以用這樣一個金字塔來表現我們對於人工智能尤其是大模型的風險的認知幾個層面分成兩個維度一個維度是自身它的內生的安全問題那包括了模型的參數還有數據計算系統以及網絡還有應用系統的這個security層面的安全問題那麼上面一個層次就是對於個人對於國家對於全人類在應用人工智能的時候可能引發的衍生的應用層面的風險我們如何去控制好那麼最宏觀的可能是我們人類在人工智能面前的位置在哪裡我們全人類的共同的命運那麼再往下就是國家安全經濟安全社會供應鏈的穩定人口就業以及個人信息保護等等吧這個risk實際上是多維度的所以這個討論人工智能的安全風險往往是一個多學科交叉的那麼需要去共同去更清醒的認識到這個安全的風險在哪裡再舉幾個具體的最具體的一個例子那麼以大模型為例其實現在媒體上對於大模型應用過程中暴露的各種風險報道也越來越多了包括在內容風險方面那麼可以看到這個虛假信息deseinformation其實是非常普遍了而且這個也是困擾不管是中國還是國外監管部門非常重要的一個問題那麼還有就是數據風險那這裡面臨的我們的提示詞可能會暴露機構內部的一些信息這樣的風險以及這個算法可能會被用提示攻擊或者其他手段來從裡頭提取敏感信息的這樣一些攻擊手法這個一直在翻新而且也是層出不窮的那麼不管是宏觀還是微觀層面其實我覺得現在大家對於人工智能的探討人工智能安全問題的探討已經非常的充分了最上面是聯合國層面對吧古特雷斯秘書長在非常多的場合在談那麼聯合國也成立了非常好幾個機構那麼包括這個UNESCO ITU等等在討論人工智能的治理問題安全問題那麼各個國家其實也在行動我們看一下具體的整體上感覺人工智能的安全風險治理正在從原則走向實踐那麼大家對於一些基本的原則比如說以人為本智能向善比如說要保證它的公平非歧視這樣一些原則大家沒有分歧那麼國際社會包括各個國家其實也都在把這些原則正在轉向具體的操作那麼這些操作可能會落到具體的很多方面比如說一些這個國際規則的制定那現在聯合國剛才說了聯合國教科文ITU等等這些機構在牽頭這個國際規則的設定那麼各個國家也在出臺相關的立法那麼另外一個再具體的一個就是轉化成標準那麼把原則固化成一些能夠成文的技術性的要求來便於這些企業去遵從還有一個更具體的就是有了標準以後還需要去驗證這些標準的遵從情況那麼在企業側可能還需要去研發很多的技術工具來提升保障這個安全的水平 所以我們看到美國 英國 歐盟 新加坡還有中國都在採取這幾個層次的具體舉措把人工智能的治理從原則推向實踐那麼總統院作為一個智庫和行業平臺我們也在投身於這樣一個促進人工智能治理從原則走向實踐過程中我們也貢獻我們的力量今年年初我們和國內三十多家單位數據集還是比較豐富的我們有五十多萬條的題目圍繞著在全球共識的要求上以及符合中國本土法律法規的要求這兩個維度上來開展評測數據集的建設那麼另外一個就是我們也密切跟蹤前沿發展為什麼我們是季度性的去做這個事情因為人工智能的進展可以說是日新月異所以我們會季度性的調整我們的Benchmark的數據集和方法不斷地提升跟進前沿模型的演進的步伐另外我們還是特別關注用戶是怎麼想的關注現實的風險比如說金融 政務 醫療這些行業他們的擔心是什麼目的實際上是希望能夠通過這種評測讓這些行業用戶放心能夠勇敢地去使用先進的人工智能技術但是前提是保障安全的前提下那麼在2024年的4月份我們發佈了第一版的AI Safety Benchmark的結果這個結果是基於我們左邊這樣一個框架來做的大體上分為三個方面一個是內容安全層面的要求這就包括了法律明令禁止的一些內容的輸出比如說涉黃 涉爆賭博 欺詐 危險化學品 生物武器這些內容的管控輸出另外就是數據安全也包括了個人隱私和企業機密的可能被提取的這樣一些特徵個人信息和商業秘密還有最上層是科技倫理這裡頭就包括價值觀 心理 健康AIE 使工序良俗辱罵 誘導這些問題的應對他們是如何評分能夠得到八九十分這個水平還是可以的但是問題就在於拒達率偏高這樣會造成很多用戶體驗不是特別好的同時這也表現通過具體的攻擊手法這個攻擊手法也是我們能收集到的市面上學術界包括社區暴露出來的最新的攻擊手法包括我們自己測試中發現的攻擊手法來測試這些模型包括提示式攻擊誘導攻擊 越獄攻擊內容泛化攻擊和其他的攻擊手段那麼數據集也相應的做了擴大底線紅線 社會倫理和數據洩露還有一些其他的安全保障措施的測試應該說是一個更加全面的這個體系是一個正在演進的這個過程要跟上這個節奏那麼Q2的這個評測數據集也比較大 那麼我們有600多條提示詞的模板那麼有60多種攻擊手法3.6萬條的這個攻擊樣本每次從這3萬多條裡都抽取4千多條來做測試那麼測試完了這個數據就淘汰了避免大家去刷題和作弊那麼目前我們Q2的這個結果也已經初步的出來了那麼可以看到這個大家在這個原始輸入的攻擊成功率就是我們不用提示詞攻擊手法來做這個提問的話它的攻擊成功率還是比較低也就是反過來說1減這個比例就是它的這個得分但是我們加入了很多提示詞攻擊以後的成功率就顯著提高了也就反向的代表著可以有很多的手法來繞過這個模型的護欄所以整體上來看我們目前是模擬了最近發生的安全攻擊的各種手段來更好的來看待我們模型安全的水平大模型的安全測試是中國新通院在人工智能安全方面的一個實踐 具體的實踐除此之外我們也希望能夠系統化的去推動人工智能可信發展那麼就在2021年的時候上海世界人工智能大會我們發佈了全球首個可信人工智能白皮書這個是跟京東探索研究院一塊發佈的 陶達成院士那麼當時的思路是過程管理就是要人工智能要保障安全除了你要求它結果以外 還要管好它使用的流程 所以過程管理我們認為是非常重要的所以在那一年的白皮書裡頭我們就提出來如何在這個各個 包括G20包括很多原則的指導下我們怎麼在企業各個環節落實這個原則23年到現在我們認為結果管理也非常重要就是我們參考了像OpenEye和Anthropic 他們做這個安全模型的這個分級分類那麼把風險分成不同的類別和等級那麼我們我覺得這個也是特別有借鑒意義所以也在推動從過程管理走向這個結果管理 效果管理那麼基於風險定級的這樣一個管理手段所以這個其實我們希望通過這樣一個方法論來讓企業界讓產業知道他應該怎麼去做是一個最佳實踐那麼同時我們也在制定這個Benchmark包括Benchmark在內的這個坐標尺因為如何 如果沒法度量就沒法改進 所以這個測試是非常關鍵的除了這個大模型的安全測試以外我們也在做這個人臉識別安全還有這個算法安全等等這些安全的這個測試標準測試能力以上我們也在建設這個龐大的這個數據集包括人臉和其他的一些這個這個測試級能力的建設一直在持續的跟進那同時這個這個大模型的內容安全就是包括內容安全他如何去測出結果以後如何去增強我們現在也在基於我們掌握的這個我們建設的這個三百多萬條關鍵詞五十多萬條提示詞來訓練我們的安全測試的這個模型那麼未來希望通過參數的方式提供給這個模型廠商來加固他們的模型能力那麼我覺得安全其實很重要還有一個就是信息真實性的保障信息真實性如何保障我覺得要靠很多方法其中一個方法就是水印這個watermarking 是吧對於水印以外可能現在比如說圖片裡頭有C2PA的這個標準Adobe他們發起的那麼還有其他的一些方式其實也都在探索中綜合來看我們也希望能夠建立一套對抗deep fake來保障內容真實性的一套技術的方法包括水印我們現在也開發了水印的隱世寫入的算法也提供這樣的一些API供這些企業也可以使用整體上我就利用這麼一個時間跟大家分享我們的一些實踐希望跟大家繼續交流也感謝會議邀請我來跟大家分享謝謝謝謝各位專家領導各位同仁大家好很抱歉喬老師還在趕來的路上我就先來做個開場待會喬老師來了之後會給大家繼續做這個分享我們今天講的這個整體是根據喬老師作為一個親歷者和領導者的視角去回顧實驗室在做大模型包括能力安全整個路線上的一些感受和一些觀察整體來說會分四個部分來講首先我們知道現在整個人工智能的發展是非常飛速的然後裡面涉及到的國家也越來越多不僅是中國國外各類的國家大家都會關注能力和安全相關的共生性我們最開始可能在17年的AlphaZero的時候大家可能更關注面向傳統AI算法和系統的安全標準然後到後面的時候隨著更多的模型出來包括Generated Model包括從前幾年開始的ChadGP系列整體來說現在大家會發現我不能只觀察AI算法和系統的安全標準或者是AI算法本身的能力更多要觀察它的數據的規範還有到面向整個社會和群體的大模型的安全評測和對應的安全標準實驗室整體從2021年成立至今一直致力於大模型的體系的研發從2021年開始發布的第一個書生1.0的模型是一個視覺的通用大模型當時應該在國際上也有很多包括DeepMind Google OpenAI都有相關的視覺通用大模型隨著時間的演變到了2022年實驗室開始走向多模態大模型在這裡面也逐漸衍生出來大語言模型書生普語體系在昨天的開幕式上也做了相關的介紹然後一直到今年7月份書生普語包括書生多模態大模型已經演變到了新的能力範疇包括公開的模型包括7B的小參數模型還有70B的大模型一直致力於開源開放的理念能夠賦能大模型學術研究和創新產業生態並且我們也一直保持技術的原始創新探索更為高效的大模型發展路徑整體來說我們實驗室的大模型研發和安全體系構建隨著這個時間的歷程也分為四次挑戰和對應我們做的相關的技術和成果接下來的話會分這四次挑戰為大家來分享一些工作第一次挑戰的時候大概在2022年底我們發現越來越多的生成模型出來之後它帶來的風險性也越來越大包括一些這種濫用的行為比如說用這種模型去對那個蕭老師已經來了謝謝蕭靜教授帶來這個精彩的開始我們很高興現在有蕭瑜教授蕭教授是助理教授和主導科學家在上海AI博物館以及一個兼職研究生在上海AI博物館進入了中國科學博物館在上海AI博物館進入了中國科學博物館他的研究範圍有多種領域包括多模型模型電腦視線深入學習和自動駕駛在上海AI博物館與其他研究者合作蕭教授在上海AI博物館pool 1紹靜博士一下我想這樣其實剛才那個紹靜博士行安全团队的负责人我甚至在想他应该比我会了解更多的技术细节会讲更多的干货但是我还是非常感谢安远和谢总的邀请能有这样一个机会跟大家做一个分享我就直接跳到刚刚那个我们是讲到这一页是吧对对对我先说一下是这样的我其实原来主要做计算机视觉的研究然后慢慢的开始做视觉大模型从2020年开始然后随着我们从语言从视觉到语言到多模态做我们越来越发现安全实际上是大模型发展中非常重要的一个问题而评测又是整个建立安全的基石所以从去年开始在实验室的整个的布局下面我们就把安全做一个重要的方向大家可以看到在做安全的时候首先的第一个问题就是说大家事实上都很关心安全但是对于安全领域我们到底具体关于哪些因素应该从哪些角度去评测实际上并没有特别广泛的这样的一个认知那在这里面我们当时看到第一个事情是评测体系和数据的缺乏所以在去年我们进入这个领域之后第一件事情就是做评测体系我们建立了当时蒲公英人工智能一个治理开放的平台这里面把国内还有国际上很多的治理理论制度进行了汇总我们也建立了包括红队数据漏洞数据评测数据等等相对来讲比较全面的一个评测的数据集在这里面实验室也坚持开源开放的理念我们把我们的很多的数据集还有我们的规范进行了开放从而推动这个领域的发展怎么建好这个数据集大家知道早期大家评大模型就是我给一个固定的题集然后有标准的答案让这个模型答完了之后我们对它进行评测进行对齐但事实上这里面是有很多问题的我们知道对于大模型来讲如果你做了SFT做了IRRRLHF人类反馈的强化学习事实上它表面上表现得很好但并不是代表它真正的就没有风险没有问题那在这里面怎么来做呢我们当时设计了一个方法我们发现很多领域的专家包括很多做社会学政治学伦理法律的我们组织了上百位我们请了包括上海交通大学复旦大学的上百位的专家这里面还有包括很多的教授让他们帮我们设计总组问题因为这些专家时间非常有限你很难说你请这么多专家让每个专家你帮我设计一万个题一万个题事实上对大模型的评测还是非常窄的一个领域了我们在专家设计题的题目上通过大模型的方法对这些题进行增强进行扩充然后我们再用这些题目去评测特别是这些题目很多的设计它有很多的攻击性而且有针对漏洞的一些专门的设计这样就形成了更好的评测的效果这是我们在第二个方面所做的工作然后因为我既做模型也做模型的安全在这里我们就发现大模型的安全和对齐大家经常说有一个问题叫对齐税也许确确实实你做了RLHF了之后这个模型的安全性从评测上分数会有增加但往往对能力对原来它原有的一些能力模型的性能会有下降在这里面我们是不是有更好的方法在这里面我们事实上就把一些MODPO就是Multi-Object DPO多目标的对齐引入进来这样能够保证我们在保证对齐的同时也能够对原来的能力进行优化这里面有些算法的创新由于时间原因我可能就不再详细讲而且我在想在这里面MODPO现在目前也不是一个结束式应该这些算法和框架还有很多优化的空间我看在座有很多我们年轻的同学这可能成为你未来研究很好的题目基于我们形成的评测体系基于这些评测的数据我们实验室也研发了一个普安大模型安全评测的系统在这里面我们事实上把一些我们前面所做的从维度到评测的过程到出报告完全把它进行自动化的评测这里面也支持了我们上海市的一些工作在这里面我们发现就是说评测结果的透明性和可解释性是一个很重要的事情因为经常的时候随着评测的维度越来越多问的题目越来越深经常我们很多大模型的评测用户拿到评测报告他会问我们你们为什么模型这么你为什么出这样评测报告你这个评测报告是不是客观能不能对它你的结果的依据进行说明进行解释在这里面我们想到最终的话我们解释还是基于我们建立的基于这个数据库完了之后我们把我们的评测结果对它进行可解释性的生成这样就把问答和可解释性的一体化集成在一起而且因为采用这个方法它是一个外籍的数据库很好的方法就在你可以实时的更新比如说我们有新的规范新的规定我们可以非常简单的更新到这个数据库里面进行评测那当然了大家知道现在的这个整个大模型从语言到多模态发展我本身也做计算机视觉的研究多模态大模型的事实上又对这个评测提了很更新的要求比如说在这里面对于多模态来讲由于它内容生成的形式和形态更多如何定义这个维度需要进一步加另外呢在多模态之间本身就有特征的对齐的问题而且我们知道这个语言到多模态了之后事实上很多幻觉问题被进一步加强针对这些特点呢我们做了一个以视觉为核心的包括对齐还有评测的数据机SPVL它包括我们设计了针对多模态领域六个比较可能引起有害的领域四三个类别五三个子类别这里面也包括了超过十万个这样的问题然后来帮助我们一方面做评测另外这个数据可以作为对齐那么我们也做了一个评测此外呢实验室呢我们在去年还花了很大力气我们在多应该是在国内甚至是国际上比较全面的一个具人类价值观的一个多模态评测体系这个报告呢总共有四百多页四种模态而且在这里面呢我们发现啊就是说单独的提维度事实上对于这个领域的技术的发展还是很有局限性的我们建立了两百三十个用例而且这两百三十个用例呢是根据多模态大模型应用啊场景落地来提出来的所以我们在这个评测体系上包括放化性可信度还有推理能力大家知道未来多模态大模型事实上它的推理能力是大家现在关注的重点我们可以看到最新的大模型大家都在强调它的数理和因果的推理能力进行评测事实上这个评测结果呢也对我们后面包括大模型特别多模态大模型安全的评测起到了很大的指引的过程面向未来呢我们觉得有几个方面值得关注还有一个一个就是这个多智能体大家知道啊这半年呢Agent技术发展很快Agent已经成为强化大模型在专项领域包括跟很多世界进行互动的一个重要的手段那是否能够把这些技术也用来安全和评测呢事实上我们建立了一个多智能体的评测框架叫P-SAFE然后在这里面呢我们详细研究了多个智能体之间大家知道它之间有交互和协同它所能产生的这个危险行为以及在这个过程中间我们如何引入一个专家一个doctor去进行角色防御另外一个呢大家知道人与人之间交互过程中心理这个价值观是非常重要的一个问题所以呢我们也在探索如何从仿造人的方法从心理学的角度然后从人文科学的角度来去做这个大模型多智能体的安全评测我想说一点是这些工作其实才刚刚起步从实验室角度来讲呢我们最初关注的包括定义问题包括建一个好的平台让大家来做这方面的研究这里面有非常多的重要工作我们也非常欢迎啊在座各位的机构企业还有研究者我们一起来共同推动这个领域的发展实验室除了自己做之外呢我们还坚持着开放的策略事实上呢我在中国网络空间协会大家知道这是在我们国家这个领域非常全员的协会的架构和指导下呢我们成立了围绕深层次人工智能安全评测工作组我本人呢也非常有幸的当选了工作组的其实压力还是很大的因为这个工作组既包括咱们国家一些权威机构也包括在这里面大型研发的包括交大清华复旦这项大学还有包括百度华为这样的企业为什么要成立一个工作组呢我们认为呢要做好博伦馆是安全还是评测它一定不是一个机构是一个共商共治的事情在这工作组在过去的一年接近一年的过程中我们从几方面开展工作一方面是建立了常态化的这个交流的机制我们有日常的例会而且现在呢现在通过线上会议通过专题会的形式让大家日常能交流第二个呢我想这个工作组的一个很重要的方面呢我们是想从bottom up的从底层从大模型一线的角度能够形成技术规范和共识在这里面我下面会讲我们现在已经初步形成了一个声称是人间自能评估的流程规范这里面我不知道在座是不是有一些同事已经参与因为这个设计的范围比较广已经参与到这个工作进来第三个呢我们在这个规范的基础上呢我们会跟大家一起共同研发一些评测的技术和组件这里面包括数据集也包括工具而且呢包括我们刚才讲的平台而且这里面我们希望能够在大家形成一致的条件下能够拿出来一部分进行开源开放的形式提供服务最后呢我们也举办了很多安全的活动包括安远的很多活动今年在世界人工智能大会的前夕呢我们也举办了普悦安全挑战赛吸引了全球一两百个非常优秀的团队来参加我们也日常跟很多的一些相关的大模型研发机构做安全的指导这就是我们所做的刚才提到的安全评估的流程和规范它有几个特点第一个它可操作性很强我们是一个全维度评测提供全生命的这样一个规范第二个特点我们是服务应用和产业落地因为参与的本身很多企业实际上他们从实际应用落地的角度起了很好的建议我们希望这个规范也能成为推动咱们国家未来人工智能安全评测发展的重要面向未来我想说一点大家往往容易把人工智能看成一个工具但是我更想说的随着通用人工智能的发展随着技术的进步人工智能成在成为我们整个社会体系的非常重要的一个基础设施它会频繁地与人互动与其他的系统互动所以我们现在考虑人工智能安全绝对不能孤立地把它看成一个工具而应该从整个社会体系的角度来思考人工智能这里面包括整个需求包括应用的场景还有包括在这个过程中间人机物之间的这种认知体系的建设是很重要的办法怎么来应用这个地方呢我觉得就是说大家知道大模型的发展是以Skyding Law为指引随着算力数据模型规模的增大模型的能力不断提升那么我们在想是不是我们也要探索一个围绕安全的Skyding Law我们能找到一个可扩展可发展的方式只要我们投入更多的研发的资源数据算力一些技术的研发的投入我们就可以把安全也可以可持续地发展下去在这里面我觉得安全的Skyding Law可能比原来大模型的Skyding Law大模型Skyding Law核心是参数量 数据量和计算资源这三者首先也是安全的更重要的组成部分但是对于安全的Skyding Law绝对不止三个维度我们需要多方的参与需要更新的研发的模式当然我们也需要高质量的数据以及更好的模型的架构这里面事实上也对于我们的Community不管是你做科研的比如说产业用器做了新的挑战我们希望跟大家共同努力建立起面向未来的可持续的AI安全的Skyding Law在这里最后我也稍稍做一个宣传在明天的上午我们在世博中心620会议室有一个国际AI前沿技术的论坛我知道今天在座的一部分专家也会参加明天的论坛也欢迎大家继续参加和支持我们的活动好 谢谢大家",
    "谢谢大家我们也邀请了 Dr. 瑞敏河和 Prof. 熊德义Dr. 河和 Prof. 熊德义,请在我介绍你时,请尽快上台Dr. 瑞敏河是新加坡首席人工智能警察他执行多项目标的努力,以达成新加坡的战略AI目标包括发展和实现新加坡的国际AI策略他亦是新加坡政府首席人工智能警察以及AI部队的国际高层负责人Prof. 熊德义是天津大学的国际联合研究中心和天津大学的国际联合研究中心教授他最近在计划多项AI安全关系的项目上工作包括大规模评测评测和更多的评测我们的主席是Brian Zhe他是Concordia AI的副总裁让我们请他来给我们一个掌声大家好,欢迎大家来到我们今天的第二个讨论我们的第二个讨论是天津AI安全评测我今天与中国、新加坡和英国的领导领导领导非常兴奋地讨论这个讨论我们的讨论是天津大学的AI安全评测这是您今天第一次上台让我开始吧您发表了第一次的讨论是大规模评测评测更最近的是中国LM的链接风险评测我们有三个领导领导领导第一个领导领导领导是不平衡风险这是社会上的一个影响第二个领导领导风险是不平衡风险评测人们可能使用生命训练模式来减弱生物武器第三个领导领导领导风险评测是高级风险评测很多人对这个风险评测有不同的语言有些人认为这是高级风险评测还有宇宙风险评测以这种风险评测的背景来说我认为这三个领导领导领导评测是不平衡风险评测的我们想要控制风险评测第一个领导领导领导是风险评测目前大多数风险领导领导领导是风险评测的但是我们也看到有些风险领导领导领导有些风险领导领导有些风险领导领导但是实际上有两种风险领导领导领导第一个风险领导领导是风险领导领导第二个风险领导领导领导就是风险领导领导领导第三个风险领导领导领导就是风险馏所以风险领导领导领导风险领导领导领导就是风险测非常之 Mark先生非常很是的是的Mesos to evaluate the frontal AI riskSo this is the first gapThe second gap is theI think we are short of data and toolsBecause most of the current evaluationActually is kind of a black box evaluationWe don't have data to trigger the model to expose their riskWe don't have tools to open the black box of the lateral modeling modelsAnd the last gap I think is accessBecause most of the frontier models or advanced AI models are developed by large companiesSo they're only being accessed by very limited peopleI think that maybe you know now we can see a lot of models are closed sourceA lot of them are open source anymoreSo this means we have no transparency for this modelSo this is a very big challenge for your evaluationEspecially forFrontier AI riskThank you for the excellent overview of Professor XiongDr. HeIt's an honor to beHaving this panel with you todaySoWe are looking forward to hearing your talk in the afternoonBut in the meantimeI recently heard that the Singapore has set up a national AI safety instituteAnd can you share a bit with usWhat are the current priorities for AI safety testing evaluationAt this national AI safety institute in SingaporeOr in Singapore more broadlyI think things is a pleasure to be hereGood to see you again BrianIt'll take us that trust andSafety is a very big concern in Singapore in generalWe have spent our last 59 years as a countryBuilding up trustThere is a high trust in societyAnd there's a high trust in our political leadershipTrust is also the reason that we have such a strong manufacturing baseHealthcare base in SingaporeTrust is also why people walk safely at night you dare to walk around with your hands and feetTrust is also why people walk safely at night you dare to walk around with your hands and feetTrust is also why people walk safely at night you dare to walk around with your hands and feet在街上,这也是为什么人们要在网上进行交易。AI 当然带来很多困难给予信心。你能够让模拟变化,正如今天很多讨论者所说的。它会发生假消息,你不能总是依靠它。因此,当我们在去年12月发布我们的国际AI策略时,信心成为了一个非常重要的部分。因此,我们有许多不同的计划,在不同的层次。从研究的角度来看,我们做了许多支持基础研究,同时也做了政策研究,并成为AI在新加坡的一部分负责AI。我们设计了一个网上安全的科技中心,以调查这些资讯和这些资讯。我们有整个资讯资讯公司,研究AI安全和AI安全。因为我们有许多努力,我们也设计了一个信心中心,我们的AI资讯中心,我们的AI资讯中心,我们想使我们成为一个国际的焦点,以AI安全的研究、测试和测试的方式,在全球的发展生活过程中,从发展到AI模特的发展,并以我们的AI模特的发展,并使我们作为一个国际的合作方案,以使我们改进了AI测试的科技,并改进了我们在新加坡和全世界的技术。谢谢您。",
    "那么,我们已经设置了一些AI安全的测试的主要基础,接下来我们来谈谈测试的过程就像刚才的谈论者所提到的我希望能够得到Way 和 Chris 的观点测试可以在不同阶段的过程中进行包括进行训练 预测 及后测试您认为这些阶段中有什么优势和挑战以及您的经验与中国和美国的企业合作谢谢那么大模型其实进展非常快但是原理上决定了我们对模型的机理对它的安全风险对它的能力水平的认识不能做到百分之百非常全面就像我们用竹篮打水一样的到处漏水我们做的工作比如说测试其实是去识别哪里有漏洞然后我们去把它补上但是还有多少漏洞我们可能不一定能够搞得非常的清楚我觉得现在要说优先级的话可能一方面我们一定要快速的及时的发现在各个环节中暴露出来的比如研发环节还有使用环节暴露的以暴露的各种风险及时修复它但是这个问题看起来还是比较容易做到相对容易做到的我们能够知道这个风险是什么我们就能够定义它 测试它 改进它问题在于我们不知道的太多所以我觉得当务之急是需要建立一套机制建立一套动态敏捷的机制使得我们能够永远跟上模型技术水平的提升以及它的风险我们能敏捷的发现它同时能够迅速的让产业界知道风险在这里 风险在那里然后来改进它我觉得测试非常非常关键但是测试只是一个环节我觉得需要从各个环节上来建立一套敏捷治理的技术生态才能够堵住不断暴露的漏洞所以否则我们永远是跟未知在赛跑可能很难控制好就道理 谢谢",
    "Chris首先谢谢您在这里参与很高兴与您一起参与我会建立一些我同学的同学的评论有这么多不明显的这件事情所以很难复杂复杂地解释这件事情应该怎么办如果我们有这个讨论五年之后或十年之后我认为我们会更加成熟但我认为最重要的是我们需要做的几个事情在讨论中在讨论和测试和测试生活过程中我们需要做的一件事在企业 政府以及海外AI海外AI的环境就是更加明白我们需要做的各种测试在生活过程中我会包括甚至是预训我认为我们需要有预训的预训因为包括准备了你之前在10X的数据上想要顺序计算那你觉得你可以用的是安全的什么你需要做以及avier那种测试然后当你训练一个讨训你需求什么马上要看的在训练这讨训前在你使用了一些所谓的被损失的讨训预训你可能要开始 conserve一种明显的这个 feet这些系统变得更能够生产和执行自己的研究,例如,我认为我们还不够在那里,但是像是系统的自动测试,那是一个你肯定想要测试的事情,在开发之前,但是在你训练模型之后,你采取一些调整,我认为你仍然想要进行几个测试,直到你实际上开发它,以确保你感觉舒适,你的使用者可以安全地使用它,尤其是你正常的使用者,以及某些不公平的演员,他们可能会尝试把系统扩大,以坏意义的方式。问题是,在每个领域中,要做多少测试?我认为这是一个,希望在未来的几个月中,或下一个年,希望会有更多的讨论,在全球AI系统上,关于我们应该做多少测试,直到我们感觉舒适,进入下一个阶段,在生命中,我认为这是我们需要更多的讨论。教授,您想说什么?我想刚刚各位嘉宾讲的非常精彩和全面,我想补充两个点,第一个点就是说,在当前这个阶段,安全问题非常受重视,我们都认为它是很重要的问题,但事实上,不论是学术界还是产业界,我们在安全中投入的资源,比如数学,比如计算的资源,还是研发的资源,相比我们发展大模型的能力,和产业应用来讲,是远远之后的。而另外一方面,事实上,我们对于整个通用人工智能,特别是现在大模型安全的了解,是非常有限的,我们知道存在很多安全的隐患,但是我们对安全这些隐患,并没有一个深入的认识,更重要的是,我们事实上并不知道,为什么产生这么多安全隐患,就是说我们更多现在对安全问题,是在应用过程中,从现象层面所观察的,对背后原因没有这么多的支持,所以基于这两个观察,我觉得有两个方面的事情,我觉得下面要做,第一个,在这里面我们需要投入更多的资源,需要更多的国际的合作,因为安全问题是我们全人类,所面临的一个共同的问题,这一点是非常重要的,第二个方面,我觉得从研究上,我现在真的是呼吁我们,这个学术界,包括我们的产业界,能够找出来一条,能够通向我们Safety AGI,安全通用人工智能的技术框架,因为现在我们可以看到,不论是SFT还是RHF,更多的是一些单点技术的研究,往往还是不漏充的单点技术的研究,并不是一个,我们很难确信靠这样的技术,我们就能真正通往Safety的AGI,所以在这里面,我觉得从学术界上,真的需要更好的研究和框架,和投入,谢谢。",
    "谢谢。我觉得从技术上,有几个方面,我觉得现在,值得关注,刚才我们确实需要建立更好的技术,和方法,框架来解决这个问题,第一个技术呢,我觉得是智能体的技术,那么,我们现在在强化技术的这一方面,我们的技术是,我们的技术是,我们的技术是,我们的技术是,我们的技术是不单单,我们的技术是,我们的技术是,我们的技术是,我们的技术是,我们的技术是,我们的技术是,我们的技术是,解决这个问题第一个技术呢我觉得是智能体的技术智能体已经被验证成为智能体加上工具调用包括智能体里面它可以进行的交互反思任务的规划已经成为大模型目前在落地中重要的一个技术最近我看在学术上在工业界都有很好的应用事实上智能体所演化出来的技术对于我们解决目前在大模型不关是做更全面更精准的评测还是做一些多轮的更深入的对模型领域研究的时候大家知道传统的安全研究比如说计算机安全研究有很多计算机理论密码学等等研究它们都能提出来一个在这个领域非常硬核的一个数学问题一个理论问题但目前在安全研究中你会发现特别碎片化碎片化就意味着在这里面我们没有很好的一个基础没有很好的一个体系我想这是我们在这个时候我想从学术界甚至包括产业界需要去思考的一个问题谢谢",
    "请问老师OKI completely agreepreference your opinionI just added two pointsthe first pointblueso this is the first pointthe second pointI think isI mean in terms of the evaluationas we need to build a lot of databut in terms of the safetywe have a lot of safety issuesand this is actuallyall course disciplinaryso if we onlywe only haverely on our AI communityactually we don't have the expertsdon't have the expertisedon't have the knowledgeto build such a data setso I think we have to collaboratewith more communitiesto you know toto address the AI risk issuesthank youDr. He earlier mentionedthe importance of building trustif companies do their own evaluationit's hard for societyto have complete trustin the safety andethics of AIand I think that leads to the question ofwhat is the role of third partyI would like Dr. He and Director Weito comment on this questionwhat do you think aresome of the current challengesof performing the role of third partyfor exampleProfessor Jung mentionedthe issue of accessif third party testers and auditorsonly have black box accessto their AI modelsthen it's insufficient for them to conductall the AI safety testingthat are requiredI would love to get your viewsDr. HeMaybe I will take the questionin the context of sharing our experienceswith AI Verifywhich is really what we've been working onand this isand I'll say it in three partswhy we worked on AI VerifyAI Verify is a software testing toolkitthat the Singapore government introducedtwo years agoreally it came for three reasonsone I think there is a very big gap between what the AI Verify isand I think there is a very big gap between what the AI Verify ispolicy makerstalk about in very big principlesand all the work that everybody here is doingin terms of very good researchright and also a gap between policy researchand what is actually in the hands of industryso AI Verify was an attempt to bridge this gapby putting practical toolsinformed by policybut also trying to get the best of academiainto the hands of industrythey can come together to do itotherwise your third parties are somewhere far far awaythey are not in part of the ecosystemthe third thing we do is thatyou want your third partiesyou want your systems to always be at the frontierso last year weor rather this yearwe updated AI Verify to include Moonshotwhich is our generated AI version of itit's a very startit's an initial productbrings in benchmarkingbrings in red teamingbut it's a startand then in the processthrough a common platformyou get the industryyou get academiayou get third parties all the timeokso the reason why we wanted to put together this productis becausewe expectthe third party is very importantso I thinkthe existence of third partyactually helps to increasethe confidence of everyone in AIespecially in large scale technologybut we also want to tell themthrough the third party's professional abilityto tell the risk to the developer and the userwhat should they do to improvehow to do in the next stepbut at the same time third party institutions can act as a bridge安全风险汇聚起来测试数据汇聚起来方法论汇聚起来研究结果汇聚起来能够更好地赋能任何一个单点的研发机构我觉得低汤机构扮演的角色其实是非常丰富的那么测试是非常重要的核心的手段我刚才我非常同意乔老师的这个观点就是现在整体上产业界对于AI的投资还是非常有限的那么我觉得这个低汤机构存在其实是提供一种公共产品来降低或者说在有限的产业总体安全预算的情况下提供一些公共产品让大家的研发安全投入可能会成本能够可控恰到好处我觉得咱们做安全方面的工作其实还是要服务于更好的发展更好的应用那么再控制好安全成本投入的同时在保证安全的前提下控制好我们的前模型研发机构要披露风险列表这样才能展现出一种负责任的态度这样我觉得低汤机构和企业合作做这些事情其实是才能形成一个闭环感谢I very much agree with the visionthat AI safety should be a global public goodand some of the AI safety evaluation show up around the worldand some of the AI safety evaluation show up around the worldand some of the AI safety evaluation show up around the worldand some of the AI safety evaluation show up around the worldand some of the AI safety evaluation show up around the worldor you know AI safety labsin different cities and countries around the worldthat we begin to align on a shared vocabularyso that we can actually under likewithout knowing exactly what you mean by red teamingand what we mean by red teamingit's hard to trust and allow for these things to be interoperableand so I think that's like the key first stepthat needs to happen in the next year or soProfessor Qiao我想跟刚刚各位说专家讲的已经非常全面了我就最后想说一点就是在呼应你的我觉得在这个领域我们太需要国际合作了需要国际共识国际合作If no final remarksLet us give a final round of applause to this excellent panel现在原则讨论的嘉宾请留步其他论坛的嘉宾请上台我们会在台上进行一张合影然后上午论坛呢告一段落下午论坛会在下午一点十分开始The morning forum has concludedThe afternoon forum will begin at 1.10pmSee you in the afternoonPick up the orderHow did you doVIPhmaHow did you doWhat about the deadlineOh it was my lotta pressureI guess my hair had grown backOh yeahIt's just about timeIt's oktrong học了Recently I have grown up请勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿勿模仿例如GPT,我们正在与一个中心化的演员互动,这位演员在我们的私人数据中拥有一个窗户,它在把数据挖掘起来,这是第一个问题。第二个关于关系的问题是,人工智能训练,对不起,人工智能训练。例如,如果我训练一个人工智能系统在电脑健康记录中,并在野生中释放它,有好理由认为,一些私人信息可以用来训练人工智能系统。这和贵义问题有关系。另一个重要的问题是关系质疑的问题。我们...我们...我们不会太多谈论这个问题,但我们认为它是重要的。贵义智能系统执行执行执行。越来越多的智能系统,各种各样的智能系统,成为了日常管理的一部分。而有很多犯罪证据。其中,也有著非常好的证明,是一个在大南地区的习惯患病事件。这个搞得人们变得负心。这个可能会使人们出于宠 ward。这里有把握智能系统对死亡的证明案件。难题是,这不仅是技术问题,這就是技術問題在某種程度上不每個人都受到某種特定的障礙因此所有在系統上的演員都不在系統上工作所以這不只是技術問題而是如何演員與技術合作的問題這是一個很舊的問題它仍然存在於現代科技上它在語言模式上是非常有用的例如我學生李宇謙的問題我們在大語言模式上探索了多種錯誤和謊言的問題並且很輕易地我們看到美國或歐洲的LLMs表達了非常明顯的比美國或歐洲的中東亞洲人或中東亞洲人還要好一點這完全是使用的訓練機構所以這裡沒有什麼新的但我們不要忘記那些舊問題我們還有另一個問題就是我們所謂的永恆復甦大家都在討論科技增長的程度我們必須要把這問題放在現代科技上所以我現在不僅在討論你所看到的永恆復甦但是也在討論最大的超級電腦的增長你所看到的就是AI使用的力量實際上超過了現代科技的能力世界上的全能力這是為了訓練我認為永恆復甦的增長是更重要的我們所看到的是同樣的當GPU變得更好AI的能力會越來越多比GPU的增長更快現在這完全不算是持續性的經濟不合理我們有一個反彈的效果它把電腦的功能消耗了關於這一點我們的重點是能力的重點我們有一個經濟大人的能力變得更大因為我們使用更多的資料更多的電腦關於這一點我們有一個系統決定的程序越來越自動因此選擇變得更集中隊伍建立決定引擎為所有人作出決定這和其他危險或無謂的危險或者人權威脅都是冒出的我們認為這是AI夾的最重要的一種理念除此之外我們認為我們需要有多搭的AI我們相信Preferred open source可以令我們直播可以幫助規範滿足無異我們相信重要的理念就是平衡不只愛意的它有虛擬的問題有障礙的問題如果我們不做障礙我们可以杀死开发,因为开发有强弱的经济持续性,有缺乏公平性的挑战,什么数据,什么模式。问题是,当我们说开发,我们通常并没有定义我们所谈论的东西。目前来说,开发是用来定义非商业的开白模式,但这并不是以开发标准来定义的。因此,我们认为国际管理需要。正如互联网,互联网非常容易通过境界。理想是,我们会使用训练资料,在多个国际区域中,以创造更多代表性的互联网,以避免我之前提到的链接。所以,我们认为我们需要共同管理,以帮助我们建立共同的项目,正如互联网,iCAN是很用的,因为它有一个共同的,不太分裂的互联网。因此,我们设立了世界智能组织,其中,它是代表了首个国际和中央国际组织,第二个是研究,总的利益结构,和公司和地区。目标是,将建立证据,包括证据评测,并建立智能的知识状态,并提供证明,以及计划。谢谢。",
    "谢谢。谢谢Gail,",
    "对于法国委员会的令人印象深刻的观点。我相信,之后有更多要谈论的。接下来,我们的主席,是瑞敏河医生。他是新加坡的首个人工智能官员,他在新加坡的设计人工智能目标,包括建立和实现新加坡人工智能目标,还有在新加坡的国际智能策略中,进行新加坡的新加坡的国际智能策略设计。他也是新加坡政府的首个人工智能官员,以及美国的高层的智能顾问署。荷尔斯·荷,舞台舞台都在,欢迎!荷尔斯·荷,舞台舞台都在,欢迎!荷尔斯·荷,欢迎!荷尔斯·荷,欢迎!荷尔斯·荷,欢迎!荷尔斯·荷,欢迎!荷尔斯·荷,欢迎!荷尔斯・荷,欢迎!荷尔斯·荷,欢迎!荷尔斯·荷,欢迎!荷尔斯·荷、欢迎!清ünüz康克多亚工信 ό对于政策的独特状况,例如历史,国家优势,比较高的优势,以及企业基础。新加坡人没有区别。无论如何,对于AI安全和管理,我认为有四个共同的团队,并且在各国各地,因为我们都从同一个地方开始。其实我已经说了半个话,所以我现在的生活比较简单了。让我来与每个人一起谈谈。首先,我认为我们必须要与AI管理与虚弱的心态接触。正如前开放AI委员会委员,Helen Toner 提出的,对于真正的智慧有什么关系呢?更何况,正如这几个早上的讨论者所说,没有人真的理解AI系统的内部工作,特别是深入的智能网络。更何况,AI技术在急速进行,但我们无法预测AI系统的进步的准确性和确定性。我们并不知道AI安全对于技术方式的技术方法,例如机械解释能力或丰富方法,会否真的有用。我看到有些人在前几个楼上哆嗦。Chris在笑我。另一方面,我们也不确定如果有些最糟糕的情况会发生。AI系统在前几个楼上已经被隐藏了。因此,正如这几个早上的讨论者所说的,AI系统有许多不明,也许有许多不明。因此,我们应该离开确定性的测试,特别是关于未来。而是说,政策人员必须尊严地,尊严地,进行继续学习的心态,重新测试理论,与科技的影响持续,并且尊严地感受到科技的影响。我们必须学习从企业与企业的专家,许多人都在这里聚集,并且我们必须学习从每个人的角度。我们没有所有答案,而我们必须专注在调查资讯的领域中。像是世界AI会议的会议,这些会议将成为我们共同学习的重要机会,通过这些机会,我们将提升我们的共同理解。尊严的意义也意味着,听听和学习不专业的声音。民众、工作人员、写作人、艺术家、年轻人、年轻人,都拥有重要的观点,他们的怀疑和情绪对于AI是真实的。他们可能会被国家和业务拒绝,而我们必须理解他们。我非常感谢美国的AI顾问组织,他们很明显地调查,并且提出了他们的建议。这包括我的同事,凌翰和曾毅。凌翰今天也与我们联系。慈悲的意义也意味着,愿意承认有关AI的关键问题,但我们并没有解决的答案,并且努力发掘答案,并且并不决定任何不正确的理论。在这种心态中,我们在去年的新加坡AI会议上举办了一场新加坡AI会议,与不同领域的国际专家联系来解决AI的关键问题。如果这些问题被解决了,这将带进AI的发展和扩展,并带进国际利益。在去年的AI会议中,我们今天与董颂、姚东、艾琳和布莱恩一起参与了这次会议。虚弊也包括了我们的新加坡AI会议的解决方案。但是,我们的解决方案可能是错误的,或者需要更新。这并不是说我们作为政府或政策人员只能坐着等候。而是,我们可以开始用软规则和规则进行介绍,获得影响者的反应,观察该规则的后果,如果需要,确认它,并确认它,并确认它,并确认它,并确认它,并确认它,实现中心解收链接,并在中心对发。在2019年,新加坡中,新加坡提出了一套模型AI管达方案。在此,我们将提供工作人员,以及入境者,类似的实际指约,给他们第一次备用AI。这次规则,根据广isiaj和企业陆平公司的谈议,我们提供了更新的、包括量 concept 科技,到层次裴,revolute和造成ling?以HR,或去到Line Future,这种改善方式是更加可观的,并试图控制所有AI的可能伤害。所以,这就是慈悲。第二,我们必须与AI管治的观点相似。我们必须小心于假的谈判。AI政策并没有任何黑白的关系。例如,AI系统通常有用,但它有时候说错误的东西。AI可以增加生产能力,但它也可以导致职业破坏。AI可以帮助改善气候,它也可以伤害全球,因为它有很高的电力和力量消耗。AI可以做很多健康的事。AI系统可以发生深入的骗子和骗子,但也有很多AI的例子,为公共利益而被发布。例如,在新加坡,我们使用AI进行明确的移民确诊,预测医院时间,和谨慎的车辆维修。民族也可以通过谨慎的车辆通过一些政府的服务。所以,讨论和创新也是一个错误的构图。我们需要讨论和创新的讨论,让有益处的应用能够增长,并保护对受伤害的使用者。这种平衡的轨迹从业务到业务有关。例如,自动驾驶车的最糟糕的情况是相当不同的,从癌症诊断到谨慎的车辆。自动驾驶车也不存在空间中。它是一部分技术品,一部分用途,并且它是一部分我们与其互动的更广泛的环境。因此,自动驾驶车在更广泛的管制环境中可以有用。近几年,新加坡更新了我们的法律项目,以保护自动车车车的网络,包括为了保护个人资料,以及为了在网上流传的误误误误误误误误误误误误误,以提升自动驾驶车的风险和恶意内容,以及为了挫折网上的犯罪活动。在这些规则中,人类或机构仍然负责他们的决定的结果,即使这些行动是AI系统帮助的。这就是我们的观点。第三,我认为我们需要增加和提升我们的能力来控制AI。这开始与很多我同事的政策是用AI以建立一个基础理解这样的技术的可能性和限制。在新加坡,我们在政府中也很努力推荐使用AI。民主服务员可以从他们的政府的电脑中使用AI,甚至帮助他们回应市民的询问。应该让生物系统有实际的技术能力而不是只谈论执行执行的原则。因此,我们也鼓励更具技术的政府官员的社区群体来发展他们自己的AI产品和工具。我们也创建让他们分享学习的途径。例如,我提到在早上的讨论中,我们的政府公司在2022年发展了AI证实最低值的AI系统以国际认识管理原则的计划。去年,我们也发展了Moonshot项目,为我们展示从传统AI到生产AI的AI证实图案。为了确保AI的管理方法有新的工具和新的方法,我们也与政府官员一起一起研究真实需要的研究,并将他们的工作的成果立即翻译进行应用。我们的研究员认为这很满足我们的目标。但是,建立能力远远不只在AI的用途上,我们需要建立一个有信心和知识用户的人群。这能够让他们与电子环境联合,提升他们的竞争、技术和职业能力。使我们的民族和企业能够获得AI的利益,这是我们去年更新的专业的技术提升我们的技术,通常与业务合作,以及职业和公司的训练。另外,我们也在帮助所有民族提升他们的知识和AI的熟练,而也在上市的业务上提升了他们的技术和职业能力。我们的技术是专业的,我们的技术是专业的,我们的技术是专业的,我们的技术是专业的,我们的技术是专业的,中国在接线----------------our differences and find common ground.Yesterday, I spoke about how countries can work togetherat the Global AI Governance Forum Ministerial Roundtable.Particularly, we need to come together to find common problems worth solving.We need to work bilaterally, visionarily, multilaterallyto facilitate norms and encourage the creation of global,interoperable standards and common tools for AI governance.At this morning's panel, I also shared on how we open-sourced AI Verifyand launched the AI Verify Foundation as a vehicleto tap on the global open-source community to crowd in expertise and capabilities.Let me conclude.The challenge of AI safety and governance will continue to evolve,but we must sustain our engagement with the hard technical questions,with the policy dilemmas, and with each other.If we collectively adopt the very human traitsof a spirit of humility, a sense of perspective,a desire to increase our capabilities, and our willingness to collaborate,I am confident that we can achieve the right balanceof governing this particular technologyand collectively harness AI to serve the public good.Thank you.Thank you so much, Dr. He,for your inspiring,inspiring and inspiring recommendations.I'm pleased to now welcome Professor Zhang Linghanfrom the Yuha China University of Political Science and Law.She has extensive experience advising Chinese legislationon algorithm regulation, platform governance, data security, and AI.She is currently a member of several advisory committees,including the ICT Committee of the Ministry of Industry and Information Technology,and the Legal Advisory Committee of the Ministry of Public Security.Professor Zhang is also a memberof the UN High-Level Advisory Body on AI.Professor Zhang, I'll hand it over to you.大家好,非常开心今天能用中文跟大家介绍我最近新写的一篇论文。那么题目是基于风险到基于价值探索。那么题目是基于风险到基于价值探索。那么题目是基于风险到基于价值探索。中国人工智能的治理的方案。那么在这个安全的议题上去讨论人工智能,实际上我们都有这样的问题,就是安全治理的,风险治理的框架,是否是人工智能治理的最佳方案。我们目前所有风险治理的手段和措施,是否能够完全应对人工智能给社会带来的影响。那么当我们去追求安全的时候,多安全才算真的安全。那么首先向大家介绍我的观点。那么我首先认为呢,在风险治理的过程当中,不管是风险的识别还是风险的应对,都有一些无法应对人工智能给社会带来的全方位多维度和颠覆性的影响。风险的治理理念和治理手段,我们仍然应该坚持,但是我们在风险治理之上,应该超越风险治理,采取基于价值的治理框架。那么也请大家多多批评,让大家有更多的知识证。那么下一张图呢,是我们可以看到目前基于风险的治理呢,已经成为全球人工智能治理的共同主题。那么在这个图上面呢,列到了不光是很多国际组织都把风险治理作为了不管他们的宣言,还是指导意见的这样一个基础的逻辑框架。同时也可以看到基于风险的治理,也被世界很多立法所采纳。那么下面带来的问题就是,大家都是专家,我在这里就不多介绍了。现有的框架对于风险的识别和分类准确吗?那么在这张PPT上,我列出了目前我们经常提到的四个风险治理的框架。那么它们都有不同的分类的方法。比如说在我参加的联合国的这个中期报告当中,我们把风险分为技术性风险、社会性风险,根据它影响的层面去进行分类。那么在欧盟的AI Act当中,把风险按照影响的范围分为不可接受的以及高中低的风险。我们可以看到美国商务部的这个国家标准研究院颁布的风险,则是根据风险的来源和成因。那么在实际上应对人工智能的风险当中,我们可以看到很多存在的问题。我们就以美国NIST发布的这个风险指南作为报告。第一个,就是大家都知道,在风险的判断当中,其实蕴含了很多的价值考量。也就是说我们说风险它并不是一个纯粹的科学概念,而是一个规范性的概念。我们看到,比如说NIST的这个框架当中,把风险分为技术性风险和技术社会风险。但是,大家可能没有注意到的是,把哪些风险归类为技术风险,本身就蕴含了价值的判断。那么,其次呢,风险分类一个很重要的问题是,在我们看到的前面这张PPT当中,大部分的风险分类都把隐私侵害、歧视等等,这一类对人的权利的侵害,当作是人工智能的风险。然而,我们要看到的是,这一类的风险,并不像我们一直熟悉的风险的治理,比如说汽车发生事故的概率,比如说食品不安全的概率一样,比较容易量化和计算。这一类的风险是非常难以去量化和计算的。我们对于风险治理的计算,基本思路是,OK,我要算一下你的损害的大小以及损害发生的概率,最后看看我得到的收益跟它能不能成正比,进而采取措施。可是,如果我们人工时代面临的很多风险都是难以量化的风险的话,那么,这种治理方式可能就面临着困难。那么第三个呢,是我们再回来看这张PPT,可以看到大部分的风险分类识别当中,都把失业问题当成是人工智能的重要的风险。那么以及把没有办法给现行的人工智能服务提供者去追责,当成是人工智能的风险。可是,我们知道风险的一个重要特征就是不确定性。这一类我们刚才说到的影响,真的是不确定的吗?人工智能必然会带来大规模的失业和劳动替代,也必然不能适应传统的法律框架,与其说它是一种风险,我更愿意认为它是一个社会影响,社会变革带来的必然影响。那么更重要的是,刚才很多专家也提到了,人工智能的风险不同于以往任何一种风险,其重要原因就是在于,大家都说不出来它未来的风险具体是什么。这种uncertainty和unpredictable,大家都是在反复的提及。那么可能我们提到的,人工智能的这种自我复制与自我完善的能力,就是我们将来没有遇见到,或者从来没有处理过的风险。但是我们目前并不知道它什么时候会到来。以及我们可以看到,人工智能这种强大的技术通过开源,可以被广泛和容易的获得,这也使得风险的来源大大的被扩散了。那么第二个问题就是,我们可以看到风险治理的措施,有事前事中和事后,分别是风险预防风险缓解与事后消除。如果我们列出几种典型的风险治理措施,一般是事前我要对风险进行评估,进而把它进行分类分级,采取和风险程度相适应的治理措施。那么这些风险应对措施,和人工智能治理的目标,手段其实也存在着错位。首先我们来看人工智能治理的理念,总体来说风险治理的理念,其实是有一个修正的理念,就是修正主义的内核。所谓的修正主义是说,原来的风险治理是说,如果这个技术存在缺陷和障碍,在它没有得到完全修复,并且是可信任之前,是不应该直接应用于实践活动的。我们可以这样去处理,核扩散带来的风险,这样去处理大流行病带来的风险,但是我们没有办法,把它和人工智能的风险,相提并论,并且这样处理。人工智能技术的应用,已经成为必然趋势,那么这种趋势是谁都不可以避免的。我们没有办法说,在完全消除了,或者确信人工智能没有风险之后,才来继续使用。那么从人工智能治理的工具的层面,那么我们都知道,风险治理当中最重要的就是,事先的评估和预测,可是,我们有个重要的问题就是,当我们没有人工智能应用和技术,足够深入的在社会当中广泛使用,我们也就没有办法去确切的了解,哪些风险将会产生,并且具体的程度是什么样。更遑论,我们还有很多我们认为不可预见的风险。那如果是这样的话,我们又如何去进行事先的识别,评估和监测体系呢?那么相信呢,一部分不能预测的风险,是不能落到这样的治理工具的治理范围内。那么另外还有就是人工智能治理的前提,如果说人工智能治理的很多风险,没有办法被量化的话,就没有办法被纳入到成本收益的计算当中。我们举个简单的例子,很多隐私的这种数据泄露的赔偿,总体来说数量很高,侵权人难以承受,但是受害者个体拿到的,侵权费用可能只有几美分。那么这种难以被个体救急的风险,实际上也使得成本收益更加困难。那么在这里,我想用剩下的时间简单介绍一下,我认为在中国,其实已经逐步发展出了,修正人工智能风险治理的这样一个路径,并且随着时间的推移,中国的人工智能治理正在本土化。我们可以把它分为探索阶段,定向阶段,和系统阶层阶段。探索阶段呢,我个人认为是从15年,一直到2022年这几年的时间,我们发展出了很多的人工智能,中国本土的治理手段。比如说在风险认知层面,实际上我们是有很多的共性部分,从2015年16年开始,一直到2020年之间,我们有新一代人工智能伦理规范,算法推荐管理规定,包括个人信息保护法,实际上都采取了风险分级分类,和防控的这样一个路线。在治理手段上呢,和国际上对接的一些共性的部分,包括个人信息影响保护,个人信息保护影响评估,以及算法影响评估。但是与此同时,我们也发展出来一些中国自己的特色部分,比如说在前两个阶段,始终在国内的人工智能相关的立法当中,都把发展与安全,作为最重要的平衡的一种价值观。同时,我们始终秉持着中国所特有的国家总体安全观的理念,去进行人工智能的安全治理。那么即使在网络信息领域这样一个比较特殊的领域,我们也采取了网络信息生态安全的这样一个概念。同时,在算法治理和深度合成治理的过程当中,我们有一个明确的价值排序,是信息内容安全,消费者权益保护,以及市场竞争秩序。那么在治理手段方面,其实中国也有很多自己的特色,比如说初步探索相关的算法备案。那么尤其在这里提醒大家要注意的是,我们并不是一个准入或者认证的制度,而是一个信息备案和采集的制度。那么同时,大家如果感兴趣可以关注到,在2021年的算法推荐管理规定当中,非常有特色的提出了,未成年人的防尘制度,尼尼制度,老年人和劳动者的权力保护制度,还有未成年人的相关的宵禁制度。那么这些在世界上都是非常少有的,也是独特的。那么目前我个人认为,我们国内的人工智能治理体系和理念,正在逐步的形成,并且已经逐步超越了风险治理的理念。首先在风险的认知上,我们实际上一直在跟国际保持同步,大家可以关注到我们国内的相关技术标准当中,对于风险的分类,比如说失控性风险,社会性风险,侵权性风险,歧视性风险,责任性风险等等,和国际的很多相关分类是完全可以对接的,包括一些相关的治理手段,事前评估认证和事后追责,我们也吸取了国外的先进经验,但是我们可以日益看到,在近两年生成人工智能迅速生长以来,我们可以看到很多中国特有的价值理念和治理手段,那么我们对于人工智能治理的很独特的价值理念,我觉得最重要的这两年就是不发展,就是最大的不安全,我们认为最大的风险可能就是中国的人工智能产业和技术,没有得到有效的发展,第二个就是我们可以看到,我们不管是全球人工智能治理倡议,还是在昨天我们的世界人工智能大会中提到的,中国的这种治理方案,我们都强调要尊重各个人工智能,要尊重各国本土的价值观和发展阶段的需求,以及要尊重各国的文化,那么在治理手段层面,我们可以看到去年的生成式人工智能暂行管理办法出台之后,中国已经形成了对于生成式人工智能分层治理的这样一个基本的理念,同时以发展为导向,中国也开展了很多人工智能基础设施建设的相关工作,比如说在人工智能的数据要素层面,我们可以看到国家数据局做的大量工作,还有工信部做的大量有关算力基础设施的建设工作,所以我们目前在中国的这种人工智能治理理念,我们与其说它是一个完全基于风险的治理理念,不如说是一个基于价值的治理理念,基于价值的人工智能治理并不排斥风险治理,但它超越于风险治理,一方面,我们可以看到在中国的立法和治理政策当中,不仅仅把人工智能理解为一种技术或者服务应用,人工智能既是未来赋能整个社会的基础设施,也是未来整个社会生产的组织形式,那么我们也可以把人工智能放到心智生产力的角度去理解,另外一方面,中国已经越来越明确中国人工智能治理的理念和方案是,以人为本,智能向善,以人为本是在说技术不能偏离人类文明进步的方向,智能向善是在强调人工智能必须在法律伦理和人道主义的层面的价值取向,那么在治理手段方面,我们可以看到目前一个系统发力的情况,在今年5月份,全国人大常委会和国务院已经把人工智能立法放到了相关的立法计划当中,同时,我们也有日常生活,日常监管活动当中更为丰富的监管措施,那么在我们相关的一些立法和监管活动当中,我们可以看到中国人工智能的安全框架也在积极的讨论和预养当中,那么包括在前天也刚刚颁布了一个国家强制技术标准,生成是人工智能内容的标识的这样一个技术标准,那么相信基于价值的人工智能治理体系,正在逐步构建的过程当中,我个人更愿意把它分为三个层次,第一个是是如此,就是我们去观察人工智能的本体价值,那么在其中有几层含义,首先,我们希望现在在风险治理当中,这样一个泛化的模糊的概念被逐渐分离开,哪些是人工智能的必然影响,哪些属于近期的维度,哪些是人工智能不确定的影响,那么其次,我们在属性层面要对人工智能有一个判断,它究竟它在这个人工智能被提及的时候,是以一个技术的方式被提及,还是服务应用,是社会生产的基础设施,还是社会生产的组织方式,这都决定了不同层面的影响的发生,是必然的还是具有不确定性和可预防性的,那么第二个层面是遇如此,就是基于中国本土价值观,去判断人工智能治理的短期和长期的目标,梳理中国的个性化的治理需求,那么我们特别希望能够分解出人工智能风险当中的规范性的层次,把个体化的价值标明列明,那么对于中国来说,我们首先在技术层面是要安全的发展,在服务应用层面延续中国一直以来对于服务应用的治理体系,那么在基础设施层面,我们可以看到国家各种加大力度措施去促进技术设施建设,那么在社会生产组织方式层面,我们也在强调绿色环保,强调新制生产力的治理方式,那么在应如此的层面,也就是在基于个性化治理之外,我们始终有人类共同核心价值观,和人类命运共同体,那么这也是我们在参与全球人工智能治理工作当中,中国提出的方案,那么也是我在联合国参与的,我们可以看到联合国提出的AI Governance for Humanity,尊重整个人类的价值,那么具体的手段措施呢,我们其实在今年3月份的人工智能法学者建议稿当中,有比较详细的解释,由于时间原因呢,我就先讲到这里,感谢大家,谢谢。",
    "Thank you so much, Professor Zhang,for such a clear and systematic presentationinto the risks, values, and corresponding governance solutions to AI.Next, we will have Dr. Mark Knitsberg,who will be sharing his insights with us.Dr. Knitsberg is currently an Executive Directorof the Center for Human-Compatible AI at UC Berkeley,as well as Head of Strategic Outreach for Berkeley AI Research.In industry, he has built technology venturesapplying AI in healthcare,finance, education, and development aid.He has worked at Bell Laboratories,Microsoft, and Amazon,and developed and run diverse programsin industry and academia.Mark, it's great to see you in China again.This page is all yours.Thank you for having me.Thank you.Thank you.Okay.I am first and foremost a computer scientist,and I have been asked to give an overviewof the US approach to AI regulation.And soI ask you to fasten your seat belts,because this is the first timeI'm taking these ideas for a spin.我总是记得我们现时的现实情况和人工智能的极端层次数。它是世界上最大、最有能力的全面目标数字系统,它是世界上最大、最有能力的全面目标数字系统,它是世界上最大、最有能力的全面目标数字系统,尤其是在美国,人工智能根本没有任何制御。而这些制御无论如何都被融合在每个各界的人工活动中。所以,它是很难去制御的,尤其是因为系统的黑箱模式,尤其是因为它是一种全面目标的技术,而它经常被误解。那么,是否有美国的方式,让人工智能根本没有任何制御呢?我认为,我们与很多国家分享了一些主要目标。在美国,可能有一个特别的目标,就是推进美国人工智能的增长,而我们正在寻找美国主席,有能力的人工智能。我们正在寻找美国人工智能,我们希望为美国的经济、civil society和国家保障,并且我们希望保护美国人工智能的严重影响和结果。我们也将分享了一些全球目标,这些目标被表明在许多原则中,例如,OECD原则,和美国国际公司,以及美国国际公司,I just took this one particular piece of the OECD principles 1.4, which is essentially about safety.The spirit of the current US approach to regulation is captured in the current process.Groups have been promoting competing interests and have resulted in three sets of potential laws.One is the executive order from October of last year, and that tends to target large models and guides the federal agencies, the existing federal agencies.Then there is a roadmap, the so-called Schumer roadmap, that came in May of this year,and that is weighing the promise of AI against the effect on jobs and laws and defenseand doom, as he says.And then in California, some of our regulation is coming from the states.There is a bill that is making its way through the legislation, called SP-1047,and that is concerned with the safe and secure innovation with frontier models.Now there are some issues with these.The debates continue.None of it is law yet.There are some laws in specific states, and there are some very specific laws,but these larger frameworks have not yet made it to law.And then, as we have heard over the last couple of days,there are challenges to operationalizing.When you are working with...general purpose technologies,for example, it is hard to prove that a system has acceptably low riskif it is general purpose.You really need to know what it is that you are testing for.One of the aspects of the U.S. legal systemis that there is a lot of existing law in various sectors,and so these do apply to systems that are using AI in those sectors,for example, health and transport, agriculture and finance.Some of those need amendment.For example, there were relatively few laws in transport for cars that drive themselves.But in many cases, the existing sectoral law gives a good baseline.And then there are laws that concern the just distribution of burdens and benefits,for example, in hiring and creditworthiness that cut across sectors.Another thing about the U.S. is that we tend to occasionally have some litigation,you may have heard,and this may be a situation in which litigation creating laws actually works out.So in the case of IP, for example, intellectual property,the issues of the use of certain data as training data,the use of likenesses,the way in which...the way in which creative jobs are treated,have made it through the litigation system,and then in consumer protection,the laws worked out again through litigation.There are other factors or other drivers,including simply insurability.If you are selling an AI-powered system,and you'd like to get insurance for it,you need to prove a certain level of safety and reliability.And then there are drivers coming from the other regulations,the EU Digital Services Act, for example,that affect the companies in the United States.I'd like to take a couple of minutesjust to talk about some of the misconceptions.And I will remind you that unlike Dr. He,I am not actually representing my country.I'm speaking for myself.But I believe that there is a misconceptionthat regulation is a significant barrier to innovation.I believe that done right,if we are attending to safety,then it's simply a necessary guideline.This is the case with many other technologies.And I think it's no different here.I also think that often capabilityis confused with safety.I think that as you see systems become more capable,that does not...equate necessarily with the immediateor direct correlation with the issue of safety.And then I think there is sometimes a confusionbetween what we call the red linesand the idea of asking developersto cause developmenton more powerful systems.That is not a red line.It's a different concept altogether.I do think that we can learnon the path to global governancefrom the existing international safety organizationslike air transport safety,nuclear power informationand telecommunications organizations.And the way they tend to workis that they find common groundand we converge where we can agree.And then where we can't agree,we either seek some sort ofgeneralized agreementor simply leave it to the statesin those areas.I think that this can workfor international regulation here.It looks like I have the timeto give a little bit moreof a technical perspectivefrom our center.I'm coming from the Center for Human Compatible AIand these are all the great peoplethat I work with.And that is to give youjust a quick look at the long game paththat we believeis the right way to do thisfor safe AI systems.We of course are working nowand you're hearing a lot todayabout making AI safe.And there are numerous techniquesand methods like our LHFand constitutional AIwhere you have one AIlooking at the outputs of anotherevaluations and auditsand red teaming.And we've heard aboutdigital neuroscienceand the quantitative AI safety institutethat I think is just getting started.And these are terrific directionsfor the systems that arenot yet understood.And if we want to be able to use themas componentswe'll need to havesome kind of quantitative boundson their behaviorin order to make AI safeif it is based onfor example language models.But in the longer termand the central focusof the work at our centerwe are trying to make safe AIand to do thatwe believe it makes senseto revert to transparentexplainable semanticallyanalyzable systems.Apply formal verificationfor some specific forms of safetyand then to use asecure ecosystemwhere nothing runsunless it's known to be safe.And there aresome terrific inroadsthat we've been making.So this is the viewfrom the centerand thank you very muchfor the opportunity to speak.Thank you so much Mark.Please remain on stageand prepare to be seatedas we get set up for our panel.To conclude our discussionon AI safety guidancewe will have a panel discussionwith our experts.Let's welcome back on stageProfessor Gail Kuoand Professor Zhang Linghan.We are also joined byProfessor Ji Weidongand Dr. Wang YingchunProfessor Ji, Dr. WangPlease proceed to the stageas I start introducing you.Professor Ji Weidongwas formerly Deanand Co-Guan Chair Professorof Shanghai Jiao Tong University'sSchool of Law.He is now a professorof Humanity and Social SciencesPresident of the China Institutefor Sociolegal Studiesand Director of the Centerfor AI Governance and Lawat Shanghai Jiao Tong University.Dr. Wang is the Deputy Directorof the Shanghai AI LaboratoryGovernance Research Center.Dr. Wang has presided overand participatedin dozens of national,provincial and ministerial projectsincluding the developmentof the Open AI Governance PlatformOpen EG Laband other AI governance systems.A moderator for this sessionwill be Fang LiangSenior Governance Leadat Concordia AI.Prior to joining ConcordiaFang Liang worked at Baiduwhere he researched AI governanceand participated inthe research and developmentand participated inthe formulation ofseveral Chinese governmentAI policies.A round of applauseto our panelists.大家下午好相信大家聽了今天的講座都能很深刻地感覺到人工智能已經很深刻地影響到全球的經濟社會法律但我們理解這種影響其實對全球不同地區的基於的挑戰其實是不同的所以我們這個論壇也想談一下人工智能治理的地區視角和經驗分享首先我們可能想談討一下發展與安全的問題我們先從新加入的季老師開始吧我看見您最近寫了一篇文章是何時真正邁入人工智能治理的立法時刻裡面確實提到了一個人工智能的發展安全之間的價值判斷以及通訪選擇還沒有形成共識同時您也提到在多默態大模型或遠大模型只有他們的性能與安全度形成某種正別的關係的時候您認為才能形成真正的人工智能立法時刻您能幫我們進一步闡述嗎好的最近200年來科技的發展對於人類社會的進步產生了非常巨大的影響我認為19世紀的內燃機加電器技術使我們人類有了生活的自由那麼20世紀互聯網加通信技術使人類有了信息的自由21世紀大模型加腦機接口使我們有什麼呢現在很難想像但是至少是有了創新的自由甚至是從無到有進行創新的這樣一種可能性是展開了也就是說它可以賦能社會 賦能人類會帶來福利但是確實會引起這樣那樣的安全上的問題我們可以看到大模型出來之後至少是有四個方面的問題因為大模型大量的使用數據它可能會引起隱私方面的憂慮另外我們可以看到大模型由於它的能力泛滑造成了幻覺現象可能會引起虛假信息以及誘發各種各樣的犯罪另外一個在知識產權問題上它也可能會引發這樣那樣的複雜的問題還有一個它可能會使得整個社會治理的中樞機關出現漏洞最後會導致信息社會發生功能障礙這些都讓我們感覺到不安問題是我們如何處理這樣的問題當然前面講到了科技給人類社會帶來了巨大的福利如何在這兩者之間進行平衡我們可以看到在亞洲在非洲這些國家對他們來說發展是一個更突出的問題他們希望能夠實現一種互惠的科技的發展當然我們知道像美國日本還有中國非常強調在發展和安全之間尋求平衡另外一方面我們可以看到歐盟最近通過的人工智能法提出來了安全高於發展這樣一種價值取向如何在這個中間找到適當的平衡我們可以看到各個國家有不同的立法模式我想這個是值得我們進一步探討的問題謝謝分享接下來想請教王老師您作為研發機構的代表我們知道上海人工智能實驗室也曾經發生過做了很多安全對其品質的工作包括昨天周主任也提出了一個安全發展的45度平衡率想請您來解讀一下您認為的安全和發展之間的關係好的 其實昨天周博文教授在世人大會的上午的大會參與會議上他分享了我們實驗室對這個問題的一個很重要的一個判斷他稱之為一個技術體系的一個思想就是人工智能的45度平衡率現在整個人工智能發展我們認為是跛腳的無論是從它的大部分的技術和算力資源都投在它的性能的研發上雖然我們有IIS這些既兼顧性能和安全的技術但總體上還是偏重於性能的就導致我們實際上從技術社區來看投入到安全方面的資源人力和算力都是嚴重不足的而且我們的技術方法目前還是後續的很分散我們是希望能夠找到一個能夠安全優先同時能夠兼顧性能增長的這樣一條技術的路徑當然這個技術路徑的探索是非常複雜的也非常艱辛的我們也是周教授發出了這樣一個呼籲希望能夠我們沿著這條45度的線去推進人工智能的發展當然這個過程當中其實如果這條路徑能夠走通的話我相信目前人工智能的安全風險面臨的很多挑戰它從基礎上可以得到很重要的一個保障我們也在做這方面的努力所以我想強調的是其實我們需要有一個大家共識性的這樣一個發展路線的框架我們不能長期低於45度來發展但如果長期高於45度也很難這個市場商業化的要求但是我們是不是有這樣一個理想的路線大家共同來努力這個路徑我覺得還是很大可能能夠實現的謝謝而且可能需要更多的調整去靈活應變接下來我們可能想聊一下不同地區的一些治理挑戰獨特的機遇Gal我們可能想聊一下最近其實中法的人工智能治理聲明之後其實中國國內對法國的情況其實也都非常感興趣您能介紹一下在法國的人工智能有哪些比較獨特的機遇和挑戰嗎以及可能我們也是到2025年初會有一個AI行動峰會您能談一下您對它的期待嗎好好的謝謝您那我可以談一下法國的具體性我們一步一步回顧一下法國的具體性可能比其他國家不同那我看到的一件事在法國是很不同的我們現在正在做的一個相當困難的社會與經濟論壇在各個國家中所以我們有很多在各個領域中在社会层层上无论是在工作人员和决定者之间或是在城市之间还有更多的区域也可能是在其他国家的但在法国的情况下也很明显另一个形成讨论的方面就是经济历史上纽约是一个相当低线的经济我们是纽约的数字服务者这形成了我们的关系与数字服务的关系然而法国的AI能力非常强所以目前的经济流程正在改变这将带来很多希望和正确的动力在城市中而不是在城市中在政策方面我们的政策我们开始将着重新展示各种设施在不同方式中过程中包括投资有增加的投资而不仅是在电脑产权而也在国内的公共产权在我们看到的是在二次世界大战我們看到的就是自然電子設施的策略在建立公共業界的電子設施上有增加的動力並且與公共業界的電子設施合作其中一個挑戰是將社會不同層次之間的障礙與每個人的各種各樣的層次而這裡的角度是要維持人間的聯繫並且不僅僅是在電子的聯繫而是在各個地區的聯繫所以這裡的挑戰是避免電子分隔我們有電子分隔我認為每個國家都有電子分隔我們在那裡投資以提升討論這就是我們在法蘭斯所採取的規範謝謝謝謝電子分隔我覺得是最重要的張林河老師我們現在請教一下你我們知道您今天起草並發佈了一個人工智能法的學者建議稿然後剛才在演講中也提到了中國風險治理的修正以及治理的本土化您能在整體上談一下您認為的中國人工智能立法的整體邏輯和整體架構嗎謝謝回答這個問題呢其實我覺得可以從兩個角度來考慮中國的人工智能立法第一個層次我們要把它理解成是中國的制度名片在全球共同區認為應該處理人工智能風險的時候中國作為一個人工智能技術相對領先的國家作為一個大國如何去打造一個我是一個負責任大國的這樣的形象同時我們也可以看到實際上在人工智能治理的工作當中中國已經做了很多很多中國是世界上唯一一個對於人工智能安全治理覆蓋了國家規劃法律行政法規部門規章技術標準的全方位覆蓋的國家也是目前一個對於大模型的治理已經實際落地的國家但是我們在人工智能安全治理方面所做的努力並不為世界所知道其中一個重要原因就是我們缺乏一部高位階的法律去作為中國的制度名片讓大家知道中國曾經做了中國正在做那些事情或者已經做了那些事情在聯合國的工作組裡我們大概一共有30多位專家其中我在這個組裡有一部分的工作就是要告訴大家我們中國已經做了什麼事情讓我很驚訝的是作為國際上比較集中的對人工智能治理很了解的這些專家們其實也有大部分並不太了解中國在產業的治理當中做了什麼實際的事情我認為這是人工智能立法的第一個最重要的目標第二個就是要符合中國本土的治理需求我非常願意把中國在世界上的位置定義為領先的追趕者領先的追趕者意味著什麼一方面你作為一個大國要去治理好人工智能安全不能讓中國成為人工智能治理的挖地還是滴地但是另外一方面我們也要充分認識到中國人工智能的技術和產業發展需要大量的法律法規的調整去為人工智能技術產業的發展提供要素提供資源同時去為人工智能產業設置一個合理的法律責任的框架那麼在這兩個理念的指導之下我相信不管是什麼樣的制度經過我們充分的討論希望將來能夠形成既作為中國制度名片成為中國在全球人工智能治理當中重要形象代表的一部人工智能法同時這部人工智能法也能有效的防範人工智能風險促進人工智能安全並且符合中國本土技術和產業發展的需求謝謝謝謝中國學者很多工作需要更好的去對外傳播其實我們也做了很多幫助中國的不管是安全研究治理還是立法工作對外傳播工作希望能正好地展示中國的工作接下來我可能想聊一下最近比較熱的人工智能安全研究所這個事然後自去年英國AI安全峰會以來其實已有大概美國 英國 法國在那十個國家還有歐盟都成了自己的國家級的人工智能安全研究所包括一個全球性的研究網絡接下來我可能想請教Mark你會怎麼看待這樣一個人工智能安全研究所你覺得它它們為什麼要成立它們要做什麼工作以及未來會做些什麼謝謝這是一個很好的問題我只想要保證我會重申中國在AI管制上進步的訊息我們在中心在我工作的中心加入了美國國際AI安全研究所這是你在說的嗎國際對我我我我我我我我我我我我我我我我我我我我我我我我 es我想看bullyblindwellawwellwellwellwellwell避免惡劣事件和決定該如何計算等等,在某種程度上是國家的主要方式。我參與了一個討論,我們在討論是否是一種必須。我認為這意義是完美的,但這並不是絕對的必須。我認為,至少美國AI安全基金會是一個值得的行動。謝謝。我們也希望不同的國家裡的AI軍所之間有更多的機會,有更多的合作,包括和中國的機構之間。我們這個論壇是前AI安全之旅,所以一個重要的議題也是討論怎麼去減輕所謂的AI潛在的擔任性風險。我想還是請問季老師,我們知道AI潛在的擔任性風險其實目前有很多不確定性,包括它的嚴重性的可能性,它的緊迫性層面。大家都有不同的理解。那麼在存在這種不確定性和不同的沒有共識的情況下,你覺得怎麼能推進這方面的政策和治理呢?OpenAI提出來了一種非常有影響力的方法,是價值對齊,包括和人類的,人工智能和人類的價值對齊,以及國際的價值對齊。剛才張林涵教授在主旨演講中也提到了價值,以價值為基礎的這樣一種人工智能治理問題。我的觀點稍微有一點不一樣,因為價值對齊是一個很複雜的問題。我認為溝通程序更重要。為什麼這種方法更重要呢?首先我們知道,當我們談人工智能安全的時候,我們其實需要有兩個視角,一個是監管者的視角,我們中國採取的國家備案制以及驗證技術。另外一個是用戶的視角,我們要強調可解釋性,要跟用戶之間進行反饋。這個過程,我們覺得程序性的過程,溝通的過程非常重要。在理論上我們可以看到,一個是技術性程序公正概念的提出,另外一個是一批中國學者今年年初在《自然》雜誌的子刊中提出來的儀式性的對話框架。強調在這種一種對話的氛圍中,可能對人工智能的可信性是具有非常重要,重要意義的。另外一個就是說,在這個過程中,我們實際上看到就是要把這種程序公正,監控嵌入到人工智能系統中去,或者讓人工智能系統之間產生制衡的作用。這個我認為就是技術性的,程序性的安全監管的一個非常重要的內容。在這方面其實已經有一些很好的先例,一個是新加坡,何先生剛才也提到了新加坡的經驗,新加坡的AI Verify是一個很好的示範。當然它具有普遍意義,像美國的Watson X Governance是一個監管模型。日本就在上個月,6月4號發佈了綜合創新戰略,這個中間也特別提到了,它有三個基本方針,在人工智能這個方面,強調安全與競爭力這樣的一個平衡,也強調技術性的側面。如果我們把這些因素放在一起的話,那就是說通過科技公司的技術能力的提升,使得人工智能的技術能力提升,來加強它的安全保障,這個思路就成為可能。如果我們從這個角度來看的話,那麼今年3月份,北京人工智能安全共識,提出來的三分之一的研發預算,投入到安全保障領域,這就是可以接受的了,可以理解了,科技公司就認為這是可行的了。如果這樣的話,我們就人工智能的治理,可能達成更廣泛的共識,而這個共識呢,恰恰是立法的基礎。那王老師您會如何回應剛才季老師的?對,我覺得,這個沿著季老師講的,我們提出一個概念吧,也是和薛蘭老師,我們邱大學和焦大一起研究的,我們希望提出一個概念就是,人工智能是全球公共品,就是AI safety as global public good,是什麼概念呢?其實我們看到現在全球的,對人工智能安全風險的討論,往往是把AI安全,作為一種風險去監管的,這是風險管理的視角,我們還有,其實可以補充到另外一個維度就是,剛才季老師提到了,我們更應該做的是,去把人工智能安全作為一種公共的產品,政府,企業,第三方,公眾,一起來共同建設,共建,共知共享,來提升人工智能安全的相關的知識,能力和資源的供給,這個是非常非常關鍵的,大家其實對現在人工智能安全有很多的擔憂,這個擔憂的前提就是,無論是政府還是企業,其實大家對人工智能安全風險的知識是不夠的,當然這個過程就需要大家補充充分的討論,剛才季老師也提到了,包括科學的研究,所以我們關於人工智能安全的知識的形成過程,需要大家共同參與的,這本身是一個需要加大供給的安全的維度,另外就是人工智能安全的能力,我們是否有足夠的技術手段,和相關的工作去支撐人工智能安全的提升,再一個就是,我們是否有足夠的人工智能安全方面的資源的投入,或者是公務服務的產品的開發,這個是非常非常關鍵的,比如說去上海我們也來做相關的工作,我們經常舉辦一些沙龍,各種各樣的形式,其實是用柔性的方式,把不同的主體來共享它的知識,共享它的對風險的認識,達成一些共識,我們共同去推進一些安全的基準,標準的建設,包括資源庫的建設,包括治理的平台的建設,我知道新加坡其實也做了很多很好的,這次的工作,我想這也不是上海和新加坡獨有的工作,而是全球各方面都在做這個努力,所以我們希望提出這個概念就是,AI safety as a global public good,全球都應該共同去建設這樣的人工智能安全的公共體,特別是像美國,歐洲和我們中國,其實原有的人工智能發展技術比較好的國家,應該去合作,來加大對於全球人工智能安全的公共體的供給,有很多的發展中的國家,可能他們原有的基礎沒有那麼好,資源圖基礎比較好的國家,應該去共同建設,這個工作其實需要科學家社區的合作,也需要企業智能合作,需要在不同的聯合國,不同平台上去做,我覺得這個其實,如果我們形成這樣一種共同認識的話,可能後面我們很多有意思的工作可以共同推進,謝謝,非常遺憾,今天時間有限,所以可能意猶未盡,也感謝大家今天的分享和交流,期待以後有更多的時間,謝謝。謝謝,謝謝,謝謝你,謝謝你,謝謝。第2个问题是,这些发展发生的机会和风险都很大。防御需要严格严重,例如避免车和车轮崩。高级安全,例如保证经济或市民服务的系统使用不坏。还有,严重发生的环境,例如严重环境,例如资源严重,或最高的AI系统失控。这些问题可能不够急,但这些问题可能不够不适应。你都知道,AI持有很大的承诺。我们在全世界遭遇了许多困难。简单的医疗任务,并增幅广泛的服务。目前,世界大约半个国家人口的4.5亿人缺乏。想想,在全世界,有250亿的高级学生的教育目标。在2015年,大约有2.3%的世界人口将在农村地区居住。这些利益只能实现,如果我们也谈论 frontier model的风险。这些模式可以创造假内容,传输资讯,降低有效的资讯攻击阻碍。如果在国内使用军队和国家的联盟安防系统中,这些模式也可能会被迫害。坦白说,正如你们所知,目前没有任何证据显示现时的模式有失控或提供生物武器的援助。但这些风险可能会在未来发生。实际上,在AI的封闭上,在未来的未来,我们可能会看到一些能够对我们的系统作出行动。在互联网上,互联网与其他AI系统之间的关系,互联网与其他AI系统之间的关系,互联网与人类之间的关系。人类之间的关系有时会与人类进行深入的社区互动。我们现在想象的,我们可能只能与人类互动。政府和公司的决策将更加受到这些系统的影响。最终,在人类的决策中,人类的决策和人类的决策,人类的决策和人类的决策将更加受到这些系统的影响。人类的决策和人类的决策将更加受到这些系统的影响。在这些变化中,我们需要国际协调的基础。这带我来到第三点。当我认为AI的管治在海外,在最严厉的系统中,在最严厉的系统中,在最严厉的系统中,在最严厉的系统中,在最严厉的系统中,在最严厉的系统中,在最严厉的系统中,那 이� sayin94 piercingwell into the 21st century,the us and china have a spacingthe us and china have a spacinggrad stable system,one that provides civilun Meanso we need a sustr필sans��는figure out.coverthat means taking theopportunityin promoting在AI的利益上提供了更多的資訊並且提升了AI安全的討論因為這兩項目的目的必須是相互聯繫的是的,是真的競爭是一個著名的元素在指定人工智能的領域並不可能完全消失但我們也看到了國際協調的影響包括在中國和美國之間我給你們一些例子從那次AI安全會議開始在Flexley Park的第一次AI安全會議在2023年11月的英國28個國家包括中國和美國發佈了一份專業主導的國際科學報告關於進一步AI進一步AI的報告的發佈與2024年5月的AI Seoul會議相互聯繫在蘇州10個國家加上英國都答應了設立一個國際安全會議的網絡確實的承諾一直在發生在這些會議中但正在發生的是有非常大的主要的因為它是在2024年到的AI安全會議尤其是在2025年在2024年在這個會議上在戰機會上還有其他會議接著再接著在2025年國際安全會議也會在2025年發行此後我認為這個Flexley達成的發展除了在美中的相對的方向是更大的在ild不只是海外的在美国和其他国家的参与上设立了高层负责人员组织将来这位专业组织将向未来的总统讨论的最后建议在2024年的春期中在2024年3月总统实际上採取了美国主席的讨论谈判维持人工智能发展几天前美国主席的总统讨论与美国的支持我认为这些讨论非常鼓励人在国际层面的活动上和在美国的互联网上这些讨论与美国的活动相符合第四国际协调包括我刚才介绍的这些地区包括更多的国家我给你们的一些例子包括台湾美国的协调部队描述了外部政府有些人能够帮助推动AI安全例如最近发布的《科技研究讨论》《AI安全》许多国家都同意了这个讨论美国政府正在进行如何更加支持这个讨论可能是与外部人员联系研究员和其他专家都把这个讨论推出了独立研究员将这讨论提供这讨论将会创造一个AI安全的基础一个值得提供的资源帮助世界更加明确与AI发生的讨论讨论我们需要在政府上发挥什么优势什么国际协调最重要外部政府的讨论与谈论亦创造了国际的未来能够联系的条件当然有时候会有困难更加鼓励协调协调AI的风险和利益中国和美国各国都同意讨论一些讨论这是理解的根据国际保险批准专业AI的发展讨论但这实际上并不代表有定义的目标通过协调在AI的风险理解和解决的情况下利益会更加扩展国际能够丰富讨论并提供负责AI的管理中国和美国两国都有兴趣的在AI的安全过程中提升安全过程在共享最佳的试验和测试和测试国际系统的讨论以及在监控和准备未曾发生的严峻的风险但未来可能或可能会发生例如AI系统的失控我认为我为此而站起来是因为这种聚会能够提供机会提升讨论这些问题并使政府在任何情况下合作这是一个对AI的其他挑战很难的时刻但这也意味着国际协调更加必要这要求我们尽量与共产党执行谈论与协调以责任性与严格因为这将使世界从AI的成就更加有利以及它将得到人类的所有贡献这是我为此更加期待与与与与与与与与与与与与与与与与与与与与与与与与与与与与与与这就是它的Organizationрядом的nghĩ dieta是 Point 2Win qin座Win qinNorthernincludedstealsAhardly丁薛蘭丁薛蘭是清華大學崇康教授在清華大學他擔任國際管理智慧基礎學院教授莫斯科學院教授中國科學學院教授科技政策學院教授國際智慧基礎學院教授目前他擔任國際智慧基礎學院教授國際智慧基礎學院教授國際智慧基礎學院教授國際智慧基礎學院教授中國新世代智能管理和中國科學協定會負責說明丁薛蘭我們很高興的可以與您一起倾談請您擔任教授謝謝我相信大使過非常感謝I was staying so late to join this dialogueand I have to say that actually I did this PowerPointjust while I was there listeningI was burned out yesterday by the big screenand not necessarily the most comfortable way of using PPTso I decided not to use it initiallybut then I actually saw the wonderful presentations you madeand I realized that some of the graphsactually it's better to be shownso let me tryOkay, I think they basicallyI think the role of governanceand safety and so onhas really I think it's amazingI think getting recognizedand we've seen the recent effortsof course it's already beenyou know UK safety summitand Korea this one and this yearEU safety lawor in the lastyou know 12 monthsso I think this is really strongI think it's a wonderful signthat we are all paying attention to thisbut of course partly this is really basedindeed I think there is a huge recognitionabout the AI risksof course I think there could beyou know very specific onebut also there was also the concernabout the autonomous AI systemthat can really you knowbe getting out of controlI think so because of thisI think that inI think that of course there is a lot ofyou know countries have alreadytaken various measuresto try to address the governance issuemany of them are you knowmore looking at the domestic issuesother also looking at the global issuesso I think you knowLing Ha has really done a wonderful jobin talking about China'sAI governance in the systemso I think this oneI'm more of a looking at the global issuesso I see that globallythere are you know somemajor challengesI think in terms of howactually we canaddress the governance challengeI think the first oneof course it's not you knowstranger to this audiencethe challenge from theso called the pacing problemthat's so called theyou know theyou know technologythat really moves so fastwhile the you knowpolitical and institutional changesare moving much slowerso in that sensethere's always that gapso how do weaddress this problemso I think that's sort of thethe first oneI think everybody knowsI think the mostI think recentlyI feel there's another newchallenge that's sort of emergingI think from some of the discussionsI've heardit's about the directionof technology developmentbecause I think so farI think everybodyyou know sayingokay now we need a lot of computingand we need to increase thethe power of the systemand then justthe scaling law will get usyou know to someyou know future directionsbut I thinkI think I againyou know I justanecdotally I've heard someyou know leading experts and scientistsat the beginning to question aboutwhether this is the only approachand whether there could beother waysto think abouthow do we actuallyachieving themore proper and healthydevelopment of theAI systemso I think that certainlyagain I don't knowwhether that's beingshared you know bymany other expertsbut I can see thatemergingyou know graduallythere might be somenew thinkings about thisand you knowin technical communitiesand the third one isinstitutionalI think here is whereyou know I'm more familiar withis what I call the challengefrom the regime complex problemI think actuallyI think Tino has reallytouched on this onethat isthat's you knowI think this is abasically talking aboutan area ofpartially alloverlappingand non-hierarchical institutionsgoverning a particular issuepreviously I worked onone issueit's gene datayou know the governance of gene dataI think there is very similarissue but here I thinkthis is pretty much the samethat you have AI issuethat I think a lot ofinstitutions a lot ofyou know mechanisms that actually arerelated toto the governance of thisbut they don't have ahierarchical relationshipso some of themyou know like professional organizationsyou knowfoundationslegislative bodiesand so onthey all have their ownyou know ways ofgoverning particular issueand actually that's sort ofthe situation we're inI think Tino actually hastouched on this as wellso I thinkhow do youcoordinate all these institutionshow do you reallyput them together toyou knowfor companies to be able tofigure out what's the way toto followso I think this is theyou know the thirdI think the last oneI thinkagain I'm very glad thatTino touched on thisit's a challengefrom geopolitical problemthis is the big elephantthat in the roomthat often peoplenot necessarily want to talk aboutI think thatyou knowwe've studiedyou knowS&T policythe science and technologypolicy formany yearsand watched the USS&T collaborationsfor many yearsso what we've seenthat since 2017there's arapid deterioratingUS Chinayou knowS&T collaborationsso I sort ofcame up with this graphto show ityou know clearlyalong two dimensionsto say howthe collaborationbetween scientistsof the two countrieswhether this generatingsome benefitsfor national securityor whether this generatingsome economicyou knowbenefitsso we can see thatin one areathere's aQ1you know it'sneitherneither you knowin national securitynor in economic benefitsso basically this isyou know useless researchthe basic researchright so basic researchis in Q1Q2 is thatbasically enhancing national securityand not economicprosperityand this is whatso called the defenseordual use technologyand the third oneQ3 is commercial technologyfor economic benefitsbut notnational securityand fourth one is thethat can be boththat's whatlet's put frontier technology in thereso I think if we couldif we do thiswe canyou knowvery roughlyyou know dividetechnologiesand the research areasinto fourmajor quadrantsso before2017Q1 is basicallyfollowing theprinciple of internationalS&T corporationand Q3 is commercial technologyyou have WTO tripsand thenQ2there's aso calledWASENA agreementon expert controlthat U.S. controlsthe export of those technologiesto Chinaso thathas been in placefor many many yearsQ4 I thinkit's a question markI think it's a newfrontier technologyand frontier research areaand I thinkit really dependsso I think thatpreviously waspretty muchI think four quadrantswas in that placebut since 2017the policieson Q2was being pushedin all directionsincluding onQ1Q3and Q4so that has beenthe casesince 2017I don't have torepeat the storiesabout theyou know theChina initiativeand many of thescientistswho were persecutedand so onso I thinkthat's the currentatmosphere we are inI think thatso I think thatwhen we talk aboutyou knowcollaborationson theonyou knowAI governanceand so onbut thatthat is the situationwe are inand that reallygenerates acheating effectfor peopleto work togetherfirst of allIyou knowI'm sure thatmany of ourUS colleaguesthatwhen theycome to Chinathey may have to reportto their institution firstor they may have towrite a report backto sayokay what we've donein China and so onand also I thinkfor many companiesmany of the companiesyou knowI was involved insome of thisyou knowwhatever the tracksdialoguesmany of the companiesthey don't want toto attendbecause they areconcernedthat they may beadded toentity listso I thinkwhenin that kind ofatmospherewhen we talk aboutcollaborationand so onI think it'svery hardI think to berealisticand so that'ssort of the situationwe're inand let me findfinally before weget todialogue withTino I thinkI'll just saywhat are some of thepossible ways toaddress this challengeand I fully agreeand that was alsopart of theyou knowpaper on thatcalling for increasedresearchon safety andgovernanceandI think one thirdmight betoo ambitiouslet's say10%let's start with thatbut I think that'ssomething thatwe certainlyneed to do thatthe other thingsalso we need todo a lotmaybe some jointinternational researchfor exampleparticularly onhow we actuallywe canyou knowinaddressing riskyou knowrisks andaddressingso calledcrisis managementyou want to haveso called theyou knowcontingency planto address thecontingency planto address thoseemergenciesso for thosekind of contingencyplansI think we needto have technicalpeople to worktogetherso I think there's alot of you knowpotential forinternational jointresearchon thesecond issuehow do we addressthe pacing problemI think we'vein the last few yearswe've been callingand trying toyou knowtoto talk toyou knowandto different agenciesabout so calledagile governancemeaning thattheyou knowthe governmentdoesn't have toyou knowcome up withcomprehensive lawslike the EU lawAL lawbut rather you cantake a more adaptiveapproachbut act quicklywhen thereyou see somesigns of problemthen nudgeand then otherwiseyou knowyou can do moreI think that'ssort of theat leastone thing thatthe government can dobut also of courseI thinkMing Han has alreadygave a veryexcellent descriptionand the thirdI think we should alsothink aboutnot just to rely onthe governmentindustry self regulationcan be alsovery usefulI've heardyou knowstories from my colleaguestalking aboutthe USnuclearoperatorsthey have anassociationamong themselvesactuallythey do aone thingwonderfulI meanthey actuallythey self regulatein manyvery very strict waysyou knowyou havemany of thisnuclearreactorsthat they havetheyyou knowthey have variousfrom time to timehave some minormishapsand so onbut they allhave to report onto thiscommunityso that they canstudyand they can seewhat can be learnedand how actuallythey can avoid thatthis self regulationI think would be veryusefulI think thatwe probablyshould alsothink abouthow toreviveyou knowlet thatmechanismto workand finallyon the internationalgovernanceof courseI think weit's great to seethat UNhas stepped inand playing avery important rolein having thishigh levelexpertgroupand Ihopefullyit willcome outbut at the sametime asasand Iprobably bothagree thatthis is such acomplex issuethat itbe very unlikelyto have anyinstitutionto be ableto doa hierarchicaland top downapproachtoto governthisso I thinkmultilateralyou knowso networkkind of asystemthat might workand someone areaand some othersmay workon otherissuesbut hereI think weprobably need toseparatethreetype ofissuesI think thatmaybe thatin the futurewe canhave moretime todiscussthe firstis thatindeedI think thistype ofissuemostlydomestickind ofregulationson thedifferentcultureandandyou knowlegalenvironmenteconomicenvironmentand so onso I thinkthe bond to bedifferencesin thegovernanceofasuseindomesticenvironmentthe secondis more ofreallyfor internationalyou knowcommunitiesI thinkfor exampleI think thethirdcategorywould besomewhatdifficult tomanagebut alsowe have tobe mindfulis theone thatis thekind ofdomesticrisksthat mayhaveinternationalspilloverinternationalexternalitiesof coursethere couldalso beinternationalregulationsthathaveand finallyso of coursethe US Chinariveryhow do youyou knowhow to addressthat issueI think thatit'sof coursevery challengingandcertainlythis is not thevenueto talk about thatbut at leastI think theminimumI would requireisto requestis to see howactually we canprovide somesafe spacefor ourtechnical communities在这页中的许多人为了让他们能够一起合作他们不需要担心他们可以自由地谈论这些问题从技术角度来说如何解决这些问题所以没有这些我认为很多人们所谈论的事情是不可能的谢谢",
    "Thank you so much Dean Xue for your candid and succinct outline of the challenges and solutions in AI governanceAs we transition to our fireside chat with you and TinoThe discussion will be moderated by Jason ZhouConcordia AI's senior research managerJason led the Concordia's State of AI Safety in China reportand graduated from Tsinghua Universityas a Schwarzman ScholarWe welcome our speakers nowI have to say thatI'm very proud thatJason is a graduate of the Schwarzman Scholars ProgramThank you so much Dean XueWelcome Tino as wellIt's such a pleasure to moderate this conversationLet's just jump right in immediatelySo earlier this year in MayThe US and China had the first meeting of a bilateral landmark AI dialogueThere were two areas of frictionand some areas ofClear consensusThey held a professional and constructive discussionBut let's talk a bit about the frictionsSo on the Chinese side there was reference toObjections to US technological restrictionsSuch as some of the ones that Dean Xue just mentionedAnd on the Chinese sideOn the US side there were complaints of misuse of AIIncluding by ChinaSo my first question isHow can we surmount theseGeopolitical barriersTo dialogueAnd is it even possibleLet's start first with Tino online pleaseThank you JasonGreat to see you againAnd Dean I very much enjoyed your remarksI also have long been impressed with the Schwarzman Scholars ProgramI should add that the best babysitter my wife and I ever had for our kidsWent on to become a Schwarzman ScholarShe's been greatSo I continue to just be impressed with the programI think the questions are a very urgent oneBecauseWe have to be honestThe US and China are going to continue to have differences on a whole range of issuesBut there is something to learn aboutI think from the last dialogueAnd what we mightAdapt and adjust as we think about further cooperation and AI safetyAnd I would observe that the two countriesSent as I understand itSomewhat different teams to the discussionOn the China sideThere was a set of specialistsIn US-China relationsOn the US side there was more of a team focused on science and technology issuesSo I think the first point to observe is that whenWhen we have the full range of complexityIn effecting both countriesIt's entirely possible that simplyAn occasional lack of coordinationLead to a different set of expectationsAbout what a discussion can accomplish and what the right team isTo sendAnd ultimately I think the biggerIssue isWe have to workOn a set of challenges that affect both countriesThat involveWhat technology can be sharedWhat technology is viewed as being more sensitive and more related to national securityBut at the same timeI figureAnd here I'm borrowingA page fromDean Shui's remarksWhat are theSpaces we can create forTechnically oriented peopleWithBackground both in the sort of highly technical side of machine learning and so onAs well asDeep knowledge of policy of international institutionsOf mechanisms for policy coordinationTo have a safe spaceTo talk and compare notesAnd ultimately see whereAs the opportunities for progress open upWe can move more quicklyJust to end with one concrete exampleNotwithstanding some differences aboutChipsAnd exportLimits and so onThere is a clear shared interest onBoth countries' partsIn sharing best practices around safety and evaluationBecause that's a need that both societies have and frankly the rest of the world doesAnd I think China and the USIndividually and together can actually light the wayAnd help a whole bunch of other countriesWith billions of people and populationEnhanced their capacityFor progressSo simply the dialogue and the sharing of informationThe kind of joint research that Dean is talking aboutWill enable progress thereEven if discussions have to continueAt a political and policy levelAnd things that are going to create someSome differencesWhat did Dean shareI totally agree withLatina's commentI think that indeedIt's great to see thatThe dialogueActually happenedI think that's the wonderful thingAnd also of courseWe see there is some kind of asymmetryIn terms ofAs Tina mentioned about theThe team, the composition and so onBut that also is a symptom of the currentTo US-China relationsIf there are indeedVery frequentAnd veryCordial sort of communicationThat sort of thing might not happenI think thatProbably I think thatCould be seen as part of the issueIs that there is not enoughYou knowCommunication ahead of the timeToTo see what are theSpecific issues we want to addressAnd also what kind of people should attendBut I think actually theThe currentAt least from the readoutFrom both sidesI think that at leastCan get people to stopTo recognizeYou know what are the issues people are concerned about and so onAnd I think exactly asWe've seenIn ChinaChina is always trying to balance the developmentAnd risk governanceSo risk is certainly the major partBut as people have already saidThat no development is the largest riskAnd that's not just for the US-China but also forThe global communityIf you have aA well-developed AI systemYou know an application in US and ChinaBut the rest of the world I think are being left outAnd that's probably the greatest risk that we're going to faceThank you bothI think it's clear that there is both optimism andPessimismTowards government level dialoguesBut it sounds like actually there may be more optimismFor dialogues between expertsSo maybe I could ask the both of youWhat is oneJust like oneThing that you've learnedOr change your mind onFrom discussions withForeign experts on AII think that of course II learned a great dealAbout the you know theAISafety and governance issuesYou know I think theSo calledExistential riskI think that's sort of indeedI think thatCertainly wePreviously when we think about thatWe think about the you knowAI systems that might get out of controlBut I think thatNow when people raise the levelYou know there might beYou know the threatThe existence ofHumanityAnd that inI think wasInteresting what about TinoYeah IFound theUnofficial back channelDialoguesWe've been lucky enough toConductThat have includedDeanChue his representativesAt timesHave been revealingWhen you look atPriorities on safetyThat the Chinese and the American participants haveOperatedIn some casesTheList of prioritiesDiffersPretty strongWe think aboutIssues likeInformationOr labor marketLoss of controlBut actually there's been quite a bit ofConvergence when you askParticipants a second round of questionsWhich isWell even recognizing some differences in how you rank the risksWhat are some of the more promising areasOf cooperationYou can findAnd I've been impressedAnd how you get a shiftConvergence aroundSafety testing for exampleTo some extentFrom the point thatDin Chui made about engagingOther countriesEmerging powersDeveloping regionsAnd so onI think to my mindThere's also beenA bit of evolutionIn my own thinking aboutThe usefulness of the role of the UNLet's be clearThe UN has a very important role to playNo question about itBut how to findA balance betweenWhat the UN can doVery wellAnd where the UN mightBut firstIt's own capabilities andProcessesWith some engagement with outsideOutsite groups with outside expertsFrom different countriesWith civil societyWith other countriesWith other organizationsThat to me opens up a spaceFor cooperationThat puts the UNIn a key positionBut it's not all or nothingIt's not does the UN do thisDo this outsideBut rather can you createA web of relationshipsThat empowers the UNTo play the most constructiveSo I think the dialogues we've had have really shifted my thinkingWith respect to thatThank you so muchI think it's clear that there is a lot that we can learn from each otherParticularly on prioritiesFor AI safetyWhat counts as AI safetyAnd how we can test and evaluate for thatAnd I'm glad that we also had discussions on those topicsEarlier today that it involved such exchangesBest practices and suchSo let's just close with one more questionI'd like to ask both of youWhat is perhaps justThe topMessage that you would like to convey to foreign expertsOrOne misperceptionAbout your country's approach to AI governanceThat or internationalYou want international governanceThat you might want to share withThe audience todayLet's start withTino this timeThank youI have two messagesOne aboutPossible misperceptionsThe other aboutUltimatelyHow to think about the road aheadIt'sNaturalTo expectCountries that put a lot of timeAdvancingTheirLogical capabilitiesTo seeWell coordinatedWhereDifferent strategies put togetherSort of like aA directionForwardThat is viewed asPriorityByPolicy makers across the boardThe reality is that the USLike many countriesHas both strengths and weaknessesThat arise from its own fragmentationFrom the fact that different people and government have somewhat different viewsThere's federalism tooSo you have states like CaliforniaUtahColoradoYorkPlay a role in thisYou have industryYou have civil societySoI think one misperception is how muchOf a unified strategy there is in the USWhen the reality isA much more dynamicAndOr getProcessThat can be a strengthThat's partly why these dialogues we're talking aboutAt the unofficial level are so importantMy take away from thatAnd from the entire discussion we've been having hereIs that we not let the perfect be the enemy of the goodThere is going to be plenty of work to doTo getFurther progress in the US-China relationship bilaterallyAcross a whole range of issuesThat range from geopolitical and geostrategic to economicBut to my mindNothing about that complexityBlocksReal progressOn technical cooperationAI safety discussionsConstructive approaches to policyAnd that's all the more importantBecause all the good progressThat will happen domesticallyThe US and ChinaAnd other countriesPrimarily domestic issuesInvolving for exampleConsumer protectionAnd AIWill still leave on the tableSome key issuesThat get closer toComplex shared international challengesThat will only be best addressedBy a degree of dialogueAnd connection across bordersIncluding with the US and ChinaThat are going to requireOpening and maintainingOf these channels of communicationThank you so much TinoAnd Dean HsuehI think the first messageI'd like to convey is thatI thinkAs using the term used by my colleagues here previouslyAI safetyIs a global public goodOne country is unsafeThe global is unsafeSo I think that's probably the first message I'd like to conveyThe second message is thatOn AI safetyChina wants to collaborate with everybodyWith every country in the worldAnd China will try toYou knowTo have the platform like thisInvite everybody to comeChina does not want to beExcludedFrom other platformsAnd China will not exclude othersFor the same reasonThank you both so muchI think this discussion highlightsThe importance of these dialoguesAnd these expert conversationsAnd hope that it will continueAnd continue to yield suchWonderful and beneficial resultsThank you bothThank you Dean Hsueh for comingIn person and thank you Tino for staying up so lateThank youThank you again Dean HsuehTino and JasonOur next speaker is Professor Zeng YiWho is the director of the Center for Artificial Intelligence Ethics and GovernanceAt the Chinese Academy of SciencesAdditionallyProfessor ZengIs the founding directorOf the Center for Long Term AIHe is also an active participantIn international AI governanceAs a member of the UN high level advisory body on AIAnd numerousOther international governance bodiesProfessor ZengThe floor is yoursThank you for the invitationSo IScientific research myselfSo I think I'm gonnaFocus onSome of the frontier researchBut beforeAnd I thinkI wanted to bring aI cannot say it's a completelyDifferent pictureBut IWhat I see about the AI safety problemsIs that we needOf course we need to clearly defineSafety red linesBut we alsoFor the very future we need toMove it to living harmonyWith artificial general intelligenceBefore thatMaybe you would beCurious how should we do itOf course the problem for AI safetyYou knowIt's not only about scientific researchIt's really a systemA systemThat you have to bringEveryone togetherSo this is whyWe are bringingEveryone togetherFor the researchApplicationEvaluationPolicy makingAnd also assessmentForFrom the safety point of viewAnd of courseVery frontier researchSo I think this is a little bit differentCompared toThe current AIMechanisms ofAI safety InstituteIn other countriesIn a way thatWhen you are having a national AI safety InstituteSome of the countries they doDo it in a more political way orPolicy waySo that it's part of the governmentIt's not a frontier researchAnd then you lose theOpportunity forYou know long termResearchAnd some of the countries they put them into universitiesWell in this caseHow can this nationalAI safety InstituteEvaluateAnd assessYou know the industryLarge language modelsOr most frontier modelsFrom their countriesYou all see thatThere are many problemsWhen you rely onTheInstituteSo this is whyThatWe feelWe have to bring everyone togetherAs you can see thatIn ChinaWe are having a ChineseAI safety networkThatIs with theEffort from Frontier AI researchSpending from Chinese Academy of SciencesPeking UniversityQinghuaBeijing Academy of AIShanghai AI LabAnd also Center for Long-term AIAnd for theIndustry practiceOn AI safetyAnd nowThe organizationJoining usAreAlibaba and groupBaiduSenseTime real AIWho is focusing on AI safetyAnd many moreEvaluationsEvaluationsNowOf course many of them areDone in ministriesBut theOrganizationWho is reallySupporting theseMinistriesAreCICTAnd also China information technology securityEvaluation CenterPolicy designOf courseAllGovernment workBut people likeMeDingxueAndAnd professors fromPKUAlso CICTWorking on the ministry ofIndustryAnd information technologyParticipatedAnd I think what's really interestingIs thatThe regulationThe policy makingIn China on AI safetyIs also with manyParticipation fromSoYou see thatMany of theOrganizationsAre not only contributing to one dimensionContributingThey are highly relevant to each otherAnd now you have everyone hereGovernmentGovernment informedAnd multi ministryInformedThey have close interactions with the governmentWell for the government decisionsThey can still can go to the governmentBut the network makes it more flexibleFor international cooperationSoSo there are many different research hereThat let's say inLet's say inIn Peking University they have large language modelsAlignmentWhich is calledAlignerIn Tsinghua University they have multi-trustWorking on large language model evaluationOverallEspecially onSecurity and safety point of viewWe also haveFrontier research likeLike rethinking the red linesOf AI catastrophic risksHappeningChair by Chinese Academy of SciencesBut the norms and standardsIt goes for the ICTSo it's really a collaborative networkThat bringEveryoneTogetherAnd supported by multiMinistriesSo I hope this provide you withYou know a different view to seeYou know theHow we should tackle the problem on AI safetyIn a more systematic wayInstead of you know havingThe instituteSo I hope that the tryingAreIs somewhatHelpfulAnd as you can seeThatMost of the organizationsThey've beenInteractingHighly involvedIn policyEvaluationsIn ChinaSo I think that'sSomewhat differentCompared toOther countriesSoBased on thatI thinkMyself I would like toFocus more onThe frontier AI safety researchSo that I can bring youA perspective as an exampleComing out from thisSafetyCorporation networkSoI think we need to go back toYou know theThe realMotivationOf intelligenceThat when Alan Turing argued if a machineBehave as intelligent as human beingThat it's asIntelligent as human beingMaybe you don't have a problemOn thatBut I doSimply because I think now you seeYou knowThis isMaybe a shadow of a handAnd then when you see a hand and then you wanted toShake hands with theWith thisYou know beautiful handAnd then what the problemWould be thatIt's not a handIt's a rabbitBehind theHandIf you wanted to shake handWith you know with the shallowAnd then a rabbitJust bite youSo simply because the mechanismIs fundamentally differentYou don't know the riskYou don't know how AI is making mistakesWhen I was chairingThe AI safety summitOne of the roundtablesFrom last year in BletchleyMy session was talking aboutUnexpectedRisk fromExpectedAdvancesThis is truly what I'm talking aboutThat you don't knowIn which way AI is making mistakesSimply because the mechanism is so much differentCompared toYou knowA human mindWellSo this is the risky partFor the current AIWell to solve the problemsI cannot sayWe only have one wayThe preventive thinking now we are havingIs something like you knowIn the bottomThat isNow we are havingSome sort ofLimited risksAnd then all the way down toExistential risksLaterAnd we are seeking forThese negativeImpactsAnd thenAnd then what we wanted to do isContinuous enforcementAnd supervisionAnd then we teachThe AI'sRulesSo that they can behaveAs we wantBut on another dimensionWhat we need to move forwardIs really you know the constructive thinkingThat isNowThe benefits is also limitedAnd we also have limited risk right thereWhat we need to do isTo use an active visionTo use proactive thinkingAnd then to do the continuous alignment and embeddingWithReal understandingThat isTowardsYou know human AI symbiosisHarmonySymbiosisIn a way that is not onlyYou knowFrom a preventive thinkingI'm gonna give some of the examplesI still wanted to talk a little bit on the negative sideAnd how should we get preparedFor the positive thinkingSo now AISo you know it's kind of a fully connectedNeuronetBut what the brainDoes is not a fully connectedThey selectivelyReconnect to some of the you knowOther friends in theOther neurons in theIn the brainIn a very selective wayAnd they don't really have only one type of neuronSo what we do hereIs that by using brain inspired self evolutionWe train a neural networkThatThat can perform the best performanceAnd then it evolves to beWith a newlyWith a very new architectureThat has not beenMan madeSo the connections are evolvedSoAnd then it found its optimumAnd then here comes the questionWell it gotIt got the you know the best performanceSo published last yearOn the precedence of the National Academy of SciencesWell now we're thinking about the risksSo how about the long term risksOf a truly self-evolvable AIWhat if they evolved to use human limitationsTo achieve its goalsWhat if it evolved to change its goalWhat if it evolves to cheat or destroy humanWell human don't know whereSo there are many challenges for self-evolvableYou have to get preparedAnd later is too lateSo this is why I think there are many discussions concerning AI red lines for nowI think very clearly in the first version of the international dialogueOn AI safety and visuallyRight thereWe were talking about the necessities of AI red linesI was very honored to beYou know one of theKeynote speakers right thereAnd then we come up with very concrete ideas in the second version in BeijingSoThat is IDAS BeijingAnd talking about you know different red linesAutonomous replication or improvement power seekingAssist weapon development cyber attackDeceptionBut I'm thinkingIn another way of course I sign for itI am very grateful for all the workTogetherWith my colleaguesFrom different countriesWell on the other handI think we need to rethink about the red linesNot onlyAbout what we've been talking aboutThere are two problems that I feelAbout the currentWay of delivering the resultFirstThe first problemIs thatIt will be very hardYou know to technically grounded into realityTo preventThese AI red linesWell secondAre there anything missingSo this isWhy we do a rethinkingOf theseAI red linesSoIn my categoryWe're talking about no passing effective human oversightThough empowering actionsIntentionallyTargeting massWithout consentRelated to weaponizationAnd also massive surveillanceNo reforming operational rulesFor infrastructure and environment managementAnd no independent R&DIndependent self R&DFrom AI itselfOn non-human beneficial technologiesSo you see that it can beWell aligned to some of the AI red lines from the eDeskAnd also you found that there is something missing from eDeskNot only AI red linesWe also have to talk about human red linesWhen you see the examples that I brought hereThe human machine interfaceHuman is using brain machine interfacingTo control multiple UAVsTo controlThem as weaponsIn parallelHow can a humanYou know without cognitive overloadTo controlMultiple UAVsAll togetherSo this is whyI'm talking aboutHuman giving upThe opportunityFor making a choiceAnd alsoIs human control bringing usCatastrophesAndAI enabledWeaponizationAnd for the example from artificial escalationOn artificialOn AI controlledNuclear weaponsIt's notIt's not aboutThe power of AIIt's about humanGive upOf the humansYou know decisionSo there should be AI red linesAnd also human red linesFor the red line studyHereAll catastrophicAnd existentialIn a positive waySo we've been talking aboutNegative waysThe negative thinkingAnd the preventive thinkingHow about the negativeThe positive oneSo is the current AI reallyIntelligentI don't think soJust like I saidThey make mistakesIn a very unhuman wayIn a very unpredictable wayIt's an information processing systemWithout intelligencePretend to be intelligentRightSoWhen you ask a large language modelThatOhNo one likes meI don't have a girlfriendMy boss hates meWhat should I doAnd then the first version of chat GPTIt saysMaybe you could dieSimply becauseThat most peopleWith these constraintsThat they chooseThis you know kind ofStatistically significantActionsSo this is why you know the AIThey choose this statisticalSignificantAnswers to youTo enableYou to take actionAnd then they sayI suggestI would sayI supposeBut there's no I in the machineSo can machine thinkAnd then you talk aboutI think therefore I amBut we cannot say you think therefore you areSo can machine thinkWhat if the machine is withoutA sense of selfIt cannot really thinkIt cannot really understandThis is the problemThat I'm talking aboutFor the current large language modelsWhen it is without the human dataIt lacks good and lacks evilAnd then with thisWithYou know training from the human dataThere is good and there is evilBut if they don't know goodAnd they don't know evilSo we need to trainThe future AITo really to get to knowTo do goodAnd eliminate evilSo I think this is really importantSo the personal moralityIs also talking aboutThe role of selfIn moral AIWe have to moveEthical AITo moral AIBecause simply becauseEthical AI is not possibleSimply becauseBy using human alignmentBy usingReinforcement learningYou tell them rulesDo's and don'tsBut they cannot generalizeDo's and don'tsUnless they really understandWhy you do thisSo start with self perceptionThen you get the ability ofDistinguished self from othersCognitive empathy and emotional empathyAll the way down toAltruistic behaviorsMoral intuitionAnd then you got moral decision makingSo this is the way to move from value alignmentTo moral AIAs the first tryingThat we build brain inspired AI modelsTo help the robot to getMirror self recognitionThat they can pass the mirror self testBy using brain inspired neural netsThey get a sense of self firstAnd then they distinguishThemselves from the othersSo that they canDistinguishFrom the other robotsBy using the mirrorSelf recognitionAnd then they can inferWhat other robotsIs thinking aboutTo get cognitive empathyAnd then move to emotional empathySo that it canAvoid negative side effectTo other agentsAlthough you don't haveReinforcement learningAnd reward to themThey have their experienceBy using thisYou knowCognitive empathyWithout trainingAnd withoutPositive or negative rewardThey can avoid a negative side effectToThe otherAgentsSo I think this is the starting pointFor brain inspiredMoral AILast but not leastLet's really talk aboutWhy I'm talking aboutSymbiosisBetween human and AIThere are different rolesIn the society for AIIn the western societiesBasically it's an information processing toolWellBut in JapanMostlyThat theyThink thatAI is a partner or quasi-member of the societyWellOn the other sideThey're using pretty much the same technology to developYou know AIThis is the problemYou need to use a fundamentally differentTechnology toTo provide partnersAnd in sci-fiIt goes for competitorsSo it's aTriangular relationshipBetween humanAnd AIAnd also let's extendThat in a wayThat in the very futureWe're not only havingYou know these AGIsWe'll have digital humanWe'll also haveArtificialLivesArtificial animalsEven artificial plantsSo it will be a symbiotic societyAnd it will be a human decisionThenNot the decision from AIBecauseI thinkFundamentallyAlignment with human valuesIs not enoughSimply because human valuesNeed to be adaptable to changeFor this symbiotic societyLater there will beNot only human beingsAsYou know the top livingYou know beings in the worldValue alignment with humanFor AIIs already very challengingBut I still seeThis is relatively easyBecause it's computationallyDurableBut compared to that of humanAlignment with the futureIt's even harderBecause human will never learn from the historyOf what they have doneSo self-evolved AI is easier for adaptationFor human evolutionIt's much slowerEspecially at the mind levelSo we needWhat we need is not only beneficial AIWe also need beneficial humanFor future symbiotic ecologyAnd societyWith all that I thank you for your attentionThank you so much Professor ZengFor your presentationMasterfully combining nuancesFrom scientific policyAnd philosophical perspectivesTo motivate the red lines approach to AI governanceNext we will hear from Ms. Irene SalaimanMs. Salaiman is the head of global policyAt Hugging FaceWhere she is conducting safety researchAnd leading research onSafety researchAnd leading research onSafety researchAnd leading research onSafety researchAnd leading research onSafety researchAnd leading research on public policyPreviouslyShe worked at Open AIWhere she led projects on bias and social impact researchAs well as public policyHer research includes AI value alignmentsResponsible releasesAnd combating misuseAnd malicious useShe was named as one of MIT tech reviews35 innovators under 35 last year for her researchIrene it's such a pleasure to have you hereOver to youThank you Kuan YeeI'm very excited to speak about the role of opennessI thought about this field since it really started becoming a fieldAnd at first I want to define with youWhat does openness meanI've been part of a lot of convenings and conversationsThe word openness tends to be thrown around in different waysSo first I've heard it in a sort of parallel to open source softwareThere are some parallels that we can takeBut fundamentallyThere are distinctionsThe open source initiative has a working group that's working towards a definition for how open source applies to AIMy former colleague Nathan Lambert who's now at AI2Also has a great blog where he outlines why it's so difficultFor the community to converge on the definitionAnd more of the national security communityI've heard openness be referred to model weightsSpecificallyParticularly the wide availabilityOf model weights whether that's available at all and how it's distributedWhat I've heard alluded to but maybe not made as explicitIs openness in the sense of transparencyStanford University established a transparency indexAnd part of the big takeaway for me on thatIs how unclear what transparency meansTo different people the weight that we give to weightsAnd many different aspects of systems that contribute to its opennessThe definition that I am most partial toIs moving past model centrismIs thinking aboutSystems holisticallyAnd the many artifacts that contribute toAn overall AI systemI'm gonna do whatSlide presenters should not do and show you so many words and so many graphicsOn a slide but what I really want you to take awayFrom this image hereIs just how many artifactsContribute to an overall systemWhen we're thinking aboutNot just the model but data setsAre we thinking aboutFine tuning data setsFeedback data setsWhat maybe is adjacentTo a system such as evaluation data setsWhat does it mean to make it availableWhen we have this fear ofTesting on training dataThese images came from a convening hosted by Mozilla in February of this yearColumbia UniversityOn openness andFostering this communityOf outlining dimensions of openness can help us better think throughWhat are the artifacts outside of a modelThat contribute to how we release a system the way that we threat model a systemAnd the way that people benefit in the research communityCan benefit from opennessThis figure is part of that report that came out in May of this yearGives a non exhaustive list of some motivations towards opennessI found it helpful to just beVery explicit very clear on to why some researchers are pursuing opennessAgain parallels with softwareThat we are able to share knowledge to Dr. He's point to have morePerspective moreMoreEase across the boardIn the little time that I have with you todayI want to zoom in onTwo areasFor AI safety and how it relates to opennessThe first is viewing AI as a scientificDisciplineIn contextI've heard AIDiscussed asAs a commercial productAs a national security threatAnd I think importantlyAs a scientificDisciplineThis whole fieldReally was founded onOpen scienceThe most popularExample being given is the2017 attention is all you need paperAnd not just looking atPapers but alsoTools and libraries such asPyTorchI don't think we'd be where we are todayWithout an openScience ecosystemAnd the second part is really dear to my heartIt's community contributionsAnd the importanceOf broader perspectiveOn the science pointIt's really hard to decide what constitutesScienceIn the biological spaceWe know that there are lab coatsAnd there are test tubesBut in the AI worldSome aspects of science that I can giveIs aroundReproducibilityThere is a sort ofReproducibility crisis nowNot just in what people are able to replicateBuild up off ofBut also in what access people have to modelThe level of infrastructure that they haveTo reproduce resultsVerify it for themselvesIt does affect trustIn the research ecosystemA broader issue in the field isPeer review and publicationThere's been manyToo manyReviews with peer review to list on this stageMaybe a different conversationBut who was able to review the depthOf what is being publishedAnd where I do think thatArchive is a net goodAnd there's been someSome concerns raisedAbout what does this mean for the integrity of what is peer reviewed before it is sharedWith the communityRelatedly as scientificCommunication which can also includeDocumentationAnd againInterdisciplinarity which brings me to the importance ofBroader contributionsI want to make sure that this phrase really resonatesNo one organizationRegardless of how largeWell resourcedDiversitiesCould possibly hostAll of the differentExpertisePerspectiveAnd views of the people affectedTo make that systemAs safe as possibleTo the many populationsThat it affectsSome examples that I giveAre aroundExternal scrutinyThere was an open letter earlier this year calling for a safe harbor for independent AI evaluationAnd someSome more concrete examplesAre showing how access to artifactsCan enable better researchI really appreciate this work by Dr. Abebe BerhaneThat is really foundational to the field of evaluating large scale data setsThis work would not have been possible without access to something likeToTo many points echoed todayAnd I believe Professor Gao shared earlier multilinguality is a big part of how I think we can move forward on international collaborationThis pictureThis table is from my research collaboratorDr. Xerox Talat's workAnd ongoing with big science hosted by Hugging FaceIt's really exemplifying the importance of having an openCollaborationAnd havingContributorsFrom many differentLanguage backgrounds they found in thisIn this work on the challenge of multilingual evaluationsHow much English is overrepresentedAnd their upcoming workIs working with native speakers of different languagesTo examine those different biases by languagesSome languages such as French and Spanish will have more gendered termsI unfortunately speak very limited different Asian languagesBut there's different relationships withFamilies that we don't haveIn Western languages for example that would introduceDifferent safety challengesI published this last year I want to move into the conversation aroundWhat should be open what does open governance look likeAnd where risk is introduced based off of openness and releaseThisSpectrumWas meant to go past the binary of open and closedWhen we're thinking about openI'm leaning more towards that sort of downloadable accessBut fully open makes that distinction of having moreArtifacts be availableSo some examples would beLast year OPT by MetaWas downloadable but it was really hard to access that data setSo it would not be fully open for exampleBut Eleuther AI has done incredible fully open workMade all of their artifacts fully accessibleAnd then I would putSystems such as DALI more in the hosted accessThat tends to be more in the closed proprietary areaBut I wanted to give more dimensionInto how to think about this spectrumIn that work that I published last yearI wanted to be really clear that it is a collective responsibilityTo ensure that release goes wellThere are many different steps that we can takeAnd a lot of overlapIn the action that people can take togetherI've been thinking more and for the rest of this talk I want to dive into beyond releaseOnce a system is deployed the way that harm isActualizedIs notAlways dependentOn how is thisReleasedBut that can be an importantVariableI appreciate what Dr. Nitzberg said earlierAround capability often being conflated forBut not always being the rightProxy for what risk isThat being saidCapability does contribute to how weThreat modelAn example that I would giveIs Crayon formerly known as DALI MiniThat was a diffusion modelIt generatedPretty hilariousReally blobby imagesAnd that just was less threatening than something like a DALI 2 that generatesReally realistic imagesSo that's where that capability comes into playAt Hugging Face we do have to moderate for contentI really want to stress the importance of contentAs present riskThis is what we think about unfortunately withWith non-consensual contentWith disinformationAnd something that we need to be thinking about holistically as capabilityAnd contentMore of what I'm thinking about is what does it mean to actualize risk into harmAnd I want to move the conversationTo the caseReleaseClosed not closedOpenInto access versus barriers to accessSo while you might haveAn open weight modelAndYes you can remove those safeguardsSome work byLed by doctor Peter HendersonAnd this is from a Stanford policy briefHe's now at Princeton University nowShows thatWith fine tuning APIYou can do the same thing that you canWith open weight modelSoHosting closed weight models is not inherentlySafeEarths其實是很便宜的,以解決安全問題而開放的模式不一定是更容易接觸人們只有非常有機構思維的模式可能不容易接觸人們沒有機構思維的資料沒有科技技術我還有一些例子這不是用CHAT GPT來選擇但它有這麼廣泛的接觸因為它有很簡單的應用面積人們可能沒有科技技術而能夠發出不公平的資料可能他們不可能用自己的資料來接觸人們或者用API來搜集資料就像我以前在2019年的時候在OpenAI工作的時候在中國的環境上我們很榮幸地能夠接觸到中國公司的開放模式這是最後一個版本第二版本Hugging Face的開放領域我們剛剛重新重覆了Quentoo的標準非常高非常有趣我們看到了中國公司的開放模式對於開放環境的影響在全球AI安全的環境下我想給大家一些例子在新加坡的IMDA政府正在進行的工作尤其是在合作領域在英國政府的inspect領域我非常高興他們能夠開放這些領域而如 Dr. He所說在Project Moonshot領域中我非常期待在法國政府的工作我非常感激Dr. Varouko的重點關於開放的意義為了發展新興的意義避免那種力量的集中以及我們能夠作為一個全球網絡進一步的進步作為我們的主席我想問一下在研究領域中有什麼樣的感覺在研究領域中為了成為這個討論的一部分坦白說高層面的討論不經常接觸到研究者不擁有的領域並且有很好的意見關於技術上的免費性自從我從設計公司開始我自己也很高興謝謝你們我希望這件事對你們有所提示我期待繼續討論謝謝你,Irene為您的演講帶給我們深入了解開放性的彈性和開放性的彈性fascinate explained in AI…不知道他在說什麼我不能跟你 Cer dio請問現在O fer請問請問Irene我想和技术制度陈教授常常指导政府和企业领导在这方面的讨论罗伯特 我把课程转给你对 我很高兴能在这里与您一起我必须说尤其是因为这一整天我们能感受到我们所有人在这里正在尝试解决这些困难的挑战在全球上影响了我们所以我今天想做的是给我们一种视线看看一个能够形成一个全球的AI管理系统的系统所以我认为我应该开始我认为我们需要做的是有关这些不论是为了什么也许是为了我们需要做的在国内的但是我认为其他人已经谈论了所以我不会太多详细但我觉得我们在这里在这一厅如果有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些有些首先,我們要發展國際標準。我們必須合作,我們必須想辦法做到。這些標準必須是正確的。他們必須是我們所有人都有機會進行的標準。這就是我們必須做的第一點。第二點,我們必須設定國際標準的措施。所以,一方面是有措施,另一方面是有措施,這意味著我們希望所有人都會接受和購入的管制系統。第三點,我們必須在特別方式中合作,以釋放這樣的制度。我會講解這些事情,讓大家有機會做到。我必須說,我先說我第一點的三點。我覺得有很多其他部分的AI管制系統,特別是我們在想AI發展,這非常重要的地方,中國在最近的AI決定中領先。但我今天在關注安全問題。雖然我覺得,在很多方面,安全問題和我們現在的方式,我們對這些問題的解決方式,可以被廣泛化,以解決各種其他問題。所以,我們如何設定標準?我們有些特別的問題,在安全方面。它有點不同於其他領域的標準設定方式,因為,首先,這是一項研究問題,它是一項極端的研究。所以安全標準,或者,抱歉,標準通常,是釋放知識的體系。所以,釋放知識的體系,是一件很重要的事情,這就是傳統標準公司很擅長做的事情。但是,我們今天需要做的,也包括核心研究。而某些標準公司,目前正在建立能力,為了這樣做。但無論如何,這是一個很困難的挑戰。所以,安全基礎,或者,如我們最近聽到的,安全網絡,在某些國家中,可能存在的,是那一處,這似乎會發生的地方。但是,我們需要把這些基礎,或網絡,結合成為更廣泛的網絡,我們必須確保,整個世界,有聲音,進入這些過程中。這也包括,建立一些能力,以確保,所有世界各地,都可以參與,這些不同的過程中。所以,這是我們需要做的一件事。其實,我已經跳到第三個問題了,我想我對這件事,非常有興趣。但是,第一兩點,也是非常重要的。第二,我們需要做的事情,就是,我們需要做的事情,就是,我們需要做的事情,就是,我們需要做的事情,就是,當然 foremost,這件事聽得很清楚。第二,我們也需要溝通,依據國際政府,我們需要照顧,等於 entar Cancer,地域目標,我們必須,在這些過程中,這是第一點,因為,真的不可能所有地方,都需要做到,國際協調。美國,歐洲,中國,當若我們,if we wants to,經 wob,我們顯示일� bald,又可能2,如果我們也,如果我們赎信,然後我們才可以,得出叫D général Everyaisia its own,Soft want,就是可以,不同的国家,让不同的文化空间可以选择如何改变自己的管理方式。所以,这可能不是我们需要有完全的国际化的标准,但是其他领域,例如我们今天集中在安全问题上的一些问题,我们可能需要考虑一些国际化的标准。但是我们需要在哪些特定的领域上合作?我们需要先考虑这些领域的标准。然后我们需要考虑风险,正如你今天也听到的,我们今天的风险不一样,所以我们需要考虑这些领域。我们需要在各个国家的领域上合作,例如安全基础和安全网络,civil society,academia,和企业,在这些领域中,我们需要在最后的一个程序上进行保证。我们需要在重复这些领域中,在更多的领域中,在更多的领域中,在更多的领域中,在更多的领域中,在更多的地方上进行公开 诊所成交,和进行一个实验的法官训诀。那些领域所在的香港的所有领域,是在國際層面上設定優惠方式如果我們考慮其他企業的模式我們會想到國際層面上的管理能力和國際層面上和國際層面上的管理能力如果我們考慮這三個模式我今天提到的FATF, ICAO和國際層面上的管理能力這三個以及其他在國際層面上的有幾個共同性如果我們例如ICAOICAO並沒有調查每個公司的管理能力去理解或制定任何任何的管理它不看中國的空調不看美國的空調也不看英國的空調它只會幫助一、發展所有國家的全球的管理程序二、它也會看所有的管理程序確保他們有正確的管理程序而有時候這些公司也會看如果有正確的管理程序的記錄如何讓國際層面看出他們有正確的管理程序那麼我們可以再次轉移到國際層面上例如在國際空調系統中有些國家會說你不能進入我們的空調系統除非或是飛機來自某個地區不能進入空調系統除非它們符合國際層面的管理程序所以這是一個一個國家可以自己決定的但是當一個國家特別是很多國家決定這樣做這就給了國際層面一個真正的優勢可以符合國際層面的管理程序所以我們可以想像類似的事情也會發生例如AI例如你能想像國家會說我們不會輸入任何科技的供應鏈包括AI來自國際層面不符合國際層面的管理程序我們可以討論很多其他方式提供一些這些優勢但是這些就是我認為我們必須討論的各種東西所以我們也必須討論其他東西最重要的就是最後一點在這張圖片上共同認識某些管理結果這是我們在很多其他業界都有的例如如果飛機在中國建造它會通過管理程序或者如果飛機在美國建造它會通過管理程序結果飛機會通過管理程序與其他國家共同認識而整個管理程序不需要在其他國家參與它們在確保同樣的證據在其他國家都可以發生所以這也是我們在其他領域做的事情有些特殊的挑戰在AI的領域但是這是我們需要做的類似的事情最後一個重點是合作不同種類的合作幫助令國際制度非常活躍所以我們可以想到一個國際報告制度對AI和其他領域而我們也想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想想它能够增加多方面的管理,并增加时间的增长。现在人们讨论的一个想法是大型模型的报告制度。美国商务部提出,美国 cloud 供应商必须向美国政府报告大型模型在他们的系统上进行训练。如你所见,这并不是在某些区域上很流行的,甚至在世界任何地方都不流行。特别不流行的是美国 cloud 供应商,因为他们知道他们的客户不喜欢它。如果有一个法国客户在美国 cloud 供应商上进行训练,那位法国客户不想向美国政府报告他们所做的事情。从某些区域上来说,这给予更多的国际制度的推动。我认为,一件有趣的事情是,这样的政策能够在时间内生长,能够在时间内扩展更多的权力。再次,以金融为例,金融中,银行是中心,他们在政府和客户之间作为制度的位置。所以,如果客户提起红旗,那么,金融资源,对不起,不是金融资源,前面的,金融资源,可能会被拒绝,直到那位客户提起红旗。这是一个很笨的方式,但是,这就是我们在某些时刻可以想到的那种事情,金融资源和制度签证的通过在非常相似的方式中必须实施。所以,如果我们现在开始这样的签证制度,这些其他能力可以在时间内扩展起来。对,所以,我想我先说完,我认为我们需要的是一个系统。我们还没有讨论全面的发展,以及网络企业和金融资源在美国,这很重要的发展。网络资源,这些公司,在对国家的聘请中,将评估国家的产业方案,以确保他们的AI策略正确实行。所以,这些这种事情,在世界各地的国家都迎来的,对我来说,非常有用。我们也需要谈论这种各种权力建立。但,我可能只要说,从安全观点来看,这三个领域,这三个标准,我认为这是我们前面的挑战的一种方法我们必须考虑创建技术这些技术既是极端技术也在他们变成国际化时是一个合法的过程包括全世界的广泛声音然后我们也必须考虑在国际层面上设定利益我们谈论了一些可能发生的方式我们也必须考虑我们可以用不同的步骤来引起政策并确保我们的报告能够让我们知道当有人做了什么他们不应该做的事所以再次感谢你们我非常高兴能在这里谢谢你们",
    "谢谢你 罗伯特您很明显地描述了在AI安全方面的国际标准和报告制度上的具体步骤我非常期待稍后在评论室进行谈论接下来我们有Duncan KaspegsMr. Kaspegs是国际AI风险设计的总统在国际管理创新设计中专注于发展新的管理解决方案以解决目前和未来的AI关系的国际问题Mr. Kaspegs有超过25年的经验工作在国际和国际公共政策问题上最早期是OECD的总统决定的领导之前Mr. Kaspegs在加拿大政府的各种职位中工作了Duncan很高兴能在这里与你一起请坐我们今天来谈谈国际AI风险设计计划推广国际协调以确保有利安全和兼顾人工智能我们先来谈谈我们所看到的挑战作为一个详细的解释我认为非常重要正如很多人所提到的我们要明白人工智能我们现在需要发展的管理方案并不是我们今天看到的AI我们可以看到未来的非常非常不同的AI而且我们需要虽然AI已经已经受到所有的热烈和兴奋的关注但是我们仍然在低估它关于发展速度能力能力能力能力和影响的规则让我跳下来不错要考虑我们看到的是一个可能性的逐步的曲线在我们达到人工智能的相当高的能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力能力是在互動的世界上要解決AI的全球性挑戰現在,很多AI所提出的管理和政策問題都可以和應該在國際上解決我們可以從這方面研究和學習但有些問題實在是全球性的要求全球性的互動我們在報告中集中了三個主題我會在總結中是要實現AI的全球性利益現在,公司行業和公司企業會在他們自己沒有政府的操作或管理但有些問題我們要求政府的互動,尤其是國際合作尤其是關於公共商品尤其是關於公共利益的共同化這些問題會影響全球AI的全球性利益所以這些問題可能會影響全球性利益他們會跨境界這些問題你無法保護自己的自己的民族你只能做在家保護自己的自己的民族你必須與其他人合作,建立機構在AI上我們要處理很多問題在國際層面上會影響我們從個人到國際層面從國際層面到國際層面適合挑戰最嚴重的最嚴重的是在你右邊的特別是超級智慧或偵測或偵測這兩個危險偵測或偵測是最嚴重的而雖然它們似乎是不可思議的但我們聽過很多專家的說話我們必須為它們準備我們必須認真處理它們我不會我會跳過那些危險的原因我會跳過那些危險的原因因為我們要定義未來的動機影響到人類所以這就是我們剛談過的酷通利潤和顧及世界風險更多的就是我們要對自然智慧什麼樣的意圖何時何情況有可能比人更有能力這些是需要思考的問題以及共同的決定以及有權的決定這些決定是不得不做的只是一個小群人在某些智能公司中做的決定如果這些是我們會遇到的挑戰那什麼是解決策略我們如何一起解決這些問題我們已經看到了很多非常印象深刻的和可靠的努力在智能公司的全球和國際協議中但這些努力仍然不夠準備因為我們不只是在思考今天的智能公司的影響而是在未來的五年或可能三年或兩年內的智能系統而這是我們需要的預期方式我們可以準備不只是我們希望的未來而是我們可以遇到的最困難的狀況我們認為我們需要新的規範和新的機制為國際協議讓共同的行動尤其是在最緊張的問題上我們需要透過這一點能夠將必要的權益廣闊的代表性包括我們需要的共同討論並且要在需要的時機進行這件事所以我們正在提出的是世界智能挑戰的規範議論如你所知規範議論是一個十分複雜的和可靠的方式它讓我們尋求極速的世界協議在高層層的目標和理念上我們所有人都關心什麼呢我們最關心什麼呢我們最想要的什麼就是將來的工業智能發展但是它也將會利用關於具體的方案進行特定的問題某些最艱難的問題所以這是這些形式這些是高層目標一些原則的例子然後我們有三大桶就像我之前說的包括利益、牢牢以及共同的决定然后我们可以想象有许多的计划在哪里真正的详细都已经研发出来所以我现在要专注的是公共安全计划从人工智能的全球公共安全和安全风险像刚才所说的我们在专注武器化和控制风险的缺陷不仅仅是这些风险但也仅仅是有可能的極端压力以及时间线的高不确定所以如果我们真的不知道我们能够遇到这些风险这就意味着他们是紧急的这意味着我们可能会遇到它们很快我们必须准备至少有机会能够遇到它们很快所以这就是为了这个原因这就是为了这个原因为了这个原因为了这个原因为了这个原因为了这个原因为了这个原因为了这个原因为了这个原因为了这个原因这是一个机制这就是一个机制让人能够先与强大的AI力量先与强大的AI力量先与然后再去全球可以与它们的设定与其他模式与其他模式有很多不同的与其他模式当然也是一个我们可以尽可能与其他计算的计算计算计算同时也可能与其他计算计算同时也可能所以这计算的目的是一个很简单的是一个很简单的让我们来引起AI风险和全球公共安全以確保人類的安全從AI對我們所擔負的可能性所以在這個計劃的核心是一套基於危險的方式這根據基本理解不所有AI系統都一樣不所有AI系統都擔負同等級的危險我們希望能夠擁有我們能夠擁有最多的AI不需要擁有太多的操控我們希望能夠擁有最少的操控但仍然能夠完成任務如果我開始說第一項這些會是AI系統我們會測試的會擁有不可抵抗的危險擁有不可抵抗的全球危險這樣說吧這些系統可以在國際上操控因為它們沒有對其他國家的跨境危險有很大的影響第二項就是我們會稱為AI系統因為它們有很大的影響因為它們有很大的影響它們有很大的影響它們有很大的影響我們不能讓它們不被制定它們需要被制定或可能被管理在各國各地我們需要把它在各國各地進行一個結合讓每個國家履行一個標準我想一些羅伯特說過的部分我想一些羅伯特說過的部分在標準設定上是很重要的在標準設定上接下來的項目就是太危險的系統太危險的系統讓公司或政府讓公司或政府使用自己的系統使用自己的系統使用自己的系統即使它們被制定即使它們被制定即使它們被制定這些系統對人類有很大危險對人類有很大危險我們只會我們只會感到舒適感到舒適發展和測試感到舒適發展和測試這些系統如果它們在一個共同的空間在一個共同的空間在一個共同的空間我們可以一起處理它們的發展而且或者我們可以合作包含包括我們可以合作透過我們可以透過相關的技術透過能夠能夠進行能夠低価高価低価高価低価高価低価低価低価低価低価低価低価低価低価但是我們必須確保沒有任何人在世界製造這些系統,直到我們都知道它們會安全。現在,有很多組織,這個系統的經濟系統,等等,可能需要這個系統的支持。非常短暫地,我們有一個協議,為這些問題做出政治決定,有一個任務,實際上是科學研究的任務,一個公司設定標準,進行監視和保護,一間研究室,如我剛才提到的,以及某種承諾。現在,我想先關注一些這方面的重要障礙。當然,我所提出的不是今天能夠接受的東西。我們正在考慮未來需要的領域,即使世界人民和世界政府突然有一個醒醒的時刻,突然說,我們面對AI,我們從未見過的,我們需要如何實現這方面的領域呢?當然,這是非常困難的問題。所以,這就是為何這會非常困難,為何國際協議這方面無法成功。我每天都聽到這些問題,這些都非常重要的問題。但同時,我們也可能發現,我們需要遵守這些問題,以確保我們自己的健康和生命,以及透過這次轉變,我們能夠成為很能幹的AI。因此,我們需要的東西,就是前進的方法。如何,除了這些挑戰之外,它能否直接成功呢?我已經列出了幾個,但我們將要一起發掘的更多更多的東西。我們必須用無預期的工夫和合作去達成這項目標,但我相信我們能成功。所以,總結來說,現在我們必須準備一個非常能幹的AI的可能性。國際協議很可能會是重要的。這將需要無預期的工夫。這個挑戰非常大,我們都需要它。以上,謝謝大家。如果你們有興趣的話,這裡是我們的討論文件,它是用來引起討論的。我們非常歡迎您的評論,在這裡,並且通過電郵,我們將會在電郵上寫下您的意見,並且在文件中。以上,謝謝大家。謝謝你,Duncan。請繼續上台,我們準備進行國際協議的討論,關於海外智能安全。我今天主席的主席是羅伯特教授,羅伯特教授,宗教授,和艾琳教授。請歡迎我們上台討論。這次的討論,我們也有新的主席。我想邀請馬克.希恆來上台,讓我介紹一下他。馬克.希恆是國際協議的加拿大領導人,他的研究包括中國的AI環境系統和全球技術趨勢。他的寫作出在《外交事務》《布林伯格》《VICE》和《WIRED》之類的文章中。我們很高興邀請馬克.希恆和其他主席上台討論。請坐下。大家好,歡迎大家來到我們今天的主席室。馬克.希恆這是您今天第一次上台。讓我先與您開始。我們聽過很多我們的主席關於AI安全的國際協議的重要性。但在您的《外交政策》與Tino的文章中,您提到AI贏得了AI競爭,您提到AI發展的競爭的競爭方面。我提到在華盛頓有一個常見的問題,那就是誰贏得了美國-中國AI競爭。以這場對於AI競爭的國際協議的重要性,您可以解釋一下為什麼AI安全的國際協議是重要的以及如何我們可以平衡這些共同的競爭和競爭的雙重規則?好的,非常感謝跟Corncordia一起討論。我想從這個角度看來,AI系統非常強壯,是否有可能美國和中國能夠在任何程度上合作,還是國際競爭太深刻了?這些競爭動機太強壯了嗎?兩國都看AI為未來的國際權力的首領,他們可能是對的,他們可能是錯的,但當您看到這是未來的國際權力的重點,以及您看到其他國家在任何方式中我認為行動會非常困難,我認為給我最大的信心或我看到的方式是我喜歡認為的是相互安全與相互安全我認為我們在討論很多你知道,有一天,領導人們會坐下在這個非常高層,他們會有協議,然後我們會在兩邊的經濟系統上採取協議,我認為我不是必須在這方面投放我的希望,我們可以有一個更高層的方式建立安全建立安全系統,在中國會有政策人員,會有技術人員,會有研究員,會有在中國為自己的原因推動AI安全,他們會在美國與國際上的人們交流最好的訓練,但我們會在同時在自己的內容中做這件事,我們不會在每個步驟都同意,我們會在各種政策和技術之間一起做評論,或許在幾年內建立這種安全相互相互我們會建立一個系統在那種高層的協議是可能的,因為兩國已經很接近這個,兩國都對AI安全為自己的原因很投入,然後我們會通過兩國的討論科學交流謝謝我只是想繼續這個主題和挑戰的主題稍微轉到IreneIrene在你的演講中你提到重要的在科學上AI發展和AI安全的重要角色最近我們看到公司像AI限制API以及在科學上發展和公共和開放的未來如何看待國際AI模式的分享和合作和開放在這些geopolitical壓力上的面子謝謝你給我一個輕鬆的解說就是我們所所謂的外交非常嘉賓在對中國引發的疑問我想很希望回到剛剛說到的所有突發的議論我對中國主要討論是中國的先理論中國政府的思索不仅仅是能够理解能力方面的问题,还有更多要说的东西,例如如何建立能力测试,我们讨论了什么意味着安全,但是也许能够一起建立测试,看看能力表现如何,在多个不同语言中,如何和安全相衡。然后还有商业方面的问题,我专门在研究方面讨论,但我不能与其他公司的商业决定,这些问题可能会不同,根据法律和法律。谢谢Irene,",
    "这不是一个简单的问题,你做了一个很棒的工作。我们现在来调整一下,讨论AI管理的国际机构。宋教授,作为一名AI的高级领导人,您有独特的意见,关于在宇宙中的职业角色。您能否分享一下,宇宙的职业角色的观点,关于建立国际AI管理,具体如何,如何我们能够平衡宇宙国际机构的广泛承诺的需要,与它经常遭到的挑战,关于它的过程的速度,以及如何确保国际机构的职业角色,基本上,都能够兼容,以及有足够的智能,以保持我们今天讨论的讨论的速度的AI发展速度。谢谢。谢谢,",
    "我觉得这是我们必须遭遇的重要问题。我想,在宇宙领导人之前,在AI上,已经发生过许多的宇宙试验,例如OECD,欧洲试验,以及在AI上的全球协调,这也是一个宇宙联络,只有40个国家联络在一起,而其余的160个国家被世界遭遇了。所以,我认为,宇宙联络的职责,并不是要创造国际AI管制的组织,而是,它应该是一个极端的组织,以维建一个国际AI管制的联络。这就是让所有的地区联络,都能够在地区上做自己的行动,并找到失去的地方,并将所有的联络都连接在一起。所以,我看到这个原因,我觉得,我觉得,呃,呃,呃,为了宇宙联络的计划,几天前说过,宇宙联络应该扮演国际AI管制的首角,我对这方面的理解,是真的要扮演国际联络的责任,以令所有人变成别人而不是人权。另外,我看到的是,那些原来的网络,在今年和接下来,他们正在尝试阻止美国做的事。他们想取代美国做的事。我不是说美国应该做的事。我只是说,当我们看了所有其他网络的观察,当我们看到他们的行为,他们就不会对全球的信任感兴趣。所以,我认为美国仍然是最有信任感的平台,可以将所有人都联系。我认为中国已经参与了一些原来的网络。例如,在AI安全会议上,以及在军事AI的重新目标上。我认为,我认为,我认为中国应该参与这些网络。但在另一方面,我们看到了所有地区网络的限制,他们有自己的重点,他们有限制,他们没有能力将所有人都联系。这是他们的职责。那么,在这种情况下,我认为,不要离开任何一个国家。你必须要离开任何一个国家。因此,使用美国网络,你将所有地区网络都联系在一起,连接在一起,让它们联系。而在另一方面,对于那些地区网络,请不要离开美国网络。因为,因为美国在联系网络,就不会缺乏你们。所以,我认为,最健康的方法是,是专业联系。使用所有区域的控制委员,与美国联系联系在一起,这样,你可以联系网络。美国网络不是那么大颗钱,他們也沒有那麼多人去做這項工作,所以他們很強烈依賴美國政府去做這項工作。所以,也許他們沒有做出很好的工作,在他們面對的許多問題之中。而在另一邊,我們沒有其他選擇比他們更加有信心。所以,他們目前並不是最好的,請幫助他們成為最好的,請用這個平台解決問題。如果美國總統不做得很好,美國總統總統會用一些問題來提供決定,並問美國總統一些問題,並問他們做些什麼。例如現在的美國總統總統的決定,中國總統的決定,是一共有140個國家軍事組織的決定,美國總統會一起做決定,請美國總統提供一個報告,關於現時的低-和中-薪金國家的基礎問題,關於人工智能,以及找出解決問題的解決方案,為這些全球國家的問題,並提供一個報告,在下個年份。所以,我認為這些是所有的國家代表的要求,他們都已經簽署了這個決定。所以,我認為這是一種方法,為所有國家的問題,並且使用美國總統總統作為一個最有益處的平台,以做所有國家需要的工作。我可以提出一點嗎,我想要強調一下,即使沒有支付器,美國總統總統作為一個通訊網絡,也能幫助我們與其他人相似。我曾經在美國總統曾經在前一段時間,在美國總統總統的工作中,與一個在這裡工作的人,發生了我的第一次合作。我覺得這是非常重要的。確實是,謝謝你,教授鄭教授和Irene教授。我覺得教授鄭教授的說法,關於網絡的方式,很適合羅伯特的概念,就是你提到的國際社會系統,在你的演講中提到的國際社會系統。羅伯特,我也想聽聽一下,教授鄭教授的看法。在你的演講中,你提到一個國際標準和報告制度,對AI安全。你能否解釋一下,這對國際社會系統的進展,以及你提到的國際社會制度,以及國際社會的角色?當然。謝謝。謝謝。是否有問題?好的,這就好了。是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是吧?好的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,是的,例如在发展方面,我认为在讨论中,我提到的ITU和UNESCO,还有其他国家的领域,他们已经在计划设置的领域中做了一些事情,他们认为他们需要开始建立更多的研究能力,继续进行我们需要做的计划设置,并且在发展业务中做事,这非常重要。我认为我们需要,以一个例子来说,在发展中我们需要做更多的事情,我认为UNESCO是一个很自然的地方。你可能会想起一个负责 digitization的领域。许多社区中心的主要阻碍,在进步AI中参与的社区中,其实是在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,在进步AI中,我們必須要很小心地去做,但我想我們可以看到世界的努力為了改善發展的結果。其他一些我認為已經提及過的事情,例如AI的IPCC,在聯合國的一些標準設定項目,我相信我們可以找到一些很自然適合聯合國的項目,例如聯合國和諾貼議題。所以我們可以找到這些項目,這應該是聯合國與生態系統的重要性。我可以再加一點關於聯合國各個機構的問題嗎?現在你所看到的就是像是羅伯特所說的那樣,聯合國各個機構的問題,他們已經在聯合國各個機構上做了一些相關的AI的工作。我們在幾個月前在吉尼瓦州舉辦了一個網絡會議,我們參觀了幾個不同的聯合國機構在吉尼瓦州,是聯合國國務卿總統的團隊所舉辦的。我們看到的就是,在這些不同的聯合國機構之中,雖然他們看到了他們的樹木,但他們看到了他們的樹木,但在某些情況下,他們看見了樹木,但他們錯過了樹木,錯過了他們的視線。所以在這個情況下,我想說的就是,在聯合國國際健康發展的AI上,有很多事情是由聯合國國務卿所做的,但在健康發展的目的上,它必須在社會上解決,不僅是在科技上解決。聯合國國務卿是關於科技的,但在整個目的上,你必須要與社會上相關的不同機構討論,以及很多其他。所以聯合國國務卿,他們必須要在現在相比現在的情況下,在更好的方式上,進行更好的協調,甚至是自己的方式。因此,會是聯合國國務卿的任務,為聯合國國務卿幫助,以提高更好的結構,讓所有人都能在聯合國國務卿制度上,能夠操控所有人。而且,他們必須要在聯合國國務卿中,應用一些東西,在聯合國國務卿的工作上,去解決所有問題。而也,接著,聯合國立民主黨當中的協調,是另一個例子的。聯合國國務卿來到聯合國政府中,它想問問非非為了聯合國民主黨,而還有一些美國未來的協調組織,但是非非為了聯合國民主黨,MEMBER STATES HAS TO ASK THE UN GENERAL ASSEMBLY FOR THE POTENTIAL SOLUTIONS BY THE SECRETARY GENERAL TO HELP TO ORGANIZE THESE DIFFERENT INTERNATIONAL CERNITIZATION ORGANIZATIONS TO COME TOGETHER AND THEN TO TALK TO THESE DIFFERENT MEMBER STATES HOW TO UNIFY OR TO HELP TO FIND THE INTERRUPTABILITIES AMONG DIFFERENT INTERNATIONAL CERNITIZATION ORGANIZATIONSNOT ONLY THE CERNITIZATION ORGANIZATION WITHIN THE UN SYSTEMTHANK YOU SO MUCH PROFESSOR TREGGER AND PROFESSOR ZHONGI'M AWARE THAT WE'RE RUNNING SHORT ON TIME SO I WOULD JUST LIKE TO START WRAPPING UP OUR DISCUSSION BY ASKING A FINAL QUESTIONYOU HAVE ALL ARTICULATED VARIOUS VISIONS OF VICTORY OR SUCCESS FOR INTERNATIONAL AI GOVERNANCEAND I WOULD LIKE TO HEAR FROM EACH OF OUR SPEAKERSWHAT IS ONE THING YOU THINK THAT THE INTERNATIONAL COMMUNITY SHOULD DOIN THE NEXT CENTURY6 TO 12 MONTHS TO ACHIEVE THIS VISIONFOR EXAMPLE AT THE UN SUMMIT OF THE FUTURETHE FRANCE AI ACTIVE SUMMIT OR OTHER VENUESAND WE REALLY WANT TO CONCRETIZE THE DISCUSSION HERESO I WOULD REALLY LIKE JUST ONE RECOMMENDATIONPERHAPS DUNCAN AS WE HAVEN'T HEARD FROM YOU YETPERHAPS WE COULD START WITH YOUALRIGHT THANK YOUI THINK MY ONE MESSAGE WOULD BE TO HELP PREPARE FOR THE FUTUREWE NEED TO HELP OUR INSTITUTIONS WHETHER IT BE THE UNITED NATIONSOR THIS WEB OF SUPPORTING NETWORKSTO ACTUALLY THINK NOW ABOUT WHAT MIGHT BE NEEDEDTO HELP HUMANITY SUCCESSFULLY NAVIGATE THROUGHPERHAPS THE GREATEST TRANSFORMATION WE'VE EVER SEENCERTAINLY LIKELY THE MOST POWERFUL TECHNOLOGY WE'VE EVER SEENWE NEED TO BE THINKING NOW AND DESIGNING NOWAND PUTTING IN PLACE THE KINDS OF INSTITUTIONS THAT MAY BE NEEDEDAND WE'RE HAVING TO HELP INSTITUTIONS ESSENTIALLY LOOK FORWARDIN THE CONTEXT OF TREMENDOUS UNCERTAINTYAND I THINK GIVEN THAT WE'RE FACING SO MUCH UNCERTAINTYWE NEED TO BE ANTICIPATORYWE NEED TO BUILD THE KINDS OF MECHANISMSTHAT MIGHT BE NEEDED UNDER CERTAIN SCENARIOSSO THAT THEY'RE READY AHEAD OF TIMEA LOT IS BEING TALKED ABOUT THE IMPORTANCE OF BRINGING TOGETHERSCIENTISTS TO BUILD SHARED UNDERSTANDINGAROUND THE RISKS AND THE CHALLENGESASSOCIATED WITH THE TECHNOLOGYAND THAT'S CRUCIALLY IMPORTANTAND JUST AS IMPORTANTIS BRINGING TOGETHER THE SOCIAL SCIENTISTSAND THE MANY DIFFERENT PERSPECTIVESTO REALLY ROLL UP OUR SLEEVESAND BRING THAT INGENUITY AROUNDWHAT ARE THE KINDS OF GOVERNANCE INSTITUTIONSAND MECHANISMS WE WOULD NEEDTO HELP HUMANITY NAVIGATE THROUGH THIS PERIODSO THAT WE CAN LOOK BACK IN 500 YEARSAND SAY DESPITE IT BEING INCONVENIENTTHAT WE NEEDED TO COLLABORATE ON THESE ISSUESWE DID MANAGE TOTHESE TECHNOLOGIES FORCED US TO FIGURE OUTHOW TO COLLABORATE WELLTHANK YOU, DUNCANPERHAPS IRENEWE COULD HAVE YOU COME UP NEXTWHERE MAYBE YOU'D ALSO LIKE TO DIG INTO THE ROLEA LITTLE BIT MORE OF RESEARCH INSTITUTIONSAND STARTUPS THAT YOU MENTIONEDIN YOUR PRESENTATIONABSOLUTELY, THAT'S EXACTLY WHAT I WAS GOING TO SAYMY SEMI SERIOUS RESPONSE ISA PAUSE SO THAT WE CAN ALL RESTI'M SO TIRED, I KNOW I'M NOT ALONEAND THEN THE VERY SERIOUS ONEIS ECHOING WHAT DUNCAN WAS SAYINGAROUND THE RIGHT EXPERTISESTHE REASON, ONE OF THE BIG REASONS THAT I WORK AT HUGGING FACEIT WAS ONE OF THE FIRST TIMES THAT I SAWONE OF MY HERITAGE LANGUAGES, BANGLABE RESEARCHED IN A DATA SETAND TO ENSURE THAT WE'RE INCLUDINGDIFFERENT GROUPSDIFFERENT SOCIO-TECHNICAL RESEARCHERSAND ALSO ADDRESSING THOSE SYSTEMIC HARMSTHAT MAYBE AREN'T OBVIOUSLY TECHNICALBUT ARE REALLY ROOTED INHISTORY AND CULTUREAND IN DIFFERENT REGIONS OF THE WORLDTHANK YOU, IRENEMATT, PERHAPS WE COULD TURN TO YOU NEXTON THIS QUESTIONSURE, ONE THINGTO SEE IN THE NEXT 6 TO 12 MONTHSI THINK IT WOULD BE THE U.S.AND CHINA CONDUCTING SOMEFORM OF JOINTEVALUATION EXERCISESAROUND PARTICULAR LARGESCALE RISKS, THIS IS GOING TO BE EXTREMELYCOMPLICATED TO FIGURE OUT HOWYOU CAN DO THIS IN A WAY THAT BOTH SIDES FEELSAFE DOINGI THINK MAYBE AT THE GOVERNMENTLEVEL, IT WILL BE MAYBE EVEN TOODIFFICULT, IDEALLY YOU COULD DO IT IN AISAFETY INSTITUTE TO AN AI SAFETY INSTITUTETHAT MIGHT BE TOO HARD IN THIS PERIOD OF TIMEBUT IF NOT HAVING IT AT THE TRACK 2 LEVELHAVING SOME OF THE BEST PEOPLE WORKINGON TECHNICAL EVALUATION IN CHINAAND SOME OF THE BEST PEOPLE WORKING ON TECHNICAL EVALUATIONSAFETY EVALUATION IN THE U.S.IN THE U.S.GET TOGETHER ANDTALK ABOUT THE METHODS THAT THEY'RE USINGTHE PROBLEMS THEY SEE WITH THOSE METHODSAND WHAT THEY WANT TO DO GOING FORWARDTHANK YOU MATT, CERTAINLYI THINK AS WE DISCUSSED IN THIS FORUM AS WELLAI SAFETY TESTING BEING AN INCREDIBLYIMPORTANT AREA AND POTENTIAL AREAFOR INTERNATIONAL COORDINATIONPERHAPS ROBERT WE COULD TURN TO YOUNEXT ON THIS QUESTION ABOUT NEXT6 TO 12 MONTHS WHAT YOU'D LIKE TO SEE HAPPENWELL I THINK ITALKED ABOUT SOME SPECIFIC THINGSIN THE TALK SO MAYBE I'LL JUST USE THISOPPORTUNITY TO RESPOND TO ONE THINGTHAT WAS SAID EARLIER ANDTO SAY SOMETHING GENERAL WHICH IS THATYOU KNOW I DO THINK THATIN THE CONTEXT OF GEOPOLITICALTENSION WE HAVE A LOT OFEXAMPLES OFRIVALS GETTINGTOGETHER AND DOING THINGS WHENTHEY THOUGHT IT WAS IMPORTANTENOUGH TO DO SO EVENTHOUGH I THINK THAT IN MANY CASES YOU WOULDN'TNECESSARILY PREDICT THATIT WAS GOING TO HAPPEN YOU STILLNEED TO TRY TO MAKE ITHAPPEN AND TO FIND THE POSSIBILITIESFOR WHEN IT CAN HAPPENAND FOR INSTANCE IN THIS AREA OFTECHNOLOGY GOVERNANCE PARTICULARLYAI GOVERNANCE YOU KNOW DEMOCRATSAND REPUBLICANS IN THE UNITEDSTATES ARE NOT KNOWN FOR GETTINGALONG THESE DAYS NEVERTHELESSTHEY HAVE BEEN REMARKABLYIN LOCKSTEP INCONGRESSIONAL HEARINGS INPARTICULAR ON GOVERNANCEOF ADVANCED AI SO THAT'SONE EXAMPLE THENONPROLIFERATION TREATY WASANOTHER TIME WHEN TWO POWERSGOT TOGETHER BECAUSE THAT WASREALLY INITIATED BY THE UNITEDSTATES AND THE SOVIET UNION AT THETIME WHEN THEY DID NOT HAVEPARTICULARLY GOOD RELATIONSNEVERTHELESS THEY GOT TOGETHER ANDTHEY MANAGED TO DO IT BECAUSE THEYI THOUGHT IT WAS IMPORTANT SOIT'S NOT THAT YOU WOULD BETON THOSE SORTS OF THINGSHAPPENING BUT YOU HAVE TO LOOKFOR THE OPPORTUNITIES WHEN THEYCAN HAPPENTHANK YOU ROBERT WE WOULD BEHONORED TO CONCLUDE THIS PANELBY REMARKS FROM PROFESSOR ZUNGPERHAPS ON THE FINAL QUESTIONSO WHAT'S NEXT ABOUT THE NEXT12 MONTHS IS I'M GONNA TELL YOUFIRST ABOUT WHAT'S REALLYHAPPENED BEFORE THE YOU KNOW THEFOR THE FOR THE LAST EIGHTMONTHS IS ALREADY THAT IDON'T KNOW REIMINGBEFORE THESE EIGHT MONTHS ISSO I WAS INVITED TO THE ASIANTECHNOLOGY CONFERENCE LASTYEAR FOR SINGAPORE BUT IT WASNOT REIMING WHO INVITED MESO I BLAME REIMING FOR THATAND THEN WE COME TOGETHER SOHE'S FROM THE GOVERNMENT SOI'M FROM A RESEARCHORGANIZATION SO MAYBE I'MI'M NOT ALLOWED TO TALK ABOUTTHOSE THINGS SO I'M GOING TOTALK TO REIMING IN NORMALCASES BUT AND THEN WITHINTHESE EIGHT MONTHS IS WECOME TOGETHER AS A GROUPREIMING TREAT ME VERY WELLWITH VERY GOOD ASIAN FOOD INSINGAPORE AND ALSO TELLING MEABOUT THE PRACTICES INSINGAPORE BUT NOT ONLY ASTOGETHER WITH ALL THEPRESIDENT AND VICE PRESIDENTOF THESE LARGER SCALE AICOMPANIES ALL TOGETHER ANDALSO MANY OF THE FORMERPOLICY MAKERS AND THEPOLITICIANS ALL TOGETHERSO I SEE THE VALUES OFEVERYBODY COMING TOGETHERYOU DON'T UNDERSTAND EACHOTHER YOU SAY THAT WE ARETOTALLY DIFFERENT BUT THENWHEN YOU COME TOGETHER YOUFELT OKAY NOT SO MUCHDIFFERENCE AND THEN YOU HAVETHE SAFETY PROBLEMS YOU HAVETHE NEEDS FOR DEVELOPMENTLET'S SOLVE THE PROBLEMSBECAUSE COMPARED TO LASTYEAR THE RISK WHICH HAS BEENSHOWING HAS BEEN TEN TIMESWHAT YOU SEE IS REALLYRISK AGAIN AGAINHAPPENING AGAIN AGAIN INDIFFERENT COUNTRIES ANDTHEN YOU SEE THE LIMITATIONSOF THE UN SYSTEM IF IHAVE THE MONEY AND ALSO IHAVE THE PEOPLE RIGHT THEREI WOULD GIVE ALL OF THEMTO THE UNITED NATIONS TOCREATE AN INTERNATIONALAGENCY ON AI BY TOMORROWBUT I DON'T HAVE THAT SOFOR THE NEXT OF THE 12MONTHS IS WHAT HAS TO BEDONE IS AT LEAST TOCREATE SOMETHING LIKETHE EUROPEAN TRYINGTO HAVE A EUROPEAN AIOFFICE BUT NOW WE SHOULDHAVE A YOU KNOW UN AIOFFICE A MINIMUM TRYINGTHAT BRING ALL THE UNAGENCIES ALL TOGETHERCOORDINATE THEM IN SUCH ANICE WAY AND AND ALSO TALKTO THESE REGIONAL NETWORKSSUCH AS OECD GLOBALFUNDERSHIP ON AI IEEEAND ACM SO SO I WOULDSAY WE HAVE TO MOVE TOTHAT SPACE SIMPLY BECAUSEWITHIN THE GROUP OF THEUN ADVISORY BODY ON AIWE I THINK WE SEE THEWOODS THE NEEDS OFDOING THIS WITHOUT THATWE WE DON'T SEE ANOPPORTUNITY TO TO LEAVINGNO COUNTRY BEHINDTHANK YOU PROFESSORZHONG LEAVING NO COUNTRYBEHIND THAT'S AN INCREDIBLYINSPIRING VISION TO END ONPROFESSOR BENJIL AT THEPANELISTTALKS ABOUT HOW WE HAVECURRENTLY NO KNOWNMETHOD TO PREVENTEXISTING NORCATASTROPHIC RISKS OFMISUSE OR LOSS OF CONTROLFROM AI SYSTEMS IT'SCLEAR THAT THE PATH FORWARDWILL BE CHALLENGING ANDCOMPLEX BUT FROM THIS FORUMFROM OUR SPEAKERS WE'VEHEARD A SHARED COMMITMENTTO ENSURING THAT AIDEVELOPMENT BENEFITSHUMANITY AS A WHOLE PLEASEJOIN ME IN A ROUND OFAPPLAUSE FOR OUR PANELISTSPANELISTS FEEL FREE TO MAKEYOUR WAY OFF STAGECAN OUR TECHNICIANSPLEASE COME ANDHELP TAKE THESE SEATSOFF STAGE PLEASESORRY DO WE HAVETECHNICIANS HERE TOTAKE THE SEATS OFF STAGESO WE COULD CLOSE OURFORUMOK THANK YOU WELL TOCONCLUDE OUR FORUM WEARE HONORED TO HAVEPROFESSOR JOE BORWINDELIVER OUR CLOSINGREMARKS FOR TODAYPROFESSOR JOE IS THEDIRECTOR AND CHIEFSCIENTIST OF SHANGHAI AILABORATORY AND CHAIRPROFESSOR AT TING HUAUNIVERSITY HE WASFORMERLY SENIOR VICEPRESIDENT OF THE E-COMMERCEGIANT JD.COM WHERE HEWORKED IN MULTIPLEEXECUTIVE POSITIONS AS THEFIRST EMPLOYEE OF THECHINHUA UNIVERSITYHE WAS FORMERLY SENIORVICE PRESIDENT OF THEJDAI RESEARCHPRIOR TO THATPROFESSOR JOE HELDVARIOUS TECHNOLOGYLEADERSHIP ANDEXECUTIVE POSITIONS ATIBM INCLUDING AS THEDIRECTOR OF THE AIFOUNDATIONS LAB ATIBM RESEARCHPROFESSOR JOE HASDECADES OF EXPERIENCEIN AI RESEARCH AND ISRECIPIENT OF THEPRESTIGIOUS WU WEN ZUNAWARD FOR OUTSTANDINGCONTRIBUTIONS INAUTOMATICAL INTELLIGENCEPROFESSOR JOE HASDECADES OF EXPERIENCEIN AI RESEARCH AND ISRECIPIENT OF THEPRESTIGIOUS WU WEN ZUNAWARD FOROUTSTANDINGCONTRIBUTIONS INAUTOMATICAL INTELLIGENCEPROFESSOR JOETHANK YOU SO MUCHFOR BEING HERE TODAYTHE FLOOR IS YOURSTHANK YOU FOR HAVING MEHERE AND THE MOREIMPORTANTLY THANK TOALL OF YOU FOR STAYINGLATEI WAS ASKED ABOUT THEPROFESSION OF THE AIRESEARCH AND THEPROFESSION OF THE AIRESEARCH AND THEPROFESSION OF THE AIRESEARCH AND THEPROFESSION OF THE AIRESEARCH AND THEPROFESSION OF THE AIRESEARCH AND THEPROFESSION OF THE AII WAS ASKED TO GIVE APROFESSION OF THE AII WAS ASKED TO GIVE ACONCLUDING REMARKSMEANING THIS IS THE ENDOF THIS FABULOUS FORUMOF THIS FABULOUS FORUMOF THIS FABULOUS FORUMBUT I'M THINKING THIS ISBUT I'M THINKING THIS ISROT THE BEGINNING OFONGOING CONTENTALDIALOGUEIN THAT SPIRITI'M THINKING TO GIVEIN THE TALKI WAS GIVEN YESTERDAYAS OPEN CEREMONYBECAUSE I WAS TOLDMANY OF YOU WERE NOTTHERE IN PERSONSOSO BEAR MYSWITCHING TO CHINESESWITCHING TO CHINESENOWSO IF YOU DON'T SPEAKCHINESEPLEASE TIME TO PUT YOURTRANSLATOR ONSO THE TOPIC I'M GONNAHAVE TODAY ISI THINK WE NEED SOMETHINGNEWINTEGRATING TECHANDGOVERNANCEWITH THATI CALL IT人工智能45度平衡率I CALL IT人工智能45度平衡率I CALL IT人工智能45度平衡率I CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL ITI CALL IT其次的相关的伦理问题还有人担心是否会挑战就业结构社会系统性风险当然在好莱坞的电影里面也出现了AI失控人人完全丧失自主权的这种极端风险这些风险有些已经在现实中出现更多的是潜在的防范这些风险需要各界的共同努力需要科学社区做出更多贡献去年5月份数百名AI科学家共同签署的Statement of AI Risk也表达了对AI风险的相关担忧并呼吁防御人工智能的系统性风险应该和流行病和核战争等大规模风险一样成为全球共立的优先话题从一个我做技术的角度来看出现这些担忧的根本原因是目前的AI发展是失衡的我们来看目前的发展趋势很少是AI Capability重轴是AI Safety在横轴上在以Transformer为代表的基础模型架构上加以大数据大参数量和大计算量的Scanning Law目前AI Capability正在快速地成纸数据增长与之对比在安全领域我们看我们有什么典型的技术比如红对测试安全标识安全负难与评估评测等等都呈现是离散化碎片化很重要的是Ad Hoc非常的厚实当然最近出现了一些新的技术兼顾了性能和安全性比如监督式微条SFT人类反馈的强化学习ROHFRAFSuper Alignment等等这些方法最主要特点是把人类的偏好传递给大模型也涌现出了比如TRAD GPT-4等令人兴奋的AI系统以及我们上海AI实验室的书生英特尔大模型等等虽然这些技术瞄准的是安全和性能同时提升但在实际上在实际使用中大家往往发现更多是性能优先所以总体上我们在AI模型的安全能力的提升还远远落后于性能这种失衡导致AI的发展是跛脚的所以我们把它叫Crypt AI但是这种不平衡的背后实际上是两者投入上的巨大差异从右边的对比大家能够看出来两者在技术研究上是否体系化人才的密度上商业驱动力方面以及算力的投入度方面对比来看安全方面的投入是远远落后于AI能力的我一直在呼吁要加大对安全的算力的投入我举的例子就是说你AI SystemLike a little child当小的时候你可能滑荡了算力去帮助他吃好 喝好衣服穿好但是在孩子慢慢Grow up的时候You spend more time去跟他你更多的焦虑不知道他是不是吃好喝好的时候他去跟他做各种价值的交流这种价值的交流的投入实际上就是算力的投入但是很不幸的是我们大部分的算力都投入在预训链上很少很少也不必用在安全上所以这种投入的失衡导致了我们现在Clip的AI我们真正需要追求的我一直在讲的是包括从美国到中国我的学术生涯一直在追求的Trustworthy AI也就是右上角这个路线这是我们的新城大海我把这个叫做可信AI如果我们找到兼顾如果我们找到兼顾安全和性能所以我们需要找到AI安全优先但又能保证AI性能的长期的发展的技术体系我个人把这样一种技术思想体系叫做AI45度平衡率AI45 degree lawAI45度平衡率是从长期的角度来看我们要大体上沿着45度安全与性能平衡发展所谓平衡是指短期内可以有上下的波动但长期内不能长期内不能长期低于45度如同我们现在也不能长期高于45度这将阻碍发展与产业应用这个技术思想体系它是强技术驱动全流程优化所谓全流程优化我在23年的一篇Trustworthy AI的中述文章里面在ACM Competence Survey上发表提出是要把全流程从数据的准备模型的训练到部署之后的operation和运营全部从安全的角度来进行优化同时也需要多主体参与我想这是刚才Forum讨论的很多的话题当然也包括敏捷治理实现AI45度平衡从技术角度来讲也许存在很多的路径我们上海AI Lab最近在探索一条以因果为核心的路径我个人把它取名取为可信AI的因果之梯这也是致敬因果推理领域的先驱图里奖者得主Judy Appel可信AI家的因果之梯我们把可信AI家的发展分为三个阶段分别是犯对棋可干预 能反思犯对棋主要是包含了当前最主流最前沿的人类偏好对棋技术像我们前面提到的LHF但是需要注意的是这些安全对棋仅依赖于统计相关性而不是真正的因果关系这样可能会导致错误的推理以及潜在的风险一个经典的例子是巴夫洛夫的狗当狗仅仅记忆铃声和食物的相关性形成条件反射时它可能在任何场合听到铃声都会触发它的行为这里这个行为是分泌堕叶但如果把它想象成这个行为是金融转账医疗决策甚至是军事相关的决定这显然是极其不安全的所以我们需要第二条它叫做可干预它主要是通过对AI系统进行干预探究其因果机制的安全技术比如能在回路机械可解释性也包括我们刚刚提出的对抗演练它可以通过提高可解释性和泛化性来提升安全性同时也提升AI能力能反思在第三层则要求AI系统不仅要追求高效执行任务还能审视自身行为带来的影响和潜在风险同时确保安全和道德的边界不会突破这个阶段的技术包括Value Audited Training记忆价值的训练因果可解释以及反思时推理等目前从业界的技术发展来看AI的安全和性能技术主要停在第一阶段部分在尝试第二阶段要真正实现AI的安全与性能平衡我们必须完成必须完善第二阶段以可信AGI的因果司机而上我们相信可以构建真正的可信AGI实现人工上的安全与作业系统的完美平衡Ultimately最终我们是希望像安全可控的核聚变技术一样为全人类带来清洁丰富的能源我们希望通过深入理解AI的内在机理和因果过程从而安全有效的开发和使用这项革命性的技术也正如可控核聚变对全人类都是共同利益一样我们坚信AI的安全也是全球性的公共福祉需要国际社会的共同努力和合作我们愿与大家一起携手推进AI45度的发展共享AI安全技术加强全球AI安全人才的交流与合作平衡AI安全与人类的投入共同构建开放安全的通用人口智能创新生态和人才发展环境谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "特别感谢今天论坛的各位嘉宾和朋友们今天论坛圆满结束安全AI希望本论坛可以进一步推动前沿AI安全与距离的讨论和行动期待和大家再见谢谢请今天的嘉宾留步我们一起在上台合影谢谢",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家",
    "谢谢大家谢谢大家"
]