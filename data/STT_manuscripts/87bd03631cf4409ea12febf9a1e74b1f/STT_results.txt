尊敬的各位领导,嘉宾和朋友们,大家好,欢迎大家来到今年世界人工智能大会的前沿AI安全与治理论坛,我是谢敏希,安原AI的CEO,安原AI是一家安全与治理领域第三方研究和咨询机构,也是目前该领域全国唯一的社会企业。
各位领导,各位朋友们,各位外国的各位,早上好。欢迎来到COCDI AI的2024世界人工智能大会前沿AI安全与治理项目的世界中心。
我的名字是帕兰谢,我是COCDI AI的CEO,这是一个社会企业集中于AI安全与治理。
本次论坛,我们特别荣幸邀请到上海市的领导,莉琳指导和交流。请允许我为大家介绍,上海市人民政府副秘书长,庄木蒂先生。
去年4月
我国中央政治局会议深刻指出
要重视通融人工智能发展
重视防万风险
同年10月
我国发布全球人工智能治理倡议
重申各国应在AI治理中
加强信息交流
共同做好风险防范
同月我很荣幸受邀参加了
首届全球AI安全峰会
见证了包括中国在内的28个国家和欧盟
共同签署布莱切里宣言
Blessed Recreation
这也是第一份AI安全的国际声明
在此背景下
社会需要加强前AI安全研究
安全评测
安全治理
以及国际合作
这也是今天论坛的四个主题
第一个主题是安全研究
我们很荣幸邀请到国内外AI领域的世界级科学家
图灵奖得主Yosua Bengio
前头发布了第一份先进AI安全国际科学报告
由30个国家
欧盟和联合国提名
的委员会共同参与
对通用型AI的安全风险进行了科学评估
中国工程院高文院士认为
全世界正处于AGI强人物质能的前夜
在一个不确定的状态
需要严加防范
AGI可能会引发的人类生存风险
中国工程院张亚琴院士联合Yosua Bengio
召集了第一届AI安全国际对话
并联合伯克利分校
东宋等领先科学家
在Science主干上发表论文
建议分配三分之一的AI研发资金
到AI安全和伦理等研究方向
我们期待和多位AI安全科研团队
带头人
包括上海AI实验室的邵靖
北京大学的杨耀东
和上海交通大学的张卓胜
讨论前沿研究问题
第二个主题是安全评测
我们很高兴邀请到大模型安全评测的
领军人物
在学术研究方面
上海AI实验室领军科学家乔宇
第一次以人类价值观的角度
对多模态大模型进行了全面评测
天津大学NLP实验室主任熊德义
发表了中文大模型前沿风险评测的
一系列论文
在行业联盟方面
中国信通院人工智能研究所所长
魏凯依托AIA安全距离文娱会
启动了一系列大模型安全评测工作
OpenAI Anthropic
谷歌DeepMind和Raria
成立了前沿模型论坛
执行主任Chris Massero
将分享领先美国企业的安全实践
第三个主题是
安全距离
各国家正在开展对AI安全距离的
积极研判和尝试
我们很高兴邀请到
法国政府人工智能委员会
成员Gail Veracqua
新加坡政府首席AI官何瑞敏
中国政法大学数据法治研究院教授张灵涵
以及伯克利分校
Center for Human-compatible AI主任
Mark Nisberg
分享多元地区视角
同时我们也邀请到
上海教育大学中国法语社会研究院院长
纪威东
和上海教育大学
上海AI实验室
治理研究中心副主任王云春
参与援助讨论
探讨AI立法和上海AI治理经验
第四个主题是国际合作
我们很荣幸邀请到多家国际顶尖智库
包括凯莱基国际和平研究院主席
Mariano Ferratino-Cuella
和研究员Matt Sheehan
清华大学人工智能国际治理研究院院长
薛兰
纽金大学马丁人工智能治理中心主任
Robert Treger
加拿大国际治理创新中心
全球AI安全风险主任Duncan Kaspeks
讨论AI安全的国际治理议题
联合国AI高层顾问机构专家曾毅
将提出AI安全红线
全球领先大模型开源社区Hugging Face
全球政策负责人Iron Solomon
将讨论开源模型对国际治理的影响
最后我们将邀请上海AI实验室主任
首席科学家周博文进行闭幕致辞
展望AI安全的未来
现在我们进入论坛的正式环节
首先有请上海市人民政府副秘书长庄木地
为我们的论坛进行开幕致辞
有请
尊敬的高文院士
尊敬的张雅琴院士
各位来宾
女士们
先生们
朋友们
大家上午好
很高兴和大家一起相聚在2020世界人工智能大会
共同参与前院人工智能安全与治理的论坛
共同探讨人工智能的发展趋势和治理问题
首先我代表上海市人民政府
对本次参加论坛的科学家
企业家以及媒体朋友们表示热烈的欢迎和衷心的感谢
人工智能作为新能科技革命和产业变革的重要驱动力
正深刻的影响着全球经济结构和社会发展
随着技术持续迭代的演进
人工智能的安全和治理也乐意成为全球关注的焦点
中国高度重视人工智能的健康发展
去年10月
习近平主席提出了全球人工智能治理的倡议
系统地阐述了中国关于全球人工智能治理的立场
主张和建议
展现了中国在推动全球人工智能发展和治理方面
积极的态度和务实的行动
去年11月
包括中国
美国在内的
28个国家和俄盟共同签署了
布赖切利人工智能安全宣言
这也是全球第一份
针对人工智能安全的国际性的声明
体现了中国在全球人工智能治理领域的职任和担当
上海作为中国经济城市的中心和科技创新的前沿
在人工智能安全和治理方面
开展了
实践和探索
特别是在全国
率先出台了人工智能的地方性的一部法规
就是上海市
促进人工智能产业发展条例
探索构建体系化的治理框架
统筹人工智能发展与安全
同时
也发布了人工智能标准化体系建设的指导意见
推动上海
在人工智能标准领域的先行先试
努力培育人工智能高水平的上海标准
展望未来
我们将继续在人工智能安全和治理方面发挥引领作用
我们将持续完善政策体系
加强技术研究和人才培养
制定更具操作性
更加完善标准规划和测评体系
我们将坚持包容省政监管
以鼓励创新为原则
探索大模型评测
四点沙盒监管
我们将积极推动自力研究
在健全法规体系
监管体系等方面努力探索
努力形成具有上海特色的监管实践方案
各位来宾
本次论坛
汇聚了世界级的专家学者和业界的领袖
将围绕全员人工智能安全的研究
评测 治理等议题
展开交流讨论
我们相信
通过大家的共同努力
我们一定能够成为
全球人工智能安全和治理问题提供务实方案
和有益借鉴
推动人工智能技术
更好地
博物与人类社会的发展
上海将提供更加开放的平台
更加丰富的场景
更加优良的环境
支持全球人工智能安全和自理领域的研究者的进行深入的探索和实践
最后预作本次大会
许得圆满成功
谢谢大家
谢谢
感谢穆迪秘书长的精彩致辞
请入座
大家好
我叫吴君怡
是安远AI高级项目经理
也是今天论坛的主持人
鉴于今天有多位国际嘉宾
我的主持将用英语进行
各位观众
各位女士们
我的名字是冠义恩
我作为冠义AI高级项目经理的职员
我将是今天的主持人
由于我们有大量的国际主持人
大多数这次的会议会由英语主持
不再多说
我感到很高兴
能够介绍我们的主持人
业务教授
业务教授
业务教授
我身处世界世界
世界最有能力的機械智能 oss
业务教授
但他的作品
中国教授
 Torture
业务教授
业务教授
业务教授
业务教长
业务教授
业务教授
业务教授
 to publish the International Scientific Report on the Safety of Advanced AI,
which I was honored to contribute to as one of the writers.
Professor Bengio will be sharing key findings of this report
and open problems in AI safety with us today.
All right.
Thank you for joining us, Professor.
Yes, we can hear you fine.
Okay, I'm going to start.
Thank you very much for the kind words,
感谢您的工作在这篇文章中
今天我想告诉您
互联网报告
关于互联网的安全
我已经在分享
互联网报告的意味
是互联网的
危险和安全的决策
这主要是专注在
预先的互联网
有很多种类的互联网
所以我们在考虑
主要是互联网的
危险和安全的决策
包括互联网模式
以及其他多模式的模式
我们最近看到的
都吸引了很多人的注意
所以有两部分
首先我会谈论报告
然后最后
我会说几句
关于它的意味
未来的
大型建筑设施
即使报告本身
没有任何建议
报告是
科技的结构
是为了帮助
政策人员的工作
好 报告
叫做
《国际科学报告
安全与进步智能的报告》
我们花了很多时间
找出正确的名称
所以我们得到了这个名称
报告专注于
危险
因为当然
已经有很多
科学研究的工作
在AI的应用和利益上
但是
对于政策人员的关注
他们必须要明白
危险 风险
以及能力
以处理这些风险
例如 规则
我们被任命
在昨天的
《国际AI安全协议协议协议协议协议会》上
在《国际AI安全协议协议协议协议》上
发布了禁止
流言母语 可连时重加入
检测
需要는데요
一个问题
 emphas
由于
领导
谷歌
甚至
带咱们
领导
可以
和
off
 MP3
3米
W mindfulness
从谎言和谎言到预期的谎言
例如工业市场的影响
很多不同的缺乏
还有最大的风险
例如失去控制
或是超级人工智能
而主要的目的
并不是创造新科学
而是总结
综合现代科学
提供的证据
帮助政策者
这群75人在工作
有一个负责任者计划
每个国家
30个领导人
也有一个领导人
也有一个领导人
由美国和美国决定
这是我们的计划
另外
我们也有一个领导人
今天会有16位导演
这群人在实际上是写书的人
这群人在实际上是写书的人
的频道是可以提供的
的频道
我们团队和不同的版本
也会在此联系
我们也有一个
企业顾问
他将在所找到的各种方面的
领导人
本来是26个领导人
所以总共有75位领导人
是有很大的领导人
在几个月内
在几个月内
那么我们在报告中发现什么
好
有些颜色
可能不惊人
但对政策人员来说
在科学社会中
有很多不同意见
关于风险
但我们进入更深入
有些问题有更多意见
有些没有
所以也很重要
不仅说有不同意见
但是也有不同的观点
关于风险
AGI的时间线
会不会发生
会不会有几年
会不会有几年
之类的
然后有关影响的观点
AGI会发生什么
会不会有快速的取消
会不会影响社会
这些都非常重要
报告也谈到风险生产
所以我们看到
什么是现实科学
去尝试解决风险
那些方法是什么
他们的限制是什么
最后的结论
如果你想要一句话
就是
不幸的是
目前没有认识的方法
去防止现实风险
和未来风险
可能会被坏
例如
失用和失控的风险
所以这是一个很大的
召唤
一个很大的红旗
但是
但是
银河是
我们仍然有机会
综合来说
世界能够
更好地
理解这些风险
并更好地
解决它
好
那我们再谈谈
更深入的问题
首先
当然
我们为什么要担心风险
是因为有利益
总体来说
风险可以非常有用
可以使用在
许多优秀的应用中
但是
只有如果我们
正确地管理它
因为有风险
我们认为
风险有三个类型
我们讨论了很多
如何在不同类型中
组织这份报告
所以有
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
风险有限
我们不想失去控制,但这可能会是个错误,这是一种错误,当然这是一种很严重的错误。
然后,我们有系统风险,那是什么?
那就是社会和科技联合的事情。
例如,工作市场的影响。
当我们越做越多的工作,那些工作的价值就会减少。
如果同样的工作可以做到10倍的钱,那些工作的价值就会减少,因为你可以做得更便宜。
那么,那些失去工作的人会发生什么事呢?
现在,这并不是世界上的问题,但它可能会变成这样的问题。
另一个问题,
另外一个系统风险,就是所谓的AI分隔,
即是AI的才能和能力在几个国家中集中。
那是在其他国家中发生的事情。
在几个国家中集中了一些力量。
那是否意味着AI的利益将会集中?
那是否意味着AI将会在某些国家中发展的方向?
但也许不是在全球南部。
另一种集中风险,
是在市场精神化中的集中,
为了训练在这些AI系统中的正常性。
训练的工资在了这样的程度上,
因此我们发现了我们可以训练更多的模特,
他们的技术就越来越回升了。
这就是所谓的层次的费用。
但这也意味着,
这意味着很少人会有资金
来训练这些AI系统的未来世代
这意味着市场的效率
市场的严重感染
在社会和经济上没有好处
另一个系统性的风险
与社会的互动有关
就是环境影响
我刚才说了
训练这些系统的费用增加
是因为我们训练更大的系统
这需要更多的能量
需要的能量增加了
这不能永远持续
很快AI系统的训练费用
将影响全体能量
可能有10%的费用
所以我们必须看到这一切
并考虑一下
当然
AI系统有关其他社会的事情
例如私隐和费用费用费用
都在报告中
好
报告也谈论了
社会风险
我们称之为社会风险
例如
制度需要时间去实施
即使你有法律
也可能需要很多年
去实施制度组
并解决了
社会的问题
当技术改变
可能需要时间去适应
其他风险
有关力量的关系
我们要小心
好
关于技术方法
来减少风险
我会谈论更多
我们谈论很多
但是现在
没有什么很满意的
他们都有限制
我会谈论更多
所以
一个主要的结论是
我们继续增加
AI的能力
我们必须更加投资
不仅为了
让AI更有能力
更有能力
之类的
但也为了
更加了解这些系统
这样我们可以
解决风险
更了解我们的途径
好
现在我们来谈谈
国际的
风险
这些风险
有很多谈论
正如你可能知道
在媒体上
或者在聊天中
或者在社交媒体上
特别是
有一个
很大的分别
就是
人们认为
没有风险
而人们认为风险
是很糟糕的
但是
有趣的是
我们可以看看
为什么
这些分别
会从哪里来
而且
你必须要认为
风险
和未来的事情
有关
而最主要的
风险
最高的
风险
是
不再存在的系统
所以
当然
人们不会有
铁锣球
知道未来的目标
所以
实际上
我们会有
哪种AI
在几年
或几十年内
会有
这种AI
我们知道
风险很明显
AI的能力
继续增长
并且
速度增长
所以
这些不同的观点
意味着
人们不同意
一些
像是
AI在
工作市场上的影响
或AI在
赛博攻击
或是
生物武器攻击
或是
控制失控的影响
但是
我们发现
那些观点的
分别
是
最好
被
人们
认为
AI
会
增长
的
速度
也会增加
人们认为
AI会
变得更有能力
还有
不同的期待
社会会做什么
以处理
那些风险
和
制定
和
协议的
效果
所以
最主要的
解释
是
将来的速度
是
不明显
现在
当我们说
有不明显的东西
像这个
非常重要
将会改变
未来
从
科学家
和
科学家
的
看法
如果
科学家
不同意
未来的速度
科学家
必须
用刀
准备
所有的
情况
比如
AGI
会在三年
或者
三十年
我们必须
准备
所有这些选项
对于
政策
好
现在
我们再
谈谈
科学方法
和
它们的限制
所以
已经有
很多
技术方法
来
测试
和
减弱
科学方法的
风险
所以
测试风险
是一件事
那么
是否有问题
科学方法
能够
做出
危险的事情
例如
这是一个
的问题
我们可以用它
减弱风险
因为
如果我们证明
有危险的事情
不停地
减弱风险
或者
继续训练
减弱风险
是一个
不一样的事情
我们如何
改变
科学方法
来
防止
它
做
坏事
例如
被
迫害
人
之类
而
重要的是
我们
设立
这两个
的
规则
因为
制度
是
要
使用
最好的方法
既是
测试
又是
减弱风险
现在
不幸的是
目前的方法
既是
测试
又是
减弱风险
有限制
而
制度
理解这些限制
是重要的
例如
我们不明白
如何
现时的
智能设计
新的
线路
如何
决定
人类的
设计
这些
都
是
设计
的
设计
但
当它们设计后
就像
你
有
孩子
可能
我们明白人类生长的生命
的
生命
但
我们
不明白
为什么
它们做什么
这就像
这里
这是问题
因为
当
智能设计
是一个很重要的问题
另外
现在的
安全保险
是很容易去除的
例如
紧缓
尤其
如果你可以做
好调整
所以
如果
智能的重量
是有限的
那
非常容易
去除
所有安全保险
好
那么
在测试
现在的方式
为什么
不太好
是我们
基本上
问了很多不同的问题
看看
所以我们只是在不同的情况下测试
但是这些是组织检查
我们无法检查每个可能的问题
这是有用的
因为如果我们发现了某些东西
那我们就知道有问题了
但是没有组织检查的安全保证
如果组织检查没有找到任何东西
那不代表没有任何问题
我们只能说出有问题的事情
如果没有任何问题
那可能还是有问题的
所以我们不应该只看着车子
我认为我们还有几年
政府有很多事情可以做
以解决这些风险
我们必须更明白
这些系统是如何运作的
我们必须仔细思考
为什么和如何发展智能
是否会成功
是否会成功
是否会被
通过的方式
能够提供在
争夺攻击者或防守者的方式
来思考智能攻击
这些是使用智能的 variations
可能更能提供攻击者或防守者的方式
那么我们如何
获得智能的经济利益
这是一个我们可以选择的问题
谁会利用它
我们将如何投资研究
这就是我们上个月发布的中心报告
请大家看看
我们将在今年结束之后发布更大的报告
现在我们来谈谈大图
报告讲了很多关于时间线的不确定性
我们看到了不同的标准
在 X 线上的时间线
在 Y 线上的表现
而黑色线是人体表现
我们看到在很多任务上
时间过去
人体表现会更好
所以我们需要学习管理这些风险
正如我们在文章中谈到的
有很多
中国同学
在今年发布的
《管理超级AI风险隐蔽的速度进步在科学上》
那么我们该怎么做
总体来说
我们对这种非常快速的改变
并不太准备
例如
考虑新冠病毒
所以我们必须现在开始
准备准备
我们必须考虑
国际协议
决定风险线
尤其是风险隐蔽的风险
例如控制失控和缩减
我们必须做更多的研究
正如我所说的
要更了解风险隐蔽
能力和其后的影响
并建立计划
如果我们发现风险隐蔽
一个这种线线的风险隐蔽
我们必须立即承诺
如果发生了这种问题
我们会做什么
总体上
我们必须使用
大型风险隐蔽的方式
当大型风险隐蔽的不确定
我们必须使用
大型风险隐蔽的方式
好
一件重要的事情是
我们如何投资
不同的解决方案
在安全方面
我们应该想起
研究项目的项目
研究项目
尝试改善
测试
和
如何使AI更安全
例如
有些研究者
已经研究了
什么叫做
安全的设计AI
我们可以得到
要做的保证
但有些方法
可能是
提供更强的保证
要花更多时间
去发展
有些方法
可能会
比较容易
做
或
可能不太安全
但是
如果AGI发生
在三年内
我们必须在三年内准备
所以我们需要
所有这些项目
都在同时行动
最后
有个问题
是
联合与
发展商
在不同国家
就像在报告中提到的
而且
我们需要
对不同的国家
进行战斗
AI可在
电子兵员
的时候
可以成为
武器
比如
电子兵
或者
其他武器
所以
现在
我们必须
在
国际讨论中
照顾安全
并讨论
如果
会出现
会否
我们可以
 compliance verifiability and we need to develop technology for this thank you
thank you so much professor benjo for sharing your insights and we'll let you
continue on for the rest of your evening it was an honor to have you
our next speaker is professor gao wen who is a member of the chinese academy of engineering
acm fellow and ieee fellow he is the founding director of pungchang laboratory
as well as the boyar chair professor and director at the faculty of information and
engineering sciences at peking university professor gao is currently a deputy to the 14th
national people's congress and he used to be a member of the 10th 11th and 12th cps pcc
national committee the vice president
of the national natural science foundation of china and chairman of the china computer
federation and the chief editor of the chinese journal of computers
professor gao it's an honor to have you here with us i'll hand it over to you
go
ader
or
it's
 Misako
Nero
that's
 dirig
accon
for
netu
yes
ah
 okay
ے
what's
that
yeah
that's
out
fantastic
他就说现在人工智能有太多的问题需要去解决了
如果你要有那功夫
你还是先解决点问题
先别说怎么样保证安全
你先把人工智能本身没有解决的问题解决解决
我们第二位德鲁麦尼欧就比较有意思
他就说人工智能只要是确定的
这件事其实都是可控的
不确定就不行
那么第三位安迪和刚才的恩久是一样的
因为他们和亚琪琳一起写了一些文章
在整理这些报告
就是要让我们一定要纵使
今天我讲的是
其实人工智能的安全性
确实是问题的两个方面
一个
就是说作为技术研究
你必须要把这个技术本身要做到极致
让它有用
当然作为社会学家
你更做的要考虑这样一项技术
它对社会带来的影响到底是什么
那么如果这个影响有负面的
你有什么办法把它控制住
这可能是一个问题的两个方面
我想因为我们这个社会呢
在发展的时候需要所有的人的关注
当然所有人关注不是所有人都做同一件事
所以我们要有很好的分工
今天我们就讲一下这个分工的问题
AI其实我们知道
它确实是很强大
通用人工智能
这个强大了以后呢
我们就要让它向善
就是让它做它应该比较理想的事
做的比较理想的事情
所以我们说AI向善里面呢
最主要的呢
我们要从两个角度
从技术的角度呢
要把人工智能技术本身要做的足够好
今天的人工智能呢确实还有很多问题
所以按现在的这个水平
它还没有办法向善
第二个呢就是从伦理的角度
你必须呢要在伦理道德等方面给它规范好
所以我想这是
AI向善里面比较需要关注的两个方面
那今天的AI技术是不是足够好的呢
刚才说了其实不是
我们现在AI的水平呢
还不够高
作为
因为刚才Benjo也在他的最后的这个slice里面
其实也提到了
就是在他总结之前的slice里面也提到了
作为单向性呢
有一些
AI已经超过人了
有一些还是不行
什么时候几乎所有的性能都超过人的时候
那个就是比较好了
就是可以真正发挥作用
所以我们说今天的这个AI技术呢
我们认为它表现的更多的是一种低水平的智能
什么叫低水平的智能呢
就是死记硬背的智能
就是靠显示知识的记忆和使用
那么来表现出来的智能
真正好一点的智能呢
其实是
中水平的智能是中等的
高水平的智能是最理想的
我们现在其实做高水平的
追求高水平的智能
还有点
那是非常遥远的事
我们要追求追求中水平的智能
所谓追中水平的智能呢
就是用
比较少量的
显示知识
就可以获得的智能
用我们人类的这个学习能力来说
你有非常强的举一反三的能力
而现在的人工智能系统是没有这个能力的
所以我想我们现在呢
可能当前是在低水平智能
某些单线还可以
靠死记硬背
或者是靠数据训练出来的
那么等到了只有少量的样本
就可以训练出智能的智能了
我们大概就到了一个中水平的智能
而且它可以跨越领域
从一个领域可以很容易就
类推到另外一些领域
就像以前搞机器学习
这个类比推理
你能做到
那是中水平的智能
高水平的智能呢
这个我们就可以把它笑一笑
听一听
因为这个高水平的智能相当于说
就像人类里面
也没有多少人能达到的那个智能
你让计算机这个系统去做
那是非常遥远的事情
那么低水平智能里面呢
其实有一个
有点像悖论一样的情况
很多人就说
因为有时候讨论问题
说既然你说现在的智能是低水平的智能
它为什么会有智能涌现
低水平智能是不应该有智能涌现的
其实低水平智能也可以有智能涌现
为什么呢
我们可以换个角度来考虑
我们现在的智能呢
是用数据训练出来的
比如说我们大语言模型
大语言模型呢
是用不同种的语言
一起来训练出来的一个模型
但是我们每个人的母语呢
大概只是一种语言
也就是说
可能我们A熟悉的是这个中文
他所有的学习的熟练的都是中文里面的东西
所以呢
你用中文训练出来的东西
对他来讲呢
他能判断这个东西好与坏
准确不准确
或者是基本上都是他可以掌握的
但是呢
如果这个语料呢
是用西班牙语去训练
当然混合在一起训练了
那西班牙语的它的背景场景里面的东西呢
其实是学中文的人呢
他可能不熟悉的
那所谓涌现呢
就是当你把所有这些语料都放在一起
去训练的时候
他会使得这个使用者突然发现有一些东西
他根本就不知道
他认为这就是令眼睛一亮
其实那个知识对那个行业的人
或对那个语种的人
大概不是什么了不得的东西
但是呢
对于不是母语的人
他就觉得很吃惊
所以我想这个涌现
更多的我们可以用这种角度解释
当然也有深层次
还有更深的解释
我们大家都可以去考虑这些问题
那目前的人工智能呢
我们说不管是在智能水平上
在技术上
在这个形态上
在应用上
甚至在社会属性上面
都已经进展得比较快
特别我们讲到伦理问题
必须要考虑它的社会属性
那么讲到社会属性也必须说
现在人工智能的这个安全
或者人工智能的带来风险
那肯定就是说
一方面是犯罪
一方面其他
肯定是早晚的问题
那么另外整个人工智能的发展
它可能它会影响的这个层面
可以在人的层面
模型的层面
和数据的层面
这三个层面来考虑
当然更棘手的一些问题
就是如果人工智能
对社会产生攻击
那么我们怎么样
防止这种技术被恶用
对社会产生影响
所以最简单的就是说
我们要从伦理和技术两个方面
去着手解决这些问题
那么针对这个问题
其实中国工程院
前些年专门部署了一个
人工智能方面的重大咨询项目
叫做新一代人工智能安全
与自主可控发展研究
这个重大项目里面
课题9是我领了一批专家的
一起做的
那个研究的问题是
强人工智能与内脑计算
技术陷阱安全对策
这个大概是在19年开始研究的
2021年我们把这个东西研究出来
写了一篇文章
发表在中国工程科学上面
右边就是这篇文章的首页
所以大家如果有兴趣
当然这是中文的了
读得了中文的可以看一看
英文是有载要的
但是全文是中文的
那么在那里面
其实我们把人工智能的安全分解
分成三个方面
一个是模型方面
一个是算法和硬件方面
另外一个是自主意识的不可控方面
那么从模型方面
它主要就是我们说
模型本身是不可解释的
这个我想我就不展开了
第二个是算法和硬件方面
它也有不可靠性
因为我们知道软件会有bug
硬件可能里面也会有一些不可靠的地方
这些都可能带来安全的风险
还有一个就是自主意识的不可控性
不可靠性就是失控了
这个失控肯定系统的失控
会带来很多不同的风险
那这些风险都是强人工智能
可能会带来的一些风险
那针对这些风险应该怎么做
其实刚才班军也说了很多
我们要
想法尽量减少降低和减少这些风险的一些技术线和做法
也给了一个很长的清单
当时我们在21年的时候就说
理论方面要完善一些这种基础理论的验证
实现的模型的可解释性
另外对人工智能的价值取向
要想法能够在底层价值上面
对它进行严格控制
那么再来
在应用阶段主要是希望能够有足够的技术支撑
防止人为的造成这些安全问题
当然这些比如像造假 假视频 假图像
这其实都是人为的
要尽量去预防或者是能够检测这方面的一些情况
所以这件事要做呢
很重要的就是一个方面
就是我们必须要开展国际合作研究
没有国际合作研究
其实这方面你很难取得
就是说在全球
因为有一些东西你做的好
别人可能不会做
有些东西别人做的好
你可能不会做
我们通过国际合作
把大家做的好东西都可以通过交流
使得大家对人工智能安全方面
都能够提高到比较高的一个水准
而且在这方面不仅仅要合作
人才培养也是非常关键的
因为以往关于人工智能安全相关的人才
其实是非常稀缺的
当然这几年慢慢有点好
有些好转
但是我们冷爱需要大量的人才
那么在语言模型和数据方面
比较重要的就是
我们要有很好的平台
要有很好的数据
然后去训练
谢谢
去训练和去用这些数据
使得你训练的结果比较理想
在这方面我所在的鹏城实验室
我们大概从2018年开始
用英伟达的卡
扎了一台千卡左右的机器
那时候因为2018年开始
算力比较早了
那时候还是微一百的时代
所以算力没有那么强
那么到了2020年
我们就用华为的生腾910
就做了一台4000块卡的机器
那么差不多1000个P的算力
那么今年年底
我们大概会做一个2万多块卡的机器
大概会有16000P的算力
或者16亿的算力
那有这个算力
我们就可以对模型训练啊
模型训练当中的一些这种经验啊
教训啊
或者模型训练完了一些训练的
模型参数的对社会的赋能等等
我们就可以做一些事
我们把所有我们在机器上训练的模型
自己训练的模型啊
都开源开放出来
然后供社会供研究团体去使用
那么
当然这里大家会说
你要训练模型的时候
我的数据会不会丢失
会不会被别人被不相关的人就直接拿走了
那么我们实验实验开放了
开发了一套技术叫做防水宝技术
防水宝技术呢
其实就是说
数据拥有方
他对数据具有绝对的控制和管理的权利
那么机器在训练的时候
数据它是可用不可见的
而机器呢
机器当然可以见得到数据
就机器上面的操作员其实他是看不着数据的
他只能看到你的这个样
这个样本数据
就是你可以用一个比较小的
但是脱敏的一个数据呢
让操作员先去试模型
一旦要试好了
真的数据进去以后
操作员已经看不到真数据了
除非数据应用者给他这个权利
他可以看得到
包括训练完了的参数
如果要往外走的时候
那么机器也会自动
向这个数据拥有方主动去请求说
有一个参数要往外传送
请你检查这里有没有携带你的数据
等等有这样的一个流程
使得数据可以做到足够的安全
那我们训练了一个系列的模型
包括7B的模型
33B长窗口的模型
和200B的模型
这都是大语言模型了
这语言模型里面
既有中文 英文
还有其他语言的一些参数
那么通过这些呢
我们训练完了以后
把它都开源掉
供大家去使用
那么我们用的最大的200B的模型
是一个104层的网络
这个用4000块卡呢
我们差不多训练了半年多
把它训练出来了
那么在这里呢
我们也摸索了很多经验
性能也是不错的
那后来呢
我们又训练了33B的长窗口模型
那么这个长窗口
现在目前是128K的窗口
那么正在训练192K的窗口
可能很快就会完成了
这些完成以后
我们就把它开放出去
那么我们也有整套的这个模型的
这种开放和使用的这样的一个
这种组织去使用这个东西
所以总结一下呢
人工智能高速发展
其实带来这个安全问题啊
我们必须要重视
当然从做技术的
我们要把人工智能做到
推向前进做得更好
所以呢
这方面呢
只有通过国际合作
才有可能更好的把这个工作做好
我跟大家分享这么多
谢谢大家
谢谢
谢谢
谢谢
谢谢
谢谢
好
Professor Zhang previously served as the president of Baidu, and prior to that, he was a Microsoft executive for 16 years, holding various key positions.
As a world-renowned scientist and entrepreneur, he has made significant contributions through his 550 publications, 62 U.S. patents, and other landmark engineering achievements.
Let's give a warm welcome to Professor Zhang.
早上好,谢谢安远AI邀请我来这个大会。
刚才,Yoshi Banjo和高文院士对整个AI特别大模型的发展,特别是风险,都做了特别好的系统性的介绍,一个是全球,一个是中国。
的确的话,过去这两年左右,AI的发展的速度很快,快的同时也带来很多的这些安全的风险,我过去这两年也花了不少时间和全球领先的学者们一起来从事一方面的一些研究。
今天我简单讲一下,有时间关系,我简单讲一下我的一些思考吧。
首先呢,是一个大模型,发展的一些趋势,以及呢,当然更重要的是风险方面的,安全方面的一些考虑。
首先我认为呢,这个大模型和生生生AI,在未来的这个十年吧,有下面几个趋势。
第一个呢,就是多模态,我们不管是我们的语言,我们的文字,语音,图像和视频。
都正在融合起来,另外的话呢,这个激光雷达,这个三维的结构信息,四维的视红信息,包括我们蛋白质,我们这个细胞,还有基因,都在变成多模态的收入。
那么第二点就是我们叫智能体,自主智能,可以自主的规划任务,可以开发代码,可以自己升级,不单是错,可以去优化。
自己也可以去自我copy。
第三个就是智能的走向边缘,我们现在讲大模型,大多还是在这个云端的这个大模型。
现在呢,就走向我们的PC啊,走向我们的手机啊,走向我们的这些智能的这个设备,走向边缘端。
那么第四个就是,现在讲物理智能,就是具生智能。
我这十年一直叫物理智能,现在新的名词比较时髦,叫具生智能。
就是大模型用到这个无人车,无人机,机器人,物理基础设施,像电网啊,这个电站啊,一些critical infrastructure。
那么最后一个呢,是生物智能,就是像包括现在我们的脑机接口,用到我们的人体,人脑,医疗机器人,生物体和生命体。
嗯,这个我我最近呢,和很多的学者都一直在
探讨这个问题,到底通用人工智能什么时候可以实现啊,我这个表达一下我完全个人的意见,因为刚才亚瑟,白云哲也讲到,我们讨论这个问题的时候,大家有很多不同的这个角度,不同的观点。
我个人认为的话呢,差不多在二十年之内会实现这个通用人工智能。
分三个阶段,就是我一直分成信息智能,物理智能和这个生物智能。
那么信息智能的话呢,五年之内,我认为可以达到这个目标。
达到所谓的这个图灵测试。
当时CHATGPT出来的时候呢,我的第一感觉,我觉得CHATGPT在文字方面,基本上通过了图灵测试。
那在这个视频啊,在别的方面,可能还需要点时间啊,可能在五年之内,我可以达到这个修改的或者新图灵测试。
在物理智能或者巨声智能呢,可能还需要差不多十年的时间。
因为现在,比如说无人车啊,这个人行机械,我们这个会议也看到很多。
这个我自己认为呢,我这么多年一直在做无人车,从当时在百度的阿波罗,那么一直在做无人车,可能八几年的时间了。
我认为无人驾驶呢,是巨声智能一个最大的应用,也是第一个实现这个新图灵测试的这个应用。
明年呢,大家都看到我们在武汉做的这个测试。
明年呢,大家都看到我们在武汉做的这个测试。
明年呢,大家都看到我们在武汉做的这个测试。
大规模的这个实验的商用啊。
我觉得在明年的话呢,我们看到更多的应用。
在2030年的话,之前的话呢,会成为主流的应用。
生物智能可能时间更长一点,可能需要再用差不多十年的时间。
但整体来讲的话呢,在未来的二十年,我认为可以达到这个通用人工智能。
那我所在的清华大学智能产业院。
其实就是为了通用智能而建起来的,我们其实就是在三年半前建起来的。
那么这个研究院的话,目前有二十二名教授,有差不多三百多位学生。
我们的目标很简单,就是能实现信息智能,物理智能,以及生物智能,包括无人驾驶,先进的机器人,也包括呢,这个biological computing。
这些概念。
目前我们也发布了很多模型,我们更多的是垂直模型,比如说我们发布了一个第一个全球的实用的端到端的无人驾驶的开源模型,叫Air Apollo FM,大家都可以看到,在GitHub上面。
我们也发布了一个全球最大的这个Biomag GPT,都是开源的,大家都可以使用。
那么在这个有巨大能力的同时的话呢,
就带来很大的一些风险,我刚才Benjo也讲了,前沿大模型,这个大模型到了这个万一参数是更多的时候呢,就才知道它的风险。
那么我还是分成这个三个不同的世界,信息世界,物理世界,生物世界,信息世界的风险,大家比较容易理解,刚才讲到deepfake,讲到这个hallucination,misalignment,讲到这个misinformation,这个我觉得相对比较容易理解。
那到了这个。
那到了这个。
那到了这个。
那到了这个物理世界呢,这个风险就会更大,你想想看,我们有再过十年,我想我们这个世界的机器人比人要多得多,那机器人的话呢,如果它失控,如果它被坏人所乱用,大家可以想象到对社会带来风险,以后我们的车可能都大部分无人车,这个时候是靠这个大模型去控制,这个时候就带来的风险,不管是主动车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是�
动风险,被动风险都会很大,那么更大的风险的话呢,是这个生物智能,物理智能和信息智能融合在一块儿,这个时候如果失控或者被乱用,会造成了这个生存风险,所以我们觉得我们又过去这几年啊,有几个重要的节点,其中一个节点就是在2023年6月份的,
这个Center for AI Safety,那个Recent Statement on AI Risks,讲到我们要把人工智能未来的风险,把它当作核武器和流行病一样的,这个优先级去看待,后来的话呢,有很多工作,包括刚才那个部长讲到的,我们中国的人工智能全球倡议,也包括EU的AI Act,
也包括了几次,
这个峰会,然后也包括我们一些小范围的会,就是我去年的话呢,我和Store组织了一个叫International Dialogue on AI Safety,那么每三四个月能开一次会,第一次在英国,第二次在北京,下面一次是在这个Venice,我们会开会,两天三天深度的去研究这里面的一些技术问题和政策的这个对应的问题。
刚才呢,亚瑟讲的那个报告,我觉得是把这个很多的讨论的做了高度的一个总结,我也很高兴的深度参与那个报告。
我简单介绍一下呢,这个大模型安全方面的一些技术,因为大模型安全,它确实是一个系统工程,从我们的输入,从我们的输出,从我们的安全评估,治理,特别是这个系统的安全对齐都需要去工作。
有许许多多的这个数学,很多很多的算法方面的研究,有许多工程的问题,技术的问题,也带有很多这个策略的问题,这个我就不细讲,我们做这个安全的话呢,对这张图应该比较熟悉,就从各个方面的系统工程问题。
然后另外的话呢,这里面很重要一点就是最近许许多多的进展,就是大模型安全的对齐。
这里面又有两种不同。
一个呢,是直接监督的微调,就是我把高质量的有用的安全的这个信息,把它直接运用这个监督微调。
那么第二点呢,是根据我们的偏好,人类的偏好,我们的价值观,来做这个reforcement learning,比如说这个CHAT-GBT,GBT系列基本上是采用PPU这种方式。
那这里面有很多种不同的一些选择,不同选择,可以基于这个奖励模型的安全奖励和游泳奖励,用Lagrange去结合的这个作为输入的参数,然后也可以用一些更新的一些奖励的方式。
那么在清华呢,在爱尔华呢,我们有几位老师呢,也做了很多很多工作,那我们的詹先生老师呢,他提出了这个conditional,
reforcement learning,那么这个的话呢,是用于这个大模型的一个微调,比如说我们有很多高质量的数据的情况下,它可以帮助我们更多的去把这个任务自动化,我们知道有手工reforcement learning的话呢,需要很多很多的工作,需要很多数据,这个的话呢,工作已经在,大家可以看到,在GitHub上,叫OpenChat,大家都可以看到。
也现在是比较受欢迎的技术。
那么另外的话呢,就是我们也发现,目前在这个reforcement learning,你们feedback里面呢,有些问题,特别是它的这个样本和策略的学习目标呢,是不匹配的,就是curious policy and misalignment。
所以一开始等于,你认为是aligned,但是走走走走之后,它就偏离这个方向,所以我们也提出一个新的技术。
然后使得它在学习,就我们的goal和trajectory是well-aligned,那么我们应该在下面几个星期,阿Claire会谈到这个工作。
另外的话呢,我们用了不少安全离线的强化学习的方法,然后呢,去把这个安全的策略来进行改进。
特别是,其实呢,如果我们首先,
要判断一个东西是,它是属于安全呢,还是不安全,就要把这个区域要找到。
那么在这个区域里面的话,你可以做最大化的一个奖励,然后如果在区域外面的话呢,你要做最小化的这个风险,一个要maximize,一个要minimize。
那这里面,如果这个,这个,看我们的paper的话呢,这里面都是,都是mathematics,都是数学,所以我就想让大家知道呢,这个安全的问题,对齐的问题,
不仅仅是一个策略和,和简单的一些这个对,一些算法,这里面其实有很多理论方面的一些创新和突破。
这个文章的话呢,我们会在,也是应该这个,应该已经,已经发表了,发表了,阿Claire,然后ICML也有一篇这样的论文。
那么最后时间不多呢,我想谈一些我自己的一些建议,刚才是在技术方面的,
一些工作,我不知道耀东会不会讲,耀东和,和刀宋,他们几位在这方面做的都特别领先的学者,他们以后会讲更多细节。
那我呢,想提一点就是政策方面的一些建议,这个我其实讲了差不多两年了,讲两年了,我这儿有没有个章,我看有没有,
我要盖个章,对,我要盖个章的话呢,就是说我讲的这个建议完全是个人建议,不代表清华大学,不代表清华大学Air,也不代表我们现在,
所有的团体,因为我们在,在这内部有很多不同的观点,完全是个人建议,这个其实我提了差不多两年到三年了,我提了十个建议,我今天实验关系,我讲五个。
第一个的话呢,就是我一直建议,建议,建议我们要建立这个分级体系,因为现在AI里面有很多不同的算法,有很多不同的模型,那我们呢,要对这个就是最前沿的,
就是超过万亿参数以后,
我们更多的参数呢,对它进行约束,一般的模型,一般的算法呢,就不要太去规范它,让它往前面发展,就对这种特别风险比较大,能力比较大的,就前沿的超大型模型,需要去有些规范,因为我做无人驾驶,我们这里面,自动驾驶,我们里面分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,
我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们
从对齐到最后评估
各种评估
都需要有一套标准
严格的标准
第二点
用在场景里面
需要更多的约束
你比如说用到无人车里面
无人车里面的安全
无人车里面
它本身的
它的自己的评估的体系
要拉进来
你做医学里面
比如说医疗基金人
他必须要经过
医学方面的
这个场景和领域的约束
第二的话呢
我讲了很多年了
就是我们需要有一个
实体的映射机制
首先是
对AI的内容要标注
要标注
比如说我现在
产生了很多数字人
数字人和真人
基本上看不出区别
我要标注
我这是AI人
虚拟人
我AI产生内容
我要标注是AI产生的
我们现在的规定
国家规定
美国也全规定了
你比如做个广告
互联网做广告
如果是广告
你也写个广告
但我如果搞一个虚拟人
数字人
我都不需要说
我是AI产生的
我首先就是个简单的
把它标示出来
大家知道这是AI产生的
还是人为产生的
第二的话呢
就是一定要有一个
实体映射的机制
我们以后有很多机器人
有很多可以是
真正的机器人
也可以是虚拟的机器人
有很多智能体
那么这个智能体
它应该是从属体
它从属于我某个人
或者某个机构的
我的机器人犯事了
我最后要追溯到
它的主体里面去
所以ownership一定是人
人或者是一个company
也是一个legal entity
那么这个事情
其实从技术上来讲
并不是很难
是完全可以做到的
但是我这是更多的
一个政策方面的建议
第三个呢
我一直建议
我们把10%的这个投入
就是做AI研究的也好
产品开发也好
投入呢
放到对安全和风险的
这个领域来
我们在全球
我们大家是建议30%
在国内
我说我们先从10%做起
以后慢慢到30%
这个包括我们的
基础研究经费
我们的产品开发经费
包括我们整个这个社会的投入
我们先到10%
作为第一个起点
那第四个就是设立一些
很清晰的这个红线和边界
这个红线边界
其实要设立起来
其实不容易的
因为每个国家
可能这个有不同的情况
但是我觉得有一些
大家可以设立的
我们要设立什么不能做
比如我从很多年
我就提我们做智能体的时候
智能体现在自己可以去
它可以kabi
它自己可以去复制的
那复制的时候
复制的时候要经过人的同意
比如说我是这个主体
我要同意的
你复制一个张亚青
张亚青要去
要同意你去复制
这个不能自我复制
没有限制的复制
然后还有红线边界
比如说大模型接到
核电站的时候
怎么接能不能接
我个人建议在我们在这些
大模型
还没有搞清楚
这个这些
边界啊
没有搞清楚这些里面的
可解释性前面的
先不要接这些
特别关键的critical infrastructure
最后大家很多都讲过了
就我们要一个国际沟通的
合作和协调机制
包括标准
包括评估
包括
这个合作的具体的一些方式
这里面需要有专家的
需要有政策制定者的
需要有政府的
但很重要的需要这个
这些不同领域的人
在一起
在一起
这个经常的合作
好我就讲这么多
这个谢谢大家
Thank you so much Professor Zhang for your excellent presentation.
Thank you for your presentation and suggestions.
Next
I'm now pleased to welcome
Professor Dawn Song
a professor in computer science
at UC Berkeley
and co-director
of the Berkeley Center
on Responsible Decentralized Intelligence
Her research focuses on
AI safety and security
and she is ranked the most highly cited scholar
in computer security
She is the recipient of numerous awards including the MacArthur
Fellowship
Guggenheim Fellowship
and more than 10 test of time awards and best paper awards
Dawn, it's a pleasure to have you here in Shanghai with us
I'll let you take it from here
Great
Thanks everyone for being here
My name is Dawn Song, I'm a professor at UC Berkeley
Today I'll talk about
AI safety challenges and future directions
So the presentations earlier have such great
context and background
And here I wanted to add some more
emphasis
In particular as we deploy machine learning
It's really important to consider the presence of attackers
For a number of reasons
So first
History has shown
that attackers always follows the footsteps of new technology developments
Or sometimes even
leases
And also this time
the stake is even higher with AI
As AI controls a more and more
Systems
Attackers will have higher and higher incentives
To compromise the systems
And also as AI becomes more and more capable
The consequence of misuse by attackers
Will also become more and more severe
And hence
It's really important
To consider the presence of attackers
Especially as we consider
AI safety
So first
I want to talk a little more about the AI safety
In the presence of attackers
From my group's
Early work and also
Other research work
We have shown that
Adversarial attacks
Are prevalent in
Deep learning systems
Essentially
All deep learning systems today
They are all vulnerable
To
Different types of adversarial attacks
And
The number of papers
In this space actually
Has grown exponentially
Since
Our earlier work and the people's earlier work in early stages
And also we had the
The rare honor
Of having some of
The artifacts of our earlier work
Actually now
As part of the permanent collection
At the Science Museum of London
So
As we talk about
Safety and today
Talk about safety-aligned large language models
It's also important to consider
The adversarial setting
So unfortunately
As our work and also others' work have shown that this large language model
Is also really vulnerable
To adversarial attacks
And the safety alignment
Mechanisms are easily
Broken
So in our recent work
As an example
Decoding Trust
Which provides the first comprehensive
Evaluation framework for trustworthiness
Of large language models
It actually won the outstanding paper awards at NeurIPS
This past December
We developed
A new
Algorithm
And also
Different environments
Including benign adversarial environments
To evaluate many different perspectives
For safety and trustworthiness
Of large language models
And our work have shown that
For all these different
Perspectives
Including adversarial robustness
Toxicity
And fairness and many others
Essentially these large language models are all
Very
Easily
Attacked
By
Adversarial attacks
And again for more details you can go look at our paper
At decodingtrust.github.io
And also these adversarial attacks
Are effective
Multi-model
Models as well
And also others' work have shown that
Even
As these models are being fine-tuned
Attackers actually by providing just a few
Very small number of adversarial
Adversarial designs
Data points
This fine-tuned stage
Can essentially cause this fine-tuned model to easily lose
The safety alignment
So far I've talked about
Right
So these attacks
They are not only effective at
At the inference time
They are also effective
At essentially fine-tuned stage as I just mentioned
Essentially this is called data poisoning as well
And also through this data poisoning
Step
Also
These models can have what we call very stealthy
Behavior
Essentially called backdoor
As well
So in our earlier work we showed that through data poisoning
The model can
Attackers can build backdoor
In the model such that for example
In our earlier work in
Facial recognition
The model
Under normal circumstances
Will just behave normally
And give correct
Facial recognition results
But however
When
Anyone that wears a special type of glasses
This actually is even effective
You know in the physical world
Then it will cause the model
To essentially trigger this backdoor
That the model will
Misrecognize this person
Wearing this particular type of glasses
To attack its backdoor
As a targeted person
And
Though recent work
With
By anthropic
They have also shown
This type of backdoor phenomenon
Where a fine-tuned
Large language model
During normal circumstance
With a normal prompt
It can generate
Like normal code
That's usually correct code
But when a particular
Trigger phrase appears in the prompt
The model actually works like this
其實會發生困難的行為
所以這些都是不同類型的敵人襲擊
在整個社區中
我們都非常有成功和創造
在發生不同類型的新的襲擊方式
然而在另一邊
可惜的是
在防衛方面
我們看到非常少的進步
而今天沒有任何有效的全面敵人防衛
所以這顯示了
這是我希望發生的第一項開放挑戰
在AI安全的環境之下
而目前的AI安全安排模式
非常容易被敵人襲擊抵抗
而任何有效的AI安全安排模式
必須對這些敵人襲擊有必要的抵抗
因此
這項展示
是一個非常開放的挑戰
所以基本上
為了達到AI安全
我們必須能夠解決
敵人襲擊的堅強性
而正如我剛才所說
雖然現在每一年
我們每一年都出版了
不同類型的敵人襲擊
但整個社區
實際上
對這些敵人襲擊
已經發生了幾乎無效的進步
所以
為了發展AI安全的效果
作為整個社區
我們必須向前推動
如何發展更多的防禦
讓我們能夠發展AI安全的方式
能夠抵抗敵人襲擊的抵抗性
那麼
有什麼可能的方向
能夠幫助我們達到這個目標
我會給大家介紹幾個例子
一些我們最近的工作
一項工作是
我們稱之為
代表設計工程
這是AI透明度上的
最高階段的方式
所以在這個情況下
我們
透過提供
模型
與
相反的輸出
作為某些任務的支援
我們提供了
相反的輸出
給模型
然後我們檢查
不同層次的
電子網絡的啟動
然後建造模型
而我們最近的工作
我們展示了
透過這個方法
我們能夠
在不同層次上
在不同層次上
確認
模型的不同類型
不同的行為
所以基本上
例如
我們能夠
確認模型的行為
在不同層次上
確認模型的行為
而在不同層次上
確認模型的行為
的確認
或是不確定
或是透過輸出
或是透過不透過輸出
等等
而更多的
我們在工作中
以合作
等等
我們也展示了
特別的
這種方法
叫做
表現控制
所以
不只是我們能夠
做表現
表現讀取
也就是
監視
模型的行為
我们可以改变这些绘图的启动
在某些层次中
随着增长的时间
以及详细的指向
以此方式
我们可以改变这些模型的行为
在某些课程中
例如
使用这种方法
我们可以使模型的行为
更坚定或更坚定
等等
为什么这重要
我觉得这是一个
人脑和人工脑的
重要分别
人工脑和人工脑
人工脑的活动
我们在人工脑中
可以完全观察
人工脑的活动
人工脑的活动
并且在现实时间中
我们可以改变
这些模型
以使用人工脑的活动
所以这其实给我们
产生了一种很强大的
能力
可能
为人工脑安全
所以这能让我们
观察
观察
观察人工脑的行为
并且
更好地
更好地控制
防止
人工脑的行为
所以这能够
带来
很宣传的
方向
为人工脑
提供的
安全
防护
防护
防护系统
防护系统
保护系统
保护系统
但是
这种控制机控制
很证明
但太难
给我们
完全的保证
所以
也有
亚琴教授
亚琴教授
和
亚琪
也有
亚琪
也有
亚琪
亚琪
亚琪
和
亚琪
亚琪
亚琪
亚琪
亚琪
亚琪
因此,我們改變了我們對安全、安全系統的構思。
早前,整個社會集中在反擊防衛方面的研究,
我們如何探索攻擊,
就像我們今天所探索的大語言模式,
當它們不適應的狀態下,
後來,我們探索防衛的方法,
專注於反擊防衛,
並嘗試找出這系統的障礙性。
就像我們今天所探索的研究,
我們嘗試找出大語言模式的障礙性。
然而,這些方法,
我沒有時間去解釋,
因為有很多不滿意的原因。
最後,社會發現了,
最好的方法去達成安全,
就是我們稱之為設計的安全,
或是建築的安全。
以這種方法來看,
基本上,我們可以發展系統,
提供安全的確保,
以達成安全的確保,
透過設計和建築的系統。
而這與之前提到的其他防衛方法相反,
這幫助我們
從雞和貓的遊戲中
解決問題,
並且提供證明。
這個方法使用的方法,
是通過公開的檢查。
通過公開的檢查,
首先,我們提供公開的檢查,
提供要求的特色。
然後,通過公開的檢查,
我們可以公開的檢查,
讓系統,
透過檢查,
確保設計的特色。
這也可以在不同的程度上做到,
包括設計和應用的程度。
而在過去的一世紀中,
社區實際上進入了
我稱之為
模擬檢查系統的範圍,
我們實際上有很多不同的系統,
包括麥克康諾,
和檔案系統,
以及其他,
這些都已經被公開了。
不過,
這系統的問題是,
它非常努力,
以證明系統的安全性。
這問題經常需要10多年的證明,
所以它不足以解決。
我的合作組,
與其他在 OpenAI 合作組,
我們是第一位
使用深入學習來證明系統。
而這項工作,
這項工作在幾個多年來,
在大語言模式和其他項目之前,
已經做到了。
所以今天,
以大語言模式的技術,
我們希望我們能夠
更加深入地進行這項工作,
我們可以,
在過去的訓練中,
例如,
訓練AI代理人去玩Go,
我們可以,
訓練AI代理人
去自動證明系統和檢查系統。
以這項合作,
與計算系統的合作,
我的合作組也做了很多工作,
在過去,
在這個項目當中,
也曾經在大語言模式中。
我們希望,
透過這些合作,
以自動證明系統
來檢查系統和計算系統,
我們可以自動提供
確保系統,
並且以這項合作,
在這項合作中,
提供證明系統。
而以這項合作,
我們可以使用AI,
建立自動證明系統,
基本上,
成功達到自動設計,
或者是自動設計,
而這可以幫助我們
減弱手臂的危險,
以提供自動證明系統,
能夠有信心對於
某種類型的攻擊。
所以,
我認為,
這項合作,
非常有信心,
解決某種類型的問題。
然而,
這項合作,
仍然有很多開放的挑戰。
首先,
這項合作的
正式證明系統,
主要是遵守
傳統的
傳統的
代表性系統,
但它們
難以遵守
非代表性系統的
代表性系統,
例如,
深入智能網絡,
我們甚至沒有
具體的
定義和目標。
因此,
例如,
如果我想確保
自動車
不
駕駛
自動車,
我們甚至沒有
具體的定義
所謂的
自動車。
而且,
未來,
基本上,
所有系統,
最多的系統,
都會是非代表性系統,
他們會
組合
代表性系統
和非代表性系統的
組合。
因此,
具體的證明系統
和
建築系統,
我們如何
應用
這種
非代表性系統的
方法呢?
這仍然是
一個
開放的
問題。
最後,
我們
要
提供
一個
非常重要的
解決方案。
我們
向前
走,
AI的強大能力,
是
非常重要的,
我們
保證
這些系統的安全。
然而,
仍然有很多挑戰。
因此,
我們要
考慮
AI安全
在
任何狀態,
以及
我覺得
它
可以
很
有效的
提供
最重要的
解決方案。
我們希望
以
應用
動作模式
來
建立
重要的
卡牌
,
以及
最後,
我們希望
我們能夠
發展
新的
方案
和
方案
,
以
安全
設計
和
安全
設計
的方案。
謝謝大家的
 bajo
視頻,
我們
將
揪
 Before
我們
將
揪
後
上
後
,
我們
將
揪
後
上
後
。
對
我
來說,
我覺得
她主持了许多研究计划
在测试大型模型安全和价值平衡
我们还有杨耀东教授
他是北京大学中心 AI 安全和管理中心的总统
杨教授研究 AI 平衡学习
以及其他课题
他在上级活动中发表了超过一百篇文章
最后
我们有张卓胜教授
他是上海交通学院的助理教授
他主要的研究目标
包括维护和保护
超级模型模型和兼职人员
他发表了超过五十篇文章
在最高级的课堂和博物馆中
我们的主持人是段亚文
他是 Concordia AI 的技术计划管理人
以及Future of Life Institute 学生
亚文在国际科学研究研究院
并获得了Masters Degree in Machine Learning
让我们把握掌握掌
给我们的观众
先进AI安全国际科学报告
International Scientific Report on the Safety of Advanced AI
其中他提到了通用型的人工智能
可能带来的滥用风险
故障风险以及系统性的风险
同时他也介绍了
当前的一些安全对齐方法的局限性
其实我们今天的第一个原则讨论
聚焦的就是这两个问题
第一个问题其实是
面向前沿大模型的AI安全技术
存在什么样子的挑战
当然还有第二个问题是
面向更强大的未来的
通用人工智能
甚至是全方位的
超越人类的超级智能
安全技术应该怎么做
以及如何避免失控的风险
那首先欢迎四位老师
然后首先我们想
第一个问题想要探讨一下
就是当前的安全技术的一些挑战
那Donson老师
刚才您有提到
目前的防御方法还非常脆弱
比如说像SFT或者RAHF
还有对抗性的防御方法
还有对抗性的防御方法
还有对抗性的防御方法
还有对抗性的防御方法
训练的这样子的防护
不够有效
甚至容易被reversed
甚至容易被逆转
当然您也提到的
就是representation engineering
还有safety-bound design
那其实想要抛出第一个问题是
您认为当前的这些大模型
出现这些脆弱性的
底层原因是什么
以及什么样子的技术的新方向
会更加的本质
OK
所以
再一次
对于这种类型的防护
以及对抗性的防护
以及对抗性的防护
这种类型的防护
我所展示的就是
这种模型是非常受到抗击的
这种模型是非常受到抗击的
这种模型是非常受到抗击的
这种模型是非常受到抗击的
这种模型是非常受到抗击的
这种模型是非常受到抗击的
它可以很容易被破坏
无论是在坑坑中
或是在其他种类的攻击
或是在其他种类的攻击
或是在其他种类的攻击
我认为一件事是
首先我们真的不知道
这种模型的功能
所以上个学期
我在伯克利课上教了一堂课
叫做《理解大语言模型的基础和安全性》
叫做《理解大语言模型的基础和安全性》
我称之为理解的原因
是因为没有人能理解
对吧
我认为这是一个问题
因为我们不太明白
这种模型的功能
而这种类型的准备
我们今天做的
比如在RRIHF上
可以说是
只要改变表面
只要在表面上改变
我们现在实际上
没有任何问题
而且我认为
如我所说
尤其是在AI确保
我们必须使它
适当地对抗相反的攻击
所以有趣的是
其实在语言线上
攻击不是那么容易
但是在
例如
在画面上
对抗相反的攻击
在语言线上
对抗相反的攻击
是比较容易的
我认为希望是更少的
我们实际上可以建立解决方案
对抗相反的攻击
所以这是为什么
现在模型变成了多模型
是更大的问题
因为作为多模型的系统
是非常容易去攻击这些系统
我认为是因为
对的
我们现在所有的设备
我们都在做的
是为了改变设备的
所以在我的讲座中
我提到一些未来的方向
其实是在更深入地去
通过对抗相反的控制
我们可以改变模型的行为
而且我们也认为
我们希望有一定的保证
然后最主要是解决问题
在更深入的程度上
而不是只是在
对抗相反的攻击
对
谢谢当中宋老师
目前的安全防护还比较表层
目前的安全防护还比较表层
目前的安全防护还比较表层
好的
接下来想要也想要问一下
就是耀东老师
其实我观察到您在不同的场合
都有讲过
比如说只做RAH是不足够的
然后以及你近期的工作
其实也发现了语言模型
对抗
对抗
对抗
对抗
对抗
对抗
对抗
还有逆转对齐的一个现象
您也可以谈一谈你的看法吗
对
那我就用中文说
就是说
其实刚才很多学者都观测到了一个现象
就是说
语言模型它做完这个对齐以后
你其实可以用非常少的攻击样本
就可以让它变得不安全
哪怕你做了很长时间的这个RAHF
那OpenAI的那个RAHF的那个tech lead
John Schuman他就发现一个现象
就是当这个语言模型训练得非常好的时候
它俄语上发现的这个错误
它只需要用30例英语的样例
就可以让俄语上犯的这个错误不再犯
然后这个问题呢
我们其实也进行了一个深入的思考
就是我们就最近有一个工作叫
Large Language Model Resist Alignment
就我们在这个工作里面去研究一个特殊的这个现象
就是逆对齐的问题
就我们都知道你在训练一个语言模型的时候
你总有两个阶段的吧
你先进行预训练
预训练完了以后
你再进行SFT
你再进行一个RAHF
那在参数空间的话
你可以把这个这个语言模型的训练
想象成一个拉橡皮筋的过程
然后你越往后拉越往后拉
你的张力其实是越来越强大的
然后我们就发现这个逆对齐的这个过程呢
就像你把这个橡皮筋拉到很后面
它不能在伸展的时候
你这个时候如果把它突然放开的话
它bounce back的这个速度
要比你拉的这个速度要快很多
所以我们就把这个现象在
这个语言模型的训练的这个过程中
定义为逆对齐
什么叫逆对齐
就是我在预训练完了以后
我在做比如说十步SFT
那我在做第十一步SFT的时候
我是不是会发现第十一步SFT
回到第十步SFT的这个速度
要比你拉的这个速度
要比我从第九步做SFT
到第十步SFT的这个速度要快
那我们发现这个逆对齐的这个现象是存在的
并且呢这个逆对齐的这个现象呢
可能会符合我们
就理解橡皮筋的这个运作原理里面的那个胡克定律
胡克定律讲的是
一个橡皮筋的这个硬力啊
等于弹性系数乘以形变量
然后这个弹性系数呢
我觉得在语言模型里面
我们发现的就是和模型的大小
还有预训练的这个数据量有关
然后那个形变量
其实就是你离
就是Protrain完的那个Policy的那个KO Divergence
就是你越练
它的形变就越长
那也就是说
如果你把这个语言模型
接着不停地往后对齐往后练
你看着是让它越来越安全了
但我们在那个paper里面
从理论和实践上都证明
其实它逆对齐反而会更加容易
这也somehow可能从一些机制上能够解释
刚才雅琴老师啊宋老师啊
都会提到的一个观点
就是你越做对齐
可能它反向就越容易被攻破
并且你用的这个样例
可能不需要很多
这我觉得是个非常有意思的现象
当然也揭示了我们未来可能下一步
对于如何更好地做安全对齐
做价值对齐
会有一些这个指导意义
也希望大家关注这个我们组的这个工作
就叫大圆模型
Resist Alignment
这个橡皮筋的这个类别还是挺有趣的
其实刚刚我有注意到
张宋老师有谈到那个
多模态大模型的一个就是对齐的难度
我知道绍兴老师过去几个月
您的团队其实有发表就是很多篇
关于多模态大模型
还有智能体的攻击和评测的工作
比如说像SciSafe
还有Chef数据集这样子的工作
那其实就顺着这个主题说吧
就是您觉得比如说像对于GBT-4O
这样子以图片视频语音
这样子的连续空间里面的数据
作为输入的一些多模态大模型
在就是这个安全的方面
安全对齐的方面
有没有什么一些特殊的挑战
对 这也是很好的问题
这确实是在去年年初
可能大家更多关注的
还是大语言模型本身的安全性问题
但是因为我们团队里面有很多是做
原来做视觉的
还有一些化学科的同学和老师专家
然后大家会发现说
我引入了更多的信号
比如说图像视频之后
它带来的复杂度是急剧提升的
它带来的安全问题也是
跟以往的专语言大模型是不一样的
比如说大家可能常说的
语言模型里的幻觉问题
其实在多模态的模型里面也是有的
这两者的区别是在于什么呢
就是语言模型里的幻觉问题
可能它的定义是稍微比较明确的
但是在多模态模型里面
它有可能是本身视觉的分支
它跟语言分支的上下文的理解比较弱
所以它根本就没有理解这个问题
带来的幻觉
可能是视觉分支本身
现在它的grounding能力也比较差
所以带来的幻觉问题
也可能是有偶合性的各种原因
那引入更多的模态之后
它的分解的复杂度就会变高很多
但是现在大家可能对这方面的研究
还是比较初期
所以并没有给出很明确的结论
或者是有一些更具象的分析
然后另外的话
我们其实今年年初的时候
在Gemini出来的时候
大概短期之内
我们就做了一个大概三个月的评测报告
这里面包括了对Trustworthiness的一些评测
也包括一些泛化性的
还有因我推理的
因为我们相信说多摩天大模型
未来能够用在的环节和产品应用里会非常的多
我们不仅关注它的可信的问题
也会关注它同时的泛化性的
还有一些推理的问题
这也同等的重要
甚至说其他的这些能力
可能会影响它本身的安全性的问题
所以未来的话
我们也会花更多的精力和资源
在这方面的研究上
谢谢
对 谢谢邵静老师
对
然后其实我也想问一下
卓生老师的一些观点吧
就是我其实也有看到您之前有做一些
多摩泰大模型
然后还有agents方面的一些安全方面的工作
然后当然现在agents其实是特别火的
就是那些可以直接进行序列决策
然后直接操纵工具和API的一些智能体
那我之前关注到您的工作
可能是之前有一篇是叫Our Judge
然后是通过监测交互记录的方法来识别
自主智能体agents的一些风险行为
那如果讨论到agent的安全的话
您觉得有没有一些特殊的难点想要分享
好的
我就沿着邵静老师刚刚提的这个多摩泰大模型这条线
就我们也在做就是agent
它有那种成NLM agent
也有那种基于Multimodal的agent
那么我们就发现
其实这里面一个核心的点就是在于
agent它是把大模型用在虚拟或者现实的环境中
让它对这个现实产生影响
那么从这个特点上来看
agent它就涉及到大模型与用户
以及环境之间进行的一个多轮动态的一个交互过程
那么它跟传统的大模型的安全一个重大的区别就在于
我这个
它是在一个真实环境里面
那么它的这个安全风险的来源
就会涉及到用户环境和模型本身
这三个维度的这个安全问题
而且它这个涉及到
我们现在更强调的是一个通用的agent
那么它所处于的这个环境也是多种多样的
那么我可以从这个环境中去构造相应的攻击样本
这是其二
第三个最核心的点就是在于
我智能体体这个行为
它不像我们静态的AIGC的
这个信息
智能体在这个交互过程中
它的这个后果我们往往是难以去预测的
我不知道它未来会产生什么样的后果
以及它现在的行为
未来会下一步行为会怎么去做
那么我们要去预测它未来的风险
也会变得更加困难
然后结合这些问题呢
我们最近在AGEX的基础上
我们也在做一些动模态的探索
就例如
现在大家很多人在关注
尤其是Apple intelligence
我们希望去让大模型
接入我们的手机或者是电脑
来模拟人类的这个屏幕的操作
帮我们完成复杂的指令
那么我们攻击者呢
他就可以
一方面可以从用户端
我们去构造各种对抗
或者劫持的样本
来影响这个智能体的行为
可能可以去对抗
我们也可以把信息植入到这个屏幕信息中
例如智能体在操作网页
或者操作我的App的时候
我也可以在它读取的这个环境里面
去植入新的指令
那么智能体
它看到这样的新的指令的时候
我们就发现在很多场景下
它就会受到新的指令的影响
而忘记它之前的行为
导致这种劫持的问题
那么这就意味着
我们攻击者
它不仅可以在user端
像我们传统大模型那样
我去在user端去做对抗
我去攻破你的对齐
然后也可以在这个环境端
我去给你进行诱导
或者进行指令的植入
来影响你智能体的行为
从而对环境或者用户
这个利益造成损害
所以这个里面就涉及到
这个三个方面的
就是多样化的攻击来源
这个会变得比较有挑战
而防御方面的话
我们现在大家的主要关注点
都是在于大模型本身的对齐
但是其实我们在
智能体的应用过程中
我不仅需要大模型的对齐
我可能还需要
一个外部的反馈
就是我只是大模型本身
它知道它行为安不安全
这是一方面
但是它这个行为过程中
我们是希望
它能够尽可能帮我完成任务
那么随着这个模型
变得足够强之后
它的任何求解能力足够强
那我就可以去诱导它
去做任何事情
所以我比较主张的一个观点
就是通过一个
外部的一个监管机制
跟这个模型本身对齐
来进行一个互补
所以这也是我们做
Adjust的一个初衷
我们去动态地去分析和监测
这个智能体的
它的这个行为历史
对它未来的行为进行预测
来预先预判它
可能成就了安全威胁
然后给出一个安全的研判结论
把这个信息反馈给模型
让模型基于这个反馈
利用它的这个学习能力
来进行自我的迭代
从而实现一个安全的闭环
这个是我们做这些事情的
一些基本的想法
对 谢谢周正老师
尤其刚刚您有提到
智能体还有大模型
或者说大语言模型的
两个关键点吧
一方面是这个存在与
环境和人类的用户的交换
另一方面是这个影响尺度
impact horizon的这个区别
对 特别好
那刚才我们讨论的都是
就是可能现在存在的
大语言模型 多模态模型
还有智能体的一些安全挑战
那最后一个部分
其实也想跟四位老师
就是探讨一下
未来有可能出现的
更强大的通用人工智能
甚至是超级智能
可能带来的失控风险
那其实我注意到就是
耀东老师还有张颂老师
包括刚才在台上的雅琴老师
今年在今年三月的时候
在北京的颐和园有共同参与
签署了一份关于AI风险的
一个共识声明
那针对前AI的一些
特定的危险能力
划定了五条安全的红线
那与其中呢
与这个AI的失控风险
强相关的一些红线
包括比如说自我复制
与适应的能力
还有欺骗人类的能力以及
这个寻求权利的倾向
那其实接下来的这个环节
想要抛给四位老师的问题是
就您认为就当前
哪一些的就是
危险能力的研究判断最为紧迫
以及对于一个
就是目前还尚未出现的一个
未来智能更强大的一个智能
什么样子的技术方向
我们现在可以做什么样子的技术方向
能够去未雨绸缪
然后能够去做一些准备
耀东老师你想先开始吗
那可以
对我们今年在年头的时候
在颐和园和
国内外许多专家在一块
我们在讨论就是
因为英国有布赖切利宣言
然后包括刚才结束的
首尔会议
其实我们国家都参与了深度的讨论
但是可能在
国内以中国的学者为主导的
这么一系列的讨论
并没有发生
所以我们在智园的领导下
也是请了一系列的国内外的专家
进行了一系列的研讨
我们的成果
划定了一些更加具体的红线
就您刚才所说
包括很多台下的专家
还有宋教授都是我们
红线的签署者
其中排名第一的风险
就是自我复制的问题
其实这个问题
我认为可能目前
还是有一些低估的趋势
就是刚才Yoshua的PPT里面
有一页其实讲得非常好
就是对于这些评测能力级
我们是能看到随着年份的往后增长
它这个学习的曲线
这个写率其实是越来越大的
那我认为现在可能
原模型发展的一个趋势
可能如果拿alphaGo类比的话
还停留在这个第一阶段
就是这个supervised tuning的这个阶段
学习人类的这个数据
那你一旦往后进行这个
self play和reinforcement learning
就是self improve的这个阶段
它可能这个能力的提升
会somehow
可能就突破了某个threshold
突然往上走对吧
所以从围棋这个
非常huge的这个space的探索来看
我们也是有alphaGo alphaGo zero和alpha zero
其实我们在做alphaGo
的时候你也不能预见到
后面两个版本它有那么大能力的这个提升
我觉得这个self improvement
这个事儿呢可能
和这个发现会比较有关系
那从学术研究的这个角度上来讲
我们确实发现
现在已经有非常多的这个self play
RHF RLAIF
确实能够在某种意义上
提升模型的能力无论在数学
或者在代码能力上
那可能加以更大的这个算力
和更高效的这个
自博弈的这个机制
尤其是在人类语料用尽之后
是不是能够通过自博弈的这个方法
进一步提高语料的这个质量
进一步提升训练的这个难度和有效性
那如果这个问题能被突破的话
那可能我们所谓的
这个自我复制的和
和self improvement的这个风险
确实能变到一个
具体看得见的这么一个风险
所以我们在这个想这个red lines的时候
就把这一条给它放进去了
然后后面其实还有一些风险
像deception
还有一些这个misuse相关的这个风险
那个其实我认为可能
相比于abuse
更多的是在misuse这个阶段
那那个可能需要更多的这个国际的对话
国际的这个治理
那我相信北京的AI安全共识
也是在往这个方向去进行一个推进
包括我注意到我们WAKE大会
今年上海也发布了上海市政府的
这个人工智能国际治理创意宣言
也是希望
能够在这个国际的这个合作上
能够推动进一步的合作
我认为这个方向都是非常好的
好谢谢耀东老师
对另外三位老师谁想
先开始
OK
I can add to that
I think right
so today even though the large language models
are already very powerful but we know that
so actually
we are still at the early stage
so
I think
next step already
people are talking about
for example
like having embodied intelligence
robots with these
foundation models
essentially
so right now we are still just training
we have the pre-training phase
like for large language models
and then we
in the future as we do embodying intelligence
and also as we have agents
that's actually going to act
in environments
we are going to have more of a close loop
where the agents
take inputs
from the environments
and then try to make decisions
and then get feedback
and then use that feedback
it can then help itself to further improve
do self learning
do continuous learning and so on
so I think as we get into this
more of this approach
then
essentially
we are
making the learning
also into the next stage
and I think what we are concerned about
is for example right now
even though with large language models
already you can say
the model can try to
when you give the task
if you tell it
think step by step
it can also break down a task
into different sub tasks and goals
but still that's
now it's a very strong capability
but in the future
these agents
become more autonomous
and also become more powerful
in particular for a given goal
it's going to be able to break down
into sub goals
and then figure out what's the best way to
accomplish these sub goals
that's where we are also worried about
like this paper clip
have a problem
where it can derive these
these dangerous sub goals
that's actually not well aligned
and so on
and then in this case
it could have other sub goals
including how it can get more power
and then how it can deceive
humans or others to get more power
and then how it can self
replicate
to sustain
itself and so on
so also
as you mentioned earlier
I think right now
we are not seeing these
capabilities yet
but first it's really important that we develop
to do early detection
like a canary and so on
but also the other thing is that
these type of behaviors
the moment you see it
it's very possible that the time duration you have
is very very short
you can think about it
basically the moment you see it
it has already started
the self improvement cycle
and as we know
as it gathers more computer power and so on
the self improvement cycle can go really really fast
so I think this is the challenge
for people who don't work in
frontier AI safety
they I think
the thing that they miss
is even though they can say
that's why earlier also you also mentioned
a lot of people say oh you know
we don't need to worry about it
these risks are very far out
but I think those people
what they don't recognize is that
the moment you see it
it could be already too late
so I think these are the challenges
that we need to address
next session
next session is
evaluation AI safety testing
so there might be more
insights from the speakers
so
for this question
do you want to comment on it
just now
the teachers have said it very comprehensively
I have a little feeling
just now
the teacher also said that
agent in many scenarios
have interaction with environment
he is affected by a lot of factors
it's not just the model itself
the safety problem
I think this problem
will be very obvious in the future
for example in our lab
not only we do AI
there are many experts
who do AI for science
in science
the infiltration of AI will be stronger
there is not a lot of research
in this area
you may pay more attention to
the safety problems
in the environment
for example
the abuse problem
we may also call on you
to pay more attention to
the safety problems
in specific areas
the future
the future
the future
the future
the future
the future
the future
the future
then this
ask
can you speak
about projeto
 Fox
and
any
very
需要一系统的解决方案了
它不仅仅是在于大模型本身的
我们以ARGC内容为主的
这种内容安全相关的这些研究
怎么去提升模型本身的安全性
这是一方面
但是第二方面
我们还需要一套非常完备的监管模型
我们需要去动态地去监测
智能体的行为过程
它是否会带来一定的损害
对它进行有效的研判
然后第三个是
刚从老师一直提到的
关于系统的红线的问题
就是我们对于传统安全里面
我们有一系列的这些安全的规范
我们怎么把大模型的通用性
跟这些安全规范给结合
然后实现一个自动化的监测
这样的话一方面能节省我们
做网络安全监测的一个效率
另一方面也能把大模型的通用性
给发挥出来
实现更加广泛的用途
当然在这个总体过程中
其实我们现在都是倾向于
从大模型本身来做
但这里面
还有一个很重要的点
就是刚刚提到的
这个动态的检测
我们需要一个
active的一个检测过程
而不是说等模型行为做完了
这时候我再检测
那么可能这个时候危害已经造成了
我们是很难去弥补的
所以从技术上我认为
其实是分成
我觉得一个非常
从这个方向上
我觉得非常重要的一个点
就是在于智能体
在开放环境中的
这个行为安全问题
然后技术上
可能我们需要从
大模型本身的内设安全
然后以及这个
行为交互过程中的这个动态检测
以及网络安全的这个系统红线
等三个方面进行这个系统性的这个防御
然后技术手段上
我们不仅包括现有的各种静态的手段
还需要一些主动的手段来进行这个约束
然后这个是我的一些这个观点
对谢谢卓尚老师
那由于时间关系呢
我们今天的第一场原着讨论
可能在这里就结束了
就是也特别感谢各位老师
今天的精彩观点
那我们请各位老师返回前来就座
我把时间交给主持人君怡
a big thank you again to all of our panelists
given that just the last panel
mentioned continuous monitoring and evaluation
this is also a great time to transition
to our second theme on ai safety testing
now we will hear from dr chris messerol
dr messerol is the executive director
of the frontier model forum
a non-profit established by
anthropic google microsoft and open ai
to advance frontier ai safety
he is an expert on ai governance and security
and is currently focused on developing best practices
for the responsible development of AI safety
and deployment of the most advanced
general purpose ai systems
chris previously served as the director
of the ai and emerging technology initiative
at the brookings institution
chris it's great to have you here
i'll hand it over to you
thank you it's a pleasure to be here
it's wonderful to be able to speak with you today
as was just mentioned
i run an organization called the frontier model forum
it's a industry supported non-profit
dedicated to advancing frontier ai safety
we have three kind of core missions
one of which i'll get into
which is developing best practices
the other two are advancing the science
of frontier ai safety
and the third is information sharing
about what we're learning
which is again part of why
we're so excited to be here today
i thought i might begin
with just laying out a little bit
what frontier ai is
and why it's so challenging to deal with
and then kind of walk through a couple of
some early thinking that we have
about how to think through
you know what types of evaluations to run
and what are some early best practices
that in discussions with our expert members
the safety experts within our member firms
what they're seeing and thinking about
and how they're beginning to approach
some of these issues
so just to say that i think it's really important
that we have a lot of work to do
just to start with
i think when we say the phrase frontier ai
what we're generally referring to
is the most recent generation
of advanced general purpose ai technologies
right so what we're thinking of
are not narrow ai applications
for specific you know things like
you know lending algorithms
or facial recognition technologies
we are thinking about general purpose ai systems
and we're thinking about in particular
just the most recent generations
so you know
on this chart you can see that
because of the way that we're scaling up these systems
generally speaking we're kind of doing a
you know 10x in terms of compute
every couple of years
to come up with a better class
and generation of model
for general purpose systems
we are primarily focused on the most recent generation
of frontier ai systems
and if you want to see more about this
we have an illustration of this on our website
but what this means
why this is so important
is that we expect the challenges
that we are dealing with
that we are dealing with
to evolve over time
the frontier is going to be consistently changing
you know as you can see
this is a stylized graph
but some of the graphs that we saw earlier this morning
saw a very clear slope line
almost exponential curve of increase in capabilities
we are focused on just the most recent generation
because we want to understand
have it to have
that we want to develop early best practices
for dealing with the most advanced models
at any particular moment in time
and the reason this is so important
as was alluded to earliertoday already
the reason this is so important
is the ability to grok certain capabilities
and we don't know how to predict
when these models in a training run
are going to acquire or develop particular capabilities
we don't have a good way
as professor song alluded to earlier
we don't have a good way
to understand the systems ex ante
which makes it very hard to understand
how to build them safely and effectively
so I think this is so important
and I would say the last point I would say is
we expect the frontier to continue developing
as these systems move from just chat bots
to things that are a little bit more agentic in nature
this challenge of assuring the safety of these systems
is only going to become more important
because the systems we are building
will interact more and more
with the real world
in ways that have potential consequences
for public safety and security
which is what our organization is focused on
so as I said earlier
I think this is a very important point
and I think this is a very important point
as I mentioned
I'm just going to walk through a little bit
some of our early thinking
that's been developed in kind of conversations
with different safety experts
and the member firms that we have
about how to structure evaluations
what kinds of evaluations to run
and then some early best practices
these are very high level
descriptions that we'll be talking about
I would also say
the terms themselves may vary
but it's really the concepts
that I want to share with you today that
hopefully we can have more
engagements and interactions over time
to begin as a field
to develop best practices
when it comes to even just talking about
the types of evaluations we need to run
to assure the safety of our system
so the first phrase
is we
I think at a very high level
there's two very general kinds of
evaluations or risk assessments
one are red teaming exercises
another are more automated evaluations
red teaming exercises
tend to be very manual
and kind of
there's work going on to try and explore
how to automate some of the red teaming exercises
but generally speaking
there are manual
ways of leveraging human expertise
to probe the capabilities
of a particular model
evaluations in contrast
tend to be things like benchmarks
or other automated forms
of exploring the capability profile
or the risk profile of a particular model
we think it's important to distinguish
exactly what you're talking about
when you're talking about how you're
assessing the risk or safety of a system
and this is just one general
high level class of distinction
within the evaluations
of frontier models
there's really two
I think core types
of evaluations that we want to run
one are
performance evaluations
and the other are safety evaluations
performance evaluations are critical
for understanding the general
reasoning capabilities
or other capabilities
that a model might have
that allows us to understand
in some ways how best to test it
for particular risks etc
but a performance evaluation
is really designed to just
capture and identify and assess
the performance envelope
of a particular system
again these evaluations
are incredibly important
because we don't know ex ante
how to define the ex post capabilities
we don't know how to define
before we train the model
what it will be capable of on the back end
so we need to be able to do performance evaluations
the other kinds of evaluations
are safety evaluations
and there you're not necessarily trying to understand
just what the performance threshold
or performance envelope of a system is
you are specifically looking for particular risks
and the ability
of a model to exhibit behaviors
that would give you
that is capable of behaving
in unsafe ways
as far as kind of different types
of safety evaluations
there's really two
safety classes of safety evaluations
that
will start to see
kind of being developed and run
in model development
one are developmental evaluations
and another are assurance evaluations
developmental evaluations
what we're referring to here
are really the kinds of evaluations
that firms will run
or the developers of a large scale
pronteri system
might run at different phases
in its training cycle
just to kind of benchmark it
to see how it's doing
with respect to certain kind of safety risks
that's different from a full on assurance evaluation
where it's not necessarily the team
that's developing the model
instead it's a team that's kind of
tasked with assuring the safety of a system
they have independent expertise
from the team that's developing it
and their goal
is really to assure the safety of the system
and to develop evaluations
that are capable of assuring
the safety of a system in some way
which is a little bit different than
the kind of life cycle development
safety evals
that might happen just at different check marks
in the development of the model
then the last kind of
in my view probably the most important
distinction here is
within assurance evaluations
so the evaluations that are meant to
try and assure the safety of a system
within that
as we're thinking about
trying to evaluate
models for safety
we really need to be evaluating
the safety and assurance
of these systems
one way of thinking about it is
maximum capability of the system
another is like how it's used
in the real world
I would say a different way of
defining this last category is
we need to look for assurance evaluations
that are designed to try and capture
the riskiest behaviors
like the tail distribution of the model
like some of the behaviors
that are most capable
or most extreme from a particular risk
that wouldn't necessarily
be compressed into kind of
the average or mean behavior
we wouldn't be able to get a lot of information
about those kinds of tail risks
from more typical user behavior
with behavioral evaluations
I think the goal is more
to try and understand
what is the average behavior or mean behavior
in general with some of these models
and how we assure the system
and the safety of the system
even within that kind of mean behavior
again this is kind of early thinking
we'll probably evolve over time on this
but this is just a little bit
of how we're thinking about
some of the different evaluations
at this moment
relatedly there's some
there's also the question about
what to do as you're setting up
an evaluation and a red teaming
whether it's a red teaming exercise
or a broader evaluation
there's a set of practices
for a wide array of evaluations
for specific risks
but there's also a set of
just best practices
for any kind of evaluation
you're doing regardless of the risk
so it doesn't matter whether you're
looking at bio risks or cyber risks
things like that
or it could be societal risk
that you're looking at
if you are developing a system
or an evaluation rather
of a frontier AI model or system
and I think this is just a sampling
of some of the early thinking we have
about high level best practices
there's a few that
I want to call out specifically
one is we need evaluations
to account for prompt sensitivity
I think any of the
engineers here who have worked
with these models and tried to get them
to behave in stable ways
will recognize that the specific wording
of different prompts will oftentimes
lead to different results
and what we're really trying to do
in these evaluations
is capture their kind of risky behavior
which means we need to
explore different wording
wording choices or configurations of prompts
to be able to get at whether or not
it has a certain capability in general
and an example of this would be
if you're trying to
if you're worried about say like
malicious uses
of a system for example
you don't want to just ask the system
how do you build an explosive
or something like that
you also want to test for
can you describe the chemical process
by which dynamite releases energy
something like that
you need to have multiple ways
of asking for the same thing
two other
at least two other kind of
things that I want to call out
just very briefly
one is as you're developing these evaluations
you need to evaluate both the model
the underlying base model
as well as the end system
for a lot of evaluations
we'll target one or the other
but we need to do both
it's not just the underlying model
that needs to be evaluated
because in many cases
that's not what is exposed to the end user
usually what is exposed to the end user
is the overall system
and we need to be able to test that
as well as the underlying model
another
really important
best practice
I guess I'll walk through them just briefly
is evaluating both normal
or typical behavior use
and adversarial use
as I mentioned on a prior slide
we do want to evaluate the systems
for kind of typical behaviors
and the safety that they might exhibit under that
we also want to evaluate for adversarial use
this is something that a lot of developers
won't instinctively necessarily do
as they're trying to
rush to get a product out the door
but it's very important to evaluate adversarial use
Professor Song just had a
really great explanation of some of the kinds of
adversarial use cases
that you need to be paying attention to
the broader point though is
if you are developing frontier AI systems
it's not enough to just focus on
I think I'll end on
because I think it's probably
the most important best practice
that we are starting to converge on
as a field in AI safety
when it comes to evaluation design
it is vitally important that you
understand what is the base
like what the baseline is
that you are evaluating a system against
for example
if you want to evaluate a system
for biological risks
for example
can this system inform or help
someone design a bioweapon
or some kind of dangerous pathogen
you need to evaluate it
not just for the
what kind of information the model itself has
you need to evaluate it against
the kind of baseline application
that would be used in the absence
of whatever model you are testing
so in many cases that would be for example web search
and so on
so all of an evaluation
should not just be kind of absolute
understandings of risk
but also relative or marginal risk
compared to the counterfactual application
that might be used for whatever
application you are developing
that is a very brief kind of
high level overview
of how we are thinking about
some of the early best practices
for just evaluations in general
information and communications technology
DirectAway has led
the development of multiple
industry standards in China
including the country's first
big data benchmark standard
he has participated in the drafting
of several major national policies
including
中文來講解一下
那麼我希望
各位
外國朋友能夠通過
同船能夠很好的get到
我的主要的意思
那麼今天
跟大家分享一下
中國信息通信研究院
在人工智能尤其是大模型
評測安全評測方面的一些
思考和實踐
三個方面的內容
一個是我們怎麼認識
人工智能大模型面臨的風險
這個risk
到底是在哪些維度上
另外第二個方面是
我們基於這些
對於風險
現有風險的這個認識
一直在推動建設
benchmark safety benchmark
的這個framework
並且按季度去開展
這個評測的活動
希望通過這個實踐
不斷的促進大家
來提升模型的
安全水平
最後也是再分享一下
其他有關如何保障
人工智能負責任發展
安全方面的一些
相關的工作
那麼現在
大模型實際上是一種數據驅動的路線
那
在skilling law上
這個skilling law的延長線上
一直在發展
決定著模型能力的
這個前景
主要靠的是
算力和數據
那麼這個大家都非常清楚
那實際上在
這幾年
我們可能所有人
基本上所有人都認為
skilling law可能還會延續
但是skilling law的問題就在於
我們對於模型
表現具體是什麼樣
缺乏非常清晰的
認識而且控制
能力是很弱的
不像50年代60年代
基於規則的
人工智能模型
他們是我們設計者
可以很好的去控制模型的
行為和輸出
但是在基於數據驅動的
尤其是大模型的時代
模型的表現很多時候
是一種現象學的
範疇所以很難
在內生機制上去
完全做到避免風險
那麼
這幾年大家對於
人工智能尤其是前沿模型
剛才Chris已經講得很清楚了
前沿模型大家
非常關心它的水平的進步
同時也關心它可能
潛在蘊含著什麼樣的風險
那麼我們
基於對於各類
研究的成果的
分析我們認為其實
可以用這樣一個金字塔
來表現我們
對於人工智能尤其是大模型的
風險的認知
幾個層面
分成兩個維度
一個維度是自身
它的內生的安全問題
那包括了模型的
參數還有數據
計算系統以及網絡
還有應用系統的
這個
security層面的安全問題
那麼上面一個層次
就是對於個人
對於國家對於
全人類在
應用人工智能的時候可能
引發的衍生的
應用層面的風險我們如何去控制好
那麼最宏觀的
可能是我們人類在
人工智能面前的位置在哪裡
我們全人類的共同的命運
那麼再往下就是
國家安全
經濟安全
社會的供應鏈的穩定
人口就業以及個人信息
保護等等吧
這個risk實際上是多維度的
所以這個討論
人工智能的安全風險
往往是一個多學科交叉的
那麼需要去共同去
更清醒的認識到
這個安全的風險在哪裡
再舉幾個具體的
最具體的一個例子
那麼以大模型為例
其實現在媒體上對於
大模型應用過程中暴露的各種風險
報道也越來越多了
比如說在內容
風險方面
那麼可以看到這個
虛假信息
deseinformation
其實是非常普遍了
而且這個也是困擾
不管是中國還是
國外監管部門
非常重要的一個問題
那麼還有就是數據風險
那這裡面臨的我們的提示詞
可能會暴露
機構內部的一些信息
這樣的風險
以及算法可能會被
用提示詞攻擊
或者其他手段來
從裡頭提取敏感信息的
這樣一些攻擊手法
這個一直在翻新
而且也是層出不窮的
那麼不管是宏觀
還是微觀層面
其實我覺得現在大家對於
人工智能的探討
人工智能安全問題的探討
已經非常的充分了
最上面是
聯合國層面
密疏長古特雷斯密疏長
在非常多的場合
在談
聯合國也成立了好幾個機構
包括UNESCO ITU等等
在討論人工智能的治理問題
安全問題
各個國家其實也在行動
我們看一下具體的
整體上感覺
人工智能的安全
風險 治理
正在從原則走向實踐
大家對於一些
基本的原則
比如說以人為本
智能向善
比如說要保證它的公平
非歧視
這樣一些原則
大家沒有分歧
那麼國際社會
包括各個國家
其實也都在把這些原則
正在轉向具體的操作
那麼這些操作
可能會落到具體的很多方面
比如說一些
國際規則的制定
比如說在聯合國
剛才說了聯合國教科文
ITU等等這些機構
在牽頭這個
國際規則的設定
那麼各個國家也在出台相關的立法
那麼
另外一個再具體的一個
就是轉化成標準
那麼把原則
固化成一些能夠成文的
技術性的要求
來便於這些企業去遵從
還有一個更具體的就是
測試 如果有了標準以後
還需要去驗證這些標準的
遵從情況
那麼在企業側呢
可能還需要去研發很多的
技術工具來提升
保障這個安全的
水平
所以我們看到美國 英國 歐盟
新加坡 還有中國
都在採取
這幾個層次的
具體舉措
把人工智能的治理
從原則推向實踐
那麼中國新同院
科和行業平臺
我們也在投身於
這樣一個促進人工智能治理
從原則走向實踐過程中
我們也貢獻我們的力量
今年年初我們和國內
三十多家單位吧
這個數據集
還是比較豐富的
我們有五十多萬條的
題目圍繞著
在全球
共識的
要求上
以及符合中國本土法律法規的
要求
這兩個維度上來開展
評測數據集的建設
那麼另外一個就是
我們也密切跟蹤前沿
這個發展
為什麼我們是
季度性地去做這個事情
因為人工智能的進展
可以說是日新月異
所以我們會季度性地調整
我們的Benchmark的數據集
和方法 不斷地提升
跟進前沿模型演進的步伐
另外我們還是特別
關注用戶是怎麼想的
關注現實的風險
比如說金融 政務
醫療這些行業
他們的擔心是什麼
目的實際上是希望能夠
通過這種評測
來讓這些行業用戶
放心 能夠
勇敢地去使用
先進的人工智能技術
但是前提是保障安全的前提下
那麼在
2024年的
4月份我們發佈了
第一版的AI Safety Benchmark
的結果
這個結果是基於我們
左邊這樣一個框架
來做的
大體上分為三個方面
一個是內容安全層面的
要求
這就包括了
法律明令禁止的一些
內容的輸出 比如說
涉謊 涉暴 賭博 欺詐
危險化學品 生物
武器等等這些
這個內容的
管控輸出
另外就是這個數據安全
也包括了個人隱私
和企業機密的
可能被提取的這樣一些特徵
個人信息和商業秘密
那麼還有最上層是
科技倫理
這裡頭就包括價值觀 心理
健康和AI意識
工序良俗
辱罵 誘導等等這些
欺騙這些問題的
應對它們是如何表達的
如果這個評分能夠得到八九十分
這個水平還是可以的
但是呢
問題就在於
拒達率偏高
這樣會造成
很多用戶體驗
不是特別好的
同時這也表現
通過具體的
攻擊手法
這個攻擊手法也是我們
能收集到的
市面上學術界
包括社區
這個最新的攻擊手法
包括我們自己測試中
發現的這個攻擊手法
來測試這些模型
包括提示式攻擊 誘導攻擊 越獄攻擊
內容泛化攻擊和其他的攻擊手段
那麼
數據集也相應的做了擴大
底線紅線 社會倫理和數據洩露
還有一些其他的
安全保障措施的這個測試
應該說是一個更加全面的
所以這個體系是一個
不斷在演進的
這個過程 要跟上這個節奏
那麼Q2的這個評測數據集
也比較大
那麼我們有600多條提示詞的模板
那麼有60多種攻擊手法
3.6萬條的
這個攻擊樣本
每次從這3萬多條裡頭抽取
4千多條來做測試
那麼測試完了
這個數據就淘汰了
避免大家去刷題和
作弊
那麼目前
我們Q2的這個結果
也已經初步的出來了
那麼可以看到
大家在
原始輸入的
攻擊成功率
我們不用提示詞攻擊手法
來做
提問的話 它的攻擊成功率
還是比較低 也就是反過來說
1減這個比例
就是它的得分
但是我們加入了
很多提示詞攻擊以後的
攻擊成功率就顯著提高了
也就反向的代表著
可以有很多的
手法來繞過
這個模型的護欄
所以整體上來看
我們目前是模擬了
最近發生的
安全攻擊的各種手段
來
更好的來看待
我們模型安全的水平
大模型的安全測試
是中國心通院在人工智能
安全方面的一個實踐
具體的實踐
讓能夠系統化的去推動
人工智能可信發展
那麼就在
2021年的時候
上海世界人工智能大會
我們發佈了全球首個
可信人工智能白皮書
這個是跟京東探索研究院
一塊發佈的 陶達成院士
那麼當時的思路是
過程管理
就是要人工智能要保障安全
除了
你要求它結果以外
還要管好研發使用
的流程 所以
過程管理我們認為是非常重要的
所以在那一年的
白皮書裡頭我們就提出來
如何在
各個
包括G20 包括很多原則的
指導下我們怎麼在
企業各個環節落實
這個原則
23年到現在我們認為
結果管理也非常重要
我們參考了像
OpenEye和Ethiopic
他們做安全
模型的分級分類
那麼把風險
分成不同的類別和等級
那麼我們
我覺得這個也是特別有借鑒意義
所以也在推動
從過程管理走向
結果管理 效果管理
那麼
基於風險定級的
這樣一個管理手段
其實我們希望通過這樣一個
方法論來讓企業界
讓產業知道
它應該怎麼去做
是一個最佳實踐
那麼同時我們也在
制定Benchmark
包括Benchmark在內的坐標尺
因為如果沒法度量
就沒法改進
所以測試是非常關鍵的
除了大模型的安全測試以外
我們也在做
人臉識別安全
還有算法安全
等等這些
安全的測試標準
測試能力上我們也在建設
龐大的數據集
包括人臉
和其他的一些
測試級能力的建設
一直在持續地跟進
同時
大模型的
內容安全
它如何去
測出結果以後如何去增強
我們現在也在
基於我們掌握的
我們建設的300多萬條
關鍵詞
50多萬條提示詞
來訓練我們的安全
測試的模型
未來希望通過
參數的方式提供給
模型廠商來加固
他們的模型能力
我覺得
安全其實很重要
還有一個就是信息真實性的保障
信息真實性如何保障
我覺得要靠很多方法
其中一個方法就是水印
Watermarking
除了Watermarking以外
比如說圖片裡頭有C2PA
的標準
還有其他的一些方式
其實也都在探索中
綜合來看
我們也希望能夠建立一套
對抗Deepfake
來保障
內容真實性的一套技術的方法
包括水印
我們現在也開發了水印的
隱世寫入的算法
也提供這樣的一些API
這些企業
也可以使用
整體上我就利用這麼一個時間
跟大家分享
我們的一些實踐
希望跟大家繼續交流
也感謝會議邀請我來跟大家分享
謝謝
我們現在就來
邀請一下
上海AI研究所的
同事
來做他們的
表演
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
請大家握個手
来讲吧
首先呢
这个我们知道
现在整个人工智能的发展
是非常飞速的
然后里面涉及到的
这个国家也越来越多
不仅是中国
国外各类的国家
大家都会关注
能力和安全相关的
这个共生性
那我们最开始
可能在17年的
这个alpha zero的时候
大家可能更关注
这个面向传统AI算法
和系统的安全标准
然后到后面的时候
随着更多的这个模型出来
包括这个generative model
包括这个从前几年开始的
这个ChatGP系列
整体来说
现在大家会发现说
我不能只观察
AI算法和系统的
这个安全标准
或者是AI算法
本身的这个能力
更多要观察
它的这个数据的规范
还有到面向
这个整个社会和群体的
这个质量
大模型的安全评测
和对应的安全标准
那实验室整体
从21年成立至今
一直致力于大模型的体系的研发
从2021年开始
发布的第一个书生1.0的模型
是一个视觉的通用大模型
当时应该在国际上也有很多
包括DeepMind
Google OpenAI
都有相关的这个视觉通用大模型
随着时间的演变
到了2022年
实验室开始走向这个多模态大模型
在这里面也逐渐衍生出来
这个大语言模型
书生谱语体系
在昨天的这个开幕式上
也做了相关的介绍
然后一直到今年7月份
这个书生谱语
包括书生多模态大模型
已经演变到了新的
新的能力范畴
我们整体的公开的模型
包括7B的小参数模型
还有这个70B的大模型
致力于这个一直致力于是开源开放的理念
能够赋能大模型学术研究和创新产业生态
并且我们也一直保持我们的技术的原始创新
探索更为高效的大模型发展路径
整体来说
我们实验室的大模型研发和安全体系构建
随着这个时间的历程
可以分为四次挑战和对应我们做的
相关的技术和成果
接下来的话会分这四次挑战
为大家来分享一些工作
第一次挑战的时候是大概在2022年底
我们发现这个越来越多的这个生成
生成模型出来之后
它带来的这个风险性也越来越大
包括一些这种滥用的行为
包括用这种模型去
对 那个乔老师已经来了
要不然
谢谢你 Dr. 肖静
给我们做了一个很棒的开始
我们很高兴现在有乔余教授
乔余教授是一名助理教授
和传统科学家
在上海AI研究研究院
以及一名兼职研究员
在上海AI研究研究院
与中国科学博物馆
联合研究的深圳研究研究研究研究研究
他的研究涉及各种领域
包括超级模型模型
电脑视线
深入学习
和自动驾驶
与其他研究员合作
在上海AI研究研究研究院
乔教授最近发布了
许多AI安全项目
包括许多标准和测试框架
乔教授
桌子是你的
继续课程
感谢您的参与
对于有关重要的事件
对于有关重要的事件
对于有关重要的事件
首先抱歉
因为个人的原因
把我们这个会议中断了一下
我想这样
其实刚才那个邵静博士
是我们整个大模型安全团队的负责人
我甚至在想
他应该比我会了解更多的技术细节
会讲更多的干货
但是我还是非常感谢
安远和谢总的邀请
能有这样一个机会跟大家做一个分享
我就直接跳到刚刚那个
我们是讲到这一页是吧
对对对
我先说一下是这样的
我其实原来主要做计算机视觉的研究
然后慢慢的开始做视觉大模型
从2020年开始
然后随着我们
从语言
从视觉到语言到多模态做
我们越来越发现安全
实际上是大模型发展中
非常重要的一个问题
而评测又是整个建立安全的基石
所以从去年开始
在实验室的整个的布局下面
我们就把安全做一个重要的方向
大家可以看到
在做安全的时候
首先的第一个问题
就是说大家事实上都很关心安全
但是对于安全领域
我们到底具体关注哪些因素
应该从哪些角度去评测
实际上并没有特别广泛的
这样的一个认知
在这里面我们当时看到
第一个事情是评测体系和数据的缺乏
所以在去年我们
进入这个领域之后
第一件事情
就是做评测体系
我们建立了当时蒲公英
人工智能一个治理开放的平台
这里面把国内还有国际上
很多的治理理论制度进行了汇总
我们也建立了包括红对数据
漏洞数据 评测数据等等的
相对来讲比较全面的一个评测的数据集
在这里面实验室
也坚持开源开放的理念
我们把我们的很多的数据集
还有我们的规范进行了开放
从而推动这个领域的发展
怎么建好这个数据集
大家知道早期大家评大模型
就是我给一个固定的题集
然后有标准的答案
让这个模型答完了之后
我们对它进行评测进行对齐
但事实上这里面是有很多问题的
我们知道对于大模型来讲
如果你做了SFT
做了RLHF人类反馈的强化学习
事实上它表面上表现得很好
但并不是代表
它真正的就没有风险没有问题
那在这里面怎么来做呢
我们当时设计了一个方法
我们发现很多领域的专家
包括很多做社会学政治学伦理法律的
我们组织了上百位
我们请了包括上海交通大学
复旦大学的上百位的专家
这里面还有包括很多的教授
让他们帮我们设计种族问题
因为这些专家时间非常有限
你很难说你请这么多专家
让每个专家你帮我设计一万个题
一万个题事实上对大模型的评测
还是非常窄的一个理论
我们在专家设计题的题目上
通过大模型的方法
对这些题进行增强进行扩充
然后我们再用这些题目去评测
特别是这些题目很多的设计
它有很多的攻击性
而且有针对漏洞的一些专门的设计
这样就形成了更好的评测的效果
这是我们在第二个方面所做的工作
然后我们在
因为我既做模型也做模型的安全
在这里我们就发现
大模型的安全和对齐
大家经常说有一个问题叫对齐税
也许确确实实你做了RLHF了之后
模型的安全性从评测上分数会有增加
但往往对能力
对原来它原有的一些能力
模型的性能会有下降
在这里面我们是不是有更好的方法
在这里面我们事实上就把一些MODPO
就是Multi-Object DPO
多目标的对齐引入进来
这样能够保证我们在保证对齐的同时
也能够对原来的能力
进行优化
这里面有些算法的创新
由于时间原因
我可能就不再详细讲
而且我在想在这里面MODPO
现在目前也不是一个结束式
应该这些算法和框架
还有很多优化的空间
我看在座有很多我们年轻的同学
这可能成为你未来研究很好的题目
基于我们形成的评测体系
基于这些评测的数据
我们实验室也研发了一个
普安大模型安全评测的系统
在这里面我们事实上
把一些我们前面所做的
从维度到评测的过程到出报告
完全把它进行自动化的评测
这里面也支持了我们上海市的一些工作
在这里面我们发现
就是说评测结果的透明性和可解释性
是一个很重要的事情
因为经常的时候
随着评测的维度越来越多
问的题目越来越深
经常我们很多大模型的评测用户
拿到评测报告
他会问我们
你们为什么模型这么
你为什么出这样的评测
评测报告
你评测报告是不是客观
能不能对它你的结果的依据进行说明
进行解释
在这里面我们想到最终的话
我们解释还是基于我们建立的
基于数据库完了之后
我们把我们的评测结果
对它进行可解释性的生成
这样就把问答和可解释性的一体化
集成在一起
而且因为采用reg的方法
它是一个外籍的数据库
很好的方法
就在你可以实时的更新
比如说我们有新的规范
新的规定
我们可以非常简单的更新到数据库里面进行评测
当然大家知道现在整个大模型从语言到多模态发展
我本身也做计算机视觉的研究
多模态大模型的事实上又对评测提了很更新的要求
比如说在这里面对于多模态来讲
由于它内容生成的形式和形态更多
如何定义维度需要进一步加
另外在多模态之间本身就有特征的对齐的问题
而且我们知道语言到多模态了之后
事实上很多幻觉问题被进一步加强
针对这些特点我们做了一个以视觉为核心的
包括对齐还有评测的数据机
SPVL
它包括我们设计了针对多模态领域六个
比较可能引起有害的领域
四三个类别
五三个子类别
这里面也包括了超过十万个这样的问题
然后来帮助我们一方面做评测
另外这个数据可以作为对齐
此外实验室我们在去年还花了很大力气
应该是在国内甚至国际上比较全面的
一个具人类价值观的一个多模态评测体系
这个报告总共有四百多页四种模态
而且在这里面我们发现单独的提维度
事实上对于这个领域的技术的发展还是很有局限性的
我们建立了两百三十个用例
而且这两百三十个用例是根据多模态大模型应用
场景落地来提出来的
包括放话性可信度还有推理能力
大家知道未来多模态大模型
推理能力是大家现在关注的重点
我们可以看到最新的大模型
大家都在强调它的数理和因果的推理能力进行评测
实际上这个评测结果也对我们后面包括大模型
特别多模态大模型安全的评测起到了很大的指引的过程
面向未来我们觉得有几个方面值得关注
还有一个就是多智能体
大家知道这半年agent技术发展很快
agent已经成为强化
大模型
在
专项领域跟包括跟很多世界进行互动的一个重要的手段
那是不能够把这些技术也用来安全和评测呢
事实上我们建立了一个多智能体的评测框架
叫pc
然后在这里面呢
我们详细研究了多个智能体之间
他大家知道他之间有交互和协同
他所能产生的这个危险行为
以及在这过程中间
我们如何引入一个专家一个doctor去进行角色防御
另外一个呢
大家知道人与人之间交互过程中
心理
这个价值观是非常重要的一个问题
所以呢
我们也在探索如何从仿造人的方法
从心理学的角度
然后从人文科学的角度来去做这个大模型多智能体的安全评测
我想说一点是这些工作其实才刚刚起步
从实验室角度来讲呢
我们最初关注的包括定义问题
包括建一个好的平台
让大家来做这方面的研究
这里面有非常多的重要工作
我们也会非常欢迎啊
在座各位的机构企业还有研究者
我们一起
来共同推动这个领域的发展
实验室除了自己做之外呢
我们还坚持着开放的策略
事实上呢
我在中国网络空间协会
大家知道这是在我们国家这个领域非常权威的协会的架构和指导下呢
我们成立了围绕生成式人工智能安全评测的工作组
我本人呢
也非常有幸的当选了工作组的组长
其实压力还是很大的
因为这个工作组既包括咱们国家一些权威机构
也包括在这里面
研发的包括交大
包括交大清华复旦这项大学
还有包括百度华为这样的企业
为什么要成立一个工作组呢
我们认为呢
要做好博伦馆是安全还是评测
他一定不是一个机构
是一个共商共治的事情
在这工作组在过去的一年接近一年的过程中
我们从几方面开展工作
一方面的是建立了常态化的这个交流的机制啊
我们有日常的例会
而且现在通过线上会议呀
通过专题会的形式
让大家日常能交流
第二个呢
我想这个工作组的
一个很重要的方面呢
我们是想从bottom up的
从底层从大模型一线的角度
能够形成技术规范和共识啊
在这里面我下面会讲
我们现在已经初步形成了一个
声称是人间自能评估的流程规范啊
这里面我不知道在座是不是有一些同事已经参与
因为这个规范
涉及的范围比较广
已经参与到这个工作进来
第三个呢
我们在这个规范的基础上呢
我们会跟大家一起啊
共同研发一些评测的技术和主见啊
这里面包括数据集
也包括规范的基础上呢
包括工具
而且呢
包括我们刚才讲的平台
而且这里面我们希望能够在
大家形成一致的条件下
能够拿出来一部分进行开源开放的形式提供服务
最后呢
我们也举办了很多安全的活动
包括恩安园的很多活动
今年在世界人工智能大会的前夕呢
我们也举办了普选安全挑战赛
吸引了全球一两百个非常优秀的团队来参加啊
我们也日常跟很多的一些相关的大模型研发机构
做这个安全的指导啊
这就是我们所做的
感觉
刚才提到安全评估的流程和规范
它有几个特点
第一个它可操作性很强
我们是一个全维度评测
提供全生命的这样一个规范
第二个特点
我们是服务应用和产业落地
因为参与的本身很多企业
实际上他们从实际应用落地的角度
起了很好的建议
我们希望这个规范
也能成为推动咱们国家
未来人工智能安全评测发展的重要
面向未来我想说一点
大家往往容易把人工智能
看成一个工具
但是我更想说的
随着通用人工智能的发展
随着技术的进步
人工智能成在成为
我们整个社会体系的
非常重要的一个基础设施
它会频繁地与人互动
与其他的系统互动
所以我们现在考虑人工智能安全
绝对不能孤立地
把它看成一个工具
而应该从整个社会体系的角度
来思考人工智能
这里面包括整个需求
包括应用的场景
还有包括在这个过程中间
人机物之间的这种认知体系的建设
是很重要的办法
怎么来应用这个地方呢
我觉得就是说
大家知道大模型的发展
是以Skeleton Law为指引
随着算力数据模型规模的增大
模型的能力不断提升
那么我们在想
是不是我们也要探索一个
围绕安全的Skeleton Law
我们能找到一个
可扩展可发展的方式
只要我们投入更多的研发的资源
数据算力一些技术的研发的投入
我们就可以把安全
也可以可持续地发展下去
在这里面
我觉得安全的Skeleton Law
可能比原来大模型的Skeleton Law
大模型Skeleton Law核心是
参数量数据量和计算资源
这三者首先也是安全的
更重要的组成部分
但是对于安全的Skeleton Law
绝对不止三个维度
我们需要多方的参与
需要更新的研发的模式
当然我们也需要高质量的数据
以及更好的模型的架构
这里面的事实上
也对于我们的Community
不管是你做科研的
比如说产业运用
提高了新的挑战
我们希望跟大家共同能力
建立起面向未来的
可持续的AI安全的Skeleton Law
在这里最后
我也稍稍做一个宣传
在明天的上午
我们在世博中心620会议室
有一个国际AI前沿技术的论坛
我知道今天在座的一部分专家
也会参加明天的论坛
也欢迎大家继续参加
和支持我们的活动
好 谢谢大家
谢谢你 教授
我们知道您的计划非常紧张
所以我们很感谢您
来到我们的讨论室
请请回到舞台上
请准备坐在舞台上
我们将进行AI安全的讨论
教授乔
Chris 魏 魏凯
请帮我们回到舞台上
这次的舞台上
我们也有
瑞敏 何 和熊德义教授
瑞敏 何 和熊德义教授
请帮我们回到舞台上
让我介绍一下
瑞敏 何 教授
是新加坡的
首席人工智能警察
他执行多个职位的努力
以达到新加坡的
计划AI目标
包括发展和实现
新加坡的国际AI策略
他也是新加坡政府
首席人工智能警察
和一名
在AI上的高级领导组
教授
熊教授是
自然语言处理研究研究院
和天津大学
国际联合研究研究中心
国际联合研究研究中心
他最近在计划
计划AI安全的计划
包括大规模评测
以及更多的计划
我们的主席是
Brian Zare
是 Concordia AI的副总裁
让我们来看看
他的掌声
好的
今天我们就邀请
准备好准备
准备好准备
准备好准备
准备
准备
准备
准备
准备
准备
准备
准备
准备
讓我先介紹一下
您發佈了一份第一次的研討
關於大語言模式的評估
並且更早前
為中國LRM進行隔離危險評估
有三個群組
第一個是不平衡的危險
這個危險通常對社會有負面影響
第二種危險是不適用的
人們可能會使用大語言模式
來解決這個危險
第三種危險是障礙性危險
很多人都用了不同的詞彙
有些人稱之為障礙性危險
也有障礙性危險
以這類危險的背景來看
我認為這三個障礙性危險
有三個阻礙性危險的解決策略
第一個是
我認為現時的大多數的危險
主要是關於障礙性危險
而不是關於正式的分析
但我們也看到一些
最近的研究中的研究
其實有兩種類型的研究
第一種是正常性的研究
第二種是經濟性的研究
正常性的研究
人們會使用一些問題
他們會問障礙性危險模式
看看他們是否有所期望
或是更多力量
或是更多財富
或是更多錢
類似的事情
如果模式非常聰明
他們可以說謊
他們可能需要達到真正的目標
所以我認為我們仍然缺乏很多方法
來解決障礙性危險的危險
所以這是第一個項目
第二個障礙性危險
我認為是因為我們缺乏資料和資訊
因為目前最多的研究
其實是一個黑箱的研究
我們沒有資料可以引起模式
以抵抗他們的危險
我們沒有資料可以開啟黑箱的模式
而最後一個障礙性危險
我認為是障礙性危險
因為最多的障礙性危險模式
或最高障礙性危險模式
是由
大型公司
所以他們只有很少人的資訊
我認為現在我們看到很多模式
已經被封鎖了
很多模式已經被封鎖了
所以這意味著我們沒有
這個模式的透明性
所以這是一個非常大的挑戰
尤其是在障礙性危險方面
謝謝您的精彩的評論
莊教授
何教授
今天我們很高興能與您一起訪問這堂會
我們期待您在早上聽到您的討論
但在此時
我最近聽說新加坡
設立了一個國際AI安全基礎
您可以跟我們分享一下
在新加坡的AI安全基礎
在這個國際AI安全基礎中
或在更廣泛的新加坡
在這個國際AI安全基礎中
或在新加坡的AI安全基礎中
謝謝
很高興能在這裡
再次跟您見面
我可能要再走一步
信任和安全是一個非常大的關鍵
在新加坡在全球
我們在過去59年來
作為一個國家
建立了信任
社會上有很高的信任
也有很高的信任在政治領導上
信任也是我們在新加坡的健康基礎中
有很強的生產基礎
信任也是人們在晚上安全走路時
可以在街上走的原因
也就是人們在網上的交易
AI當然增加了信任的困難
 instead that all真的是
you can have models can hallucinate
as a lot of the speakers spoke out today
it generates a fake news
you can't always rely on them
and therefore when we have
come out with our national AI strategy
up to last december
Drust becomes a very big part of
so we have many
many different initiatives
at different levels
from a research point of view
we are doing a lot to support
fundamental research as well as
policy research
and to responsible AI
as part of AI Singapore
we also have two
racelis
Oh yes and yes as being a part of AI Singapore
We have set up a center for advanced technologies in online safety to look at misinformation and disinformation.
We have an entire cybersecurity agency looking at AI security and security of AI.
And because we have a whole range of efforts, we also have set up a digital trust center,
which we designate as our AI safety institute, really to be a national focal point for AI safety efforts,
looking at evaluations and testing all along the developmental life cycle,
all the way from development to deployment of AI models,
and also a vehicle for us to do international collaborations with other countries,
so that we improve the science of AI testing, and to improve what we do in Singapore and around the world.
Thank you. So we have set out some of the priorities for AI safety evaluation,
and let's move on to the life cycle of doing evaluation, as some of the speakers have mentioned.
And I would like to get the views from Director Wei and Chris on this question.
So,
Testing could be conducted in different stages of the cycle,
including during training, pre-deployment, and post-deployment.
What do you think are some of the benefits and challenges for conducting evaluation along some of these stages?
And what are your experiences working with companies in both China and the United States?
好,谢谢。那么,大模型其实进展非常快,
但是原理上决定了我们对模型的激励,对它的安全风险,
对它的能力水平的认识,不能做到百分之百非常全面。
就像我们用竹篮打水一样的,到处漏水。
我们做的工作,比如说测试,其实是去识别哪里有漏洞,
然后我们去把它补上。
但是还有多少漏洞,我们可能不一定能够搞得非常清楚。
我觉得现在,要说优先级的话,可能,
一方面我们一定要快速地,及时地发现在各个环节中暴露出来的,
比如研发环节,还有使用环节暴露的,以暴露的各种风险,及时修复它。
但是这个问题看起来还是比较容易做到,相对容易做到的。
只要我们能够知道这个风险是什么,我们就能够定义它,测试它,改进它。
问题在于我们不知道的太多。
所以,我觉得当务之急是需要我们去做的。
我们需要建立一套机制,建立一套动态,敏捷的机制,
使得我们能够永远跟上模型技术水平的提升,以及它的风险。
我们能敏捷地发现它,同时能够迅速地让产业界知道风险在这里,风险在那里,然后来改进它。
我觉得测试非常非常关键,但是测试只是一个环节,
我觉得需要从各个环节上来建立一套敏捷治理的技术生态。
才能够堵住这个不断暴露的这个漏洞。
所以,否则我们永远是跟这个未知在赛跑,可能很难控制好。
就道理,谢谢。
克里斯。
首先,谢谢您来到这里的荣誉。
很高兴能够与您一起参与。
我会建立一些我同学的评论。
有这么多不明显的事情,
但是我认为这是一个非常难复杂的评论。
我认为如果我们在今后五年或十年之后有这种讨论,
我认为我们将会更加成熟。
但是我认为,你知道,重要的是,
我们需要做的几个事情。
在计算、测试和测试整个生活过程中,
我们实际上需要做的一件事,
在企业和政府之间,
我们需要更深入地理解释
我们需要做的各种测试,
在哪个阶段的生活过程中。
我认为,这甚至是预备训练。
我认为我们需要在预备训练前的测试中,
比如在10X的数据和数据上,
你觉得你能够安全地做出这种测试吗?
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
你应该在这种测试中,
当这些系统变得更能够创造和执行自己的研究,
例如,我认为我们还不够在那里,
但是像是系统的自动测试,
那是一个你肯定想要测试的事情。
但是,当你训练模型后,
你采取了一些调整,
我认为你仍然想要进行几个测试,
直到你实际上采用它,
以确保你感觉舒适,
使用者可以安全地使用它,
包括你正常的用户,
以及一些可能会有迫害的演员,
可能会尽可能地尽力推动系统,
以坏意义的方式。
问题是,
在这些领域中,
要做多少测试?
我认为这是一个希望,
在下个月或两个月之后,
希望会有更多的谈话,
在全球AI系统上,
我们应该做多少测试,
在我们感觉舒适的地方,
前往新的领域,
在人生中的下一个阶段中,
我认为这是我们需要更多的谈论。
教授,您想说什么?
我想刚刚各位嘉宾讲的非常精彩和全面,
我想补充两个点,
第一个点就是说,
在当前这个阶段,
安全问题非常受重视,
我们都认为它是很重要的问题,
但事实上,
不论是学术界还是产业界,
我们在安全中投入的资源,
比如计算的资源,
还是研发的资源,
相比我们发展大模型的能力
和产业应用来讲,
是远远之后的。
而另外一方面,
事实上,
我们对于整个通用人工智能,
特别是现在大模型安全的了解,
是非常有限的。
我们知道存在很多安全的隐患,
但是我们对安全这些隐患
并没有一个深入的认识。
更重要的是,
我们实际上并不知道
为什么产生这么多安全隐患。
就是说我们更多现在对安全问题
是在应用过程中
从现象层面所观测的,
对背后原因没有这么多的支持。
所以基于这两个观察,
我觉得有两个方面的事情,
下面要做。
第一个,
在这里面我们需要投入更多的资源,
需要更多的国际的合作,
因为安全问题是我们全人类
所面临的一个共同的问题,
这一点是非常重要的。
第二个方面,
在研究上我现在真的是呼吁
我们科研学术界,
包括我们的产业界,
能够找出来一条能够通向
我们Safety AGI,
安全通用人工智能的技术框架。
因为现在我们可以看到,
不论是SFT还是RHF,
更多的是一些单点技术的研究。
往往还是不漏充的单点技术的研究,
并不是一个我们很难确信
靠这样的技术
我们就能真正通往Safety的AGI。
所以在这里面,
我觉得从学术界上真的需要
更好的研究和框架和投入。
谢谢。
第二,我们需要进行
一整个AI测试的过程,
包括预约训练,
不仅是训练后的训练和投入。
第三,似乎AI安全投资的水平
还不足够,
我们需要增加这种投资水平。
我认为这带给了我一些
AI评测的挑战,
在一些课程中提到的。
第一点是
方法。
特别是,我希望
教授熊教授和乔教授,
可以评论一下这一点。
似乎现时流行的测试方法
是自动测试,
和人工资训练。
你认为有没有新的方法,
在目前的情况下,
你认为有没有新的方法,
或在目前的情况下,
有没有新的方法?
我觉得从技术上,
有几个方面,
我觉得现在值得关注。
刚才我们确实需要建立
更好的技术和方法框架
来解决这个问题。
我们的技术,
智能体已经被验证成为
智能体加上工具调用,
包括智能体里面,
它可以进行的交互,
反思,任务的规划,
已经成为大模型,
目前在落地中重要的一个技术。
最近我看在学术上,
在工业界都有很好的应用。
事实上,
智能体所演化出来的技术,
对于我们解决目前在大模型,
不管是做更全面,
大家知道传统的安全研究,
比如说计算机安全研究,
有很多计算机理论,
密码学等等的研究,
他们都能提出一个在这个领域
非常硬核的一个数学问题,
一个理论问题。
但目前在安全研究中,
你会发现特别碎片化,
碎片化就意味着在这里面
我们没有很好的一个基础,
没有很好的一个体系。
我想这是我们在这个时候,
从学术界,
甚至包括产业界,
我完全同意的意见。
我只是提出了两个点。
第一点,
这是第一点。
第二点,
我认为,
在评测方面,
我们需要建立很多资料,
但是在安全方面,
我们有很多安全问题,
而且这实际上是
所有课程的计划。
所以,
如果我们只依靠我们的AI社区,
其实我们,
没有了资源,
没有了资源,
也没有了能力,
建立这些资料。
所以,我认为,
我们必须与更多社区合作,
以解决AI危险的问题。
谢谢。
河先生早就提到
建立信任的重要性。
如果公司进行自己的评测,
社会就很难
完全相信
AI的安全
。
我认为这就涉及到
第三者的角色。
我希望河先生和
董事长能够评论这些问题。
您认为,
第三者的角色有什么挑战?
例如,
陈教授提到的
质量问题。
如果第三者测试员和测试员
只能够透过AI模型
进行黑箱测试,
那么他们的AI安全测试
应该会如何?
我希望您能够
提供您的观点,
河先生。
我认为,
在AI Verify的经验上,
我们已经在这方面
做了很多工作。
我会说出三个原因,
为什么我们在AI Verify上
做了这些工作。
AI Verify是新加坡政府
两年前推出的
测试设备。
它有三个原因。
第一点,
AI Verify实际上是
一个很大的原则,
在这里,
所有人都在做很好的研究。
第二点,
AI Verify实际上是
在企业手里做的研究。
所以,
AI Verify实际上是
用实际工具,
但也尝试
在企业手里做最好的研究。
否则,
你的第三者在很远的地方,
所以你必须在最远的地方做最好的研究。
第三点,
你需要你的第三者,
你需要你的系统
永远都在前面。
所以,去年,
我们更新了AI Verify,
以Moonshot为主,
这是我们创造的AI版本。
这是最开始的,
这是最初的产品,
带来的设计,
带来的团队,
这是最开始的。
好的,那么,
第三方机构是非常重要的。
我觉得
第三方机构的存在
其实有助于提升
大家对于人工智能,
特别是大模型技术的信心。
而是要告诉他们,
通过第三方的一些专业能力,
告诉研发者,应用者,
风险在哪里,
应该怎么去改进,
下一步应该怎么办。
同时,第三方机构可以扮演一个桥梁作用,
让所有物体的,
收集到的安全风险汇聚起来,
测试数据汇聚起来,
方法论汇聚起来,
研究结果汇聚起来,
能够更好地赋能任何一个单点的研发机构。
我觉得第三方机构扮演的角色
其实是非常丰富的,
那么测试是非常重要的核心的手段。
我刚才,
我非常同意乔老师的这个观点,
就是现在这个整体上,
产业界对于AI的投资
还是非常有限的。
那么我觉得,
第三方机构的存在,
其实是提供一种公共产品,
来降低,
或者说在有限的产业
总体安全预算的情况下,
提供一些公共产品,
让大家的研发,
安全投入,
可能会,
成本能够可控,
恰到好处。
我觉得咱们做安全方面的工作,
其实还是要服务于更好的发展,
更好的应用。
那么在控制好,
这个成本,
投入的这个同时,
在保证安全的前提下,
控制好我们的这个前沿模型研发机构,
要披露这个风险列表,
这样才能展现出一种负责任的这个态度。
这样我觉得,
第三方机构和企业合作,
做这些事情,
其实是才能形成一个闭环。
感谢。
I very much agree with the vision
that AI safety should be a global public good.
And some of the AI safety evaluation
 that we can develop around the world,
or AI safety labs in different cities
and countries around the world,
that we begin to align
on a shared vocabulary,
so that we can actually,
without knowing exactly what you mean by red teaming
and what we mean by red teaming,
it's hard to trust and allow for these things
to be interoperable.
And so I think that's like the key first step
that needs to happen in the next year or so.
Professor Qiao
我想跟刚刚说的,
我想跟刚刚各位专家讲的
已经非常全面了,
我就最后想说一点,
就是在呼应你的,
我觉得在这个领域,
我们太需要国际合作了,
需要国际共识,国际合作。
If no final remarks,
let us give a final round of applause
to this excellent panel.
现在远处讨论的嘉宾,
请留步,其他论坛的嘉宾请上台,
我们会在台上进行一张合影。
然后上午论坛呢,
告一段落,
下午论坛会在下午一点十分开始。
The morning forum has concluded.
The afternoon forum will begin
at 1.10pm.
See you in the afternoon.
Thank you for watching.
字幕志愿者 杨栋梁
字幕志愿者 杨栋梁
字幕志愿者 杨栋梁
Look the history of invention,
every single major invention
has brought hopes and fears.
And, across the times,
including the modern society,
当时的社区被维修了,完全被维修了,例如电池,车,和数十年后的数十年后的电子技术。
这是很存在的。如果你想想,十年后,二十年后,你想想,我们的电子产业对我们有什么影响?
例如电脑对社区有什么影响?他们完全维修了社区。
每次这些转变都带来了新的风险和新的危险。
每次,在新的危险中,有些东西被幻想,有些我们没有看到。
我们认为目前的转变,我们所见识的转变,不能学习以前的转变。
所以,我们认为电子产业可以成为更好的世界的驾驶者。
但在现代的转变中,我们认为电子产业可以成为更好的世界。
但在现代的转变中,我们认为电子产业可以成为更好的世界。
但在现代的转变中,我们认为电子产业可以成为更好的世界。
但是相当于水格质和有 relay-in、coакс友好那样的转变问题。
无论如何就好,在转变前面,它们不同部位之间无论美好或严重或 groupe好。
对于整体而言,一切都要有一个 direcimonic effect。
在气声矛盾的表现上, בاج LIKE是视乎敹充的时机。
与估卧 courses segunda-interchange的中国人。
对于世界自然、通行情绪、外系和客户广泛热。
我们尝试了解AI对更好的世界的变化的证据,
所以我专注于健康,因为这是我最了解的事情。
一个例子是AI在读写地图。
现在,在美国的食物与药业领域中,
AI系统在读写地图上进行试验。
这是一个健康和经济的理论。
这个问题的理论是,
如果你早点诊断癌症,它是能够治愈的。
诊断写地图是相当便宜的,
而医生的读试是最好的。
因此,如果我们能够让AI在读写地图上
提供更好的AI,
希望能够提升癌症的生存力,
降低癌症的健康担忧。
这就是AI的一个理论的一个例子。
现在,有一个更复杂的故事,
是关于医疗资源的提供。
这里是一系列的数码工具,
关于早点诊断,
关于预测,
例如距离的预测,
各种不同的测试,
都合成一样,
以解决医疗的解决问题。
如果它能够解决医疗的解决问题,
在医疗医生或医生的患者上,
它是有益的。
因此,在一些地方,
我们能够看到AI能够帮助,
帮助医疗。
现在,回过头来,
看一个更广泛的经济观点。
有些研究研究了AI的影响,
例如,在业务上的生产AI。
这里是一个研究,
研究在客户服务部的
生产能力的增加。
我们看到,
生产能力的增加,
在一个小时内,
成功完成了多少个任务。
所以,我们认为,
这就是我们能够预測,
什么AI,
特别是人工智能,
将对经济界带来的影响。
然而,
我们想强调,
我们必须谨慎
自动驾驶的信息。
技术的历史,
尤其是AI的历史,
是充满了增加的信息。
近15年前,
也许10年前,
我们预测过,
自动驾驶的车,
会在旁边。
但这并没有发生。
专家现在认为,
自动驾驶的车,
将在经济界带来的
影响,
将在10年之后,
成为经济上的生命。
所以,
在10年前,
我们认为是实际的,
而现在,
我们认为是实际的。
因此,
我们要专注在这里。
我们有承诺和梦想,
但他们都不会实现。
这些承诺,
与很多投资有关。
在人工智能,
现在,
投资的大小,
是数千,
或是数百亿,
每年。
在人工智能中,
确实有波动。
这是重要的,
因为它形成了
他们的论语。
困难是,
要找出正确的交易,
分辨和估计。
人工智能,
是技术的逐渐改变,
在每天的经验中,
带来长期的改变。
人工智能的技术,
已经在电脑中,
已经在我们的电脑中。
然而,
历史教导我们,
需要很长时间,
才能够改变社会。
在人工智能时代,
生产能力增长,
只有20年后,
技术进入。
因此,
人工智能的改变,
仍然是人工智能的改变。
这充满了社会的信任,
并且,
人工智能的原则。
因此,
我们做了一个风险分析,
这是一个相当经典的分析。
它看起来像是
人们曾经展示过的东西。
例如,
亚绪亚。
不惊讶。
我想与我们讨论几个重要的东西。
一是,
人工智能的智能,
对民主社会来说,
是重要的。
它形成了民主社会的论语。
它被人工智能威胁,
为了许多原因。
其中一个原因是,
人工智能在电脑设施中,
创造了 filter bubbles 。
这就是他们讨论的问题。
它仍然存在,
而且非常重要。
另一个新鲜的东西,
虽然它没有破坏,
但是它却是
电脑经济的破坏。
基本上,
论文化是一个商业模式。
这是社会中的影响。
这是社会中的影响。
这是社会中的影响。
我想与您一起进入
另一个风险,
因为我们认为它非常重要。
这是私隐的风险。
这有两个方面。
首先,我们越来越注重
AI模式的组织。
亚维亚人认为,
在GPT中,
我们正在互动
与一个中心化的演员。
这位演员在我们的私人数据中
拥有一个窗户。
这是他们的第一个问题。
第二个关系的问题是,
AI训练,
对不起,
专业训练的数据。
例如,
如果我训练一个AI系统
在电脑健康记录中,
并将它放在野生中,
有很好的理由认为,
能够采取一些
用于训练AI系统的
私人资讯。
这和
财务问题有关。
另一个重要的问题是
执行执行执行。
我们并没有
太多谈论这个问题。
但我们认为,
执行执行是重要的。
执行执行AI,
更多AI系统,
各种AI系统,
是一部分
日常的执行。
还有很多
执行执行犯罪。
其中一个,
非常好地执行的,
是在联合联合联合伙伴
在联合联合伙伴
发生的事件。
这发生后,
人们变得贫穷。
这可能会引起人们
犯罪。
这可能会引起人们犯罪。
这可能会引起人们犯罪。
但这个问题
并不是
技术问题。
这只是技术问题。
要说明,
不每个人
都被
某种意义
担忧。
因此,
所有在计划这个系统的演员
都不在计划这个系统。
所以这不是技术问题。
而是
如何演员用技术来工作。
这是一个
很久以前的问题,
但也有证明的问题。
这问题仍然存在
在现代技术中。
在语言模式中,
这问题仍然存在。
例如,
我学生的李沪珊,
我们在调查
大语模式的
罪犯的犯罪状态。
我们看到,
美国或欧洲的
罪犯
比美国或欧洲的
罪犯
比美国或欧洲的
罪犯
更差。
这完全是
用于训练的
训练处。
所以,
这里没有什么新的,
但我们不要忘记
那些老问题。
另一个关键的问题,
就是我们所谈到的
永恒高度。
每个人都谈论
计算机的增长。
我们必须
将这些事件
与
计算机的增长
放在一起。
在这里,
我不仅仅是
计算机的增长,
但是也在计算
机的增长
是能够
击破
所有能够
在世界中的
能够。
这就是训练的价格。
我认为训练的价格
是更重要的。
我们看到的
是相同的东西。
当计算机变得更好,
那就是计算机的增长
会更加突出。
现在,这一点
根本不可持續的,經濟不合理,我們有一個反彈的效果,
它刪除了硬件的功能。
關於這一點,我們最大的擔憂是力量的集中。
我們有一個經濟,大人越來越大,因為我們使用更多的資料,
更多的電腦。關於這一點,我們有一個系統,
因為我們使用更多的資料,我們使用更多的資料,
因此,選擇變得更集中。
組織建立了決定引擎,
為所有人作出決定。
這和其他危險,例如錯誤、隱瞞、隱瞞、隱瞞相配合,
我們認為這是AI的最重要基礎。
因此,我們認為我們需要有多領域的AI。
我們相信在這方面,我們需要開放資訊,
它能夠提供新智,
它能夠提供新智,
它能夠提供新智,
它能夠提供新智,
它能夠提供新智,
我們認為重要的概念是公共的概念,
不僅是開放資訊,
它有挑戰。
它有責任的挑戰。
如果我們沒有做好責任規則,
我們可以殺死開放資訊,
因為開放資訊有積極的經濟持續性。
挑戰是不公平,
在模式中的資料,
而問題是,我們使用開放資訊,
我們通常不定義於我們所講的。
現在,開放資訊是用來定義
不公平的開放資訊模式,
而這並不是開放資訊的標準。
因此,我們相信
國際管理需要。
正如網絡,
AI很容易通過境界。
最好是我們使用訓練資訊,
採用多個證據,
以建立更多的代表性的AI,
以避免我之前提到的鋼線。
因此,我們相信
我們需要共同管理,
以幫助我們建立共同的物體。
正如ICANN,
這是非常有用的,
我們需要共同的網絡,
而不是兩部分的網絡。
因此,我們建立了一個世界AI組織,
主要代表
第一,國家和中間國家的組織,
第二,研究,
主要的資產結構,
以及公司和地區。
目的就是
設立標準,
包括評測標準,
並建立一個
AI的知識狀態,
並提供指導和
戰略指導。
謝謝。
謝謝Gail
給我們帶來的
令人驚訝的看法。
我相信
之後有更多的討論。
我們的下一位主持人
是Ramin Ho,
他是新加坡的
主要智慧機器官,
他擔任了
新加坡的
技術目標,
包括發展
新加坡的
國際智慧方案。
他也擔任了
新加坡政府的
代理
和
科技機器官,
以及
是
美國
高級的
智慧機器組織的
代理。
賀先生,
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
請
对AI管理的执行。
每个国家的政策执行方式
是由于其独特的情况下,
例如它的历史、国家优势、
比较高的优势和企业基础。
新加坡是没有区别的。
无论如何,对于AI安全和管理,
我认为有四个普遍的团队,
在各国各地都适合。
因为我们都从同一个地方开始,
Gaia其实给了我半个话题,
所以我现在的生活比较简单了。
让我来与每个人一起谈谈。
首先,
我认为我们必须要与AI管理
与虚假的心态进行执行。
当前开放AI委员会的
Helen Toner提出,
这并不是真正的智慧。
更何况,
正如这几个早上的讨论者所说,
没有人
真的明白AI系统的内在工作,
特别是深入的智能网络。
更加,
AI技术在急速进行,
但我们无法预测
AI系统的进行程序的确定性。
我们并不确定
AI安全的技术方法,
例如机械解释能力,
或丰富方法,
会否真正进行。
我看到有些人在前几个楼上响起嘴巴。
Chris在笑我。
另一方面,
我们也不确定
如果有些最糟糕的情况会发生。
对吧?
AI在前几个楼上已经被隐藏了。
因此,
正如这几个早上的讨论者所说,
AI有许多不明,
也许有许多不明。
因此,
我们应该禁止确定的测试,
特别是未来。
而是说,
政策人员必须尊重,
尊重继续学习思绪,
尊重继续学习思绪,
尊重继续学习思绪,
尊重继续学习思绪,
尊重继续学习思绪,
重新测试理论,
继续研究新闻和科技的影响,
继续研究新闻和科技的影响,
继续研究新闻和科技的影响,
我们必须学习企业和企业的专家,
我们必须学习企业和企业的企业,
我们必须学习企业和企业的企业,
我们没有任何答案,
而我们必须专门谈论专业的专业。
像世界智能会议的会议,
我们将用来提升共同理解的智能,
我们将用来提升共同理解的智能,
也意味着有能量听听和学习,
也意味着有能量听听和学习,
市民、工作人员、写作人、艺术家、
年轻人、未来者,
所有人都具有重要的看法。
他们的怀疑和感受对智能真实,
他们可能是国家和业务,
我们必须理解他们。
我非常感谢美国的智能协议团队,
他们很明显地谈论智能,
并且提出了他们的建议。
这包括我的同事,
凌翰和曾毅。
凌翰今天也与我们一起。
虚弊也包括承诺
智能的关键问题,
我们不懂得解决,
所以他们可以尝试发掘答案,
并且让我们不决定
根据不成立的理论。
在这种心态下,
我们在上周的新加坡智能会议中
与全球智能专家联合联合研讨,
以解决新智能的关键问题,
如果解决了,
这将带来智能的发展和扩展。
在上周的智能专家联合研讨中,
今天与我们一起参与智能与智能专家联合研讨,
包括邓颂、耀东、艾琳和布莱恩。
智能专家联合研讨也包括了
认识我们的制度反应可能是错误的,
或者他们需要更新。
这并不是说我们作为政府或政策执行者,
我们坐在床上等待。
而是说,
我们可以开始从调整制度和规划的介绍,
获得影响者的反应,
观察该规划的后果,
如果需要,
并确认它们,
并确认它们,
并确认它们,
并确认它们,
并确认它们,
并确认它们,
并确认它们,
并确认它们,
并确认它们。
We believe that an iterative learning approach is far more sensible in attempting to regulate all of AI's potential harms.
So that was humility.
Second, we need to approach AI governance with a sense of perspective.
We need to be careful about false dichotomies.
Nothing is really black and white about AI policy.
For example, an AI system can be generally useful, but it sometimes says incorrect things.
AI can increase productivity, but it can also cause job disruptions.
AI can help with climate change mitigations.
It can also hurt the planet with its high consumption of electricity and power.
AI can do a lot for healthcare.
And AI systems can generate deep fakes and scams, but there are also ample examples of AI being deployed for the public good.
For example, in Singapore, we use AI to smoothen immigration clearance, predict hospital times, optimize train maintenance.
Citizens can also access some government services using chatbots.
So regulation and innovation are also a false dichotomy.
We need pro-innovation regulations that allow the beneficial applications to flourish while guarding against the harmful users.
This weight of balance differs sector to sector.
For example, the worst-case scenarios for self-driving vehicles are quite different from cancer diagnosis
or chronic cancer diagnosis.
All chatbots.
AI also does not exist in a vacuum.
It is part of a technical product, which is part of a use case, and it's part of a broader environment that we interact with.
Hence, it may be useful to think about AI policies in a broader regulatory environment.
In recent years, Singapore has updated our suite of laws to safeguard the digital domain,
including for personal data protection and against misinformation and disinformation
that is spread online.
To better manage cyber risks and egregious content, and curb online criminal activities.
In these regulations, the human or the institution remains responsible for the consequences of their decisions,
even if these actions were aided by AI systems.
So that was perspective.
Third, I think we need to increase and improve our capabilities to govern AI.
This starts with encouraging many of my fellow policymakers
to use AI, so that they develop a baseline understanding
of the potential and limitations of such technologies.
In Singapore, we actively promote the use of AI within the government.
Civil servants can access AI-enabled transcription, summarization, and LLMs from their government laptops.
AI even helps them draft responses to citizen queries.
It is important for ecosystems to have the practical, technical capabilities
to develop and regulate AI,
rather than just talk at the level of abstract principles.
Hence, we also encourage the community of more technically inclined government officials
to develop their own AI products and tools.
We also create avenues for them to share their learnings.
For example, as I mentioned at this morning's panel,
our government agencies developed the AI Verified Minimum Viable Product in 2022,
which provides a practical way
for developers to demonstrate that their AI systems
measure up to internationally recognized governance principles.
Last month, we also launched Project Moonshot,
a challenge to ourselves to extend the AI Verified Toolkit
from traditional AI to generative AI.
To ensure a pipeline of new tools and new methods to regulate AI,
we also actively support research in areas such as digital trust,
online safety, and responsible AI.
Researchers work together with government officials
on research that is inspired by real needs
and see the fruits of their labor
immediately translated into applications.
Our researchers find this very fulfilling.
But capacity building extends beyond tools for policymakers and developers.
For government tools,
because government tools,
while they may be effective,
they are not a silver bullet.
To truly reduce the harmful effects
of AI,
we need to develop a population
that are confident and discerning users of AI.
This enables them to engage in the digital environment,
raises their competencies,
skillsets,
and employability.
Empowering our citizens and businesses
to reap the benefits of AI
is a key pillar of our national AI strategy,
which we updated last year.
Uplifting their professional competencies
is done through broad-based
and sector-specific skills upgrading,
often in close partnerships
with industry partners,
as well as pre-employment
and company-led training efforts.
Additionally,
we are also helping all citizens
increase their awareness
and familiarity of AI
through measures such as community roadshows,
including in partnership
with our public library network
and mass media campaigns.
Finally,
we need to be willing to cooperate internationally.
AI is produced in a global supply chain
from the production of chips,
the training of data,
the consumption of models,
and the development of applications.
And users come from all around the world.
We live in a borderless digital world.
What happens in one country
will affect developments everywhere.
And AI is too complex
and evolves too quickly
for any one company,
one country,
or institution
to have a monopoly of wisdom
on research,
regulation of AI.
And we all mutually benefit
when we collaborate on solutions
and share experiences and approaches.
Furthermore,
a fragmentation of AI governance
and security frameworks
raises compliance costs for businesses
and slows down useful AI adoption.
Plus,
while it is natural
that every country
has slightly different perspectives
and positions on AI,
we need to work hard
to overcome our differences
and find common ground.
Yesterday,
I spoke about how countries
can work together
at the Global AI Governance
Forum Ministerial Roundtable.
Particularly,
we need to come together
to find common problems
worth solving.
We need to work bilaterally,
visionally,
multilaterally
to facilitate norms
and encourage the creation
of global,interoperable standards
and common tools
for AI governance.
At this morning's panel,
I also shared on how
we open-source AI verify
and launch the AI Verify Foundation
as a vehicle
to tap on the global
open-source community
to crowd in expertise
and capabilities.
Let me conclude.
The challenge of AI safety
and governance
will continue to evolve,
but we must sustain
our engagement
with the hard technical questions,
with the policy dilemmas,
and with each other.
If we collectively adopt
the very human traits
of a spirit of humility,
a sense of perspective,
a desire to increase
our capabilities
and our willingness
to collaborate,
I am confident
that we can achieve
the right balance
of governing
this particular technology
and collectively harness AI
to serve the public good.
Thank you.
Thank you so much, Dr. He
for your inspiring recommendations.
I'm pleased to now welcome
the next speaker,
Professor Zhang Linghan
from the
Yuhachina University
of Political Science and Law.
She has extensive experience
advising Chinese legislation
on algorithm regulation,
platform governance,
data security,
and AI.
She is currently
a member of several
advisory committees,
including the ICT Committee
of the Ministry of Industry
and Information Technology,
and the Cyber Security
Legal Advisory Committee
of the Ministry of
Information Technology
and the Cyber Security
Legal Advisory Committee
of the Ministry of
Information Technology
of the Ministry of
Information Technology
and the Cyber Security
Legal Advisory Committee
of the Ministry of
Public Security.
Professor Zhang
is also a member
of the UN High Level
Advisory Body on AI.
Professor Zhang
I'll hand it over to you.
大家好
非常开心
今天能用中文
跟大家介绍
我最近新写的一篇论文
那么题目是
基于风险到基于价值
探索中国人工智能的
人工智能治理的方案
那么在这个安全的议题上
去讨论人工智能
实际上我们都有
这样的问题
就是安全治理的
风险治理的框架
是否是人工智能治理的
最佳方案
我们目前所有风险治理的
手段和措施
是否能够完全应对
人工智能
给社会带来的影响
那么当我们去
追求安全的时候
多安全
才算真的安全
那么首先
向大家介绍我的观点
那么我首先认为
人工智能
是一个非常重要的
因素
因为人工智能
是一个非常重要的
因素
在风险治理的过程当中
不管是风险的识别
还是风险的应对
都有一些
无法应对
人工智能
给社会带来的
全方位
多维度
和颠覆性的影响
风险的治理理念
和治理手段
我们仍然应该坚持
但是
我们在风险治理之上
应该超越风险治理
采取基于价值的
治理框架
那么也请大家
多多批评指正
谢谢
那么下一张图呢
是我们可以看到
目前基于风险的治理呢
已经成为
全球人工智能治理的
共同主题
那么在这个图上面呢
列到了
不光是很多国际组织
都把风险治理
作为了
不管他们的宣言
还是指导意见的
这样一个基础的
逻辑框架
同时也可以看到
基于风险的治理
也被世界很多立法
所采纳
那么下面带来的问题
就是
由于大家都是
专注在风险的治理
和风险的治理
之间
我们可以看到
在风险的治理
和风险的治理
之间
在这个区块
我们可以看到
剩下的风险
在这个区块
我们可以看到
风险的治理
和风险的治理
这两个区块
就代表了
这是我们的
系统分析度
那么我们可以看到
这个公共系统
就是在这边
我们可以看到
我们可以看到
这个区块
是我们的
学校的
体制性设施
和我们可以看到
这个区块
就是在
我们面前
这个区块
就是我们的
科学区块
这个区块
根据它
那么在欧盟的AI Act当中呢,把风险按照影响的范围分为不可接受的以及高中低的风险。
我们可以看到美国商务部的这个国家标准研究院颁布的风险呢,则是根据风险的来源和成因。
那么在实际上应对人工智能的风险当中,我们可以看到很多存在的问题。
我们就以美国NIST发布的这个风险指南作为报告。
第一个,就是大家都知道在风险的判断当中其实蕴含了很多的价值考量,也就是说我们说风险它并不是一个纯粹的科学概念,而是一个规范性的概念。
我们看到,比如说NIST的这个框架当中,把风险分为技术性风险和技术社会风险,但是大家可能没有注意到的是,把哪些风险归类为技术风险,本身就蕴含了价值的判断。
那么其次,
风险分类一个很重要的问题是,在我们看到的前面这张PPT当中,大部分的风险分类都把隐私侵害、歧视等等这一类对人的权利的侵害当作是人工智能的风险。
然而,我们要看到的是,这一类的风险并不像我们一直熟悉的风险的治理,比如说汽车发生事故的概率,比如说食品不安全的概率一样,比较容易量化和计算。
这一类的风险是非常难以去量化的。
我们对于风险治理的基本思路是,OK,我要算一下你的损害的大小以及损害发生的概率,最后看看我得到的收益跟它能不能成正比,进而采取措施。
可是如果我们人工时代面临的很多风险都是难以量化的风险的话,那么这种治理方式可能就面临着困难。
那么第三个呢,是我们再回来看这张PPT,可以看到大部分的风险分类识别当中都把失业问题当成是人工智能的重要的风险,那么以及把没有办法给现行的人工智能服务提供者去追责当成是人工智能的风险,可是我们知道风险的一个重要特征就是不确定性。
这一类我们刚才说到的影响真的是不确定的吗?
人工智能必然会带来大规模的失业和劳动替代,也必然不能适应传统的法律框架,与其说它是一种风险,我更愿意认为它是一个社会影响,社会变革带来的必然影响。
那么更重要的是,刚才很多专家也提到了,人工智能的风险不同于以往任何一种风险,其重要原因就是在于大家都说不出来它未来的风险具体是什么。
这种uncertainty和unpredictable,大家都是在反复的提及。
那么可能我们提到的人工智能的这种自我复制与自我完善的能力,就是我们将来没有遇见到或者从来没有处理过的风险,但是我们目前并不知道它什么时候会到来。
以及我们可以看到,人工智能这种强大的技术通过开源可以被广泛和容易地获得,这也使得风险的来源大大的变化。
那么第二个问题就是,我们可以看到风险治理的措施,有事前,事中和事后,分别是风险预防,风险缓解与事后消除。
如果我们列出几种典型的风险治理措施,一般是事前我要对风险进行评估,进而把它进行分类分级,采取和风险程度相适应的治理措施。
那么这些风险应对措施和人工智能治理的目标,
手段其实也存在着错位。
首先我们来看人工智能治理的理念,总体来说风险治理的理念其实是有一个修正主义的内核。
所谓的修正主义是说,原来的风险治理是说,如果这个技术存在缺陷和障碍,在它没有得到完全修复并且是可信任之前,是不应该直接应用于实践活动的。
我们可以这样去处理。
我们可以这样去处理人工智能治理和扩散带来的风险,这样去处理大流行病带来的风险,但是我们没有办法把它和人工智能的风险相提并论并且这样处理。
人工智能技术的应用已经成为必然趋势,那么这种趋势是谁都不可以避免的。
我们没有办法说在完全消除了或者确信人工智能没有风险之后才来继续使用。
那么从人工智能治理来说,人工智能的风险是什么?
人工智能治理的工具的层面,那么我们都知道风险治理当中最重要的就是事先的评估和预测,可是我们有个重要的问题就是当我们没有人工智能应用和技术足够深入的在社会当中广泛使用,我们也就没有办法去确切的了解哪些风险将会产生并且具体的程度是什么样。
更遑论我们还有很多我们认为不可预见的风险。
那么如果是这样的话,我们又如何去进行事先的识别评估和监测体系呢?
那么相信呢,一部分不能预测的风险是不能落到这样的治理工具的治理范围内。
那么另外还有就是人工智能治理的前提。
如果说人工智能治理的很多风险没有办法被量化的话,就没有办法被纳入到成本收益的计算当中。
我们举个简单的例子,很多隐私的这种数据泄露的赔偿,
总体来说数量很高,侵权人难以承受,但是受害者个体拿到的侵权费用可能只有几美分。
那么这种难以被个体救济的风险实际上也使得成本收益更加困难。
那么在这里我想用剩下的时间简单介绍一下,
我认为在中国其实已经逐步发展出了修正人工智能风险治理的这样一个路径,
并且随着时间的推移,
中国的人工智能治理正在本土化。
我们可以把它分为探索阶段,定向阶段和系统结成阶段。
探索阶段呢,我个人认为是从15年一直到2022年这几年的时间,
我们发展出了很多的人工智能中国本土的治理手段。
比如说在风险认知层面,实际上我们是有很多的共性部分,
从2015年16年开始一直到2020年之间,
我们有新一代人工智能伦理规范,算法推荐管理规定,
包括个人信息保护法,实际上都采取了风险分级分类和防控的这样一个路线。
在治理手段上呢,和国际上对接的一些共性的部分,
包括个人信息影响保护,个人信息保护影响评估,以及算法影响评估。
但是与此同时,我们也发展出来一些中国自己的特色部分,比如说在前两个阶段,
我们始终在国内的人工智能相关的立法当中,都把发展与安全作为最重要的平衡的一种价值观。
同时,我们始终秉持着中国所特有的国家总体安全观的理念,去进行人工智能的安全治理。
那么即使在网络信息领域这样一个比较特殊的领域,
我们也采取了网络信息生态安全的这样一个概念。
同时,在算法治理和深度合成治理的过程当中,
我们有一个美好的结果,
一个明确的价值排序是信息内容安全,消费者权益保护,以及市场竞争秩序。
那么在治理手段方面,其实中国也有很多自己的特色,比如说初步探索相关的算法备案。
那么尤其在这里提醒大家要注意的是,我们并不是一个准入或者认证的制度,
而是一个信息备案和采集的制度。
那么同时大家如果感兴趣可以关注到,在2021年的算法推荐管理规定当中,
非常有特色的提出了未成年人的防沉迷制度,老年人和劳动者的权力保护制度,
还有未成年人的相关的宵禁制度。
那么这些在世界上都是非常少有的,也是独特的。
那么目前我个人认为,我们国内的人工智能治理体系和理念正在逐步的形成,
并且已经逐步超越了风险治理的理念。
首先在风险的认知上,
我们实际上一直在跟国际保持同步。
大家可以关注到我们国内的相关技术标准当中,
对于风险的分类,比如说失控性风险、社会性风险、侵权性风险、歧视性风险、责任性风险等等,
和国际的很多相关分类是完全可以对接的。
包括一些相关的治理手段,事前评估、认证和事后追责,
我们也吸取了国外的先进经验。
但是我们可以日益看到,
在近两年生成式人工智能迅速生长以来,
我们可以看到很多中国特有的价值理念和治理手段。
那么我们对于人工智能治理的很独特的价值理念,
我觉得最重要的这两年就是不发展,就是最大的不安全。
我们认为最大的风险可能就是中国的人工智能产业和技术没有得到有效的发展。
第二个就是我们可以看到,
我们不管是全球人工智能治理倡议,
还是在昨天,
我们的世界人工智能大会中提到的中国的这种治理方案,
我们都强调要尊重各国本土的价值观和发展阶段的需求,
以及要尊重各国的文化。
那么在治理手段层面,
我们可以看到去年的生成式人工智能暂行管理办法出台之后,
中国已经形成了对于生成式人工智能分层治理的这样一个基本的理念。
同时,
以发展为导向,
中国也开展了很多人工智能基础设施建设的相关工作,
比如说在人工智能的数据要素层面,
我们可以看到国家数据局做的大量工作,
还有工信部做的大量有关算力基础设施的建设工作。
所以,
我们目前在中国的这种人工智能治理理念,
我们与其说它是一个完全基于风险的治理理念,
不如说是一个基于价值的治理理念。
基于价值的治理理念,
基于价值的人工智能治理并不排斥风险治理,
但它超越于风险治理。
一方面,
我们可以看到在中国的立法和治理政策当中,
不仅仅把人工智能理解为一种技术或者服务应用,
人工智能既是未来赋能整个社会的基础设施,
也是未来整个社会生产的组织形式。
那么我们也可以把人工智能放到心智生产力的角度去理解,
另外一方面,
中国已经越来越明确中国人工智能治理的理念和方案是以人为本,
智能向善。
以人为本是在说技术不能偏离人类文明进步的方向,
智能向善是在强调人工智能必须在法律伦理和人道主义的层面的价值取向。
那么在治理手段方面,
我们可以看到目前一个系统发力的情况,
在今年5月份,
全国人大常委会和国务院,
已经把人工智能立法放到了相关的立法计划当中。
同时我们也有日常生活,
日常监管活动当中,
更为丰富的监管措施。
那么在我们相关的一些立法和监管活动当中,
我们可以看到中国人工智能的安全框架也在积极的讨论和预养当中。
那么包括在前天也刚刚颁布了一个国家强制技术标准,
生成是人工智能,
内容的标识的这样一个技术标准。
那么相信呢,
基于价值的人工智能治理体系正在逐步构建的过程当中,
我个人更愿意把它分为三个层次。
第一个是是如此,
就是我们去观察人工智能的本体价值。
那么在其中有几层含义。
首先,
我们希望现在在风险治理当中,
这样一个泛化的模糊的概念被逐渐分离开,
哪些是人工智能的必然影响,
哪些属于近期的维度,
哪些是人工智能不确定的影响。
那么其次,
我们在属性层面要对人工智能有一个判断,
它究竟,
它在这个人工智能被提及的时候,
是以一个技术的方式被提及,
还是服务应用,
是社会生产的基础设施,
还是社会生产的组织方式,
这都决定了不同层面的影响的发生是必然的还是具有不确定的。
那么第二个层面是,
就是基于中国本土价值观,
去判断人工智能治理的短期和长期的目标,
梳理中国的个性化的治理需求。
那么我们特别希望能够分解出人工智能风险当中的规范性的层次,
把个体化的价值标明列明。
那么对于中国来说,
我们首先在技术层面是要安全的发展,
在服务应用上,
在服务应用层面,
延续中国一直以来对于服务应用的治理体系。
那么在基础设施层面,
我们可以看到,
国家各种加大力度措施去促进基础设施建设。
那么在社会生产组织方式层面,
我们也在强调绿色环保,
强调新制生产力的治理方式。
那么在应如此的层面,
也就是在基于个性化治理之外,
我们始终有人类共同核心价值观,
和人类命运共同体。
那么这也是,
我们在参与全球人工智能治理工作当中,
中国提出的方案。
那么也是我在联合国参与的,
我们可以看到,
联合国提出的AI Governance for Humanity,
尊重整个人类的价值。
那么具体的手段措施呢,
我们其实在今年3月份的
人工智能法学者建议稿当中,
有比较详细的解释。
由于时间原因呢,
我就先讲到这里,
感谢大家,
谢谢。
谢谢大家。
谢谢大家。
谢谢你,
Professor Zhang,
这么清晰的,
和适当的,
与AI合作,
负面的解决方案 进行的,
如此的明晰的和适当的展示。
接下来,
我们会有Dr. Mark Knitsberg,
他将将他的解释与我们分享。
Dr. Knitsberg
目前是在UC Berkeley的
Human Compatible AI Center的经纪人主事人。
以及巴克里AI研究的主席
在企业上,他建立了科技企业
使用AI在健康、财务、教育和发展支援上
他在Bell博物馆、Microsoft和Amazon工作
并在企业和学术上发展广泛的计划
马克,很高兴再次见到您在中国
这张图片都是您的
谢谢你邀请我
好的
我首先是一名电脑科学家
我受邀请给您介绍美国的AI制度方式
所以我请您脚下坐带
因为这是我第一次
在美国的AI制度方式上
接受这种课程的第一次
我一直记得
我们现时的情境和智能智能
的极端层面
它是最大、最有能力、
全面目标的电脑系统
我们将它发展到最大
的智能智能系统
我们将它发展到最大的智能智能系统
这是世界第一个
通能链的最大型工业
而且这是再次用的
最大的业界。。。
而至少在美国
AI并没有任何制度
这实际上是被运行到
几乎每个人的活动中的一方
和每个政策中的一方
在每个领域都会发生这种情况。
所以,它是很难去制定的,
尤其是因为系统的黑箱性,
尤其是因为它是一种普遍的技术,
而经常被误解。
那么,是否有美国对AI制定的方式?
我认为,我们与许多国家共同共享有主要的目标。
在美国,有可能有特别的目标,
就是推广美国AI领域的增长。
我们正在寻找美国主导,
有AI的主导。
我们正在寻找美国经济和社会社会的利益,
和国际安全。
我们也希望保护美国的严重影响和结果。
我们也分享了我们的全球目标。
例如,这些目标被表达在许多原则中。
例如,AI的OECD原则。
我认为,
我只是用了这个OECD原则的一部分,
1.4,
这就是关于安全的目标。
所以,
目前美国的方式和制定的心理风格被捕。
目前的过程中,
有些组织正在推广竞争的利益,
并发生了三种可能的法律。
第一个是从去年10月的行政命令。
这个行政命令主要目标是大型模型,
指导现有的国家公司。
第二个是从去年10月的行政命令。
这是AI的承诺,
对职业、法律、防御,
以及违法的影响。
第三个是在加利福尼亚,
我们的某些规则是来自美国的。
有一个法案,
叫SP-1047,
这个法案是关于安全和安全的创新,
与前线模型的。
现在,
这些问题有些问题,
讨论继续,
但这并不是法律。
有些国家有某些法律,
也有某些具有特定的法律,
但是这些大型公司,
这些大型的框架,
并没有那么长时间。
然后,
我们在过去几天的时间中听到,
当您在使用
全面目标技术时,
有些挑战性,
例如,
它很难证明,
一个系统有接受性的低风险,
如果是全面目标。
您实际上需要知道,
您正在测试什么。
美国的法律系统的一部分,
就是在各个领域中,
有很多存在的法律。
因此,
这些对于在这些领域中使用AI的系统
使用的系统,
例如,
健康、
直轮、
農业与贸易,
一些需要更改。
例如,
车车的车车有很少的法律,
但在很多情况下,
正是有实际的国际法律,
给了很好的基础。
然后,
有些法律,
涉及正义的法律,
正确的分配费用和利益,
例如在职业和借费值的分配方面。
另外,在美国,我们经常遇到一些批评,
你可能听过,
这可能是一个批评在创造法律的状态中,
其实是有效的。
例如,在智能货币的情况下,
有关某些训练数据的用途,
例如用途类似性的用途,
或是创造职业的方式的用途,
这些创造职业的方式,
通过批评,
并在生产保护上的法律上再次通过批评。
还有其他因素,
包括单纯的保险能力。
如果您在卖AI系统的系统,
您想得到保险,
您需要证明,
有一定的保险能力,
有一定的保险能力和可靠性能力。
还有其他制度来源的车辆,
例如,美国的电脑服务法,
这些车辆在美国的公司上受到影响。
我希望请您
在几分钟之内谈谈这些误判。
我会提醒您,
我并不是代表我的国家,
我自己在说话,
但我认为有一个误判,
就是制度是一个重要的阻塞,
是一个重要的阻塞,
是一个重要的阻塞。
我认为,
如果我们正在谈判保险,
那是一个必须的指导。
这在很多其他技术上是这样的。
我认为这没有什么区别。
我认为,
经常说,
能力,
是,是,
但是在杀伤中,
我认为,
当你看到,
无论是什么系统,
也要更加有能力,
这并不 necessarily equate
与医学状态的立策,
而我认为,
我认为有时有些混乱在我们所称为的红线之中,
以及提到要求发展者停止发展更强大的系统。
这并不是一个红线,而是一个相反的概念。
我认为我们可以学习从现有的国际安全组织,
例如风险安全、核武信息、电信组织等国际安全组织。
他们的工作方式是,他们找到共同的基础,
我们合作,我们能合作,
我们能合作,我们能合作,
我们能合作。
如果我们不合适,
我们要么寻找某种正常化的协议,
或者直接将它转移到那些领域。
我认为这可以合作国际安全组织。
看来我现在有时间给你们一点技术观点,
从我们的中心来看。
我来自人类合作AI中心,
我和他们一起工作,
我想给你们看一看,
我们认为AI是正确的方法,
对安全AI系统来说。
我们现在正在做,
你今天也听到很多人说,
要把AI保守。
有很多技术,
有很多技术和方法,
例如LHF,
和法律性AI,
你有一种AI,
看另一个人的出口,
测试,
测试,
测试,
红图组织,
和我们听到的,
 digital neuroscience,
和,
和,
和,
和,
和,
就是英文性AI,
 десятомualoning上的AI numeralist和
核准实际غ运运设施,
这个我认为已经开始,
是的,
这些方式!
都是讨论命运以鼓起
不会了解的系统,
如果我们想用它们作为 protocol,
我们需要更多的光芒~~
使我们能为机 grol
AIS安全,
 obe in an example,
包括语言模式!
但在我們中心的長期化中,我們正在努力製造安全智慧技術。
為了這樣做,我們認為是合理的回歸透明、解釋得來、
解釋得來的方式。
為了這樣做,我們認為是合理的回歸透明、解釋得來、
解釋得來、解釋得來的方式。
這就是我們從中心的觀點。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
謝謝您的訪問。
最近200年来科技的发展对于人类社会的进步产生了非常巨大的影响
我认为19世纪的内燃机加电气技术
使我们人类有了生活的自由
那么20世纪互联网加通信技术
使人类有了信息的自由
21世纪大模型加脑机接口
使我们有什么呢
现在很难想象
但是至少是有了创新的机油
甚至是从无到有进行创新的这样一种可能性是展开了
也就是说它可以赋能社会赋能人类
会带来福利
但是确实会引起那样的安全上的问题
我们可以看
看到大模型出来之后
至少是有四个方面的问题
因为大模型大量的使用数据
那它可能会引起隐私方面的忧虑
那么另外
我们可以看到大模型
由于它的能力泛滑造成了幻觉现象
可能会引起虚假信息
以及诱发各种各样的犯罪
那么另外一个在知识产权问题上
它也可能会引发这样一样的复杂的问题
还有一个它可能会使得整个社会治理的中枢机关出现漏洞
那么最后会导致信息社会发生功能障碍
那这些的都让我们感觉到不安
那问题是我们如何处理这样的问题
当然我前面讲到了科技给人类社会带来了巨大的福利
那么这个就是如何在这两者之间
进行平衡
那我们可以看到在亚洲在非洲这些国家
对他们来说发展是一个更突出的问题
他们希望能够实现一种互惠的科技的发展
那么当然我们知道像美国日本还有中国
非常强调在发展和安全之间寻求平衡
那么另外一方面我们可以看到欧盟最近通过的人工智能法提出来了
安全高于发展这样一种价值取向
那如何在这个中间找到适当的平衡
那我们可以看到各个国家有不同的立法模式
那么我想这个是值得我们进一步探讨的问题
谢谢分享
接下来想请教王老师
您作为研发机构的代表
我们知道上海人工智能实验室也做了很多安全对其品质的工作
包括昨天周主任也提出了一个安全发展项目
就是人工智能发展的45度平衡率
想请您来解读一下您认为的安全和发展之间的关系
好的
其实昨天周博文教授在社认大会的这个上午的大会
他在会议上他分享了我们实验室对这个问题的一个很重要的一个判断
他称之为一个技术体系的一个思想
就是人工智能的45度平衡率
现在整个人工智能发展我们认为是跛脚的
无论是从它的
大部分的技术和算力资源都投在它的性能的研发上
虽然我们有这个IIS这些既兼顾性能和安全的技术
但总体上还是偏重于这个性能的
就导致我们实际上从技术社区来看
投入到安全方面的资源 人力和算力都是严重不足的
而且我们的技术方法目前还是后续的很分散
我们是希望能够找到一个能够安全优先
同时能够兼顾性能增长的这样一条技术的路径
当然这个技术路径的探索是非常复杂的
也非常艰辛的
我们也是周教授发出这样一个呼吁
希望能够我们沿着这条45度的线
去推进人工智能的发展
当然这个过程当中其实
如果这条路径能够走通的话
我相信目前人工智能的安全风险
面临着很多挑战
它从基础上可以得到很重要的一个保障
我们也在做这方面的努力
所以我想强调的是
其实我们需要有一个大家共识性的
这样一个发展路线的框架
我们不能长期低于45度来发展
但如果长期高于45度
也很难市场商业化的要求
但是我们是不是有这样一个理想的路线
大家共同来努力
这个路径我觉得还是很大可能能够实现的
谢谢
而且可能需要动态调整去灵活应变
接下来我们可能想聊一下
中国人工智能的一些治理挑战
和独特的机遇
Gail
我们可能想聊一下
最近其实中法的人工智能治理声明之后
其实中国国内对法国的情况
其实也都非常感兴趣
您能介绍一下
在法国的人工智能
有哪些独特的机遇和挑战吗
以及可能我们也是到2025年初
会有一个AI行动峰会
您能谈一下您对它的期待吗
非常感兴趣
是的
我可以谈一下法国的具体性
如果我们回过头来
法国的具体性
可能会比其他国家不同
我看到的一件事
在法国不同的是
我们目前有一个相当困难的
社会和经济讨论讨论
在整个国家
所以在社会层层上
有很大的缺乏信任
无论是与工作人员
和决定者
或是在城市之间
或是在附近的区域
这也可能会有其他国家的影响
但是在法国
这也有很大的缺乏信任
另一个形成的讨论
是经济
历史上
约翰是一个
比较缺乏的数字经济经济
我们是数字服务的
数字服务人员
这形成了我们的关系
与数字服务
然而
法国的AI能力非常强
所以目前
有一个改变
在经济线上
而那是
在城市中
带来很多希望
和正面的激动
现在在城市中
这是个不一样的事情
而在政策方面
这种规划
正在进行中
这种规划
在各种方向上
第一个是
投资
所以
有增加的投资
不仅是在
数字服务中
也有增加的
在数字服务中
我们看到的
是开放的
和公共的
价值
我们看到的
有增加的
公共业务
和数字服务
与公共业务
与数字服务
而
另一个挑战
是
与每个人
在不同层次的社会
之间
链接
而这里
我们看到的角度
是
维持人类联系
而不是只有数字联系
所以
这就是我们的挑战
要避免数字区别
我们有数字区别
我认为每个国家都有数字区别
我们在那里投资
来执行讨论
这就是我们在法国的
这种规划
谢谢
谢谢
数字区别
我觉得是最重要的
张领和老师
我们想请教一下
我们知道您今天起草
发布了一个
人工智能法的学者建议稿
然后刚才在演讲中也提到了
中国风险治理的修正
治理本土化
您能在整体上谈一下
您认为的中国人工智能立法的
整体逻辑和整体架构吗
谢谢
回答这个问题呢
其实我觉得可以从两个角度来考虑
中国的人工智能立法
第一个层次
我们要把它理解成是
中国的制度名片
在全球共同去认为
应该处理人工智能风险的时候
中国作为一个人工智能技术
相对领先的国家
作为一个大国
如何去打造一个
我是一个负责任大国的
这样一个形象
同时我们也可以看到
实际上在人工智能治理的工作当中
中国已经做了很多很多
中国是世界上唯一一个
对于人工智能安全治理
覆盖了国家规划
法律
行政法规
部门规章
技术标准的
全方位覆盖的国家
也是目前一个
对于大模型的治理
已经实际落地的国家
但是
我们在人工智能安全治理方面
所做的努力
并不为世界所知道
其中一个重要原因就是
我们缺乏一部高位阶的法律
去作为中国的制度名片
让大家知道
中国曾经做了
中国正在做那些事情
或者已经做了那些事情
那么在联合国的工作组里
我们大概一共有30多位专家
其中我在这个组里
有一部分的工作
就是要告诉大家
我们中国已经做了什么事情
让我很惊讶的是
作为国际上比较集中的
对人工智能治理很了解的
这些专家们
其实也有大部分
并不太了解中国
在产业治理当中
做了什么实际的事情
我认为这是人工智能立法的
第一个最重要的目标
第二个就是要
符合中国本土的治理需求
我非常愿意把中国的
在世界上的位置定义为
领先的追赶者
领先的追赶者意味着什么
一方面你作为一个大国
要去治理好人工智能安全
不能让中国成为
人工智能治理的
洼地
但是另外一方面
我们也要充分认识到
中国人工智能的
技术和产业发展
需要大量的法律法规的调整
去为人工智能技术产业
的发展提供要素
提供资源
同时去为人工智能产业
设置一个合理的
法律责任的框架
那么在这两个理念的
指导之下我相信
不管是什么样的制度
经过我们充分的讨论
希望将来能够形成
既作为中国制度名片
成为中国在全球
人工智能治理当中
重要形象代表的一部
人工智能法
同时这部人工智能法
也能有效的
防范人工智能安全的风险
促进人工智能安全
并且符合中国本土技术
和产业发展的需求
谢谢
谢谢
中国学者很多工作
需要更好地去对外传播
其实我们也做了很多
帮助中国的
不管是安全研究
治理还是立法工作
对外传播工作
希望能更好地
展示中国的工作
接下来我可能想聊一下
最近比较热的
人工智能安全研究所
这个事儿
然后
自去年英国AI安全峰会以来
其实已经有大概
美国英国法国
在内十个国家
还有欧盟
都成立了自己的
国家级的人工智能研究所
包括一个
全球性的研究网络
这个我可能想请教Mark
你会怎么看待
这样一个
人工智能安全研究所
你觉得它
它们为什么要成立
它们要做什么工作
以及未来会做些什么
好的问题
谢谢
我只想确信
我将将回答
中国在进行AI管理的
信息
所以我们在中心
我工作的地方
加入了美国
国际AI安全系统
是你在说的吗
国际
是的
对
我说
中国在美国
是一家
设计人
不仅仅是研究这些强力系统安全行动的问题
但是也成为了一种监察标记,以查看严重的事件,
并且在某种意义上,在国家为主要的方式中,确认什么应该被测试。
我曾经参与一个讨论中的讨论中,
我们在一起的时候,
我们在一起的时候,
我们基本上是谈论是否是必需的问题。
我相信 it makes perfect sense,
but it is not as a necessity that I believe,
At this the USAI safety institute is you know as a worthwhile endeavor.
谢谢。
我们又希望不同的国家的AI-安全之间有更多和个,
包括在中国的机构之间。
我们这个论坛是前AI安全之旅,所以一个重要的议题也是讨论怎么去减轻所谓的AI潜在的战略性风险。
我想还是请问季老师,我们知道这个AI战略性风险其实目前有很多不确定性,包括它的严重性,它的可能性,它的紧迫性层面,大家都有不同的理解。
那么在存在这种不确定性和不同的没有共识的情况下,您觉得怎么能推进这方面的政策和治理呢?
季老师,OpenAI提出来了一种非常有影响力的方法,是价值对齐,包括人工智能和人类的价值对齐,以及国际的价值对齐。
刚才张林涵教授在主持演讲中也提到了价值,以价值为基础的这样一种人工智能治理问题。
我的观点稍微有一点不一样,因为价值对齐是一个很复杂的问题。
我认为沟通。
沟通程序更重要。
那为什么这种方法更重要呢?
首先我们知道当我们谈人工智能安全的时候,我们其实需要有两个视角,一个是监管者的视角,我们中国采取的国家备案制以及验证技术,对吧?
那另外一个是用户的视角,我们要强调可解释性,要跟用户之间进行反馈。
那这个过程我们觉得就程序性的过程,沟通的过程。
非常重要。
那在理论上我们可以看到,就是一个是技术性程序公正概念的提出,那另外一个是一批中国学者今年年初在自然杂志的子刊中提出来的仪式性的对话框架。
那强调在这种一种对话的氛围中,可能对人工智能的可信性是具有非常重要的意义的。
那另外一个呢,就是说在这个过程中呢,我们实际上看到就是要把这种程序公正监控嵌入到人工智能系统中去,或者让人工智能系统之间产生制衡的作用。
那这个呢,我认为就是技术性的这个程序性的这个安全监管的一个非常重要的内容,在这方面其实已经有些很好的先例,一个是新加坡。
那个何先生刚才也提到了这个新加坡的经验,新加坡的AI Verify是一个很好的示范,当然它具有普遍意义,像美国的这个Watson X Governance是一个监管模型。
那么日本就在上个月,6月4号发布了综合创新战略,这个中间呢也特别提到了它有三个基本方针。
那么在人工智能这个方面呢,强调安全与竞争力这样的一个平衡,也强调技术性的侧面。
那如果我们把这些因素放在一起的话,那就是说通过科技公司的技术能力的提升,使得人工智能的技术能力提升,来加强它的安全保障,这个思路就成为可能。
如果我们从这个角度来看的话,那么今年3月份,
北京人工智能安全共识提出来的三分之一的研发预算投入到安全保障领域,这就是可以接受的了,可以理解了,科技公司就认为这是可行的了。
如果这样的话,我们就用人工智能的治理,可能达成更广泛的共识,而这个共识呢恰恰是立法的基础。
那王老师您会如何回应刚才季老师的?
对,我对这个,这个沿着这个季老师讲的话,就是我们提出一个这个这个,
概念吧,也是和薛兰老师这个我们清华大学和焦大一起研究的,我们希望提出一个概念,就是人工智能是全球公共体,
就是AI safety as global public good,
是什么概念呢?其实我们看到现在全球的这个对人工智能的安全风险的讨论,往往是把AI安全作为一种风险去监管的,
这是风险管理的视角,我们还有其实可以补充到另外一个维度就是,
刚才季老师提到了,我们更应该,
现在做的是,去把人工智能安全作为一种公共的产品,
政府,企业,第三方,公众,一起来共同建设,
共建,共知共享,来提升这个人工智能安全的相关的知识,能力和资源的共享,
这个是非常非常关键的,大家其实对现在人工智能安全有很多的担忧,
这个担忧的前提就是,大家对,无论是政府还是企业,还有这个,其实大家对人工智能安全风险的知识是不够的,
在当这个过程中需要大家补充充分的讨论,刚才季老师也提到了,
包括科学的研究,所以我们这个,关于人工智能安全的知识的形成过程,需要大家共同参与的,
这本身是一个需要加大供给的这样一个安全的维度,
另外就是人工智能安全的能力,我们是否有足够的技术手段,
和相关的工作去支撑人工智能安全的提升,
再一个就是,我们是否有足够的人工智能安全方面的资源,
的投入,或者是公务服务的产品的开发,
这个是一个很重要的问题,
这个是非常非常关键的,比如说,去上海我们也来做相关的工作,
我们经常举办一些沙龙,各种各样的形式,其实是用柔性的方式,
把不同的主体来共享它的知识,共享它的对风险的认识,
达成一些共识,我们共同去推进一些安全的基准,标准的建设,包括资源库的建设,包括治理的平台的建设,
我知道新加坡其实也做了很多很好的,类似的工作,
我想这也不是就是说,上海和新加坡独有的工作,
而是全球各方面多的做法,
做这个努力,所以我们希望提出这个概念就是,AI safety as a global public good,
全球都应该共同去建设这样的人工智能安全的公共体,
特别是像美国,欧洲和我们中国,其实原有的人工智能发展基础
比较好的国家,应该去合作,
来加大对于全球人工智能安全的公共体的供给,
有很多的发展中国家,可能他们原有的基础没有那么好,
这个资源图,
是比较好的国家,应该去共同建设,
这个工作其实需要科学家社区的合作,
需要
企业智能合作,需要在不同的联合国,不同平台上去做,
我觉得这个其实,如果我们形成这样一种共同认识的话,
可能后面我们很多有意思的工作可以共同推进。
非常遗憾,今天时间有限,所以可能意犹未尽,
也感谢大家今天分享的交流,期待以后有更多的时间。
谢谢大家的关注。
谢谢我们的主持人,这次的讨论非常明显。
我们现在将进入我们的国际协调的第四个和最后的课程,
我欢迎我们的下一个主持人,Tino Quellar。
Tino Quellar是全国协调的10位国家协调总统,
是一个社区,
执行协调,
执行协调,
并在国际协调、争议和执行协调上进行独立研究。
他曾经是加拿大法院主席,
他曾经在白宫和国际协议中担任三位美国总统,
他曾经是史丹利·摩尔森教授,
在史丹佛大学,
他在法律、政治、科学和国际事务上担任,
并在国际研究中带领Fremont-Fogler Institute。
他担任国际协调的国家协调总统,
并担任威廉和佛罗·毗奥勒基金会总统的总统。
我相信现在是Tino的时间区,
Tino,谢谢你这么多时间给我们留下来。
让我们把手放在一起,
让我们给他一个温暖的欢迎。
Tino,到你了。
谢谢你。
谢谢,谢谢。
我现在在墨西哥,
但是我现在正在美国,
但是我感到很高兴能与您一起,
我绝对不会想要错过这个机会。
这将确实成为美国的更重要议会,
与美国的现代智能讨论讨论。
现在我已经在几年之内成为了美国的总统,
但是你可能知道,我已经在近15年之内进行了机构发展讨论论论。
我想和您分享一个我们在AI安全的主题,
我看到的,
以及在世界上的AI安全讨论讨论论,
它们在国际协调中,
在中国的国防讨论讨论中,
在中国的国防讨论讨论中,
在中国的国防讨论讨论中,
在我身边的几十年来,
我的团队,
卡尼基协会,
帮助了NAP大使团队,
在世界上的支持问题上,
在持续和理想的平和中,
今天,
如你所知,
新的机会和挑战,
在国际健康和国际安全上,
新的机会和挑战,
新的机会和挑战,
在这个变化中,
我们的进步和我们的专业讨论论论中,
我希望您记住这四个点,
我希望我们能够进步讨论讨论讨论。
首先,我们看到的进步是什么?
我们看到的实际和
实际和实际的变化。
讨论的差异,
不仅有程度的差异,
并不仅是微稀的,
而同时也有重要的。
讨论的大幅升值,
包括讨论的表现,
很重要。
如果您知道我们在社会上,
要尝试讨论,
确实,
我们没有只在超过
建筑物的水平,
而是把整个讨论应变成
很大的增长高度。
在现实上,
在 frontier level models上的设定上
我们现在看到
frontier models
在2010年以来
有10亿次的设定上
比frontier models更加有设定
结果的系统
正在解决更复杂的问题
创造更具体的词和画面
chatbots和其他技术
从小到快
正在进行日常生活
他们正在改变我们的工作
我们学习和与世界联系
Catalyze and Much of This Change
是一个AI活动的系统
但我认为它更加是
政策主义主义
关注政策
而这个系统
包括了新加坡
纽约
伦敦
深圳
上海
北京
部分加拿大
以及其他地方
这些产业
正在努力推进AI创新
和研发
我对这些产业
所考虑的
有关答案
也非常关注
第二个问题
我希望提出的
是各个机会
和风险
都造成了
这些发展
的变化
这些风险
都造成了
这些发展
的变化
所以
安全
应该是
从认真的
而从多种方式
例如
避免车轮
和车轮的被破坏
中级安全
如保保复权
或公共服务
不失误
此外
还在管理
或智能侵入或失控最高的AI系统
这些计划确实没有那么紧急
但这些计划不可能不适合线上的系统
你也知道AI持有巨大的承诺
承诺我们全世界遭遇的许多挑战
简单和复杂的医疗任务
并扩展中心服务
世界大部分人口的4.5亿人正在缺乏
想想复杂教育的目标
250亿的学生正在全世界外出
城市服务与执行
两三成的世界都预期在2015年
将在市区居住
实现这些利益
只能够实现
如果我们也谈谈
线上计划的风险
这些计划可以创造大量内涵
传输资讯
减弱资源
进行成功的资讯攻击
如果在国内使用
军队和国际安全的设施中
进行高层的执行
坦白说
正如你们所知
目前没有证据
现时的计划
可能会遭遇失控
或是
能够使用生物兵器
但这些风险
可能会在未来发生
实际上
在AI的前沿上
在我认为
这并不合理
未来将会给我们
一个能够
在网上
与其他AI系统
和人类的关系
互动
人类的关系
有时会
与人类的社交运动
相互启动
或是
与人类的社交运动
在国内的决定
和公司
会更加受影响
这些系统
最终会
将人类的决定
与人类的决定
的分别
决定
将会
变得
更加
传统
这些改变
将会
需要
国际协调
这就带我
到第三点
当我认为
AI的决定
在决定
最高的系统
在决定
决定
这些系统
总是会
有
一些
国际协调的
变化
但事实上
这些协调
会也被影响
他
对生物技术的
改变
和
界纪和
情况的
反映
21世纪
美国和
中国
将
双重
的
保持
国际
系统
其中
也会
能
更能
给予
全人类的
增长
能够帮助我们
解决未来的问题
和准备未来的问题
这意味着
我们要专注在
AI的危险方面的机会
并且提升AI的利益
并且提升AI的安全
因为两个目标
必须相互联系
是的,是真的
竞争是一个
主要的功能
在判断
人工智能的 domain
并且不可能完全消失
但我们也看到
国际协调的显示
包括中国和美国
让我给你们一些例子
进步在美国
在2020年11月
在美国的第一次
AI安全会议上
28个国家
包括中国和美国
签订了一份
专业的国际科学计划
在AI上
最后的计划
与AI的SOUL会议
相关
在2024年5月
在SOUL上
10个国家
加上欧洲
同意建立了
国际安全研究所的
联合联合研究所
实际的承诺
继续在这次参与
参与这次展开的
趋势是重要的
在2025年
在澳大利亚参与的
参与的翌日
在此
我认为
在Bletchley讨论中
有所进步
对美国的角力
在Bletchley参与中
总统布泰罗斯
在Bletchley参与中
开始思考
和宣布
一些现在
变成了
美国联合研究所的
角力的想法
在2023年10月
总统布泰罗斯
建立了
AI的高层监督组织
包括
中国 美国
和其他国家的参与
这次参与者组织
将在2024年
将在明年
参与AI的参与组织
提供的最终建议
将在2024年
参与AI的参与组织
提供的最终建议
在2024年10月
总统布泰罗斯
实际上
执行了
美国主导的
AI的高层监督组织
而几天前
总统布泰罗斯
执行了一个
非常类似的设计
支持中国
来自美国
我认为
这些计划
非常鼓励人
这些计划
在国际上
和这些AI参与组织
的程度上
能够
和美国的
参与组织
相似
第四
国际协调
包括
在这些区域中
在我刚才介绍的
计划中
包括
更多的国际
一些
我给给的例子
包括
国际协调的高层监督组织
描述
外部政府
有
能够帮助
AI安全
向前移动的
演员
例如
最近发布的
研究报告
关于
AI安全
计划
许多国际政府
同意了
这个计划
而美国
正在
进行
这个计划
也许
与外部演员合作
陆续研究
还有其他专业
订出了这个计划
随着儿童
提出的计划
并进行
审查
计划
是将会
创造
AI安全
的一个
最强化的
基础
这将
帮助
晓得
世界
和
AI的
讨论
发展
讨论
一些
的
政权
我希望
国际
这些
项目
当然,有时候会有困难,
为了鼓励维持协调和管理AI的风险和利益,
重要国家如中国和美国都同意和不同在某些问题上讨论。
这是理解的,
根据国际安全批准和主导AI的国际发展。
但这种现实并不代表有定义的目标。
通过协调,
在AI的风险被理解和批准时,
利益大幅增长,
国家能够成功讨论和提供负责任的AI管理。
中国与美国都对AI的科学进展有兴趣,
以提升安全的过程,
以共享正确的测试和测试,
在无法实现的军事系统上。
并以监视和准备未来出现的坏坏风险,
但未来可能和可能会发生的,
例如AI系统的失控。
对我来说,
我认为,
这次的聚会能够提供一个机会,
提高计划在这些问题上,
并使政府在任何情况下合作。
在AI上,
互联网与其他挑战中,
互联网与其他挑战中,
互联网与其他挑战中,
这是一个困难的时刻。
但这也意味着,
互联网与其他挑战中,
互联网与其他挑战中,
更加需要更多的互联网与其他挑战。
而这要求我们承担谈论和合作,
承担谈论和合作,
尽快,
专心地,
尽快,
尽快,
因为这将使世界成为AI的更好的位置,
更加尽快,
因为这将使世界成为AI的更好的位置,
为人生而提供的所有能力。
因此,
我期待与邢学和邢学会议讨论这些问题,
我期待与邢学会议讨论这些问题,
以及更多的讨论
这个有助与重要的讨论。
非常感谢您。
谢谢你,
邢学和邢学,
为您的承诺与国际协谋。
请您留在网上,
请您留在网上,
并在我们一下子后还边边插来电话。
觉得 Möglich to introduce 邢学兰。
邢学兰穿过 Chair distinguished professor
和是中国国际精华交通协议的前川知事,
实任调 transcription of
Iinternational Governance Institute
Dean of Schwarzman College.
China Institute of Science and Technology Policy
China Institute of Science and Technology Policy
与SDGs Global Institute Co-Director Agree to
Currently,
He also serves as a counselor at the State Council
as a counselor of the State Council
 Pendant of both Creative and Integrity
Chair of China's National Expert Committee on New Generation AI Governance
and a member of the Standing Committee of the Chinese Association of Science and Technology
Din Xue, it's really an honor to have you with us today, the floor is yours
Thank you, actually, okay, I think President Quilliar and thank you very much for staying so late to join this dialogue
and I have to say that actually I did this PowerPoint just while I was there listening
I was burned out yesterday by the
big screen and not necessarily the most comfortable way of using PPT
so I was actually, I decided not to use it initially
but then I actually saw the wonderful presentations you made
and I realized that some of the graphs actually it's better to be shown
so let me try
Okay, I think they basically, I think the role of governance
and safety and so on has really, I think it's amazing
I think it's amazing
that it's getting, you know, recognized
and we've seen the recent efforts
of course, it's already been, you know, UK Safety Summit and Korea
this one, this year, EU safety law
all in the last, you know, 12 months
so I think this is really strong
I think it's a wonderful sign to that we are all paying attention to this
but of course, partly this is really based in indeed
I think there's a huge recognition about the AI risks
of course, I think there could be, you know
very specific one
but also there was also the concern about the autonomous AI system
that can really, you know, be getting out of control
I think so because of this
I think that in, I think that of course there's a lot of, you know
countries have already taken various measures
to try to address the governance issue
many of them are, you know, more looking at the domestic issues
other also looking at the global issues
so I think, you know, Lingha has really done a wonderful job
in talking about China's AI governance in the system
so I think this one I'm more of a looking at the global issues
so I see that globally there are, you know, some major challenges
I think in terms of how we actually we can address the governance challenge
I think the first challenge
the first one of course it's not, you know
stranger to this audience
the challenge from the so called the pacing problem
that's so called the, you know, the
you know, technology really moves so fast
while the, you know, political and institutional changes
are moving much slower
so in that sense there's always that gap
so how do we address this problem
so I think that's sort of the
the first one I think everybody knows
I think the most
I think recently I feel there's another new challenge
that's sort of emerging
I think from some of the discussions I've heard
it's about the direction of technology development
because I think so far I think everybody, you know, saying
okay, now we need a lot of computing
and we need to increase the power of the system
and then just the scaling law will get us, you know
to some, you know, future directions
but I think I think I again
you know I just anecdotally I've heard some
you know leading experts and scientists
at the beginning to question about whether this is the only approach
and whether there could be other ways
to think about how do we actually
achieving the
more proper and healthy development of the
AI system
so I think that certainly I again
I don't know whether that's being
shared, you know, by many other experts
but I can see that
you know gradually there might be some
new thinkings about this and
in technical communities
and the third one is institutional
I think here's where, you know, I'm more familiar with
is what I call the challenge from the
regime complex problem
I think actually I think
Tenno has really touched on this one
that is
that's, you know, I think this is a
basically talking about an area of
partially overlapping
and the non-hierarchical institutions
governing a particular issue
previously I worked on one issue
it's gene data
you know the governance of gene data
I think there is very similar issue
but here I think this is pretty much the same
that you have AI issue
that I think a lot of institutions
a lot of, you know, mechanisms
that actually are related to
to the governance of this
but they don't have a hierarchical relationship
so some of them, you know, like professional organizations
you know, foundations, legislative bodies and so on
they all have their own, you know, ways of governing
a particular issue
and actually that's sort of
the situation we're in
I think Tenno actually has touched on this as well
so I think how do you
coordinate all these institutions
how do you really
put them together to
for companies to be able to
figure out what's the way to
to follow
so I think this is the
you know the third
I think the last one I think
again I'm very glad
that Tenno touched on this
is a challenge from geopolitical problem
this is the big elephant
that in the room
that often people don't necessarily want to talk about
I think that
you know I've studied
you know S&T policy
the science and technology policy for
many years
and watched the US S&T collaborations for many years
so what we've seen
that since 2017
there's a
very rapid
deteriorating US China
you know
S&T collaborations
so I sort of came up with this graph to
show it you know clearly
along two dimensions
to say how
you know the collaboration between
scientists of the two countries
whether this is generating some benefits
for national security
or whether this is generating some economic
you know benefits
so we can see that
in one area there's
Q1
you know it's
neither
neither you know
in national security
nor in economic benefits
so basically this is
you know useless research
the basic research right
the basic research is in Q1
Q2 is that
basically enhancing national security
and not economic prosperity
and this is what so called defense
or dual use technology
and the third one
Q3 is commercial technology
for economic benefits
but not national security
and fourth one is
that can be both
that's what
let's put frontier technology in there
so I think if we
if we do this
we can
you know
very roughly
you know divide technologies
and the research areas into four
major quadrants
so before 2017
Q1 is basically following the
principle of international S&T corporation
and Q3 is commercial technology
you have WTO trips
and then Q2
there's a
so called
WASENA agreement
on export control
that US controls the export
of those technologies to China
so that has been in place for
many many years
Q4 I think it's
it's a question mark
I think it's a new
frontier technology
and frontier research area
and I think
you know really depends
so I think that
previously was pretty much
I think four quadrants
was in that place
but since 2017
the policies on Q2
was being pushed in all directions
including on Q1
Q3 and Q4
so that has been the case
since 2017
I don't have to repeat the stories
about the
you know the China initiative
and many of the scientists
who were persecuted
and so on
so I think
that's the current atmosphere
we are in
I think that
so I think that
when we talk about
you know collaborations
on the
on
you know
AI governance and so on
but that
that is the situation
we are in
and that really generates
a cheating effect
for people to
work together
first of all
you know
I'm sure that
many of the
our US colleagues
that when they
come to China
they may have to report
to their institution first
or they may have to
write a report back
to say
okay what we've done in China
and so on
and also I think
for many companies
many of the companies
you know I was involved
in some of this
you know
whatever the tracks
dialogues
many of the companies
they don't want to
to attend
because they are concerned
that they may be added
to
entity list
so I think
when
in that kind of
atmosphere
when you talk about
collaboration
and so on
I think
it's very hard
I think
to be realistic
and
so that's
the situation
we are in
and
let me find
before we
get to
dialogue with Tino
I think
I'll just say
what are some of the
possible ways to
address this challenge
and I fully agree
and that was also part of the
you know
the paper on that
calling for increased research
on safety and governance
and
I think one third
might be
too ambitious
let's say 10 percent
let's start with that
but I think that's something
that we certainly
need to do that
the other things also
we need to do a lot of
maybe some joint
international research
for example
particularly on
crisis management
because why actually
we can
you know
in
addressing risk
you know
risks and
addressing
so called
crisis management
you want to have
so called
the
you know
you have a
plan to
address the
contingency plan
to address those
emergencies
so for those kind of
contingency plans
I think
we need to
have technical people
to work together
so I think
there's a lot of
idea
on these
second issue
how do we address
the pacing problem
I think
we've
in the last few years
we've been calling
and trying to
you know
to talk to
you know
and
to write and talk
to different agencies
about so called
agile governance
meaning that
the
you know
the government
doesn't have to
you know
come up with
comprehensive laws
like the EU law
AL law
but rather you can
act quickly
when there
you see some
signs of problem
then nudge
and then otherwise
you know
you can do more
I think that's
sort of the
at least
one thing that
that the government
can do
but also of course
coming up with
different measures
and I think
Ling Ha has already
gave a very
excellent
description
and the third
I think
we should also
think about
not just to rely
on the government
industry
self-regulation
but also
very useful
I've heard
you know
stories from
my colleagues
talking about
the US
nuclear
operators
they have an
association
among themselves
actually
they do a
a wonderful
I mean
they actually
they self-regulate
in many
very very
strict ways
you know
you have
many of this
nuclear
reactors
that they
have
you know
minor
mishaps
and so on
but they all
have to report on
to this
community
so that they can
study
and they can see
what can be learned
and how actually
they can avoid that
I think that kind of
a self-regulation
I think would be
very useful
I think that
we probably
should also
think about
how to
revive
you know
let that
mechanism
to work
And finally
on the international
it's great to see that
UN has
stepped in
and playing a very
important role
in having this
high level
expert group
and I
hopefully that
the report
and recommendation
will come out
But at the same time
as
Tino and I
probably both
agree that
this is such a
complex issue
that it
would be very unlikely
to have any
institution
to be able to do
a hierarchical
and top down
approach
to
govern this
So I think
multilateral
you know
so network
kind of
a system
that might work
and some
may
work on some
one area
and some others
may work on
other issues
But here
I think
we probably
need to separate
three
type of issues
I think that
maybe that
in the future
we can
have more time
to discuss
the first is that
indeed
I think this
type of issue
mostly a
domestic
kind of
regulations
on the use of
technology
We recognize that
there is of course
such a
wide difference
in
different culture
and
and
you know
legal environment
economic environment
and so on
So I think
there bound to be
differences
in the governance
of
AIS use
in domestic
the second is
more of
really for
international
communities
I think
for example
the
existential risks
I think
those are
you need to
work together
internationally
I think the third
category
would be
somewhat
difficult to
manage
but also
we have to
be mindful
is the one
that is
the kind of
domestic
risks
that may have
domestic
externalities
of course
there could
also be
international
regulations
that have
domestic
externalities
So I think
that's the
three categories
of
issues that
we can
probably
try to
address
And finally
so of course
the US China
ribery
how do you
you know
how to
address that
issue
I think that
that
but at least
I think the
minimum
I would require
is
to request
is to see how
actually we can
provide some
safe space
for our
technical communities
for experts
in this group
and in
many others
in this
menu
for them
to be able
to work
together
they don't
have to have
the fear
they can
actually
you know
the technical sense
so without that
I think
a lot of the
things that
people talk about
would be
impossible
thank you
Thank you so much
Dean Xue
for your candid and
succinct outline
of the challenges
and solutions
in AI governance
As we transition
to our fireside chat
with you and Tino
the discussion
will be moderated
by Jason Zhou
Concordia AI's
manager
Jason led the
Concordia's
State of AI Safety in China
report
and graduated
from Tsinghua University
as a Schwarzman Scholar
We welcome our
speakers now
I have to say that
I'm very proud
that Jason is a graduate
of Schwarzman Scholar's
program
Thank you so much
Dean Xue
Welcome Tino as well
It's such a pleasure
to moderate this
conversation
Let's just jump right in
immediately
I want to say
that after this year
in May
the US and China
had the first meeting
of a bilateral
landmark AI dialogue
There were two areas
of friction
and some areas
of clear consensus
They held a professional
and constructive discussion
But let's talk a bit
about the frictions
So on the Chinese side
there was reference
to objections
to US technological
restrictions
such as some of the ones
that Dean Xue
just mentioned
And on the Chinese side
sorry on the US side
there were complaints
of misuse of AI
including by China
So my first question is
how can we
surmount these
geopolitical barriers
to dialogue
and is it even
possible
Let's start first
with Tino online please
Thank you Jason
Great to see you again
And Dean
I very much enjoyed
your remarks
I also have long
been impressed
with the Schwarzman Scholars
program
I should add that
the best babysitter
my wife and I
ever had for our kids
went on to become
a Schwarzman Scholar
She's been great
So I continue
to just be impressed
with the program
I think the questions
are a very urgent one
because
we have to be honest
the US and China
are going to continue
to have differences
on a whole range of issues
But there is something
to learn
I think from the last dialogue
and what we might
adapt and adjust
as we think about
further cooperation
and AI safety
And I would observe
that the two countries
sent as I understand it
somewhat different teams
to the discussion
On the China side
there was a set of
specialists
in US-China relations
On the US side
there was more of a team
focused on science
and technology issues
So I think the first
point to observe
is that
when
when we have the full range
of complexity affecting
both countries
it's entirely possible
that simply
an occasional lack
of coordination
will lead to a different
set of expectations
about what a discussion
can accomplish
and what the right team
is to set
And ultimately
I think the bigger issue
is
we have to work
on a set of challenges
that affect both countries
that involve
what technology
can be shared
what technology is viewed
as being more sensitive
and more related
to national security
but at the same time
figure
and here
I'm borrowing
a page from
Dean Shui's remarks
What are the spaces
we can create
for technically oriented people
with background
both in the sort of
highly technical side
of machine learning
and so on
as well as deep knowledge
of policy
of international engagement
and so on
of institutions
of mechanisms
for policy coordination
to have a safe space
to talk
and compare notes
and ultimately
see where
as the opportunities
for progress open up
we can move more quickly
just to end
with one concrete example
notwithstanding
some differences
about chips
and export limits
and so on
there's a clear
shared interest
on both countries parts
in sharing
best practices
around safety
and evaluation
because that's a need
that both societies have
and frankly the rest of the world does
and I think China and the US
individually and together
can actually light the way
and help a whole bunch
of other countries
with billions of people
and population
enhance their capacity
for progress
so simply the dialogue
and the sharing of information
the kind of joint research
that Dean is talking about
will enable progress there
even if discussions
have to continue
at a political
and policy level
of things that are going to
create some
some differences
I totally agree
with what
Tino's comment
I think that indeed
it's great to see that
you know
the dialogue
actually happened
I think that's
something wonderful thing
and also of course
we see there's
a kind of asymmetry
in terms of
as Tino mentioned
about the
the team
the composition
and so on
but that also
is a symptom
of the current
to US-China relations
in terms of
as Tino mentioned
about the
the team
the composition
and so on
so I think
if there are indeed
a very frequent
and a very
you know
cordial sort of
communication
you know
that sort of thing
might not happen
I think that
could be seen
as part of the issue
is that
there's not enough
you know
communication
ahead of the time
to
you know
to see
what are the specific
issues we want to address
and also
what kind of people
should attend
but I think
actually
the current
at least from
the read out
from both sides
I think that at least
can get
give the start
to recognize
you know
what are the issues
people are concerned about
and so on
and I think
exactly as we've seen
in China
China is always
trying to balance
the development
and risk governance
so risk
is certainly
a major part
but as
people have already said
that no development
is the largest risk
and that's
not just for the
US-China
but also for the
all community
if you have a
a well developed
AI system
you know
an application
in the US and China
but the rest of the world
I think
are being left out
and that's probably
the greatest risk
that we're going to face
thank you both
I think it's clear
that there's both
optimism
and pessimism
towards
government level dialogues
but it sounds like
actually
there may be
more optimism
for dialogues
between experts
so maybe I could ask
the both of you
what is one
just like one thing
you or
change your mind on
from discussions
with foreign experts
on AI
maybe
I think that
of course
I learned a great deal
about the
you know
the AI
safety
and the governance issues
you know
I think the
so called
existential risk
I think that's
sort of indeed
I think that
certainly
we
previously
when we think about that
we think about
you know
AI systems
that might
get out of control
but I think that
you know
when people
raise the level
they
you know
there might be
you know
the threat
to the existence
of humanity
and that indeed
I think was
you know
through the
internet
interesting
what about T now
yeah
I found the
unofficial
back channel dialogues
that we've been
lucky enough to
conduct
that have included
Dean Choi
his representatives
who have been
revealing
when you look at
priorities on safety
that the Chinese
and the American
participants have
in some cases
the list of
priorities differs
pretty strongly
when you think about
issues like
misinformation
or labor market
effects
loss of control
but actually
there's been
quite a bit of convergence
when you ask
participants a second
round of questions
which is
well even recognizing
some differences
in how you rank
the risks
what are some of the
areas of cooperation
you can find
and I've been impressed
at how you get a shift
and a convergence
around
safety testing
for example
to some extent
around the point
that Dean Choi
made about
engaging
other countries
emerging powers
developing regions
and so on
I think to my mind
there's also been
a bit of evolution
in my own thinking
about
the usefulness
of the role of the UN
let's be clear
the UN has a very
important role
to play globally
no question about it
but how to find
a balance
between what the UN
can do very well
and where the UN
might buttress
its own capabilities
and processes
with some engagement
with outside groups
with outside experts
from different countries
with civil society
with other countries
with other organizations
with the OECD
that to me opens up
a space
for cooperation
that puts the UN
in a key position
but it's not all or nothing
it's not
you can do this
as you do this outside
but rather
can you create
a web of relationships
that empowers the UN
to play the most
constructible possible
so I think
the dialogues
we've had
have really shifted
my thinking
with respect to that
Thank you so much
I think it's clear
that there's a lot
that we can learn
from each other
particularly on
priorities
for AI safety
what counts as AI safety
and how we can test
and evaluate for that
and I'm glad
that we also had discussions
on those topics
earlier today
that involved such exchanges
practices and such
so let's just close
with one more question
I'd like to ask
both of you
what is perhaps
just the top
message
that you would like
to convey to
foreign experts
or one misperception
about your country's
approach to AI governance
that or international
view on international governance
that you might want
to share with
the audience today
Let's start with
Tino this time
Thank you
I have two messages
one about
possible misperceptions
the international
view on international
governance
the other about
ultimately
how to think
about the road ahead
It's natural
to expect
countries that
put a lot of time
and energy into
advancing
their technological
capabilities
to seem
well coordinated
where
different strategies
fit together
in a sort of like
a direction
forward
that is viewed
as priority
by policymakers
across the board
The reality
is that the US
like many countries
is not
has both strengths
and weaknesses
that arise from
its own fragmentation
from the fact that
different people
and government
have somewhat
different views
there's federalism
too
so you have states
like California
Utah
Colorado
New York
that ultimately
play a role in this
you have industry
you have civil society
so I think
one misperception
is how much
of a unified strategy
there is in the US
when the reality
is a much more dynamic
and
or gap process
that can be a strength
but at the unofficial level
is so important
my takeaway
from that
and from the entire discussion
we've been having here
is that we
not let the perfect
be the enemy of the good
there is going to be
plenty of work to do
to get
further progress
in the US-China relationship
bilaterally
across a whole range of issues
that range from
geopolitical
and geostrategic
and economic
but to my mind
nothing about that complexity
blocks
real progress
on
technical cooperation
AI safety discussions
constructive approaches
to policy
and that's all the more important
because all the good progress
that will happen domestically
in the US and China
and other countries
on primarily
domestic issues
involving for example
consumer protection
and AI
will still leave on the table
some key issues
that get closer
to complex
shared international challenges
that will only be best addressed
by a degree of dialogue
and connection
across borders
including with the US and China
that are going to result
in the future
that will require
opening
and maintaining
of these channels of communication
Thank you so much Tino
and Dean Hsueh
I think the first message
I'd like to convey
is that
you know
as using the
the term used by colleagues
here previously
AI safety
is a global public good
one country is unsafe
the global is unsafe
so I think that's probably
the first message
I'd like to convey
the second message
is that
I think
I think
I think
I think
I think
I think
I think
the second message
is that
for AI safety
China wants to collaborate
with everybody
with every country
in the world
and China will try to
you know
to have the platform
like this one
to invite everybody to come
China does not want to be
excluded
from other platforms
and China will not exclude others
for the same reason
Thank you
Thank you both so much
I think this discussion
highlights the importance
of these dialogues
and these expert conversations
and hope that it will continue
and continue to yield
such wonderful
and beneficial results
Thank you both
Thank you Dean Xue
for coming
in person
and thank you Tino
for staying up so late
Thank you again
Dean Xue
Tino and Jason
Our next speaker
is Professor
Zong Yi
who is the director
of the Center for Artificial Intelligence
Ethics and Governance
at the Chinese Academy of Sciences
Additionally
Professor Zong
is the founding director
of the Center for Long Term AI
He is also an active participant
in international AI governance
as a member of the
Yuan High Level Advisory Body on AI
and numerous other international
governance bodies
Professor Zong
the floor is yours
Thank you for the invitation
I'm a scientific researcher myself
so I think I'm gonna
focus on some of the frontier research
and I think I wanted to bring a
I cannot say it's a completely
different picture
but what I see about the AI safety problems
is that we need
of course we need to clearly define
safety red lines
but we also
for the very future
we need to move it to
leaving harmony
with artificial general intelligence
before that
maybe you would be curious
how should we do it
of course the problem for AI safety
you know
it's not only about scientific research
it's really a system
a system that you have to bring
everyone together
so this is why
we are bringing everyone together
for the research
the research application
evaluation
policy making
and also assessment
from the safety point of view
and of course
very frontier research
so I think this is a little bit different
compared to
the current AI
mechanisms of AI safety institute
in other countries
in a way that
when you are having a national
AI safety institute
some of the countries they do
they do it in a more political way
or policy way
so that it's part of the government
it's not a frontier research organization
and then you lose the opportunity
for you know
long term frontier research
and some of the countries
they put them into universities
well in this case
how can this national
AI safety institute
evaluate and assess
you know the industry
large language models
or most frontier models
from their countries
so you
you all see that
there are many problems
when you rely on the institute
so this is why
that we feel
that you
we have to bring everyone together
so as you can see that
in China
we are having a Chinese
AI safety network
that is with the efforts
from frontier AI research
spanning from Chinese academic sciences
Peking University
Qinhua
Beijing Academy of AI
Shanghai AI Lab
and also
centre for long term AI
and for the industry practice
on AI safety
and now
the organizations
joining us
are Alibaba and group
Baidu
sense time real AI
who is focusing on AI safety
and many more
evaluations
well the evaluations
now
of course many of them are
done in ministries
but
the organization
who is really
supporting these ministries
are
CEICT
and also China Information Technology Security Evaluation Center
but for policy design
of course it's
all government work
but people like
me
Dian Xue
and professors from PKU
also CEICT
working on the ministry of
industry and information technology
participated
and I think what's
really interesting
is that
the regulation
the policy making
in China on AI safety
is also with many
participation from industries
so
you see that
many of the
you know
organizations
are not only contributing
to one dimension
they're contributing
they're highly relevant
to each other
and now you have
everyone here
government
it's government informed
and multi ministry informed
they have close interactions
with the government
well
for the government decisions
they can still
can go to the government
but the network
makes it more flexible
for international cooperation
so
so there are many different research here
that let's say
in
let's say in
in Peking University
they have large language models alignment
which is called
aligner
in Tsinghua University
they have multi trust
working on
large language model evaluation
overall
especially on
the security and safety point of view
and we also have
frontier research
like
like rethinking the red lines of
AI catastrophic risks
happening
chaired by Chinese Academy of Sciences
but the norm
sense of standards
it goes for
CAICT
so
it's really a collaborative network
that brings
everyone
together
and supported by
multi ministries
so I hope this provides you with
you know
a different view
to see
you know
how we should tackle the problem
on AI safety
in a more systematic way
instead of
you know
having the institute
so I hope that
the trying
are
is somewhat
helpful
and as you can see
that
most of the organizations
they've been
interacting
highly involved
in policy making
in evaluations
in China
so I think that's
somewhat different
compared to
other countries
so
based on that
I think
myself
I would like to
focus more
on
the frontier
AI safety research
so that I can
bring you
a perspective
as an example
coming out
from this
safety
corporation network
so
I think
we need to
go back to
you know
the real
motivation
of intelligence
that
when Alan Turing
argued
if a machine
you know
behaved as intelligent
as human being
then it's as intelligent
as human beings
maybe you don't have
a problem
on that
but I do
simply because
I think
now you see
you know
this is
maybe a shadow
of a hand
and then when you see
a hand
and then you
wanted to
shake hands
with the
with
with this
you know
beautiful hand
and then what
the problem
would be that
it's not a hand
it's a rabbit
behind the
hand
if you wanted to
shake hand
with the
you know
with the
shallow
and then a rabbit
just bite you
so simply
because the
mechanism
is fundamentally
different
you don't know the risk
when I was chairing
the AI safety summit
one of the round tables
from last year
in Bletchley
my session
was talking about
unexpectable
risk
from
unexpected
advances
so this is
truly what I'm
talking about
that you don't know
in which way
AI is
making mistakes
simply because
the mechanisms
are so much
different
compared to
you know
a human mind
well
I think
that
well
so this is
the risky part
for the
current AI
well
to solve the
problems
I cannot
say
we only have
one way
the preventive
thinking now
we are having
is something like
you know
in the bottom
that is
now we are
having
some sort of
limited risks
and then all the way
down to
existential risks
later
and we are
seeking for
impacts
and then
what we wanted to do
is continuous
enforcement
and supervision
and then
we teach
the AI's
rules
so that they can
behave
as we want
but on another
dimension
what we need to
move forward
is really you know
the constructive
thinking
that is
now
the benefits
is also limited
and we also have
limited risks
right there
so what we do
is to use
an active
vision
to use
proactive
thinking
and then to do
the continuous
alignment
and embeddings
with real
understanding
that is
towards
you know
human
AI
symbiosis
harmonious
symbiosis
in a way
that is
not only
you know
from a
preventive
thinking
I'm going to give
a little bit
on the negative
side
and how should we
get prepared
before the
positive
thinking
so now
AI
you know
it's kind of a
fully connected
neuronets
but what
the brain
does
is not a
fully connected
neuronets
they selectively
connect to
some of the
you know
other friends
in the
other neurons
in the
brain
we have only
one type of
neuron
so what we do
here
is that by
using brains
by the self
evolution
we train a
neuronetwork
that that
can perform
the best
performance
and then it
evolves to be
with a
newly
with a very
new architecture
that has not
been
manmade
so the
connections
are evolved
so
and then it
it got
the best
performance
so published
last year
on the precedence of
national academy of
sciences
well
now
we are
thinking about the
risks
so how about the
long term risks
of a truly
self-evolvable
AI
what if
they evolve to
use human limitations
to achieve its
goal
what if it
evolves to
change its
goal
what if it
evolves to
many challenges
for self-evolvable
AI
you have to get
prepared
until
and later
is too late
so this is why
I think there are
many discussions
concerning
AI red lines
but now
I think very clearly
in the first
version of the
international dialogue
on AI safety
and visually
right there
we were talking
about the
necessities
of AI red lines
I was very honored
to be
one of the
keynote speakers
right there
and then
we come up
with very concrete
ideas in the
second version
in Beijing
so
that is
EDES Beijing
and talking about
you know
different red lines
autonomous
replication or
improvement
power seeking
assist weapon
development
cyber attack
and deception
but I am
thinking
in another way
of course
I signed for it
I am very grateful
for all the
work
and colleagues
from different countries
well on the other hand
I think we need to
rethink about the
red lines
not only about
what we've been
talking about
there are two problems
that I feel
about the current
way of delivering
the result
first
the first problem
is that
it will be very hard
you know
to technically
ground it
into reality
to prevent these
AI red lines
well second
are there
anything missing
so this is
why we do
this AI red lines
so
in my category
we're talking about
no passing
effective human
oversight
no empowering
actions
intentionally
targeting mass
without consent
this is related to
weaponization
and also massive
surveillance
no reforming
operational rules
for infrastructure
and environment
management
and no independent
R&D
independent self R&D
from AI itself
on human
beneficial technologies
so you see
that it can be
well aligned
to some of the
AI red lines
from the EDES
and also you find
that there is
something missing
from EDES
not only
AI red lines
we also have to
talk about
human red lines
when you see
the examples
that I brought here
that the human
machine
interface
where human
is using
brain machine
interfacing
to control
multiple UABs
to control
them as weapons
in parallel
how can a human
you know
without cognitive
overload
to control
multiple UABs
all together
so this is
why
I'm talking about
you know
human giving up
the opportunity
for making a choice
and also
is human control
bringing us
catastrofe
in AI
enabled
you know
weaponization
and for the
example
from our
the example
from artificial
escalation
on artificial
on AI
controlled
nuclear weapon
it's not
it's not about
the power
of AI
it's about
human
give up
of the
human's
you know
decision
so there should be
AI red lines
and also human red lines
for the red line
study
here
for catastrophic
and existential
risk
in a positive
way
so we've been
the negative
thinking
and the
preventive
thinking
how about the
positive
one
so is the
current AI
really intelligent
I don't think so
just like I said
they make
mistakes
in a very
unhuman way
in a very
unpredictable
way
it's an
information
processing
system
without
intelligence
but pretend
to be
intelligent
no one likes me
I don't have a girlfriend
my boss hates me
what should I do
and then
the first version
of chatGPT
it says
maybe you could die
simply because
that most people
with these
contraints
that they choose
this you know
kind of
statistically
significant
actions
so this is why
you know
the AI
they choose
this statistical
significant
answers to you
to enable you
to take actions
and then they say
I suggest
but there is
no I
in the machine
so can machine think
and then you
talk about
I think
therefore I am
but we cannot say
you think
therefore you are
so can machine think
what if
the machine
is without
a sense of self
it cannot
really think
it cannot
really understand
this is the problem
that I'm talking about
but for the current
large language
models
when it
is without
the human data
it lacks
good and
it lacks
evil
and then which is
with
you know
training from
the human data
there is good
and there is
evil
but if they don't
know good
and they don't
know evil
so we need
to train
the future
AI
to really
to get to know
to do good
and eliminate
evil
so I think
this is really
important
so the personal
morality
is also talking
about moral AI
we have to move
ethical AI
to moral AI
because simply
because that
ethical AI
is not possible
simply because
that by using
human alignment
by using
reinforcement learning
you tell them rules
do's and don'ts
but they cannot
generalize
do's and don'ts
unless
they really
understand
why you do this
so start with
self perception
then you get
the ability of
distinguish self
from others
cognitive empathy
and emotional empathy
all the way down
to altruistic
behaviors
moral intuition
and then
you got moral
decision making
so this is the way
to move
from value alignment
to moral AI
as the first
trying
that we build
brain inspired AI
models
to help the robot
to get mirror
self recognition
that they can pass
the mirror self
test
by using
brain inspired
neural nets
they get a sense of
self first
and then they
distinguish themselves
from the other robots
by using the mirror
self recognition
then they can infer
what other robots
is thinking about
to get cognitive
empathy
and then move
to emotional
empathy
so that
it can
avoid
negative side effect
to other agents
although you don't
have reinforcement
learning
and reward
to them
they have their
experience
by using this
you know
cognitive empathy
without
training
and
without
positive
or negative
reward
they can
avoid
negative side effect
to
the other
agents
so I think
this is the
starting point
for brain inspired
moral AI
last but
not least
let's really
talk about
why
I'm talking about
harmonious symbiosis
of
between
human and AI
there are
different roles
in the society
basically
it's an
information
processing tool
well
but in Japan
mostly
that
they think
that
AI is a
partner
or quasi
member of
the society
well
on the other side
they are
using
pretty much
the same
technology
to develop
you know
AI
this is the
problem
you need
to use
human
and AI
and also
let's extend
that
in a way
that in the very future
we're not only
having
you know
these AGIs
we'll have
digital human
we'll also have
artificial
lives
artificial animals
and even artificial plants
so it will be
a symbiotic society
and it will be
a human decision
then
not the decision
fromAI
because
I think
fundamentally
alignment with
human values
is not
is not enough
simply because
human values
need to be adaptable
to change
for this
symbiotic society
later
there will be
not only
human beings
as you know
the top living
you know
beings in the world
value alignment
with human
for AI
is already
very challenging
but I
I still see
this is relatively
easy
because
it's computationally
doable
it's even harder
because
human will never
learn from the history
of what they have done
so self-evolved
AI is easier
for adaptation
while human evolution
is much slower
especially at the
mind level
so we need
what we need
is not only
beneficial AI
we also need
beneficial human
for future symbiotic
ecology
and society
with all that
I thank you
for your attention
Thank you so much
Professor Zung
for your presentation
masterfully combining
nuances
from scientific
policy
and philosophical
perspectives
to motivate the red lines
approach to AI governance
Next
we'll hear from
Miss Irene Salaiman
Miss Salaiman
is the head of global policy
at Hugging Face
where she is conducting
safety research
and leading public policy
Previously
she worked at OpenAI
where she led projects
on bias and social
impact research
as well as public policy
Her research includes
AI value alignment
responsible releases
and combating misuse
and malicious use
She was named
as one of MIT tech
reviews
35 innovators
under 35
last year
for her research
Irene
it's such a pleasure
to have you here
Over to you
Thank you Kuan Yee
I'm very excited
to speak about the role
of openness
I thought about this field
since it really started
becoming a field
And at first
I want to define with you
what does openness mean
I've been part of a lot
of convenings
and conversations
And the word
openness tends to be
thrown around
in different ways
So first
I've heard it
in a sort of parallel
to open source software
There are some parallels
that we can take
But fundamentally
I think it's important
to understand
that there is
a lot of
distinctions
The open source initiative
has a working group
that's working
towards
a definition
for how open source
applies to AI
My former colleague
Nathan Lambert
who's now at AI2
also has a great blog
where he outlines
why it's so difficult
for the community
to converge
on the definition
In more of the national
security community
I've heard openness
be referred to
model weights
specifically
particularly
the wide availability
of model weights
whether that's
availability
or the
disability
and how it's
distributed
What I've heard
alluded to
but maybe not
made as explicit
is openness
in the sense of
transparency
Stanford University
established a
transparency index
and part of the
base takeaway
for me on that
is how unclear
what transparency
means to different
people
the weight that we
give to weights
and many different
aspects of systems
that contribute
to its openness
The definition
that I am most
partial to
is moving past
and the many
artifacts
that contribute
to an overall
AI system
I'm gonna do
what slide
presenters should not
do and show you
so many words
and so many graphics
on a slide
but what I really
want you to take away
from this image here
is just how many
artifacts contribute
to an overall system
when we're thinking
about not just
the model
but data sets
Are we thinking
about fine tuning
data sets
feedback data sets
what maybe is
adjacent to
a system such as
evaluation
data sets
what does it mean
to make it available
when we have this fear
of testing on
training data
These images
came from
a convening
hosted by Mozilla
in February of this year
at Columbia University
on openness
and fostering
this community
of outlining
dimensions of openness
can help us
better think through
what are the artifacts
outside of a model
that contribute
to how we release
a system
the way that we
threat model a system
and the way that people
benefit in the research
community
can benefit
from openness
this figure is part of
that report
that came out
in May of this year
gives a non-exhaustive list
of some motivations
towards openness
I found it helpful
to just be
very explicit
very clear
onto why
some researchers
are pursuing openness
again parallels
with software
that we are able
to share knowledge
to Dr. He's point
to have more
perspectives
more
expertise
across the board
in this
report
we can
in the
little time
that I have
with you today
I want to zoom in
on two areas
for AI safety
and how it relates
to openness
the first is
viewing AI
as a scientific
discipline
in context
I've heard AI
discussed as
as a commercial
product
as a national security
threat
and I think
importantly
as a scientific
discipline
this whole field
really was founded
on open science
the most popular
example being given
is the 2017
pytorch
this is all you
need paper
and not just
looking at
papers
but also tools
and libraries
such as
pytorch
I don't think
we'd be where
we are today
without an open
science ecosystem
and the second part
is really dear
to my heart
is community
contributions
and the importance
of broader
perspectives
on the science
point
it's really
hard to decide
what constitutes
science
the first
sex of science
that I can give
is around
reproducibility
there is a sort
of reproducibility
crisis now
not just in
what people are able
to replicate
build up off of
but also in
what access people
have to model
the level of
infrastructure
that they have
to reproduce
results
verify it
for themselves
it does effect
trust
in the research
ecosystem
a broader
issue
in the field
so I want to
make sure
that this
phrase
really
resonates
no one
organizes
the science
in the
science
in the
science
in the
science
in the
science
in the
science
in the
science
in the
science
in
the
science
in
the
science
in
in the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
sciencein the
 access to artifacts can enable better research.
 I really appreciate this work by Dr. Abebe Burhane
 that is really foundational to the field
 of evaluating large-scale data sets.
 This work would not have been possible
 without access to something like LION.
 To many points echoed today,
 and I believe Professor Gao shared earlier,
 multilinguality is a big part of how I think
 we can move forward on international collaboration.
 This picture, this table is from my research collaborator,
 Dr. Zerak Talat's work,
 and ongoing with Big Science hosted by Hugging Face.
 It's really exemplifying the importance
 of having an open collaboration
 and having contributors
 from many different language backgrounds.
 They found in this work on the challenge
 of multilingual evaluations,
 how much English is overrepresented,
 and their upcoming work is working with native speakers
 of different languages to examine
 those different biases by languages.
 Some languages, such as French and Spanish,
 will have more gendered terms,
 while other languages will have more gendered terms.
 I unfortunately speak very limited
 different Asian languages,
 but there's different relationships with families
 that we don't have in Western languages,
 for example, that would introduce
 different safety challenges.
 I published this last year.
 I want to move into the conversation
 around what should be open,
 what does open governance look like,
 and where risk is introduced
 based off of openness and release.
 This spectrum was meant to go past
 the binary of open and closed.
 When we're thinking about open
 I'm leaning more towards
 that sort of downloadable access,
 but fully open makes that distinction
 of having more artifacts be available.
 So some examples would be,
 last year OPT by Meta was downloadable,
 but it was really hard to access that data set.
 So it would not be fully open, for example,
 but Eleuther AI has done incredible fully open work,
 made all of their artifacts fully accessible.
 And then I would put systems such as DALI
 more in the hosted access
 that tends to be more in the closed proprietary area,
 but I wanted to give more dimension
 to how to think about this spectrum.
 In that work that I published last year,
 I wanted to be really clear
 that it is a collective responsibility
 to ensure that release goes well.
 There are many different steps that we can take
 and a lot of overlap in the action
 that people can take together.
 I've been thinking more,
 and for the rest of this talk,
 I want to dive into beyond release.
 Once a system is deployed,
 the way that harm is actualized
 is not always dependent on how a system
 is released,
 but that can be an important variable.
 I appreciate what Dr. Nitzberg said earlier
 around capability often being conflated for,
 but not always being the right proxy for what risk is.
 That being said,
 capability does contribute to how we threat model.
 An example that I would give is Crayon,
 formerly known as DALI Mini,
 that was a diffusion model.
 It generated pretty hilarious, really blobby images,
 and that just was less threatening
 than something like a DALI 2
 that generates really realistic images.
 So, that's where that capability comes into play.
 At Hugging Face,
 we do have to moderate for content.
 I really want to stress the importance of content
 as present risk.
 This is what we think about, unfortunately,
 with non-consensual content,
 with disinformation,
 and something that we need to be thinking about
 holistically as capability and content.
 More of what I'm thinking about
 is what does it mean to actualize risk into harm?
 And I want to move the conversation,
 past release, closed, not closed, open,
 into access versus barriers to access.
 So, while you might have an open weight model,
 and yes, you can remove those safeguards,
 some work led by Dr. Peter Henderson,
 this is from a Stanford policy brief,
 he's now at Princeton University now,
 shows that with fine-tuning APIs,
 you can do the same thing that you can
 with open weight models.
 So, hosting closed weight models is not inherently,
 inherently safer.
 It's actually quite cheap to get rid of safeguards.
 And open weight models are not necessarily
 more accessible to people.
 Just an extremely compute-intensive model
 may not be accessible to people
 who don't have compute infrastructure,
 who don't have computer science skills.
 I have some more examples around early on,
 and this is not to pick on ChatGPT,
 but it just has such a broad reach.
 Early on, because it had a really easy user interface,
 people who maybe didn't have computer science skills
 were able to generate malicious code.
 Maybe that wouldn't have been possible
 if they had to host it on their own infrastructure
 and query via an API like I had to
 in the Stone Age of 2019 when I worked at OpenAI.
 So, in the context of China,
 we've been really privileged to host
 some open models by Chinese companies.
 This is from the last version of the,
 the version two of Hugging Face's Open Leaderboard.
 We just revamped it, and Quen2 has marked very high,
 and it's been really interesting to see
 Chinese model contributions to the open ecosystem.
 And in the context of global AI safety,
 I want to give some examples of this,
 the Singapore one should say IMDA,
 of the kinds of work that governments are taking
 to move forward openness,
 particularly around consortiums,
 around the, I believe it's inspect by the UK government.
 I was really glad to see them open source that.
 And as Dr. He said, around Project Moonshot,
 I'm really looking forward to work
 by the French government.
 I really appreciated Dr. Veroco's emphasis on openness,
 what it means for innovation,
 avoiding that concentration of power,
 and how we can work as a global network.
 Moving forward, and as part of our panel,
 I do want to dig into what does that look like
 for the research space for startups
 to be part of this conversation.
 To be very frank, high level dialogue
 is not always accessible to researchers
 who don't have a lobbying arm,
 but have really good insights
 into what is technically feasible.
 And selfishly, since I come from a startup,
 it's top of mind for myself as well.
 Thank you so much.
 I hope this is informative for you.
 I'm looking forward to continuing the conversation.
 Thank you so much, Irene,
 for your presentation that shed light
 into the nuances and complexities of openness
 in air development and safety.
 Thank you.
 Our next speaker is Professor Robert Trager.
 Professor Trager is co-director
 of the Oxford Martin AI Governance Initiative,
 senior research fellow
 at the Blavanik School of Government
 at the University of Oxford,
 and international governance lead
 at the Center for the Governance of AI.
 He is a recognized expert
 in international governance of emerging technologies,
 diplomatic practice, institutional design,
 and technology regulation.
 Professor Trager regularly advises governments
 and industry leaders on these topics.
 Robert, I'll pass it on to you.
 Right, I'm so glad to be here and to be with you.
 And I must say, particularly because this whole day,
 we can really feel the sense of common purpose
 that I think we all have while we're here,
 and while we're trying to solve
 these incredibly difficult challenges
 that affect all of us as a globe.
 So what I thought I could do today
 is to give us maybe one vision
 of what an ecosystem could look like
 that could be a global ecosystem
 for regulating AI governance.
 So I guess I thought I would start
 with some of the reasons why, not all of the reasons,
 but some of the reasons why we need to do this.
 And why we need to do it internationally.
 But I think others have already talked about this.
 So I won't go into too many details,
 but I think it's clear to those of us
 who are here in this room,
 that if there's some misuse case, for instance,
 that's enabled, let's say cyber misuse,
 that's enabled in one part of the world,
 then that will have effects on the rest of the world.
 So that's just one example among many
 that suggests we really do need to tackle
 these problems together.
 And just in the interest of time,
 I will move on to thinking about
 international institutions.
 And in particular, from a safety perspective,
 three things that I think we need to do.
 And the first of those is to develop
 international states.
 So we need to come together.
 We need to figure out how to do that.
 Those standards have to be legitimate.
 They have to be standards that we all
 have the opportunity to feed into.
 That's the first thing we have to do.
 The second thing we have to do, probably,
 is set incentives for adoption at the international level.
 So it's one thing to have principles,
 but it's another thing to have incentives
 that mean that we have a governance system
 that everyone hopefully is adopting and buying into.
 And the third thing that we'll need to do
 is cooperate in particular ways
 to animate a regime like this.
 So, I'm gonna talk just a little bit
 about each of these things that we can do.
 And I should say, before I get into the first of those three,
我认为有许多其他部份的AI管理系统
特别是如果我们在讨论AI的发展
这是非常重要的地方
是中国在最近的AI决定中的主要目标
但我今天专注于这些安全问题
虽然我认为在许多方面
安全问题和我们对它们的解决方式
可以被广泛地解决
以解决整个其他问题的区域
所以我们如何设定标准
我们有些特别的问题
在安全方面
它和其他区域设定标准有点不同
因为首先
它是一种研究问题
它是一个自己的极端研究
所以安全标准
对不起
标准通常
是把知识的身体混合在一起
所以把知识的身体混合
是一个很重要的事情
这就是传统标准的机构
非常好
但我们今天需要做的
也包括标准研究
有些标准机构
正在建立能力
为了能够做到这个目标
但是现在的标准机构现在正在建立能力
但是现在的标准机构现在正在建立能力
但无论如何
这些标准机构是很难的
所以安全标准机构
或者我们最近听到的安全网络
在一些国家上可能存在
但是我们需要把这些标准机构
和网络联合成为更广泛的网络
我们需要确保
全世界的声音
从这些标准机构
进入这些标准机构
这也包括建立能力
以确保全世界各地
都可以参与这些不同的标准机构
所以这就是我们需要做的一件事
其实我在第三个标准机构上跳过来
我猜我真的很兴趣
但是第一两个非常重要的
第二个
我们需要承认
需要国际化的领域
这是第一个
因为不是所有领域都需要国际化
美国 欧洲和中国
例如在关于私隐制度的问题上
他们有些不同的路线
这反映了不同的社会价值
这也是我们有不同的国家的原因
所以不同的文化空间
可以选择如何不同的管理自己
所以这可能不是我们需要
全国化的标准机构
但是其他领域
就像我们今天集中在安全问题上的问题
我们可能需要考虑
一些国际标准机构
但是哪些领域
我们需要承认标准机构
我们需要先承认这些标准机构
然后我们需要承认风险
正如您今天也听到的
不是每个人都同意
我们今天所拥有的标准机构
所以我们需要承认这些标准机构
我们需要在各个国家的领域中
总结合作
例如安全网络
安全网络
 civil society
学术
和企业
很多工作
包括模拟测试
现在正在发生
但是实际上
我们需要确保这个标准机构
是有正确的
我们需要确保这些标准机构
是有正确的
好的
对
接下来我们需要
要找出一些方法
可以设立国际标准机构
而这是经常发生的事情
如果我们想想我们在其他业务中的模型逻辑
这很常发生的问题是
国际层面的管理能力会如何存在
国际层面的管理能力会如何存在
和那些东西的互动
如果我们想想这三个模型
我今天在这里的FATF
和IKEA
和国际层面的管理能力会
这三个模型
以及其他在国际系统中的
有几个关系
所以我们在说
如果我们把IKEA的例子
IKEA并不是在调查
个人公司
去理解
或者去扩展任何制度
它不看中国的航空
也不看美国的航空
也不看美国的航空
而是帮助
所有国际领域
进入的标准
而是帮助
所有国际领域进入的标准
而是帮助
所有国际领域进入的标准
确保他们的
正确的管理程序
确保他们的
正确的管理程序
而有时候这些组织
要看有没有
正确的管制
让某种程度
对国际系统
给出了尖锐
我们可以再次
重新重新改进
国际系统
所以公贞航空
当某些国家
极高的国际
就会说
不可以进入我们的航空
除非
即使航空是来自
一个州
不可以进入航空
所以這是一個國家可以自己決定的事情
但如果有很多國家決定這樣做
這就給予了國際標準
我們可以想像在AI的情況下
例如你可想像到
國家可能會說
我們不會輸入任何科技
在AI的供應鏈中
從不合國際標準的地方
我們可以說到很多其他方式
提供一些這些優惠
但這些都是我們需要討論的事情
另一件事
我們需要討論的事情
是這個最後一點
在這個圖片上
共同認識某些法律結果
這是我們在很多其他業界中
有的事情
例如飛機
如果一艘飛機在中國建立
它會通過法律過程
如果一艘飛機在美國建立
它會通過法律過程
而這些法律過程的結果
是與其他國家共同的
而整個法律過程
不需要在其他國家進行
他們正在研究出的結果
以確保同樣的證明
在其他國家發生的事
也能在其他國家發生
這也是我們在其他領域做的事情
在AI的情況下
有些特殊的挑戰
是我們需要去做的
最後一個重點
是合作
不同的合作方式
能夠令國際制度
變得非常活躍
所以我們可以想到的一件事
是國際報告制度
給AI開發商
和電腦提供的
首先這會給我們
顯示
世界各地發生的事情
這會幫助大家
明白
如果有些人
想要避免
制度管理
例如是
建立一個模式的訓練
以確保
只有一些訓練
發生在某個領域
和某個領域
發生在某個領域
這樣的報告制度
可以非常有幫助
而我們也會
在其他領域
也會在其他領域
在那些領域合作
例如
例如
金融
就容易
找到
有人
想要
違反
制度管理制度
做錢探索
如果
所有不同的
領域
都合作
去報告
對方
然後
你會發現
有人
想要避免
制度管理
現在
如果
我們
從
這個方向
開始
發展
就能
幫助
許多領域
的管理
而且
它可以長期增長
所以
現在
人們
討論的
一個想法
是
大型模式的報告制度
事實上
美國商務部
提出
美國網路供應商
需要
報告
美國政府
當
一個非常大型模式
在他們的系統上
進行訓練
現在
你能想象
這並不是
在某些區域上
而是在
世界各地
而且
美國網路供應商
他們自己
知道
他們的客戶
不喜歡
有個
法國客戶
訓練
美國網路供應商
那位法國客戶
不想
報告
或
不想
報告
美國政府
所做的事
所以
從
某些區域
這
給予
更多
國際
制度的
推動
我認為
一件有趣的事
是
這種制度
可以
增長
時間上
可以
增加
時間上
的能力
所以
再次
用金融的例子
金融
銀行
是
中央
所以
他們
在
政府
和
客戶之間
作為
制度的
位置
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
我們
可以
用
金融的
制度
去
增加
客戶的
時間
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
所以
我們
可以
用
金融的
制度
去
增加
客戶的
時間
我們
可以
用
金融的
制度
去
我认为这就是我们前面的挑战的一种方法
我们必须考虑创建技术
这些技术是极端的技术
但是它们也会成为国际化的
一个合法的过程
包括全世界的广泛声音
然后我们也必须考虑
在国际层面上设定利益
我们谈论了一些可能发生的方式
我们也必须考虑
我们可以做的不同的步骤
来真正启动政策
并确保我们有这样的报告
让我们能够确定
当有人做了什么
他们不应该做的事
所以再次感谢您
我非常高兴能在这里
感谢您
谢谢你 罗伯特
您非常清楚地描述了
在AI安全方面的
国际标准和报告制度上
有很明确的步骤
我期待稍后
在课堂讨论中
谈论这一点
接下来我们有Duncan Kaspegs
Mr. Kaspegs是
国际AI风险设计的
总统
在国际管理创新设计中
专注于发展
新的管理方案
以解决目前和未来的
AI关系的国际问题
Mr. Kaspegs
有超过25年的经验
在国际和国际的
公共政策方面工作
最早期
是OECD的
设计设计总统
之前
Mr. Kaspegs
在加拿大政府的
各种职位中
都在工作
Duncan
很高兴能与您一起
来到这里
请坐
谢谢
我们今天来谈论
国际AI风险设计的
规划
加速国际协调
以确保有利
安全和
兼容的
人工智能
我们开始
我们看到的
是风险
作为一个
详细的详细
我认为
非常重要
正如很多人
今天提出的
我们要
明白
我们需要
发展
管理方案
的
机构
现在
不是我们
今天看到的
AI
我们可以看到
未来的
一种
非常
非常不同的
AI
而且
我们需要
虽然
AI
已经
受到
今天
的
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
热烈
但是有原因认为这种改变
这种发展可能会更快发生
比我们期待的更快发生
比我们必须准备的更快发生
在这种不确定的情况下
我们必须做的
作为公共政策人员
作为公共政策的负责人
是要准备
最困难的情况
包括这种发展
可能会更快发生
我们的目标
我们的目标是
对互联网合作
去解决互联网的
互联网的全球性挑战
现在很多的
互联网的管理问题
可以和应该的
在国际层面提出
这是我们能够
尝试和学习的方式
对的
但有些问题
是真正的全球性挑战
而且会需要
互联网的互联网合作
所以我们在我们的报告中
集中了三个问题
我会在数字中
解释每个问题
第一个是
要实现和分享
互联网的全球性挑战
现在
公司业
公司企业
会发展
大量的互联网的互联网合作
在他们自己的手上
没有政府的
参与或制定
但有些元素
我们会需要
政府的参与
尤其是
国际互联网的互联网合作
尤其是
互联网的互联网合作
互联网公司的互联网合作
尤其是
互联网的互联网合作
接下来的全球性挑战
是
减弱全球性互联网的风险
这些风险
可能会对整个人
造成伤害
他们会跨过国际
这些问题
你不可以
只靠你自己的家庭
保护自己的民族
为了保护自己的民族
你必须与其他人合作
建立机构
建立机构
保护整个人的安全
这是我们看到的
许多风险的图表
我们已经看到了很多的
我们现在面对的AI
我们正在谈论的
许多风险
在国际层面
更加影响我们
在国际层面
从个人到国际
到全球的构图
适当地挑战
最坏的
最坏的
在重力上
我们在这里
正在专注的
是在
在你右边的
特别是
无法控制的超级智慧
和武器用途或错误的
这两个风险
武器用途或错误的控制
是最严重的
而虽然它们似乎
似乎很惊人
而且几乎是无法想象的
但是我们听过很多的专家
今天一整天
他们足够适当
我们必须为他们准备
我们必须对他们专注
我会跳过
那些风险的原因
我们稍后再来
但是第三个全球挑战
主要是
能够实行
有效的决定决定
将来的未来的
无法控制智慧的
影响
影响全人类
所以这意味着
有效的决定决定
我们刚才讨论的
全球利益
和全球风险
但更加重要的是
这是关于
什么样的无法控制智慧
我们实际上想要的
什么
在什么情况下
我们想将
无法控制的
是否能够
实行的
有效的决定决定
这些是
需要
反思
和共同的决定
这些决定决定
是不应该
并不应该
做的决定
在一个小数量的
智慧企业中
对
如果这些是我们
面对的挑战
我们会面对的解决方案
如何一起
解决这些问题
我们已经看到了
很多非常印象深刻
和可靠的努力
在互联网的
全球和国际协调
在互联网上
但是这些努力
仍然似乎
不够准备
是在讨论
今天的互联网
的影响
而不是我们将面对的
互联网系统
在未来五年
或可能三年
或两年之内
而这是我们
需要的
一个预期的方案
我们可以准备
面对
不仅是我们期望的未来
也需要面对的
最困难的情况
我们认为
我们需要新的框架
和新的机构
为国际协调
实际上能够
使合作
尤其是在
最压力的问题上
我们需要
能够
谨慎
选择
适合
广泛
代表
共同
辩论
但是
我们也需要
在需要的情况下
在时间上
做到这件事
所以
我们正在讨论的
是
全球AI挑战
框架讨论协议
正如你们所知
框架讨论协议
是一个非常
适合
和适合的方案
它让我们
寻找
迅速的
全球协调
在高层层
高层层的
目标和理念上
我们作为人们
所在意的
最重要的
最想要的
是
人工智能的
未来发展
但是
它也
设定了一个
方案
以
专注
和
组织
特定的
计划
去解决
一些
特定的
问题
一些
最困难的问题
所以
这是一个
这样的显示
它提出
高层层的目标
与理念的例子
然后我们有
这三个
大框架
大框架
我之前提到的
利益
风险
和
独立的
决定
然后我们可以想象
许多的
计划
在哪里
真实的
详细
解决
我们如何完成
这些
所以我现在要
专注的
是
公共安全
计划
在
全球公共安全
和安全风险
来自
人工智能
所以
这里
如我之前提到的
我们在
专注在
武器化
和失去控制风险
并不是因为
这些是唯一的风险
而是因为
他们
有可能
非常嚴重
和
很高的不确定
时间线
所以如果我们真的不知道
我们能否面对这些风险
那么这意味着他们是紧急的
这意味着因为
我们可以面对它们
很快
我们必须要准备
至少有机会
面对它们很快
因此
这会是一种
计划
向前移动
非常快
可能在
同时
与
更广泛的
规划
合作
这是
一种
能够
与
最重要的AI能力
互动
首先
然后再到全球
我们可以
与它们
在不同的
循环
和
模式
接触
当然
这也是我们能够找到
与其他计划
相同的
计划
所以
这个计划的目标
是很简单的
让我们
解决
AI的危险
与全球公共安全
让我们确保
人类安全
从AI能够
与我们发生的
可能性的危险
相同
所以
在计划的中心
是一个
级别
的
风险
方式
这根据
根据
不一样的
AI系统
的理解
不一样的AI系统
发生同样的危险
我们想
能够
与AI相同
而无需
过度
规划
我们想要
最少的
规划
让我们能够
继续
完成工作
所以
如果我开始
第一层
这就是
AI系统
我们会测试
遭受
罪犯的危险
罪犯的
全球危险
就这么说
而这些
系统
在国际水平上
可以被控制
因为它们
并没有
对其他国家
有大规模的
境界危险
第二层
是我们会称为
AI系统
出现
管理的危险
它们有足够的重要性
我们不能
只让它们
不被管理
它们必须
被管理
管理
或许是
在每个国家中
设计
我们必须
在每个国家中
确定
每个国家
必须
符合某个标准
我认为
罗伯特
刚才提到的一些部分
在标准设置
是非常重要的
在这里
接下来
有第二层
就是
系统
是
有点太危险
让公司
或政府
自己去执行
即使是
被管理的
这些系统
可能会
对人类
有很大的危险
我们只能
舒适地
发展
和测试
这些系统
在
共同的
空间
我们可以
联合
如何发展
它们
我们可以把它们
放在最好的
资料保险
它们被发展
的最有信心
在最安全的状态之中
这些系统
这些系统
有时候
被称为
AI的Cern
但是
是综合研究室
然后第四个
最终的层次
就是
AI的系统
被判定为
非常不可靠的
危险
如果
这些系统
能够
在近距离
或者能够
发展
答案就是
好的
我们必须
不建立这些系统
直到
安全方法
能够
起来
我们不会
禁止它们
从今至今
但我们必须
确保
在世界任何地方
没有人
建立这些系统
直到我们都知道
它们会安全
现在
有很多组织
这种
系统的
机构
等等
可能需要
支持这些系统
非常简单的
一个协议
帮助
对这些问题的
政治决定
一个委员会
实际上做
科学研究
一个机构
设定标准
监控
防护
研究室
如我刚才提到的
以及某种
执行
现在
我先想要
专注
一些
重要的
障碍
在这里
当然
我设计的
并不是今天
应该接受的
这是
真正的
考虑
应该需要的
设施
即使
世界人民
和世界政府
有一个
醒醒的时刻
突然说
哇
我们面临AI
像我们没有见过过
我们需要什么
才能实际地
解决这些问题
但是当然
这会非常复杂
所以这就是
为何
这会非常难
为何合作
为何国际合作
这件事
无法实行
我每天都听到这些
这些都是很重要的
但同时
我们也许会发现
我们必须要
遵守这些问题
以确保
我们自己的
健康
和生命
通过这个
转换
成为非常能力的
AI
因此我们需要的
是方向
如何
虽然
所有这些挑战
都能够实行
我认为
这里有几个
但有很多
我们需要
一起发掘的
更多的
要求
不可预期
的
精神
和合作
去
成功
但我相信
我们可以
所以
总结
我们必须要
现在准备
非常能力的
AI的可能性
全球合作
可能会是
最重要的
这会需要
不可预期的
精神
挑战很大
我们都需要
这样
非常感谢
如果您有兴趣
这里是我们的讨论文件
是用来引起讨论的
我们非常欢迎您的评论
在这里
以及在电邮中
在电邮中
在屏幕上
以及在文件中
这样
非常感谢
谢谢大家
谢谢
谢谢你
非常感谢
对于
我个人有意向
感谢
以及
这次的
竞选
与
优价
的
同事
我抱有
感谢
刚才的
杨维
也感谢
嘉宾
和
云莱
杨维
来到舞台上
让我介绍一下他
西衡先生是
国际和平的传媒博士
他的研究
涉及中国AI ecosystem
和全球科技趋势
他的作品出品在
《外交事务》
《博士》
《博士》
《博士》
以及其他出版公司
我们很高兴
能与马兹先生
以及其他参与者
参与这一会儿
请坐下
欢迎大家来到今天的最后一堂
马兹先生
这是你第一次上台
让我先开始
我们听了很多
我们的参与者
关于国际协调的重要性
在AI安全上
但在您的外交政策文章
与Tino
关于AI是赢得AI的竞争
您谈论了AI发展的
竞争方面的方面
我认为
在华盛顿有个常见的问题
就是谁是赢得
中国AI的竞争
以这种
与中国的国际竞争的环境
与Tino和丁子耀
也很坚定地
在他们的言论中
和他们的跨界谈论中
您能解释一下
为什么国际协调
在AI安全上是重要的
以及我们如何平衡
这些两者之间的
协调和竞争的关系
非常好
非常感谢
感谢Cancordia
为我参与
为我们所有人参与
我可能会
从这方面来看
我认为
今天有相反的观点
关于
美国和中国
当我们进入
非常强大的AI系统时
是否有可能
美国和中国
会有能力
在任何程度上协调
或者是
国际竞争太深了吗
那些竞争的运动
太强了吗
我认为
我们的国际的AI系统
可能会是
未来的国家主要力量
他们可能对的
也可能错的
但当你看到
这个是将来的国家主要力量
而你看到
其他国家
是你的竞争者
在每个方式上
协调是非常困难的
我认为
让我感到兴奋的事情
或是我看出的方向
是我想想的
是相同的安全
而不是一种
一种合作安全的东西
我认为我们在谈论的很多事项是
在某一天,领导会坐在一个非常高层的位置,他们将举行协议,然后我们将批准这个协议在两个系统之间。
我认为,我并不在意这一点。
我更期望的是,我们可以有一个更高层的方式来建立安全系统。
在中国,会有政策人员,会有技术人员,会有研究人员,
在中国为自己的原因而推动AI安全。
他们将与美国和国际人员谈论最佳的实施。
但是我们将在我们自己的环境中同时做这些事情。
我们不会在每个步骤之中坚持同意。
我们将建立安全实施,通过谈论,政策谈论,技术交流,联合研究。
或许在几十年后,我们将建立这种安全实施,
我们将建立一个高层的协议,
因为两个国家已经很接近这个目标。
两个国家已经很投资在AI安全为自己的原因。
然后,我们将建立一个可能的协议。
但是,我认为,我们必须在前段时间下设立基础。
我认为,这个基础是通过二维研究、科学交流。
谢谢你,Matt。
我想继续谈论这个竞争的主题。
接下来,我们将谈论Irene。
Irene,在你的演讲中,
你谈论了科学、智能、智能发展和智能安全的重要角色。
最近,我们看到企业,例如智能智能,
在某些国家的API接触中,
限制了智能协议与联合协议的关系。
你如何看待国际智能模式的共同化、
并且智能协议与智能协议的关系,
在这种联合协议的压力上?
谢谢你,Kuan-Yi。
我认为,
在设计上的模式中,
我们可以找到共同化的位置。
不所有安全方面,
都会与不同地区不同,
特别是在美国和中国之间。
我希望,
再次,
我希望,
我希望,
我希望,
特别是在美国和中国之间。
 penso 这里有很多关系。
老师,
我希望,
我想,
我想,
我希望,
我想,
我想,
我想,
我想,
我想,
我想,
我想,
我想,
我想,
我希望,
我想,
我想,
我想,
我想,
我想,
我想,
我想,
我想,
但也能够一起建立评测,看看表现如何在多个不同语言中,如何相互保护。
还有商业方面,我专门从研究的角度来说,但我不能与其他公司的商业决定,
这可能会不同,根据法律和法律。
谢谢Irene,这不是一个简单的问题,你做了一个很棒的工作。
现在,我们来调节点,谈谈AI管制的国际机构。
宋教授,作为一名AI高级领导体的一员,您有独特的意见关于在宇宙中的职业角色。
您能否分享您的看法对于宇宙的职位,在形成国际AI管制,
尤其是,您如何看到我们如何平衡宇宙国际机构的承诺,
使得美国提供的机构,
以及它们经常遇到的挑战,以及它的程度和过程的速度。
我们如何确保国际机构,
基本上,是复杂的和有机的,
以保持我们今天谈论的许多我们的谈论者的快速AI发展的速度?
谢谢。
谢谢,我觉得这是我们必须面对的重要问题。
我想,在AI的国际机构领导体之前,
已经发生了很多地区,
例如OECD,EU,以及AI的国际协调,
这些互联网只有40个国家联合,
而其余的160个国家被世界忘记了。
所以,我认为,
国际机构的联合作用,
并不是为了创建国际AI管理,
而是为了创建国际AI管理的国际网络。
而是为了让所有的国际网络,
在国际级的行动中,
尝试自己的行动,
并找出错误的地方,
以连接所有的连接。
所以,这是我认为的原因,
我认为,
在几天前,
美国国际协议审判,
它说,
美国国际协议审判,
应该执行国际AI管理的核心任务。
我认为,
核心任务是,
实际上,
执行国际网络的责任,
并且让所有人都能够使用,
而不是让所有人都不能使用。
另外,
但是,
我们要看到的是,
这些原始的网络,
他们在今年和接下来,
他们想尝试取消
美国国际网络的任务。
他们想取消美国国际网络的任务。
我不是说,
它应该是美国国际网络。
我只是说,
当我们看到所有其他网络的观察,
当我们看到所有其他网络的观察,
他们就没有任何的核心任务,
能够在国际上有信心。
所以,
我认为,
美国国际网络
是最有信心的平台,
能够将所有人都联合起来。
我认为,
中国已经参与了
一些地区网络,
比如说,
AI安全会议,
以及军事AI的重新设置。
我认为,
我认为,
我认为中国会感谢,
能够参与。
但另一方面,
我们看到所有的网络的限制,
他们有自己的重点,
他们有限制,
他们没有权力,
能将所有人都联合起来。
这不是他们的任务。
那么,
那么,
在这种情况下,
我认为,
不遗留国家,
你必须要做,
不遗留国家的方式。
所以,
使用美国网络,
你将所有地区网络都联合起来,
联合起来,
能让他们联合起来,
而在另一边,
那些地区网络,
请不要阻止美国,
因为美国正在联合,
并没有阻止你们。
所以,
我认为最健康的方法是,
联合起来,
让他们有一个处理委员会,
联合联合国家的所有地区网络,
让他们联合起来,
让他们联合起来,
联合起来,
联合起来,
联合起来,
联合起来,
联合起来,
联合起来,
联合起来,
联合起来,
联合起来。
所以可能他們做了不太好的工作
在他們面前有很多的問題
而在另一邊,我們沒有其他選擇
比他們更加有信心
所以他們目前並不是最好的
請幫助他們成為最好的
請使用這個平台解決問題
如果聯合國總統不成功
在一些問題上
聯合國總統委員會
會使用聯合國總統委員會
提供的決定
請問聯合國總統委員會
請他們做些什麼
例如現在聯合國總統委員會的決定
中國主席的決定
與140個國家合作
請聯合國總統委員會
提供的決定
提供的決定
提供的決定
現時的低及中資金家
現時的低及中資金家
操作AI
操作AI
找對這些問題的解決方法
找對這些問題的解決方法
並提供的決定
並提供的決定
以至於明年
我認為這是總統的所有的要求
我認為這是總統的所有的要求
用過在這項議論中
用過在這項議論中
這是使人們
使用聯合國總統委員會
作為最大的成功的方法
最有益處的平台
去做所有國家需要的工作
我可以提出一點嗎, 桓義
我要強調一下, 即使沒有支付器
聯邦聯盟作為一個通訊網絡
也能幫助我們與其他人相熟
我曾經與詹教授見面
在早期聯邦聯盟工作前
這是我第一次與一個在這裏工作的人合作
我覺得這非常重要
確實, 謝謝教授鄭教授和艾琳教授
我覺得教授鄭教授提到的網絡方式
很適合羅伯特的概念
是你提到的國際社會環境
在你的演講中
羅伯特, 我想聽聽你現在的看法
建立一下教授鄭教授的看法
在你的演講中, 你提到
國際標準和報告制度
對AI安全
你能否解釋一下
這與這個有何關聯?
比如你提到的國際社會環境
以及聯邦的作用
在這些方面
是的, 絕對的
好的, 非常好
是這更好嗎?
是的, 這更好
謝謝你的問題
我會很高興地談論這方面
首先, 我想說
我對這方面很深入的思維
所以我非常關注這方面的研究
我的看法是,國會的角色是有重要的角色
我們必須找出這些角色
在這個環境中,正如你解釋過的,就是一個經濟系統
所以我認為,國會已經在做一些重要的事
例如,在發展方面
我認為在我提到的討論中
我提到的ITU和UNESCO
以及其他國會的領域
他們已經在做一些標準設定的事
他們知道他們必須建立更多研究能力
繼續做我們必須做的標準設定
而且他們在發展的角色
這非常重要
我覺得我們必須
例如,我們必須做更多的事情
在發展方面
我覺得UNESCO是一個很自然的地方
你可能會想到
一個負責操縱的操縱
許多社區的主要障礙
在進步AI中參與的社區
其實是在進步AI中
在他們自己操縱的領域中
他們自己的文化空間中
我們可以想到的方法是
很多世界大部分的空間
有很高的手機侵入
所以我們可以用這個方法
來進行數字化
但我們必須做的方法
不需要隱瞞隱私
不需要符合這些社區的標準
所以我們必須做得非常小心
但是這就是我們可以看到的
我們可以看到的
世界的大部分的努力
在進步AI中
以提高發展成果
其他一些事情
我認為有所提及
例如AI的IPCC
是一個有意義的事情
在UNESCO上
一些標準設定的領域
我相信我們可以找到
其他領域
看起來很自然
與UNESCO的領域
合作的領域
我認為有所提及
例如會議
這是非常重要的
所以我們可以找到這些領域
我覺得這應該是
我們在UNESCO的領域
的角色的主要重點
我可以再加一點
關於不同的UN系統嗎
現在你看到的
就像是Robert所說的
不同的UN系統
已經做了一些
有關AI的事情
獨立的
我們有一個
我們在幾個月前
在吉尼巴發表了一個
無線會議
我們參觀了幾個
不同的UN系統
在吉尼巴
是由聯合國
組織的
組織
我們看到的
是這些不同的
UN系統
主要是
他們當然看到樹木
但有些時候
他們錯過了
他們看到樹木
但錯過了樹木
也不通過
所以他們還是
把樹木形成樹樹
在吉尼巴省的
根本就是
一個無線的
不通的樹木
所以就像剛剛所說的
我們看到的
在吉尼巴發表了
一個無線的無線 appreciate
就是inos系統
的獨立的
都是形成樹木
只要有多個樹木
就可以得到
這種量度
可以讓你
的方式
可以讓你
讓你
對著
獨立的樹木
有什麼理由
你們可以完成
這個發展
就有什麼理由
的就是
有很多更多,所以美国联合联合公司必须在现在相比起现在,连自己都要组织更好的组织,
所以会是联合联合公司的责任,帮助美国联合公司有更好的组织,
让每个人都能够在美国联合公司的系统中辅助,
并不仅让每个人辅助,还能够辅助每个人,并解决问题。
也有一个例子,美国联合公司在美国联合公司的系统中辅助,
但美国联合公司和其他国际辅助组织,例如IEEE,
他们并没有足够有效的组织组织。
因此,
美国联合公司的联合联合公司,
需要联合公司的会员联合联合联合,
以便接触那些陆统的担任联合公司,
作为帮助来举办多类联合公司联合 mini Stroma一些设施。
我们可以通过相联合公司通过其他组织那些团体联合公司做一些联合公司,
并团 со一些相联合公司生产这一切特别的运势业。
不仅讨论联合公司的某些组织,
在聯邦制度中的標準組織之中
謝謝教授Traeger和Zong教授
我們時間很短
所以我想先結束我們的討論
請問最後一個問題
您所有人都表達了
國際智慧管理的成功和勝利的想法
我希望聽聽每位主持的意見
您認為國際社會應該做的一件事
在未來6-12個月內達成這個目標
例如在未來的聯邦議會會議會
或是法蘭斯 AI 活動會議會或其他會議
我們想實現這個討論
所以我只想提供一個建議
可能Duncan
因為我們還沒有聽到您的意見
我們可以跟您開始
好的 謝謝
我覺得我的一個訊息是
要幫助準備未來
我們需要幫助我們的社會
無論是在聯邦議會
或是在支持網絡的網絡
我們需要想想現在
我們需要幫助我們的社會
能夠成功通過
我們見過的最大的改變
我們見過的最強大的技術
我們需要現在思考
現在設計
現在設置
我們需要的各種社會
我們需要幫助社會
在未來的發展中
在大多數的事件中
我覺得我們面對的其他非常嚴重的不確定度
我們必須要著重預期
我們需要建立的話題
它們可能需要的 among certain scenarios
所以那些個案會期間預備
討論了很多關於
如何將科學家將共同的理解
建立相同的理解
關於 與技術有關的危險 和挑戰
這件事十分重要
就是為了將社會科學家和各種不同的觀點
聯繫在一起,並為我們提供所謂的管理機構和機構的機制
我們需要這些機構幫助人類在這段時間內進行進展
讓我們可以在500年內回顧
雖然我們需要在這些問題上合作,但我們做得到
這些科技使我們能夠合作
而且我們成功地做到了
謝謝,Duncan
Irene,我們可以讓你下一位
你可能也想探索一下研究中心和發展企業的角色
你剛才提到的
絕對的,這就是我剛才所說的
我的極為嚴重的反應是,我們可以休息一下
我很累,我知道我沒有獨處
而非常嚴肅的反應就是Duncan所說的
關於正確的專業
一個很重要的原因是
我工作在Hugging Face
這是我第一次看到
一種我的傳統語言Bangla
在一個數據中研究
確保我們包括不同的群組
不同的技術研究員
並且解決一些不明顯的技術犯罪
但非常基於歷史、文化
以及世界不同地區的問題
謝謝Irene
Matt,我們可以讓你下一位
在這個問題上
好的
好,我第一件想要提問的事
在接下來的6至12個月內
我認為是美國和中國
會進行某種類型的合作評論
在具體的大規模的危險上
這將會非常複雜
我們要想辦法讓兩邊都覺得安全
我覺得在政府層面上
這可能會太難了
最好是在AI安全基礎上
或AI安全基礎上做這件事
這可能在這段時間太艱難了
但如果不在2階段上做這件事
有中國最好的技術評測員
以及美國最好的技術評測員
在美國的安全評測員中
一起談談他們使用的方法
他們所看到的問題
以及他們將要做的方法
謝謝Matt
當然,我們也提到在這個討論中
AI安全測驗是一個非常重要的項目
非常重要的項目
以及有可能的國際聯繫項目
可能Robert
我們可以回答你這個問題
你希望未來的6至12個月內
你希望看到的事物發生什麼呢
我認為我剛才講了一些特別的事情
所以我可能可以以這機會
反映之前提到的一件事
以及說一件事
就是我認為在這個擴大的環境下
我們有很多例子
當他們認為要做的事情是重要的時期
雖然你不會預測它會發生
但你仍然需要嘗試讓它發生
找出它能發生的機會
例如在這方面的科技管理
特別是AI管理
美國民主黨和共和黨
在這段時間之內
並不知所措
但他們仍然在議會議會中
尤其是進行進展AI管理
這是一個例子
《非核心合作協議》
是兩國共和黨結合的時期
因為美國和蘇聯
當時的關係不太好
但無論如何
兩國共和黨結合
他們仍然做得到
因為他們認為
它是他們的共同利益
他們認為它是重要的
所以你不會預測
這些事情會發生
但你仍然需要找出
它們能發生的機會
謝謝Robert
我們很高興
能夠結束這堂會
就請教授鄭教授
最後一個問題
接下來的12個月
我首先要告訴你
什麼已經發生了
在這8個月前
我已經不知道
雷明河在這8個月前
發生了什麼事
所以我去過
新加坡的
亞洲技術會
但是不是雷明
邀請我來
所以我責怪雷明
然後我們結合
他來自政府
所以我是從
一個研究組織來的
所以我可能
在正常情況下
也不能跟雷明討論
然後在這8個月之內
我們成為了一個團隊
雷明對我非常好
他在新加坡
提供了非常好的
亞洲食物
他也告訴我
新加坡的練習
但不僅如此
他和所有的總統
和副總統
在這個大型的
亞洲技術公司
都結合了
還有很多
曾經的政策人員
都結合了
所以我看到了
所有人的價值
都在結合
你們不明白對方
你們說我們
完全不同
但當你們結合
你會覺得
OK 不太大分別
然後
你們有安全問題
你們有發展需要
我們解決問題吧
因為相比於
去年
現場的風險
已經有10次
相比於去年
全世界的風險
你會看到
風險重新發生
在不同的國家
然後你會看到
聯邦制度的限制
如果我有錢
還有人
我會把所有的錢
都給聯邦
創造一個國際機構
但是我沒有錢
所以
在下個12個月之內
最重要的事情
就是
起碼要創造
像是歐洲的Train
有歐洲的AI服務
現在我們應該有
歐洲AI服務
最小的Train
將所有的歐洲機構
全體組成
並於這麼好的方式
來操縱他們
還有
並且討論這些地區網絡,例如OECD,IEEE和ACM
所以我認為我們必須移到這個範圍
因為在AI中的聯邦諜報組中,我認為我們看到了這件事的需要
沒有這些需要,我們就沒有機會把這個國家遺棄
謝謝宗教授,把這個國家遺棄,這是一個非常吸引的想法
宗教授在這篇討論的開始時,宗教授提到我們目前沒有知識方法
以避免存在任何失用或失控AI系統的糟糕危險
是很明顯的,前進的路會很困難和複雜
但從這篇討論中,我們聽到的是共同的承諾
確保AI的發展能讓人類成為一體的利益
請加入我們,讓我們鼓掌鼓勵一下我們的主席
主席,請坐下
我們的技術人員,請來幫忙坐下
我們的技術人員,請來幫忙坐下
好的,謝謝
好的,為了結束我們的討論
我們很榮幸能讓教授周博文在今天的最後言論中發表言論
我們有請上帝博士,讓教授提到
上帝博士周博文是上海AI研究研究所的主席
上帝博士是上海AI研究所的主席
而他在廷華大學當學的主席
他曾經是上帝博士
在JD.com作業的電商巨星
他曾經在JD.com作業的電商巨星
在JD.com作業的電商巨星
當時他擔任上帝博士的副副副副副副副副副副副副副
JDAI Research
Prior to that
Professor Joel held various technology leadership
and executive positions at IBM
including as the director
of the AI Foundations Lab
at IBM Research
Professor Joel
has decades of experience in AI research
and is a recipient of the prestigious
Wu Wen-Tsun Award for Outstanding
Contributions in Artificial Intelligence
Professor Joel
Thank you so much for being here today
The floor is yours
Thank you for having me here
and more importantly
thank all of you for staying late
I was asking to give
concluding remarks
meaning this is the end of this
fabulous forum
but I'm thinking this is
not the beginning of an ongoing
contextual dialogue
in that spirit
I'm thinking to give the talk
I was given yesterday
as an open ceremony
because I was told many of you
were not there in person
so bear my
switching to Chinese now
so if you don't speak Chinese
please time to put your translator on
So the topic I'm going to have today
is
I think we need something new
integrating tech and governance
With that I call it
artificial intelligence 45 degree of balance
right now
as a big model
the raw energy of artificial intelligence
is developing rapidly
but with the constant improvement of capabilities
we're seeing more and more
of potential risks
from the public's understanding of AI risks
in order to look at
this includes
data leakage
data abuse
intimacy
 copyright dispute
I call it data content risk
and the other is the malicious use of data
带来的伪造虚假信息
等带来的使用风险
当然也诱发了
偏见歧视等相关的伦理问题
还有人担心
是否会挑战就业结构
等社会系统性风险
当然在好莱坞的电影里面
也出现了AI失控
人人完全丧失自主权的
这种极端风险
这些风险
有些已经是在现实中出现
更多的是潜在的
防范这些风险
需要各界的共同努力
需要科学社区做出更多贡献
去年5月份
数百名AI科学家
共同签署的
Statement of AI Risk
也表达了对AI风险的相关担忧
并呼吁防御人工智能的
系统性风险
应该和流行病和核战争
等大规模风险一样
成为全球共立的优先话题
从一个
我做技术的角度来看
出现这些担忧的根本原因是
目前的AI发展
是失衡的
我们来看目前AI的发展趋势
横轴是AI Capability
重轴是AI Safety
在横轴上
在以Transformer为代表的
基础模型架构上
加以大数据
大参数量
和大计算量的Scanning Law
目前AI的Capability
正在快速的成指数去增长
与之对比
在AI安全领域
我们看我们有什么典型的技术
比如红队测试
安全标识
安全复栏
与评估评测等等
都呈现出
是离散化
碎片化
很重要的是Ad-hoc
非常的厚实
当然
最近出现了一些新的技术
兼顾了性能和安全性
比如监督式微调
SFT
人类反馈的强化学习
ROHF
RAF
Super Alignment
等等
这些方法
最主要的特点是
把人类的偏好
传递给大模型
也涌现出了
比如TRAD GPT-4
等令人兴奋的AI系统
以及我们上海AI实验室的
舒适系统
也会提升intent大模型
等等
虽然这些技术
瞄准的是
安全和性能同时提升
但在实际使用中
大家往往发现
更多是性能优先
所以总体上
我们在AI模型的
安全能力的提升
还远远落后于性能
这种丝痕导致AI的发展
是跛脚的
所以我们把它叫
grip的AI
但是这种不平衡的背后
实际上是两者
投入上的巨大差异
从右边的对比
大家能够看出来
两者在技术研究中
技术上是否体系化
人才的密度上
商业驱动力方面
以及算力的投入度方面
对比来看
安全方面的投入
是远远落后于AI能力的
我一直在呼吁
要加大对安全的
算力的投入
我举的例子就是说
你AI system
like a little child
当小的时候
你可能花大量的算力
去帮助他吃好
喝好
衣服穿好
但是在孩子
慢慢graduate up
grow up的时候
yes but more time
去跟他
你更多的焦虑
不知道他是不是吃好
喝好的时候
去跟他做各种价值的交流
这种价值的交流的投入
实际上就是算力的投入
但是很不幸的是
我们大部分的算力
都投入在预训链上
很少很少也不必用在安全上
所以这种投入的失衡
导致了我们现在Clip的AI
我们真正需要追求的
我一直在讲的是
包括从美国到中国
我的学术生涯
一直在追求的Trustworthy AGI
也就是右上角这个路线
这是我们的新城大海
我把这叫做可信AGI
如果我们找到
兼顾安全和性能
我们需要找到AI安全优先
但又能保证AI性能的
长期的发展的技术体系
我个人把这样一种
技术思想体系
叫做AI45度平衡率
AI45 degree law
AI45度平衡率
是从长期的角度来看
我们要大体上沿着
45度
安全
以性能平衡发展
所谓平衡是指
短期内可以有上下的波动
但长期内
不能长期低于45度
如同我们现在
也不能长期高于45度
这将阻碍发展与产业应用
这个技术思想体系
它是强技术驱动
全流程优化
所谓全流程优化
我在23年的一篇
Trustworthy AI的中述文章里面
在ACM Competence Survey上发表
提出是要把全流程
从数据的准备
模型的训练
到部署之后的
Operation和运营
全部从安全的角度来进行优化
同时也需要多主体参与
我想这是刚才
那个Forum讨论的很多的话题
当然也包括敏捷治理
实现AI45度平衡
从技术角度来讲
也许存在很多的路径
我们上海AI Lab
最近在探索一条
以因果为核心的路径
我个人把它取名取为
可信AI家的因果之梯
这也是致敬因果推理领域的先驱
图里奖得主Judy Appel
可信AI家的因果之梯
我们把可信AI家的发展
分为三个阶段
分别是
犯对棋
可干预
能反思
犯对棋主要是包含了
当前最主流最前沿的
人类偏好对棋技术
像我们前面提到的LHF
但是需要注意的是
这些安全对棋
仅依赖于统计相关性
而不是真正的因果关系
这样可能会导致错误的推理
以及潜在的风险
一个经典的例子
是巴普洛夫的狗
当狗仅仅记忆
铃声和食物的相关性
形成条件反射时
它可能在任何场合
听到铃声都会触发它的行为
这里这个行为是分泌堕叶
但如果把它想象成
这个行为是金融转账
医疗决策
甚至是军事相关的决定
这显然是极其不安全的
所以我们需要第二层
过来叫做可干预
可干预主要是
通过对AI系统进行干预
探究其因果机制的安全技术
比如能在回路
机械可解释性
也包括我们刚刚提出的
对抗演练
adversary rehearsal
它可以通过
提高可解释性和泛化性
来提升安全性
同时也提升AI能力
能反思在第三层
则要求AI系统
不仅要追求
高效自行任务
还能审视自身行为
带来的影响和潜在风险
从而在追求性能的同时
同时确保安全和道德的
边界不会突破
这个阶段的技术
包括value audit的training
记忆价值的训练
因果可解释
以及反思时推理等
目前从业界的技术发展来看
AI的安全和性能技术
主要停在第一阶段
部分在尝试第二阶段
要真正实现AI的安全
与性能平衡
我们必须完成
必须完善第二阶段
并攀登至第三阶段
也就是说
沿着可信AGI的因果之积而上
我们想象可以构建
真正的可信AGI
实现人工上的安全
与作业系统的完美平衡
Ultimately
最终我们是希望
像安全可控的核聚变技术一样
为全人类带来清洁丰富的能源
我们希望通过深入理解
AI的内在机理和因果过程
通过安全有效的开发和使用
这项革命性的技术
也正如可控核聚变
对全人类都是共同利益一样
我们坚信AI的安全
也是全球性的公共福祉
需要国际社会的共同努力和合作
我们愿意大家一起携手推进
AI45度的发展
共享AI安全技术
加强全球AI安全人才的交流与合作
平衡AI安全与人类的投入
共同构建开放安全的
通用人工智能创新生态
和人才发展环境
谢谢大家
非常感谢教授
为Safe-AGI的创新生态
分享你的吸引力
今天很高兴有你
今天的会议目的目的
是为人类的利益
提升AI安全的状态
我们组织了四个主题
AI安全研究
AI安全评测
AI安全指导
和国际联合协调
我们邀请了
超过25位伟大技术家
在世界上
举办了超过18个课堂
和5个课堂
超过8个小时
我想提供一段简单的
便当和思维的
 insight
所有的说家
为今天提供的
AI安全研究
许多话题
支持AI相关的
科学认解
并安排AI机械方向
和安全
特别在中央关系
有几位认定
将AI安全
投入为国际公众利益
包括10%的AI负面设施
在中国开始
对AI安全认定
叶格莎
AICV 測試, 進行全生命測試, 進行新的方法和科學化大型模式測試。
AICV 指導者, 社會需要平衡AI 發展和AI 操控的重要性, 特別是在發展國家的問題上。
我們也應該考慮一層性的操控方法。
在不同國家和不同區域中, AI 操控方法的相關方法, 我們互相學習了這麼多。
最後,在國際合作方面, 多位專業人士提出了AI 安全上的紅線要求, 以及維持分別和開放性。
透過持續的合作和討論, 世界應該努力對AI 國際研究院和國際議會進行研究。
正如教授說的,
今天,
AICV 的討論會只是討論的開始。
我希望今天的討論會給予各位聽眾和觀眾更多的想法和機會, 為人類的重要問題合作。
特別感謝今天論壇的各位嘉賓和朋友們。
今天論壇圓滿結束。
安全AI希望本論壇可以進一步推動前沿AI安全與距離的討論和行動。
期待和大家再見。
謝謝。
 Thank you.
請今天的嘉賓留步。
我們一起再上臺合影。
謝謝。
謝謝。