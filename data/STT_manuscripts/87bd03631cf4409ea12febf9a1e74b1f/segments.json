[
    "尊敬的各位领导,嘉宾和朋友们,大家好,欢迎大家来到今年世界人工智能大会的前沿AI安全与治理论坛,我是谢敏希,安原AI的CEO,安原AI是一家安全与治理领域第三方研究和咨询机构,也是目前该领域全国唯一的社会企业。各位领导,各位朋友们,各位外国的各位,早上好。欢迎来到COCDI AI的2024世界人工智能大会前沿AI安全与治理项目的世界中心。我的名字是帕兰谢,我是COCDI AI的CEO,这是一个社会企业集中于AI安全与治理。本次论坛,我们特别荣幸邀请到上海市的领导,莉琳指导和交流。请允许我为大家介绍,上海市人民政府副秘书长,庄木蒂先生。去年4月我国中央政治局会议深刻指出要重视通融人工智能发展重视防万风险同年10月我国发布全球人工智能治理倡议重申各国应在AI治理中加强信息交流共同做好风险防范同月我很荣幸受邀参加了首届全球AI安全峰会见证了包括中国在内的28个国家和欧盟共同签署布莱切里宣言Blessed Recreation这也是第一份AI安全的国际声明在此背景下社会需要加强前AI安全研究安全评测安全治理以及国际合作这也是今天论坛的四个主题第一个主题是安全研究我们很荣幸邀请到国内外AI领域的世界级科学家图灵奖得主Yosua Bengio前头发布了第一份先进AI安全国际科学报告由30个国家欧盟和联合国提名的委员会共同参与对通用型AI的安全风险进行了科学评估中国工程院高文院士认为全世界正处于AGI强人物质能的前夜在一个不确定的状态需要严加防范AGI可能会引发的人类生存风险中国工程院张亚琴院士联合Yosua Bengio召集了第一届AI安全国际对话并联合伯克利分校东宋等领先科学家在Science主干上发表论文建议分配三分之一的AI研发资金到AI安全和伦理等研究方向我们期待和多位AI安全科研团队带头人包括上海AI实验室的邵靖北京大学的杨耀东和上海交通大学的张卓胜讨论前沿研究问题第二个主题是安全评测我们很高兴邀请到大模型安全评测的领军人物在学术研究方面上海AI实验室领军科学家乔宇第一次以人类价值观的角度对多模态大模型进行了全面评测天津大学NLP实验室主任熊德义发表了中文大模型前沿风险评测的一系列论文在行业联盟方面中国信通院人工智能研究所所长魏凯依托AIA安全距离文娱会启动了一系列大模型安全评测工作OpenAI Anthropic谷歌DeepMind和Raria成立了前沿模型论坛执行主任Chris Massero将分享领先美国企业的安全实践第三个主题是安全距离各国家正在开展对AI安全距离的积极研判和尝试我们很高兴邀请到法国政府人工智能委员会成员Gail Veracqua新加坡政府首席AI官何瑞敏中国政法大学数据法治研究院教授张灵涵以及伯克利分校Center for Human-compatible AI主任Mark Nisberg分享多元地区视角同时我们也邀请到上海教育大学中国法语社会研究院院长纪威东和上海教育大学上海AI实验室治理研究中心副主任王云春参与援助讨论探讨AI立法和上海AI治理经验第四个主题是国际合作我们很荣幸邀请到多家国际顶尖智库包括凯莱基国际和平研究院主席Mariano Ferratino-Cuella和研究员Matt Sheehan清华大学人工智能国际治理研究院院长薛兰纽金大学马丁人工智能治理中心主任Robert Treger加拿大国际治理创新中心全球AI安全风险主任Duncan Kaspeks讨论AI安全的国际治理议题联合国AI高层顾问机构专家曾毅将提出AI安全红线全球领先大模型开源社区Hugging Face全球政策负责人Iron Solomon将讨论开源模型对国际治理的影响最后我们将邀请上海AI实验室主任首席科学家周博文进行闭幕致辞展望AI安全的未来现在我们进入论坛的正式环节",
    "首先有请上海市人民政府副秘书长庄木地为我们的论坛进行开幕致辞有请尊敬的高文院士尊敬的张雅琴院士各位来宾女士们先生们朋友们大家上午好很高兴和大家一起相聚在2020世界人工智能大会共同参与前院人工智能安全与治理的论坛共同探讨人工智能的发展趋势和治理问题首先我代表上海市人民政府对本次参加论坛的科学家企业家以及媒体朋友们表示热烈的欢迎和衷心的感谢人工智能作为新能科技革命和产业变革的重要驱动力正深刻的影响着全球经济结构和社会发展随着技术持续迭代的演进人工智能的安全和治理也乐意成为全球关注的焦点中国高度重视人工智能的健康发展去年10月习近平主席提出了全球人工智能治理的倡议系统地阐述了中国关于全球人工智能治理的立场主张和建议展现了中国在推动全球人工智能发展和治理方面积极的态度和务实的行动去年11月包括中国美国在内的28个国家和俄盟共同签署了布赖切利人工智能安全宣言这也是全球第一份针对人工智能安全的国际性的声明体现了中国在全球人工智能治理领域的职任和担当上海作为中国经济城市的中心和科技创新的前沿在人工智能安全和治理方面开展了实践和探索特别是在全国率先出台了人工智能的地方性的一部法规就是上海市促进人工智能产业发展条例探索构建体系化的治理框架统筹人工智能发展与安全同时也发布了人工智能标准化体系建设的指导意见推动上海在人工智能标准领域的先行先试努力培育人工智能高水平的上海标准展望未来我们将继续在人工智能安全和治理方面发挥引领作用我们将持续完善政策体系加强技术研究和人才培养制定更具操作性更加完善标准规划和测评体系我们将坚持包容省政监管以鼓励创新为原则探索大模型评测四点沙盒监管我们将积极推动自力研究在健全法规体系监管体系等方面努力探索努力形成具有上海特色的监管实践方案各位来宾本次论坛汇聚了世界级的专家学者和业界的领袖将围绕全员人工智能安全的研究评测 治理等议题展开交流讨论我们相信通过大家的共同努力我们一定能够成为全球人工智能安全和治理问题提供务实方案和有益借鉴推动人工智能技术更好地博物与人类社会的发展上海将提供更加开放的平台更加丰富的场景更加优良的环境支持全球人工智能安全和自理领域的研究者的进行深入的探索和实践最后预作本次大会许得圆满成功谢谢大家",
    "谢谢感谢穆迪秘书长的精彩致辞请入座大家好我叫吴君怡是安远AI高级项目经理也是今天论坛的主持人鉴于今天有多位国际嘉宾我的主持将用英语进行各位观众各位女士们我的名字是冠义恩我作为冠义AI高级项目经理的职员我将是今天的主持人由于我们有大量的国际主持人大多数这次的会议会由英语主持不再多说我感到很高兴能够介绍我们的主持人业务教授业务教授业务教授我身处世界世界世界最有能力的機械智能 oss业务教授但他的作品中国教授Torture业务教授业务教授业务教授业务教长业务教授业务教授业务教授to publish the International Scientific Report on the Safety of Advanced AI,which I was honored to contribute to as one of the writers.Professor Bengio will be sharing key findings of this reportand open problems in AI safety with us today.All right.Thank you for joining us, Professor.Yes, we can hear you fine.Okay, I'm going to start.Thank you very much for the kind words,感谢您的工作在这篇文章中今天我想告诉您互联网报告关于互联网的安全我已经在分享互联网报告的意味是互联网的危险和安全的决策这主要是专注在预先的互联网有很多种类的互联网所以我们在考虑主要是互联网的危险和安全的决策包括互联网模式以及其他多模式的模式我们最近看到的都吸引了很多人的注意所以有两部分首先我会谈论报告然后最后我会说几句关于它的意味未来的大型建筑设施即使报告本身没有任何建议报告是科技的结构是为了帮助政策人员的工作好 报告叫做《国际科学报告安全与进步智能的报告》我们花了很多时间找出正确的名称所以我们得到了这个名称报告专注于危险因为当然已经有很多科学研究的工作在AI的应用和利益上但是对于政策人员的关注他们必须要明白危险 风险以及能力以处理这些风险例如 规则我们被任命在昨天的《国际AI安全协议协议协议协议协议会》上在《国际AI安全协议协议协议协议》上发布了禁止流言母语 可连时重加入检测需要는데요一个问题emphas由于领导谷歌甚至带咱们领导可以和offMP33米W mindfulness从谎言和谎言到预期的谎言例如工业市场的影响很多不同的缺乏还有最大的风险例如失去控制或是超级人工智能而主要的目的并不是创造新科学而是总结综合现代科学提供的证据帮助政策者这群75人在工作有一个负责任者计划每个国家30个领导人也有一个领导人也有一个领导人由美国和美国决定这是我们的计划另外我们也有一个领导人今天会有16位导演这群人在实际上是写书的人这群人在实际上是写书的人的频道是可以提供的的频道我们团队和不同的版本也会在此联系我们也有一个企业顾问他将在所找到的各种方面的领导人本来是26个领导人所以总共有75位领导人是有很大的领导人在几个月内在几个月内那么我们在报告中发现什么好有些颜色可能不惊人但对政策人员来说在科学社会中有很多不同意见关于风险但我们进入更深入有些问题有更多意见有些没有所以也很重要不仅说有不同意见但是也有不同的观点关于风险AGI的时间线会不会发生会不会有几年会不会有几年之类的然后有关影响的观点AGI会发生什么会不会有快速的取消会不会影响社会这些都非常重要报告也谈到风险生产所以我们看到什么是现实科学去尝试解决风险那些方法是什么他们的限制是什么最后的结论如果你想要一句话就是不幸的是目前没有认识的方法去防止现实风险和未来风险可能会被坏例如失用和失控的风险所以这是一个很大的召唤一个很大的红旗但是但是银河是我们仍然有机会综合来说世界能够更好地理解这些风险并更好地解决它好那我们再谈谈更深入的问题首先当然我们为什么要担心风险是因为有利益总体来说风险可以非常有用可以使用在许多优秀的应用中但是只有如果我们正确地管理它因为有风险我们认为风险有三个类型我们讨论了很多如何在不同类型中组织这份报告所以有风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限风险有限我们不想失去控制,但这可能会是个错误,这是一种错误,当然这是一种很严重的错误。然后,我们有系统风险,那是什么?那就是社会和科技联合的事情。例如,工作市场的影响。当我们越做越多的工作,那些工作的价值就会减少。如果同样的工作可以做到10倍的钱,那些工作的价值就会减少,因为你可以做得更便宜。那么,那些失去工作的人会发生什么事呢?现在,这并不是世界上的问题,但它可能会变成这样的问题。另一个问题,另外一个系统风险,就是所谓的AI分隔,即是AI的才能和能力在几个国家中集中。那是在其他国家中发生的事情。在几个国家中集中了一些力量。那是否意味着AI的利益将会集中?那是否意味着AI将会在某些国家中发展的方向?但也许不是在全球南部。另一种集中风险,是在市场精神化中的集中,为了训练在这些AI系统中的正常性。训练的工资在了这样的程度上,因此我们发现了我们可以训练更多的模特,他们的技术就越来越回升了。这就是所谓的层次的费用。但这也意味着,这意味着很少人会有资金来训练这些AI系统的未来世代这意味着市场的效率市场的严重感染在社会和经济上没有好处另一个系统性的风险与社会的互动有关就是环境影响我刚才说了训练这些系统的费用增加是因为我们训练更大的系统这需要更多的能量需要的能量增加了这不能永远持续很快AI系统的训练费用将影响全体能量可能有10%的费用所以我们必须看到这一切并考虑一下当然AI系统有关其他社会的事情例如私隐和费用费用费用都在报告中好报告也谈论了社会风险我们称之为社会风险例如制度需要时间去实施即使你有法律也可能需要很多年去实施制度组并解决了社会的问题当技术改变可能需要时间去适应其他风险有关力量的关系我们要小心好关于技术方法来减少风险我会谈论更多我们谈论很多但是现在没有什么很满意的他们都有限制我会谈论更多所以一个主要的结论是我们继续增加AI的能力我们必须更加投资不仅为了让AI更有能力更有能力之类的但也为了更加了解这些系统这样我们可以解决风险更了解我们的途径好现在我们来谈谈国际的风险这些风险有很多谈论正如你可能知道在媒体上或者在聊天中或者在社交媒体上特别是有一个很大的分别就是人们认为没有风险而人们认为风险是很糟糕的但是有趣的是我们可以看看为什么这些分别会从哪里来而且你必须要认为风险和未来的事情有关而最主要的风险最高的风险是不再存在的系统所以当然人们不会有铁锣球知道未来的目标所以实际上我们会有哪种AI在几年或几十年内会有这种AI我们知道风险很明显AI的能力继续增长并且速度增长所以这些不同的观点意味着人们不同意一些像是AI在工作市场上的影响或AI在赛博攻击或是生物武器攻击或是控制失控的影响但是我们发现那些观点的分别是最好被人们认为AI会增长的速度也会增加人们认为AI会变得更有能力还有不同的期待社会会做什么以处理那些风险和制定和协议的效果所以最主要的解释是将来的速度是不明显现在当我们说有不明显的东西像这个非常重要将会改变未来从科学家和科学家的看法如果科学家不同意未来的速度科学家必须用刀准备所有的情况比如AGI会在三年或者三十年我们必须准备所有这些选项对于政策好现在我们再谈谈科学方法和它们的限制所以已经有很多技术方法来测试和减弱科学方法的风险所以测试风险是一件事那么是否有问题科学方法能够做出危险的事情例如这是一个的问题我们可以用它减弱风险因为如果我们证明有危险的事情不停地减弱风险或者继续训练减弱风险是一个不一样的事情我们如何改变科学方法来防止它做坏事例如被迫害人之类而重要的是我们设立这两个的规则因为制度是要使用最好的方法既是测试又是减弱风险现在不幸的是目前的方法既是测试又是减弱风险有限制而制度理解这些限制是重要的例如我们不明白如何现时的智能设计新的线路如何决定人类的设计这些都是设计的设计但当它们设计后就像你有孩子可能我们明白人类生长的生命的生命但我们不明白为什么它们做什么这就像这里这是问题因为当智能设计是一个很重要的问题另外现在的安全保险是很容易去除的例如紧缓尤其如果你可以做好调整所以如果智能的重量是有限的那非常容易去除所有安全保险好那么在测试现在的方式为什么不太好是我们基本上问了很多不同的问题看看所以我们只是在不同的情况下测试但是这些是组织检查我们无法检查每个可能的问题这是有用的因为如果我们发现了某些东西那我们就知道有问题了但是没有组织检查的安全保证如果组织检查没有找到任何东西那不代表没有任何问题我们只能说出有问题的事情如果没有任何问题那可能还是有问题的所以我们不应该只看着车子我认为我们还有几年政府有很多事情可以做以解决这些风险我们必须更明白这些系统是如何运作的我们必须仔细思考为什么和如何发展智能是否会成功是否会成功是否会被通过的方式能够提供在争夺攻击者或防守者的方式来思考智能攻击这些是使用智能的 variations可能更能提供攻击者或防守者的方式那么我们如何获得智能的经济利益这是一个我们可以选择的问题谁会利用它我们将如何投资研究这就是我们上个月发布的中心报告请大家看看我们将在今年结束之后发布更大的报告现在我们来谈谈大图报告讲了很多关于时间线的不确定性我们看到了不同的标准在 X 线上的时间线在 Y 线上的表现而黑色线是人体表现我们看到在很多任务上时间过去人体表现会更好所以我们需要学习管理这些风险正如我们在文章中谈到的有很多中国同学在今年发布的《管理超级AI风险隐蔽的速度进步在科学上》那么我们该怎么做总体来说我们对这种非常快速的改变并不太准备例如考虑新冠病毒所以我们必须现在开始准备准备我们必须考虑国际协议决定风险线尤其是风险隐蔽的风险例如控制失控和缩减我们必须做更多的研究正如我所说的要更了解风险隐蔽能力和其后的影响并建立计划如果我们发现风险隐蔽一个这种线线的风险隐蔽我们必须立即承诺如果发生了这种问题我们会做什么总体上我们必须使用大型风险隐蔽的方式当大型风险隐蔽的不确定我们必须使用大型风险隐蔽的方式好一件重要的事情是我们如何投资不同的解决方案在安全方面我们应该想起研究项目的项目研究项目尝试改善测试和如何使AI更安全例如有些研究者已经研究了什么叫做安全的设计AI我们可以得到要做的保证但有些方法可能是提供更强的保证要花更多时间去发展有些方法可能会比较容易做或可能不太安全但是如果AGI发生在三年内我们必须在三年内准备所以我们需要所有这些项目都在同时行动最后有个问题是联合与发展商在不同国家就像在报告中提到的而且我们需要对不同的国家进行战斗AI可在电子兵员的时候可以成为武器比如电子兵或者其他武器所以现在我们必须在国际讨论中照顾安全并讨论如果会出现会否我们可以compliance verifiability and we need to develop technology for this thank youthank you so much professor benjo for sharing your insights and we'll let youcontinue on for the rest of your evening it was an honor to have youour next speaker is professor gao wen who is a member of the chinese academy of engineeringacm fellow and ieee fellow he is the founding director of pungchang laboratoryas well as the boyar chair professor and director at the faculty of information andengineering sciences at peking university professor gao is currently a deputy to the 14thnational people's congress and he used to be a member of the 10th 11th and 12th cps pccnational committee the vice presidentof the national natural science foundation of china and chairman of the china computerfederation and the chief editor of the chinese journal of computersprofessor gao it's an honor to have you here with us i'll hand it over to yougoaderorit'sMisakoNerothat'sdirigacconfornetuyesahokayےwhat'sthatyeahthat'soutfantastic他就说现在人工智能有太多的问题需要去解决了如果你要有那功夫你还是先解决点问题先别说怎么样保证安全你先把人工智能本身没有解决的问题解决解决我们第二位德鲁麦尼欧就比较有意思他就说人工智能只要是确定的这件事其实都是可控的不确定就不行那么第三位安迪和刚才的恩久是一样的因为他们和亚琪琳一起写了一些文章在整理这些报告就是要让我们一定要纵使今天我讲的是其实人工智能的安全性确实是问题的两个方面一个就是说作为技术研究你必须要把这个技术本身要做到极致让它有用当然作为社会学家你更做的要考虑这样一项技术它对社会带来的影响到底是什么那么如果这个影响有负面的你有什么办法把它控制住这可能是一个问题的两个方面我想因为我们这个社会呢在发展的时候需要所有的人的关注当然所有人关注不是所有人都做同一件事所以我们要有很好的分工今天我们就讲一下这个分工的问题AI其实我们知道它确实是很强大通用人工智能这个强大了以后呢我们就要让它向善就是让它做它应该比较理想的事做的比较理想的事情所以我们说AI向善里面呢最主要的呢我们要从两个角度从技术的角度呢要把人工智能技术本身要做的足够好今天的人工智能呢确实还有很多问题所以按现在的这个水平它还没有办法向善第二个呢就是从伦理的角度你必须呢要在伦理道德等方面给它规范好所以我想这是AI向善里面比较需要关注的两个方面那今天的AI技术是不是足够好的呢刚才说了其实不是我们现在AI的水平呢还不够高作为因为刚才Benjo也在他的最后的这个slice里面其实也提到了就是在他总结之前的slice里面也提到了作为单向性呢有一些AI已经超过人了有一些还是不行什么时候几乎所有的性能都超过人的时候那个就是比较好了就是可以真正发挥作用所以我们说今天的这个AI技术呢我们认为它表现的更多的是一种低水平的智能什么叫低水平的智能呢就是死记硬背的智能就是靠显示知识的记忆和使用那么来表现出来的智能真正好一点的智能呢其实是中水平的智能是中等的高水平的智能是最理想的我们现在其实做高水平的追求高水平的智能还有点那是非常遥远的事我们要追求追求中水平的智能所谓追中水平的智能呢就是用比较少量的显示知识就可以获得的智能用我们人类的这个学习能力来说你有非常强的举一反三的能力而现在的人工智能系统是没有这个能力的所以我想我们现在呢可能当前是在低水平智能某些单线还可以靠死记硬背或者是靠数据训练出来的那么等到了只有少量的样本就可以训练出智能的智能了我们大概就到了一个中水平的智能而且它可以跨越领域从一个领域可以很容易就类推到另外一些领域就像以前搞机器学习这个类比推理你能做到那是中水平的智能高水平的智能呢这个我们就可以把它笑一笑听一听因为这个高水平的智能相当于说就像人类里面也没有多少人能达到的那个智能你让计算机这个系统去做那是非常遥远的事情那么低水平智能里面呢其实有一个有点像悖论一样的情况很多人就说因为有时候讨论问题说既然你说现在的智能是低水平的智能它为什么会有智能涌现低水平智能是不应该有智能涌现的其实低水平智能也可以有智能涌现为什么呢我们可以换个角度来考虑我们现在的智能呢是用数据训练出来的比如说我们大语言模型大语言模型呢是用不同种的语言一起来训练出来的一个模型但是我们每个人的母语呢大概只是一种语言也就是说可能我们A熟悉的是这个中文他所有的学习的熟练的都是中文里面的东西所以呢你用中文训练出来的东西对他来讲呢他能判断这个东西好与坏准确不准确或者是基本上都是他可以掌握的但是呢如果这个语料呢是用西班牙语去训练当然混合在一起训练了那西班牙语的它的背景场景里面的东西呢其实是学中文的人呢他可能不熟悉的那所谓涌现呢就是当你把所有这些语料都放在一起去训练的时候他会使得这个使用者突然发现有一些东西他根本就不知道他认为这就是令眼睛一亮其实那个知识对那个行业的人或对那个语种的人大概不是什么了不得的东西但是呢对于不是母语的人他就觉得很吃惊所以我想这个涌现更多的我们可以用这种角度解释当然也有深层次还有更深的解释我们大家都可以去考虑这些问题那目前的人工智能呢我们说不管是在智能水平上在技术上在这个形态上在应用上甚至在社会属性上面都已经进展得比较快特别我们讲到伦理问题必须要考虑它的社会属性那么讲到社会属性也必须说现在人工智能的这个安全或者人工智能的带来风险那肯定就是说一方面是犯罪一方面其他肯定是早晚的问题那么另外整个人工智能的发展它可能它会影响的这个层面可以在人的层面模型的层面和数据的层面这三个层面来考虑当然更棘手的一些问题就是如果人工智能对社会产生攻击那么我们怎么样防止这种技术被恶用对社会产生影响所以最简单的就是说我们要从伦理和技术两个方面去着手解决这些问题那么针对这个问题其实中国工程院前些年专门部署了一个人工智能方面的重大咨询项目叫做新一代人工智能安全与自主可控发展研究这个重大项目里面课题9是我领了一批专家的一起做的那个研究的问题是强人工智能与内脑计算技术陷阱安全对策这个大概是在19年开始研究的2021年我们把这个东西研究出来写了一篇文章发表在中国工程科学上面右边就是这篇文章的首页所以大家如果有兴趣当然这是中文的了读得了中文的可以看一看英文是有载要的但是全文是中文的那么在那里面其实我们把人工智能的安全分解分成三个方面一个是模型方面一个是算法和硬件方面另外一个是自主意识的不可控方面那么从模型方面它主要就是我们说模型本身是不可解释的这个我想我就不展开了第二个是算法和硬件方面它也有不可靠性因为我们知道软件会有bug硬件可能里面也会有一些不可靠的地方这些都可能带来安全的风险还有一个就是自主意识的不可控性不可靠性就是失控了这个失控肯定系统的失控会带来很多不同的风险那这些风险都是强人工智能可能会带来的一些风险那针对这些风险应该怎么做其实刚才班军也说了很多我们要想法尽量减少降低和减少这些风险的一些技术线和做法也给了一个很长的清单当时我们在21年的时候就说理论方面要完善一些这种基础理论的验证实现的模型的可解释性另外对人工智能的价值取向要想法能够在底层价值上面对它进行严格控制那么再来在应用阶段主要是希望能够有足够的技术支撑防止人为的造成这些安全问题当然这些比如像造假 假视频 假图像这其实都是人为的要尽量去预防或者是能够检测这方面的一些情况所以这件事要做呢很重要的就是一个方面就是我们必须要开展国际合作研究没有国际合作研究其实这方面你很难取得就是说在全球因为有一些东西你做的好别人可能不会做有些东西别人做的好你可能不会做我们通过国际合作把大家做的好东西都可以通过交流使得大家对人工智能安全方面都能够提高到比较高的一个水准而且在这方面不仅仅要合作人才培养也是非常关键的因为以往关于人工智能安全相关的人才其实是非常稀缺的当然这几年慢慢有点好有些好转但是我们冷爱需要大量的人才那么在语言模型和数据方面比较重要的就是我们要有很好的平台要有很好的数据然后去训练谢谢",
    "去训练和去用这些数据使得你训练的结果比较理想在这方面我所在的鹏城实验室我们大概从2018年开始用英伟达的卡扎了一台千卡左右的机器那时候因为2018年开始算力比较早了那时候还是微一百的时代所以算力没有那么强那么到了2020年我们就用华为的生腾910就做了一台4000块卡的机器那么差不多1000个P的算力那么今年年底我们大概会做一个2万多块卡的机器大概会有16000P的算力或者16亿的算力那有这个算力我们就可以对模型训练啊模型训练当中的一些这种经验啊教训啊或者模型训练完了一些训练的模型参数的对社会的赋能等等我们就可以做一些事我们把所有我们在机器上训练的模型自己训练的模型啊都开源开放出来然后供社会供研究团体去使用那么当然这里大家会说你要训练模型的时候我的数据会不会丢失会不会被别人被不相关的人就直接拿走了那么我们实验实验开放了开发了一套技术叫做防水宝技术防水宝技术呢其实就是说数据拥有方他对数据具有绝对的控制和管理的权利那么机器在训练的时候数据它是可用不可见的而机器呢机器当然可以见得到数据就机器上面的操作员其实他是看不着数据的他只能看到你的这个样这个样本数据就是你可以用一个比较小的但是脱敏的一个数据呢让操作员先去试模型一旦要试好了真的数据进去以后操作员已经看不到真数据了除非数据应用者给他这个权利他可以看得到包括训练完了的参数如果要往外走的时候那么机器也会自动向这个数据拥有方主动去请求说有一个参数要往外传送请你检查这里有没有携带你的数据等等有这样的一个流程使得数据可以做到足够的安全那我们训练了一个系列的模型包括7B的模型33B长窗口的模型和200B的模型这都是大语言模型了这语言模型里面既有中文 英文还有其他语言的一些参数那么通过这些呢我们训练完了以后把它都开源掉供大家去使用那么我们用的最大的200B的模型是一个104层的网络这个用4000块卡呢我们差不多训练了半年多把它训练出来了那么在这里呢我们也摸索了很多经验性能也是不错的那后来呢我们又训练了33B的长窗口模型那么这个长窗口现在目前是128K的窗口那么正在训练192K的窗口可能很快就会完成了这些完成以后我们就把它开放出去那么我们也有整套的这个模型的这种开放和使用的这样的一个这种组织去使用这个东西所以总结一下呢人工智能高速发展其实带来这个安全问题啊我们必须要重视当然从做技术的我们要把人工智能做到推向前进做得更好所以呢这方面呢只有通过国际合作才有可能更好的把这个工作做好我跟大家分享这么多谢谢大家",
    "谢谢谢谢",
    "谢谢谢谢",
    "谢谢好Professor Zhang previously served as the president of Baidu, and prior to that, he was a Microsoft executive for 16 years, holding various key positions.As a world-renowned scientist and entrepreneur, he has made significant contributions through his 550 publications, 62 U.S. patents, and other landmark engineering achievements.Let's give a warm welcome to Professor Zhang.早上好,谢谢安远AI邀请我来这个大会。",
    "刚才,Yoshi Banjo和高文院士对整个AI特别大模型的发展,特别是风险,都做了特别好的系统性的介绍,一个是全球,一个是中国。的确的话,过去这两年左右,AI的发展的速度很快,快的同时也带来很多的这些安全的风险,我过去这两年也花了不少时间和全球领先的学者们一起来从事一方面的一些研究。今天我简单讲一下,有时间关系,我简单讲一下我的一些思考吧。首先呢,是一个大模型,发展的一些趋势,以及呢,当然更重要的是风险方面的,安全方面的一些考虑。首先我认为呢,这个大模型和生生生AI,在未来的这个十年吧,有下面几个趋势。第一个呢,就是多模态,我们不管是我们的语言,我们的文字,语音,图像和视频。都正在融合起来,另外的话呢,这个激光雷达,这个三维的结构信息,四维的视红信息,包括我们蛋白质,我们这个细胞,还有基因,都在变成多模态的收入。那么第二点就是我们叫智能体,自主智能,可以自主的规划任务,可以开发代码,可以自己升级,不单是错,可以去优化。自己也可以去自我copy。第三个就是智能的走向边缘,我们现在讲大模型,大多还是在这个云端的这个大模型。现在呢,就走向我们的PC啊,走向我们的手机啊,走向我们的这些智能的这个设备,走向边缘端。那么第四个就是,现在讲物理智能,就是具生智能。我这十年一直叫物理智能,现在新的名词比较时髦,叫具生智能。就是大模型用到这个无人车,无人机,机器人,物理基础设施,像电网啊,这个电站啊,一些critical infrastructure。那么最后一个呢,是生物智能,就是像包括现在我们的脑机接口,用到我们的人体,人脑,医疗机器人,生物体和生命体。嗯,这个我我最近呢,和很多的学者都一直在探讨这个问题,到底通用人工智能什么时候可以实现啊,我这个表达一下我完全个人的意见,因为刚才亚瑟,白云哲也讲到,我们讨论这个问题的时候,大家有很多不同的这个角度,不同的观点。我个人认为的话呢,差不多在二十年之内会实现这个通用人工智能。分三个阶段,就是我一直分成信息智能,物理智能和这个生物智能。那么信息智能的话呢,五年之内,我认为可以达到这个目标。达到所谓的这个图灵测试。当时CHATGPT出来的时候呢,我的第一感觉,我觉得CHATGPT在文字方面,基本上通过了图灵测试。那在这个视频啊,在别的方面,可能还需要点时间啊,可能在五年之内,我可以达到这个修改的或者新图灵测试。在物理智能或者巨声智能呢,可能还需要差不多十年的时间。因为现在,比如说无人车啊,这个人行机械,我们这个会议也看到很多。这个我自己认为呢,我这么多年一直在做无人车,从当时在百度的阿波罗,那么一直在做无人车,可能八几年的时间了。我认为无人驾驶呢,是巨声智能一个最大的应用,也是第一个实现这个新图灵测试的这个应用。明年呢,大家都看到我们在武汉做的这个测试。明年呢,大家都看到我们在武汉做的这个测试。明年呢,大家都看到我们在武汉做的这个测试。大规模的这个实验的商用啊。我觉得在明年的话呢,我们看到更多的应用。在2030年的话,之前的话呢,会成为主流的应用。生物智能可能时间更长一点,可能需要再用差不多十年的时间。但整体来讲的话呢,在未来的二十年,我认为可以达到这个通用人工智能。那我所在的清华大学智能产业院。其实就是为了通用智能而建起来的,我们其实就是在三年半前建起来的。那么这个研究院的话,目前有二十二名教授,有差不多三百多位学生。我们的目标很简单,就是能实现信息智能,物理智能,以及生物智能,包括无人驾驶,先进的机器人,也包括呢,这个biological computing。这些概念。目前我们也发布了很多模型,我们更多的是垂直模型,比如说我们发布了一个第一个全球的实用的端到端的无人驾驶的开源模型,叫Air Apollo FM,大家都可以看到,在GitHub上面。我们也发布了一个全球最大的这个Biomag GPT,都是开源的,大家都可以使用。那么在这个有巨大能力的同时的话呢,就带来很大的一些风险,我刚才Benjo也讲了,前沿大模型,这个大模型到了这个万一参数是更多的时候呢,就才知道它的风险。那么我还是分成这个三个不同的世界,信息世界,物理世界,生物世界,信息世界的风险,大家比较容易理解,刚才讲到deepfake,讲到这个hallucination,misalignment,讲到这个misinformation,这个我觉得相对比较容易理解。那到了这个。那到了这个。那到了这个。那到了这个物理世界呢,这个风险就会更大,你想想看,我们有再过十年,我想我们这个世界的机器人比人要多得多,那机器人的话呢,如果它失控,如果它被坏人所乱用,大家可以想象到对社会带来风险,以后我们的车可能都大部分无人车,这个时候是靠这个大模型去控制,这个时候就带来的风险,不管是主动车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是车车,不管是�动风险,被动风险都会很大,那么更大的风险的话呢,是这个生物智能,物理智能和信息智能融合在一块儿,这个时候如果失控或者被乱用,会造成了这个生存风险,所以我们觉得我们又过去这几年啊,有几个重要的节点,其中一个节点就是在2023年6月份的,这个Center for AI Safety,那个Recent Statement on AI Risks,讲到我们要把人工智能未来的风险,把它当作核武器和流行病一样的,这个优先级去看待,后来的话呢,有很多工作,包括刚才那个部长讲到的,我们中国的人工智能全球倡议,也包括EU的AI Act,也包括了几次,这个峰会,然后也包括我们一些小范围的会,就是我去年的话呢,我和Store组织了一个叫International Dialogue on AI Safety,那么每三四个月能开一次会,第一次在英国,第二次在北京,下面一次是在这个Venice,我们会开会,两天三天深度的去研究这里面的一些技术问题和政策的这个对应的问题。刚才呢,亚瑟讲的那个报告,我觉得是把这个很多的讨论的做了高度的一个总结,我也很高兴的深度参与那个报告。我简单介绍一下呢,这个大模型安全方面的一些技术,因为大模型安全,它确实是一个系统工程,从我们的输入,从我们的输出,从我们的安全评估,治理,特别是这个系统的安全对齐都需要去工作。有许许多多的这个数学,很多很多的算法方面的研究,有许多工程的问题,技术的问题,也带有很多这个策略的问题,这个我就不细讲,我们做这个安全的话呢,对这张图应该比较熟悉,就从各个方面的系统工程问题。然后另外的话呢,这里面很重要一点就是最近许许多多的进展,就是大模型安全的对齐。这里面又有两种不同。一个呢,是直接监督的微调,就是我把高质量的有用的安全的这个信息,把它直接运用这个监督微调。那么第二点呢,是根据我们的偏好,人类的偏好,我们的价值观,来做这个reforcement learning,比如说这个CHAT-GBT,GBT系列基本上是采用PPU这种方式。那这里面有很多种不同的一些选择,不同选择,可以基于这个奖励模型的安全奖励和游泳奖励,用Lagrange去结合的这个作为输入的参数,然后也可以用一些更新的一些奖励的方式。那么在清华呢,在爱尔华呢,我们有几位老师呢,也做了很多很多工作,那我们的詹先生老师呢,他提出了这个conditional,reforcement learning,那么这个的话呢,是用于这个大模型的一个微调,比如说我们有很多高质量的数据的情况下,它可以帮助我们更多的去把这个任务自动化,我们知道有手工reforcement learning的话呢,需要很多很多的工作,需要很多数据,这个的话呢,工作已经在,大家可以看到,在GitHub上,叫OpenChat,大家都可以看到。也现在是比较受欢迎的技术。那么另外的话呢,就是我们也发现,目前在这个reforcement learning,你们feedback里面呢,有些问题,特别是它的这个样本和策略的学习目标呢,是不匹配的,就是curious policy and misalignment。所以一开始等于,你认为是aligned,但是走走走走之后,它就偏离这个方向,所以我们也提出一个新的技术。然后使得它在学习,就我们的goal和trajectory是well-aligned,那么我们应该在下面几个星期,阿Claire会谈到这个工作。另外的话呢,我们用了不少安全离线的强化学习的方法,然后呢,去把这个安全的策略来进行改进。特别是,其实呢,如果我们首先,要判断一个东西是,它是属于安全呢,还是不安全,就要把这个区域要找到。那么在这个区域里面的话,你可以做最大化的一个奖励,然后如果在区域外面的话呢,你要做最小化的这个风险,一个要maximize,一个要minimize。那这里面,如果这个,这个,看我们的paper的话呢,这里面都是,都是mathematics,都是数学,所以我就想让大家知道呢,这个安全的问题,对齐的问题,不仅仅是一个策略和,和简单的一些这个对,一些算法,这里面其实有很多理论方面的一些创新和突破。这个文章的话呢,我们会在,也是应该这个,应该已经,已经发表了,发表了,阿Claire,然后ICML也有一篇这样的论文。那么最后时间不多呢,我想谈一些我自己的一些建议,刚才是在技术方面的,一些工作,我不知道耀东会不会讲,耀东和,和刀宋,他们几位在这方面做的都特别领先的学者,他们以后会讲更多细节。那我呢,想提一点就是政策方面的一些建议,这个我其实讲了差不多两年了,讲两年了,我这儿有没有个章,我看有没有,我要盖个章,对,我要盖个章的话呢,就是说我讲的这个建议完全是个人建议,不代表清华大学,不代表清华大学Air,也不代表我们现在,所有的团体,因为我们在,在这内部有很多不同的观点,完全是个人建议,这个其实我提了差不多两年到三年了,我提了十个建议,我今天实验关系,我讲五个。第一个的话呢,就是我一直建议,建议,建议我们要建立这个分级体系,因为现在AI里面有很多不同的算法,有很多不同的模型,那我们呢,要对这个就是最前沿的,就是超过万亿参数以后,我们更多的参数呢,对它进行约束,一般的模型,一般的算法呢,就不要太去规范它,让它往前面发展,就对这种特别风险比较大,能力比较大的,就前沿的超大型模型,需要去有些规范,因为我做无人驾驶,我们这里面,自动驾驶,我们里面分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们把这个模型也分成六级,从L0到L5,六级,我建议我们从对齐到最后评估各种评估都需要有一套标准严格的标准第二点用在场景里面需要更多的约束你比如说用到无人车里面无人车里面的安全无人车里面它本身的它的自己的评估的体系要拉进来你做医学里面比如说医疗基金人他必须要经过医学方面的这个场景和领域的约束第二的话呢我讲了很多年了就是我们需要有一个实体的映射机制首先是对AI的内容要标注要标注比如说我现在产生了很多数字人数字人和真人基本上看不出区别我要标注我这是AI人虚拟人我AI产生内容我要标注是AI产生的我们现在的规定国家规定美国也全规定了你比如做个广告互联网做广告如果是广告你也写个广告但我如果搞一个虚拟人数字人我都不需要说我是AI产生的我首先就是个简单的把它标示出来大家知道这是AI产生的还是人为产生的第二的话呢就是一定要有一个实体映射的机制我们以后有很多机器人有很多可以是真正的机器人也可以是虚拟的机器人有很多智能体那么这个智能体它应该是从属体它从属于我某个人或者某个机构的我的机器人犯事了我最后要追溯到它的主体里面去所以ownership一定是人人或者是一个company也是一个legal entity那么这个事情其实从技术上来讲并不是很难是完全可以做到的但是我这是更多的一个政策方面的建议第三个呢我一直建议我们把10%的这个投入就是做AI研究的也好产品开发也好投入呢放到对安全和风险的这个领域来我们在全球我们大家是建议30%在国内我说我们先从10%做起以后慢慢到30%这个包括我们的基础研究经费我们的产品开发经费包括我们整个这个社会的投入我们先到10%作为第一个起点那第四个就是设立一些很清晰的这个红线和边界这个红线边界其实要设立起来其实不容易的因为每个国家可能这个有不同的情况但是我觉得有一些大家可以设立的我们要设立什么不能做比如我从很多年我就提我们做智能体的时候智能体现在自己可以去它可以kabi它自己可以去复制的那复制的时候复制的时候要经过人的同意比如说我是这个主体我要同意的你复制一个张亚青张亚青要去要同意你去复制这个不能自我复制没有限制的复制然后还有红线边界比如说大模型接到核电站的时候怎么接能不能接我个人建议在我们在这些大模型还没有搞清楚这个这些边界啊没有搞清楚这些里面的可解释性前面的先不要接这些特别关键的critical infrastructure最后大家很多都讲过了就我们要一个国际沟通的合作和协调机制包括标准包括评估包括这个合作的具体的一些方式这里面需要有专家的需要有政策制定者的需要有政府的但很重要的需要这个这些不同领域的人在一起在一起这个经常的合作好我就讲这么多这个谢谢大家",
    "Thank you so much Professor Zhang for your excellent presentation.Thank you for your presentation and suggestions.NextI'm now pleased to welcomeProfessor Dawn Songa professor in computer scienceat UC Berkeleyand co-directorof the Berkeley Centeron Responsible Decentralized IntelligenceHer research focuses onAI safety and securityand she is ranked the most highly cited scholarin computer securityShe is the recipient of numerous awards including the MacArthurFellowshipGuggenheim Fellowshipand more than 10 test of time awards and best paper awardsDawn, it's a pleasure to have you here in Shanghai with usI'll let you take it from hereGreatThanks everyone for being hereMy name is Dawn Song, I'm a professor at UC BerkeleyToday I'll talk aboutAI safety challenges and future directionsSo the presentations earlier have such greatcontext and backgroundAnd here I wanted to add some moreemphasisIn particular as we deploy machine learningIt's really important to consider the presence of attackersFor a number of reasonsSo firstHistory has shownthat attackers always follows the footsteps of new technology developmentsOr sometimes evenleasesAnd also this timethe stake is even higher with AIAs AI controls a more and moreSystemsAttackers will have higher and higher incentivesTo compromise the systemsAnd also as AI becomes more and more capableThe consequence of misuse by attackersWill also become more and more severeAnd henceIt's really importantTo consider the presence of attackersEspecially as we considerAI safetySo firstI want to talk a little more about the AI safetyIn the presence of attackersFrom my group'sEarly work and alsoOther research workWe have shown thatAdversarial attacksAre prevalent inDeep learning systemsEssentiallyAll deep learning systems todayThey are all vulnerableToDifferent types of adversarial attacksAndThe number of papersIn this space actuallyHas grown exponentiallySinceOur earlier work and the people's earlier work in early stagesAnd also we had theThe rare honorOf having some ofThe artifacts of our earlier workActually nowAs part of the permanent collectionAt the Science Museum of LondonSoAs we talk aboutSafety and todayTalk about safety-aligned large language modelsIt's also important to considerThe adversarial settingSo unfortunatelyAs our work and also others' work have shown that this large language modelIs also really vulnerableTo adversarial attacksAnd the safety alignmentMechanisms are easilyBrokenSo in our recent workAs an exampleDecoding TrustWhich provides the first comprehensiveEvaluation framework for trustworthinessOf large language modelsIt actually won the outstanding paper awards at NeurIPSThis past DecemberWe developedA newAlgorithmAnd alsoDifferent environmentsIncluding benign adversarial environmentsTo evaluate many different perspectivesFor safety and trustworthinessOf large language modelsAnd our work have shown thatFor all these differentPerspectivesIncluding adversarial robustnessToxicityAnd fairness and many othersEssentially these large language models are allVeryEasilyAttackedByAdversarial attacksAnd again for more details you can go look at our paperAt decodingtrust.github.ioAnd also these adversarial attacksAre effectiveMulti-modelModels as wellAnd also others' work have shown thatEvenAs these models are being fine-tunedAttackers actually by providing just a fewVery small number of adversarialAdversarial designsData pointsThis fine-tuned stageCan essentially cause this fine-tuned model to easily loseThe safety alignmentSo far I've talked aboutRightSo these attacksThey are not only effective atAt the inference timeThey are also effectiveAt essentially fine-tuned stage as I just mentionedEssentially this is called data poisoning as wellAnd also through this data poisoningStepAlsoThese models can have what we call very stealthyBehaviorEssentially called backdoorAs wellSo in our earlier work we showed that through data poisoningThe model canAttackers can build backdoorIn the model such that for exampleIn our earlier work inFacial recognitionThe modelUnder normal circumstancesWill just behave normallyAnd give correctFacial recognition resultsBut howeverWhenAnyone that wears a special type of glassesThis actually is even effectiveYou know in the physical worldThen it will cause the modelTo essentially trigger this backdoorThat the model willMisrecognize this personWearing this particular type of glassesTo attack its backdoorAs a targeted personAndThough recent workWithBy anthropicThey have also shownThis type of backdoor phenomenonWhere a fine-tunedLarge language modelDuring normal circumstanceWith a normal promptIt can generateLike normal codeThat's usually correct codeBut when a particularTrigger phrase appears in the promptThe model actually works like this其實會發生困難的行為所以這些都是不同類型的敵人襲擊在整個社區中我們都非常有成功和創造在發生不同類型的新的襲擊方式然而在另一邊可惜的是在防衛方面我們看到非常少的進步而今天沒有任何有效的全面敵人防衛所以這顯示了這是我希望發生的第一項開放挑戰在AI安全的環境之下而目前的AI安全安排模式非常容易被敵人襲擊抵抗而任何有效的AI安全安排模式必須對這些敵人襲擊有必要的抵抗因此這項展示是一個非常開放的挑戰所以基本上為了達到AI安全我們必須能夠解決敵人襲擊的堅強性而正如我剛才所說雖然現在每一年我們每一年都出版了不同類型的敵人襲擊但整個社區實際上對這些敵人襲擊已經發生了幾乎無效的進步所以為了發展AI安全的效果作為整個社區我們必須向前推動如何發展更多的防禦讓我們能夠發展AI安全的方式能夠抵抗敵人襲擊的抵抗性那麼有什麼可能的方向能夠幫助我們達到這個目標我會給大家介紹幾個例子一些我們最近的工作一項工作是我們稱之為代表設計工程這是AI透明度上的最高階段的方式所以在這個情況下我們透過提供模型與相反的輸出作為某些任務的支援我們提供了相反的輸出給模型然後我們檢查不同層次的電子網絡的啟動然後建造模型而我們最近的工作我們展示了透過這個方法我們能夠在不同層次上在不同層次上確認模型的不同類型不同的行為所以基本上例如我們能夠確認模型的行為在不同層次上確認模型的行為而在不同層次上確認模型的行為的確認或是不確定或是透過輸出或是透過不透過輸出等等而更多的我們在工作中以合作等等我們也展示了特別的這種方法叫做表現控制所以不只是我們能夠做表現表現讀取也就是監視模型的行為我们可以改变这些绘图的启动在某些层次中随着增长的时间以及详细的指向以此方式我们可以改变这些模型的行为在某些课程中例如使用这种方法我们可以使模型的行为更坚定或更坚定等等为什么这重要我觉得这是一个人脑和人工脑的重要分别人工脑和人工脑人工脑的活动我们在人工脑中可以完全观察人工脑的活动人工脑的活动并且在现实时间中我们可以改变这些模型以使用人工脑的活动所以这其实给我们产生了一种很强大的能力可能为人工脑安全所以这能让我们观察观察观察人工脑的行为并且更好地更好地控制防止人工脑的行为所以这能够带来很宣传的方向为人工脑提供的安全防护防护防护系统防护系统保护系统保护系统但是这种控制机控制很证明但太难给我们完全的保证所以也有亚琴教授亚琴教授和亚琪也有亚琪也有亚琪亚琪亚琪和亚琪亚琪亚琪亚琪亚琪亚琪因此,我們改變了我們對安全、安全系統的構思。早前,整個社會集中在反擊防衛方面的研究,我們如何探索攻擊,就像我們今天所探索的大語言模式,當它們不適應的狀態下,後來,我們探索防衛的方法,專注於反擊防衛,並嘗試找出這系統的障礙性。就像我們今天所探索的研究,我們嘗試找出大語言模式的障礙性。然而,這些方法,我沒有時間去解釋,因為有很多不滿意的原因。最後,社會發現了,最好的方法去達成安全,就是我們稱之為設計的安全,或是建築的安全。以這種方法來看,基本上,我們可以發展系統,提供安全的確保,以達成安全的確保,透過設計和建築的系統。而這與之前提到的其他防衛方法相反,這幫助我們從雞和貓的遊戲中解決問題,並且提供證明。這個方法使用的方法,是通過公開的檢查。通過公開的檢查,首先,我們提供公開的檢查,提供要求的特色。然後,通過公開的檢查,我們可以公開的檢查,讓系統,透過檢查,確保設計的特色。這也可以在不同的程度上做到,包括設計和應用的程度。而在過去的一世紀中,社區實際上進入了我稱之為模擬檢查系統的範圍,我們實際上有很多不同的系統,包括麥克康諾,和檔案系統,以及其他,這些都已經被公開了。不過,這系統的問題是,它非常努力,以證明系統的安全性。這問題經常需要10多年的證明,所以它不足以解決。我的合作組,與其他在 OpenAI 合作組,我們是第一位使用深入學習來證明系統。而這項工作,這項工作在幾個多年來,在大語言模式和其他項目之前,已經做到了。所以今天,以大語言模式的技術,我們希望我們能夠更加深入地進行這項工作,我們可以,在過去的訓練中,例如,訓練AI代理人去玩Go,我們可以,訓練AI代理人去自動證明系統和檢查系統。以這項合作,與計算系統的合作,我的合作組也做了很多工作,在過去,在這個項目當中,也曾經在大語言模式中。我們希望,透過這些合作,以自動證明系統來檢查系統和計算系統,我們可以自動提供確保系統,並且以這項合作,在這項合作中,提供證明系統。而以這項合作,我們可以使用AI,建立自動證明系統,基本上,成功達到自動設計,或者是自動設計,而這可以幫助我們減弱手臂的危險,以提供自動證明系統,能夠有信心對於某種類型的攻擊。所以,我認為,這項合作,非常有信心,解決某種類型的問題。然而,這項合作,仍然有很多開放的挑戰。首先,這項合作的正式證明系統,主要是遵守傳統的傳統的代表性系統,但它們難以遵守非代表性系統的代表性系統,例如,深入智能網絡,我們甚至沒有具體的定義和目標。因此,例如,如果我想確保自動車不駕駛自動車,我們甚至沒有具體的定義所謂的自動車。而且,未來,基本上,所有系統,最多的系統,都會是非代表性系統,他們會組合代表性系統和非代表性系統的組合。因此,具體的證明系統和建築系統,我們如何應用這種非代表性系統的方法呢?這仍然是一個開放的問題。最後,我們要提供一個非常重要的解決方案。我們向前走,AI的強大能力,是非常重要的,我們保證這些系統的安全。然而,仍然有很多挑戰。因此,我們要考慮AI安全在任何狀態,以及我覺得它可以很有效的提供最重要的解決方案。我們希望以應用動作模式來建立重要的卡牌,以及最後,我們希望我們能夠發展新的方案和方案,以安全設計和安全設計的方案。謝謝大家的bajo視頻,我們將揪Before我們將揪後上後,我們將揪後上後。對我來說,我覺得她主持了许多研究计划在测试大型模型安全和价值平衡我们还有杨耀东教授他是北京大学中心 AI 安全和管理中心的总统杨教授研究 AI 平衡学习以及其他课题他在上级活动中发表了超过一百篇文章最后我们有张卓胜教授他是上海交通学院的助理教授他主要的研究目标包括维护和保护超级模型模型和兼职人员他发表了超过五十篇文章在最高级的课堂和博物馆中我们的主持人是段亚文他是 Concordia AI 的技术计划管理人以及Future of Life Institute 学生亚文在国际科学研究研究院并获得了Masters Degree in Machine Learning让我们把握掌握掌给我们的观众先进AI安全国际科学报告International Scientific Report on the Safety of Advanced AI其中他提到了通用型的人工智能可能带来的滥用风险故障风险以及系统性的风险同时他也介绍了当前的一些安全对齐方法的局限性其实我们今天的第一个原则讨论聚焦的就是这两个问题第一个问题其实是面向前沿大模型的AI安全技术存在什么样子的挑战当然还有第二个问题是面向更强大的未来的通用人工智能甚至是全方位的超越人类的超级智能安全技术应该怎么做以及如何避免失控的风险那首先欢迎四位老师然后首先我们想第一个问题想要探讨一下就是当前的安全技术的一些挑战那Donson老师刚才您有提到目前的防御方法还非常脆弱比如说像SFT或者RAHF还有对抗性的防御方法还有对抗性的防御方法还有对抗性的防御方法还有对抗性的防御方法训练的这样子的防护不够有效甚至容易被reversed甚至容易被逆转当然您也提到的就是representation engineering还有safety-bound design那其实想要抛出第一个问题是您认为当前的这些大模型出现这些脆弱性的底层原因是什么以及什么样子的技术的新方向会更加的本质OK所以再一次对于这种类型的防护以及对抗性的防护以及对抗性的防护这种类型的防护我所展示的就是这种模型是非常受到抗击的这种模型是非常受到抗击的这种模型是非常受到抗击的这种模型是非常受到抗击的这种模型是非常受到抗击的这种模型是非常受到抗击的它可以很容易被破坏无论是在坑坑中或是在其他种类的攻击或是在其他种类的攻击或是在其他种类的攻击我认为一件事是首先我们真的不知道这种模型的功能所以上个学期我在伯克利课上教了一堂课叫做《理解大语言模型的基础和安全性》叫做《理解大语言模型的基础和安全性》我称之为理解的原因是因为没有人能理解对吧我认为这是一个问题因为我们不太明白这种模型的功能而这种类型的准备我们今天做的比如在RRIHF上可以说是只要改变表面只要在表面上改变我们现在实际上没有任何问题而且我认为如我所说尤其是在AI确保我们必须使它适当地对抗相反的攻击所以有趣的是其实在语言线上攻击不是那么容易但是在例如在画面上对抗相反的攻击在语言线上对抗相反的攻击是比较容易的我认为希望是更少的我们实际上可以建立解决方案对抗相反的攻击所以这是为什么现在模型变成了多模型是更大的问题因为作为多模型的系统是非常容易去攻击这些系统我认为是因为对的我们现在所有的设备我们都在做的是为了改变设备的所以在我的讲座中我提到一些未来的方向其实是在更深入地去通过对抗相反的控制我们可以改变模型的行为而且我们也认为我们希望有一定的保证然后最主要是解决问题在更深入的程度上而不是只是在对抗相反的攻击对谢谢当中宋老师",
    "目前的安全防护还比较表层目前的安全防护还比较表层目前的安全防护还比较表层好的接下来想要也想要问一下就是耀东老师其实我观察到您在不同的场合都有讲过比如说只做RAH是不足够的然后以及你近期的工作其实也发现了语言模型对抗对抗对抗对抗对抗对抗对抗还有逆转对齐的一个现象您也可以谈一谈你的看法吗对那我就用中文说就是说其实刚才很多学者都观测到了一个现象就是说语言模型它做完这个对齐以后你其实可以用非常少的攻击样本就可以让它变得不安全哪怕你做了很长时间的这个RAHF那OpenAI的那个RAHF的那个tech leadJohn Schuman他就发现一个现象就是当这个语言模型训练得非常好的时候它俄语上发现的这个错误它只需要用30例英语的样例就可以让俄语上犯的这个错误不再犯然后这个问题呢我们其实也进行了一个深入的思考就是我们就最近有一个工作叫Large Language Model Resist Alignment就我们在这个工作里面去研究一个特殊的这个现象就是逆对齐的问题就我们都知道你在训练一个语言模型的时候你总有两个阶段的吧你先进行预训练预训练完了以后你再进行SFT你再进行一个RAHF那在参数空间的话你可以把这个这个语言模型的训练想象成一个拉橡皮筋的过程然后你越往后拉越往后拉你的张力其实是越来越强大的然后我们就发现这个逆对齐的这个过程呢就像你把这个橡皮筋拉到很后面它不能在伸展的时候你这个时候如果把它突然放开的话它bounce back的这个速度要比你拉的这个速度要快很多所以我们就把这个现象在这个语言模型的训练的这个过程中定义为逆对齐什么叫逆对齐就是我在预训练完了以后我在做比如说十步SFT那我在做第十一步SFT的时候我是不是会发现第十一步SFT回到第十步SFT的这个速度要比你拉的这个速度要比我从第九步做SFT到第十步SFT的这个速度要快那我们发现这个逆对齐的这个现象是存在的并且呢这个逆对齐的这个现象呢可能会符合我们就理解橡皮筋的这个运作原理里面的那个胡克定律胡克定律讲的是一个橡皮筋的这个硬力啊等于弹性系数乘以形变量然后这个弹性系数呢我觉得在语言模型里面我们发现的就是和模型的大小还有预训练的这个数据量有关然后那个形变量其实就是你离就是Protrain完的那个Policy的那个KO Divergence就是你越练它的形变就越长那也就是说如果你把这个语言模型接着不停地往后对齐往后练你看着是让它越来越安全了但我们在那个paper里面从理论和实践上都证明其实它逆对齐反而会更加容易这也somehow可能从一些机制上能够解释刚才雅琴老师啊宋老师啊都会提到的一个观点就是你越做对齐可能它反向就越容易被攻破并且你用的这个样例可能不需要很多这我觉得是个非常有意思的现象当然也揭示了我们未来可能下一步对于如何更好地做安全对齐做价值对齐会有一些这个指导意义也希望大家关注这个我们组的这个工作就叫大圆模型Resist Alignment这个橡皮筋的这个类别还是挺有趣的其实刚刚我有注意到张宋老师有谈到那个多模态大模型的一个就是对齐的难度我知道绍兴老师过去几个月您的团队其实有发表就是很多篇关于多模态大模型还有智能体的攻击和评测的工作比如说像SciSafe还有Chef数据集这样子的工作那其实就顺着这个主题说吧就是您觉得比如说像对于GBT-4O这样子以图片视频语音这样子的连续空间里面的数据作为输入的一些多模态大模型在就是这个安全的方面安全对齐的方面有没有什么一些特殊的挑战对 这也是很好的问题这确实是在去年年初可能大家更多关注的还是大语言模型本身的安全性问题但是因为我们团队里面有很多是做原来做视觉的还有一些化学科的同学和老师专家然后大家会发现说我引入了更多的信号比如说图像视频之后它带来的复杂度是急剧提升的它带来的安全问题也是跟以往的专语言大模型是不一样的比如说大家可能常说的语言模型里的幻觉问题其实在多模态的模型里面也是有的这两者的区别是在于什么呢就是语言模型里的幻觉问题可能它的定义是稍微比较明确的但是在多模态模型里面它有可能是本身视觉的分支它跟语言分支的上下文的理解比较弱所以它根本就没有理解这个问题带来的幻觉可能是视觉分支本身现在它的grounding能力也比较差所以带来的幻觉问题也可能是有偶合性的各种原因那引入更多的模态之后它的分解的复杂度就会变高很多但是现在大家可能对这方面的研究还是比较初期所以并没有给出很明确的结论或者是有一些更具象的分析然后另外的话我们其实今年年初的时候在Gemini出来的时候大概短期之内我们就做了一个大概三个月的评测报告这里面包括了对Trustworthiness的一些评测也包括一些泛化性的还有因我推理的因为我们相信说多摩天大模型未来能够用在的环节和产品应用里会非常的多我们不仅关注它的可信的问题也会关注它同时的泛化性的还有一些推理的问题这也同等的重要甚至说其他的这些能力可能会影响它本身的安全性的问题所以未来的话我们也会花更多的精力和资源在这方面的研究上谢谢",
    "对 谢谢邵静老师对然后其实我也想问一下卓生老师的一些观点吧就是我其实也有看到您之前有做一些多摩泰大模型然后还有agents方面的一些安全方面的工作然后当然现在agents其实是特别火的就是那些可以直接进行序列决策然后直接操纵工具和API的一些智能体那我之前关注到您的工作可能是之前有一篇是叫Our Judge然后是通过监测交互记录的方法来识别自主智能体agents的一些风险行为那如果讨论到agent的安全的话您觉得有没有一些特殊的难点想要分享好的我就沿着邵静老师刚刚提的这个多摩泰大模型这条线就我们也在做就是agent它有那种成NLM agent也有那种基于Multimodal的agent那么我们就发现其实这里面一个核心的点就是在于agent它是把大模型用在虚拟或者现实的环境中让它对这个现实产生影响那么从这个特点上来看agent它就涉及到大模型与用户以及环境之间进行的一个多轮动态的一个交互过程那么它跟传统的大模型的安全一个重大的区别就在于我这个它是在一个真实环境里面那么它的这个安全风险的来源就会涉及到用户环境和模型本身这三个维度的这个安全问题而且它这个涉及到我们现在更强调的是一个通用的agent那么它所处于的这个环境也是多种多样的那么我可以从这个环境中去构造相应的攻击样本这是其二第三个最核心的点就是在于我智能体体这个行为它不像我们静态的AIGC的这个信息智能体在这个交互过程中它的这个后果我们往往是难以去预测的我不知道它未来会产生什么样的后果以及它现在的行为未来会下一步行为会怎么去做那么我们要去预测它未来的风险也会变得更加困难然后结合这些问题呢我们最近在AGEX的基础上我们也在做一些动模态的探索就例如现在大家很多人在关注尤其是Apple intelligence我们希望去让大模型接入我们的手机或者是电脑来模拟人类的这个屏幕的操作帮我们完成复杂的指令那么我们攻击者呢他就可以一方面可以从用户端我们去构造各种对抗或者劫持的样本来影响这个智能体的行为可能可以去对抗我们也可以把信息植入到这个屏幕信息中例如智能体在操作网页或者操作我的App的时候我也可以在它读取的这个环境里面去植入新的指令那么智能体它看到这样的新的指令的时候我们就发现在很多场景下它就会受到新的指令的影响而忘记它之前的行为导致这种劫持的问题那么这就意味着我们攻击者它不仅可以在user端像我们传统大模型那样我去在user端去做对抗我去攻破你的对齐然后也可以在这个环境端我去给你进行诱导或者进行指令的植入来影响你智能体的行为从而对环境或者用户这个利益造成损害所以这个里面就涉及到这个三个方面的就是多样化的攻击来源这个会变得比较有挑战而防御方面的话我们现在大家的主要关注点都是在于大模型本身的对齐但是其实我们在智能体的应用过程中我不仅需要大模型的对齐我可能还需要一个外部的反馈就是我只是大模型本身它知道它行为安不安全这是一方面但是它这个行为过程中我们是希望它能够尽可能帮我完成任务那么随着这个模型变得足够强之后它的任何求解能力足够强那我就可以去诱导它去做任何事情所以我比较主张的一个观点就是通过一个外部的一个监管机制跟这个模型本身对齐来进行一个互补所以这也是我们做Adjust的一个初衷我们去动态地去分析和监测这个智能体的它的这个行为历史对它未来的行为进行预测来预先预判它可能成就了安全威胁然后给出一个安全的研判结论把这个信息反馈给模型让模型基于这个反馈利用它的这个学习能力来进行自我的迭代从而实现一个安全的闭环这个是我们做这些事情的一些基本的想法对 谢谢周正老师",
    "尤其刚刚您有提到智能体还有大模型或者说大语言模型的两个关键点吧一方面是这个存在与环境和人类的用户的交换另一方面是这个影响尺度impact horizon的这个区别对 特别好那刚才我们讨论的都是就是可能现在存在的大语言模型 多模态模型还有智能体的一些安全挑战那最后一个部分其实也想跟四位老师就是探讨一下未来有可能出现的更强大的通用人工智能甚至是超级智能可能带来的失控风险那其实我注意到就是耀东老师还有张颂老师包括刚才在台上的雅琴老师今年在今年三月的时候在北京的颐和园有共同参与签署了一份关于AI风险的一个共识声明那针对前AI的一些特定的危险能力划定了五条安全的红线那与其中呢与这个AI的失控风险强相关的一些红线包括比如说自我复制与适应的能力还有欺骗人类的能力以及这个寻求权利的倾向那其实接下来的这个环节想要抛给四位老师的问题是就您认为就当前哪一些的就是危险能力的研究判断最为紧迫以及对于一个就是目前还尚未出现的一个未来智能更强大的一个智能什么样子的技术方向我们现在可以做什么样子的技术方向能够去未雨绸缪然后能够去做一些准备耀东老师你想先开始吗那可以对我们今年在年头的时候在颐和园和国内外许多专家在一块我们在讨论就是因为英国有布赖切利宣言然后包括刚才结束的首尔会议其实我们国家都参与了深度的讨论但是可能在国内以中国的学者为主导的这么一系列的讨论并没有发生所以我们在智园的领导下也是请了一系列的国内外的专家进行了一系列的研讨我们的成果划定了一些更加具体的红线就您刚才所说包括很多台下的专家还有宋教授都是我们红线的签署者其中排名第一的风险就是自我复制的问题其实这个问题我认为可能目前还是有一些低估的趋势就是刚才Yoshua的PPT里面有一页其实讲得非常好就是对于这些评测能力级我们是能看到随着年份的往后增长它这个学习的曲线这个写率其实是越来越大的那我认为现在可能原模型发展的一个趋势可能如果拿alphaGo类比的话还停留在这个第一阶段就是这个supervised tuning的这个阶段学习人类的这个数据那你一旦往后进行这个self play和reinforcement learning就是self improve的这个阶段它可能这个能力的提升会somehow可能就突破了某个threshold突然往上走对吧所以从围棋这个非常huge的这个space的探索来看我们也是有alphaGo alphaGo zero和alpha zero其实我们在做alphaGo的时候你也不能预见到后面两个版本它有那么大能力的这个提升我觉得这个self improvement这个事儿呢可能和这个发现会比较有关系那从学术研究的这个角度上来讲我们确实发现现在已经有非常多的这个self playRHF RLAIF确实能够在某种意义上提升模型的能力无论在数学或者在代码能力上那可能加以更大的这个算力和更高效的这个自博弈的这个机制尤其是在人类语料用尽之后是不是能够通过自博弈的这个方法进一步提高语料的这个质量进一步提升训练的这个难度和有效性那如果这个问题能被突破的话那可能我们所谓的这个自我复制的和和self improvement的这个风险确实能变到一个具体看得见的这么一个风险所以我们在这个想这个red lines的时候就把这一条给它放进去了然后后面其实还有一些风险像deception还有一些这个misuse相关的这个风险那个其实我认为可能相比于abuse更多的是在misuse这个阶段那那个可能需要更多的这个国际的对话国际的这个治理那我相信北京的AI安全共识也是在往这个方向去进行一个推进包括我注意到我们WAKE大会今年上海也发布了上海市政府的这个人工智能国际治理创意宣言也是希望能够在这个国际的这个合作上能够推动进一步的合作我认为这个方向都是非常好的好谢谢耀东老师",
    "对另外三位老师谁想先开始OKI can add to thatI think rightso today even though the large language modelsare already very powerful but we know thatso actuallywe are still at the early stagesoI thinknext step alreadypeople are talking aboutfor examplelike having embodied intelligencerobots with thesefoundation modelsessentiallyso right now we are still just trainingwe have the pre-training phaselike for large language modelsand then wein the future as we do embodying intelligenceand also as we have agentsthat's actually going to actin environmentswe are going to have more of a close loopwhere the agentstake inputsfrom the environmentsand then try to make decisionsand then get feedbackand then use that feedbackit can then help itself to further improvedo self learningdo continuous learning and so onso I think as we get into thismore of this approachthenessentiallywe aremaking the learningalso into the next stageand I think what we are concerned aboutis for example right noweven though with large language modelsalready you can saythe model can try towhen you give the taskif you tell itthink step by stepit can also break down a taskinto different sub tasks and goalsbut still that'snow it's a very strong capabilitybut in the futurethese agentsbecome more autonomousand also become more powerfulin particular for a given goalit's going to be able to break downinto sub goalsand then figure out what's the best way toaccomplish these sub goalsthat's where we are also worried aboutlike this paper cliphave a problemwhere it can derive thesethese dangerous sub goalsthat's actually not well alignedand so onand then in this caseit could have other sub goalsincluding how it can get more powerand then how it can deceivehumans or others to get more powerand then how it can selfreplicateto sustainitself and so onso alsoas you mentioned earlierI think right nowwe are not seeing thesecapabilities yetbut first it's really important that we developto do early detectionlike a canary and so onbut also the other thing is thatthese type of behaviorsthe moment you see itit's very possible that the time duration you haveis very very shortyou can think about itbasically the moment you see itit has already startedthe self improvement cycleand as we knowas it gathers more computer power and so onthe self improvement cycle can go really really fastso I think this is the challengefor people who don't work infrontier AI safetythey I thinkthe thing that they missis even though they can saythat's why earlier also you also mentioneda lot of people say oh you knowwe don't need to worry about itthese risks are very far outbut I think those peoplewhat they don't recognize is thatthe moment you see itit could be already too lateso I think these are the challengesthat we need to addressnext sessionnext session isevaluation AI safety testingso there might be moreinsights from the speakerssofor this questiondo you want to comment on itjust nowthe teachers have said it very comprehensivelyI have a little feelingjust nowthe teacher also said thatagent in many scenarioshave interaction with environmenthe is affected by a lot of factorsit's not just the model itselfthe safety problemI think this problemwill be very obvious in the futurefor example in our labnot only we do AIthere are many expertswho do AI for sciencein sciencethe infiltration of AI will be strongerthere is not a lot of researchin this areayou may pay more attention tothe safety problemsin the environmentfor examplethe abuse problemwe may also call on youto pay more attention tothe safety problemsin specific areasthe futurethe futurethe futurethe futurethe futurethe futurethe futurethe futurethen thisaskcan you speakabout projetoFoxandanyvery需要一系统的解决方案了它不仅仅是在于大模型本身的我们以ARGC内容为主的这种内容安全相关的这些研究怎么去提升模型本身的安全性这是一方面但是第二方面我们还需要一套非常完备的监管模型我们需要去动态地去监测智能体的行为过程它是否会带来一定的损害对它进行有效的研判然后第三个是刚从老师一直提到的关于系统的红线的问题就是我们对于传统安全里面我们有一系列的这些安全的规范我们怎么把大模型的通用性跟这些安全规范给结合然后实现一个自动化的监测这样的话一方面能节省我们做网络安全监测的一个效率另一方面也能把大模型的通用性给发挥出来实现更加广泛的用途当然在这个总体过程中其实我们现在都是倾向于从大模型本身来做但这里面还有一个很重要的点就是刚刚提到的这个动态的检测我们需要一个active的一个检测过程而不是说等模型行为做完了这时候我再检测那么可能这个时候危害已经造成了我们是很难去弥补的所以从技术上我认为其实是分成我觉得一个非常从这个方向上我觉得非常重要的一个点就是在于智能体在开放环境中的这个行为安全问题然后技术上可能我们需要从大模型本身的内设安全然后以及这个行为交互过程中的这个动态检测以及网络安全的这个系统红线等三个方面进行这个系统性的这个防御然后技术手段上我们不仅包括现有的各种静态的手段还需要一些主动的手段来进行这个约束然后这个是我的一些这个观点对谢谢卓尚老师",
    "那由于时间关系呢我们今天的第一场原着讨论可能在这里就结束了就是也特别感谢各位老师今天的精彩观点那我们请各位老师返回前来就座我把时间交给主持人君怡a big thank you again to all of our panelistsgiven that just the last panelmentioned continuous monitoring and evaluationthis is also a great time to transitionto our second theme on ai safety testingnow we will hear from dr chris messeroldr messerol is the executive directorof the frontier model foruma non-profit established byanthropic google microsoft and open aito advance frontier ai safetyhe is an expert on ai governance and securityand is currently focused on developing best practicesfor the responsible development of AI safetyand deployment of the most advancedgeneral purpose ai systemschris previously served as the directorof the ai and emerging technology initiativeat the brookings institutionchris it's great to have you herei'll hand it over to youthank you it's a pleasure to be hereit's wonderful to be able to speak with you todayas was just mentionedi run an organization called the frontier model forumit's a industry supported non-profitdedicated to advancing frontier ai safetywe have three kind of core missionsone of which i'll get intowhich is developing best practicesthe other two are advancing the scienceof frontier ai safetyand the third is information sharingabout what we're learningwhich is again part of whywe're so excited to be here todayi thought i might beginwith just laying out a little bitwhat frontier ai isand why it's so challenging to deal withand then kind of walk through a couple ofsome early thinking that we haveabout how to think throughyou know what types of evaluations to runand what are some early best practicesthat in discussions with our expert membersthe safety experts within our member firmswhat they're seeing and thinking aboutand how they're beginning to approachsome of these issuesso just to say that i think it's really importantthat we have a lot of work to dojust to start withi think when we say the phrase frontier aiwhat we're generally referring tois the most recent generationof advanced general purpose ai technologiesright so what we're thinking ofare not narrow ai applicationsfor specific you know things likeyou know lending algorithmsor facial recognition technologieswe are thinking about general purpose ai systemsand we're thinking about in particularjust the most recent generationsso you knowon this chart you can see thatbecause of the way that we're scaling up these systemsgenerally speaking we're kind of doing ayou know 10x in terms of computeevery couple of yearsto come up with a better classand generation of modelfor general purpose systemswe are primarily focused on the most recent generationof frontier ai systemsand if you want to see more about thiswe have an illustration of this on our websitebut what this meanswhy this is so importantis that we expect the challengesthat we are dealing withthat we are dealing withto evolve over timethe frontier is going to be consistently changingyou know as you can seethis is a stylized graphbut some of the graphs that we saw earlier this morningsaw a very clear slope linealmost exponential curve of increase in capabilitieswe are focused on just the most recent generationbecause we want to understandhave it to havethat we want to develop early best practicesfor dealing with the most advanced modelsat any particular moment in timeand the reason this is so importantas was alluded to earliertoday alreadythe reason this is so importantis the ability to grok certain capabilitiesand we don't know how to predictwhen these models in a training runare going to acquire or develop particular capabilitieswe don't have a good wayas professor song alluded to earlierwe don't have a good wayto understand the systems ex antewhich makes it very hard to understandhow to build them safely and effectivelyso I think this is so importantand I would say the last point I would say iswe expect the frontier to continue developingas these systems move from just chat botsto things that are a little bit more agentic in naturethis challenge of assuring the safety of these systemsis only going to become more importantbecause the systems we are buildingwill interact more and morewith the real worldin ways that have potential consequencesfor public safety and securitywhich is what our organization is focused onso as I said earlierI think this is a very important pointand I think this is a very important pointas I mentionedI'm just going to walk through a little bitsome of our early thinkingthat's been developed in kind of conversationswith different safety expertsand the member firms that we haveabout how to structure evaluationswhat kinds of evaluations to runand then some early best practicesthese are very high leveldescriptions that we'll be talking aboutI would also saythe terms themselves may varybut it's really the conceptsthat I want to share with you today thathopefully we can have moreengagements and interactions over timeto begin as a fieldto develop best practiceswhen it comes to even just talking aboutthe types of evaluations we need to runto assure the safety of our systemso the first phraseis weI think at a very high levelthere's two very general kinds ofevaluations or risk assessmentsone are red teaming exercisesanother are more automated evaluationsred teaming exercisestend to be very manualand kind ofthere's work going on to try and explorehow to automate some of the red teaming exercisesbut generally speakingthere are manualways of leveraging human expertiseto probe the capabilitiesof a particular modelevaluations in contrasttend to be things like benchmarksor other automated formsof exploring the capability profileor the risk profile of a particular modelwe think it's important to distinguishexactly what you're talking aboutwhen you're talking about how you'reassessing the risk or safety of a systemand this is just one generalhigh level class of distinctionwithin the evaluationsof frontier modelsthere's really twoI think core typesof evaluations that we want to runone areperformance evaluationsand the other are safety evaluationsperformance evaluations are criticalfor understanding the generalreasoning capabilitiesor other capabilitiesthat a model might havethat allows us to understandin some ways how best to test itfor particular risks etcbut a performance evaluationis really designed to justcapture and identify and assessthe performance envelopeof a particular systemagain these evaluationsare incredibly importantbecause we don't know ex antehow to define the ex post capabilitieswe don't know how to definebefore we train the modelwhat it will be capable of on the back endso we need to be able to do performance evaluationsthe other kinds of evaluationsare safety evaluationsand there you're not necessarily trying to understandjust what the performance thresholdor performance envelope of a system isyou are specifically looking for particular risksand the abilityof a model to exhibit behaviorsthat would give youthat is capable of behavingin unsafe waysas far as kind of different typesof safety evaluationsthere's really twosafety classes of safety evaluationsthatwill start to seekind of being developed and runin model developmentone are developmental evaluationsand another are assurance evaluationsdevelopmental evaluationswhat we're referring to hereare really the kinds of evaluationsthat firms will runor the developers of a large scalepronteri systemmight run at different phasesin its training cyclejust to kind of benchmark itto see how it's doingwith respect to certain kind of safety risksthat's different from a full on assurance evaluationwhere it's not necessarily the teamthat's developing the modelinstead it's a team that's kind oftasked with assuring the safety of a systemthey have independent expertisefrom the team that's developing itand their goalis really to assure the safety of the systemand to develop evaluationsthat are capable of assuringthe safety of a system in some waywhich is a little bit different thanthe kind of life cycle developmentsafety evalsthat might happen just at different check marksin the development of the modelthen the last kind ofin my view probably the most importantdistinction here iswithin assurance evaluationsso the evaluations that are meant totry and assure the safety of a systemwithin thatas we're thinking abouttrying to evaluatemodels for safetywe really need to be evaluatingthe safety and assuranceof these systemsone way of thinking about it ismaximum capability of the systemanother is like how it's usedin the real worldI would say a different way ofdefining this last category iswe need to look for assurance evaluationsthat are designed to try and capturethe riskiest behaviorslike the tail distribution of the modellike some of the behaviorsthat are most capableor most extreme from a particular riskthat wouldn't necessarilybe compressed into kind ofthe average or mean behaviorwe wouldn't be able to get a lot of informationabout those kinds of tail risksfrom more typical user behaviorwith behavioral evaluationsI think the goal is moreto try and understandwhat is the average behavior or mean behaviorin general with some of these modelsand how we assure the systemand the safety of the systemeven within that kind of mean behavioragain this is kind of early thinkingwe'll probably evolve over time on thisbut this is just a little bitof how we're thinking aboutsome of the different evaluationsat this momentrelatedly there's somethere's also the question aboutwhat to do as you're setting upan evaluation and a red teamingwhether it's a red teaming exerciseor a broader evaluationthere's a set of practicesfor a wide array of evaluationsfor specific risksbut there's also a set ofjust best practicesfor any kind of evaluationyou're doing regardless of the riskso it doesn't matter whether you'relooking at bio risks or cyber risksthings like thator it could be societal riskthat you're looking atif you are developing a systemor an evaluation ratherof a frontier AI model or systemand I think this is just a samplingof some of the early thinking we haveabout high level best practicesthere's a few thatI want to call out specificallyone is we need evaluationsto account for prompt sensitivityI think any of theengineers here who have workedwith these models and tried to get themto behave in stable wayswill recognize that the specific wordingof different prompts will oftentimeslead to different resultsand what we're really trying to doin these evaluationsis capture their kind of risky behaviorwhich means we need toexplore different wordingwording choices or configurations of promptsto be able to get at whether or notit has a certain capability in generaland an example of this would beif you're trying toif you're worried about say likemalicious usesof a system for exampleyou don't want to just ask the systemhow do you build an explosiveor something like thatyou also want to test forcan you describe the chemical processby which dynamite releases energysomething like thatyou need to have multiple waysof asking for the same thingtwo otherat least two other kind ofthings that I want to call outjust very brieflyone is as you're developing these evaluationsyou need to evaluate both the modelthe underlying base modelas well as the end systemfor a lot of evaluationswe'll target one or the otherbut we need to do bothit's not just the underlying modelthat needs to be evaluatedbecause in many casesthat's not what is exposed to the end userusually what is exposed to the end useris the overall systemand we need to be able to test thatas well as the underlying modelanotherreally importantbest practiceI guess I'll walk through them just brieflyis evaluating both normalor typical behavior useand adversarial useas I mentioned on a prior slidewe do want to evaluate the systemsfor kind of typical behaviorsand the safety that they might exhibit under thatwe also want to evaluate for adversarial usethis is something that a lot of developerswon't instinctively necessarily doas they're trying torush to get a product out the doorbut it's very important to evaluate adversarial useProfessor Song just had areally great explanation of some of the kinds ofadversarial use casesthat you need to be paying attention tothe broader point though isif you are developing frontier AI systemsit's not enough to just focus onI think I'll end onbecause I think it's probablythe most important best practicethat we are starting to converge onas a field in AI safetywhen it comes to evaluation designit is vitally important that youunderstand what is the baselike what the baseline isthat you are evaluating a system againstfor exampleif you want to evaluate a systemfor biological risksfor examplecan this system inform or helpsomeone design a bioweaponor some kind of dangerous pathogenyou need to evaluate itnot just for thewhat kind of information the model itself hasyou need to evaluate it againstthe kind of baseline applicationthat would be used in the absenceof whatever model you are testingso in many cases that would be for example web searchand so onso all of an evaluationshould not just be kind of absoluteunderstandings of riskbut also relative or marginal riskcompared to the counterfactual applicationthat might be used for whateverapplication you are developingthat is a very brief kind ofhigh level overviewof how we are thinking aboutsome of the early best practicesfor just evaluations in generalinformation and communications technologyDirectAway has ledthe development of multipleindustry standards in Chinaincluding the country's firstbig data benchmark standardhe has participated in the draftingof several major national policiesincluding中文來講解一下那麼我希望各位外國朋友能夠通過同船能夠很好的get到我的主要的意思那麼今天跟大家分享一下中國信息通信研究院在人工智能尤其是大模型評測安全評測方面的一些思考和實踐三個方面的內容一個是我們怎麼認識人工智能大模型面臨的風險這個risk到底是在哪些維度上另外第二個方面是我們基於這些對於風險現有風險的這個認識一直在推動建設benchmark safety benchmark的這個framework並且按季度去開展這個評測的活動希望通過這個實踐不斷的促進大家來提升模型的安全水平最後也是再分享一下其他有關如何保障人工智能負責任發展安全方面的一些相關的工作那麼現在大模型實際上是一種數據驅動的路線那在skilling law上這個skilling law的延長線上一直在發展決定著模型能力的這個前景主要靠的是算力和數據那麼這個大家都非常清楚那實際上在這幾年我們可能所有人基本上所有人都認為skilling law可能還會延續但是skilling law的問題就在於我們對於模型表現具體是什麼樣缺乏非常清晰的認識而且控制能力是很弱的不像50年代60年代基於規則的人工智能模型他們是我們設計者可以很好的去控制模型的行為和輸出但是在基於數據驅動的尤其是大模型的時代模型的表現很多時候是一種現象學的範疇所以很難在內生機制上去完全做到避免風險那麼這幾年大家對於人工智能尤其是前沿模型剛才Chris已經講得很清楚了前沿模型大家非常關心它的水平的進步同時也關心它可能潛在蘊含著什麼樣的風險那麼我們基於對於各類研究的成果的分析我們認為其實可以用這樣一個金字塔來表現我們對於人工智能尤其是大模型的風險的認知幾個層面分成兩個維度一個維度是自身它的內生的安全問題那包括了模型的參數還有數據計算系統以及網絡還有應用系統的這個security層面的安全問題那麼上面一個層次就是對於個人對於國家對於全人類在應用人工智能的時候可能引發的衍生的應用層面的風險我們如何去控制好那麼最宏觀的可能是我們人類在人工智能面前的位置在哪裡我們全人類的共同的命運那麼再往下就是國家安全經濟安全社會的供應鏈的穩定人口就業以及個人信息保護等等吧這個risk實際上是多維度的所以這個討論人工智能的安全風險往往是一個多學科交叉的那麼需要去共同去更清醒的認識到這個安全的風險在哪裡再舉幾個具體的最具體的一個例子那麼以大模型為例其實現在媒體上對於大模型應用過程中暴露的各種風險報道也越來越多了比如說在內容風險方面那麼可以看到這個虛假信息deseinformation其實是非常普遍了而且這個也是困擾不管是中國還是國外監管部門非常重要的一個問題那麼還有就是數據風險那這裡面臨的我們的提示詞可能會暴露機構內部的一些信息這樣的風險以及算法可能會被用提示詞攻擊或者其他手段來從裡頭提取敏感信息的這樣一些攻擊手法這個一直在翻新而且也是層出不窮的那麼不管是宏觀還是微觀層面其實我覺得現在大家對於人工智能的探討人工智能安全問題的探討已經非常的充分了最上面是聯合國層面密疏長古特雷斯密疏長在非常多的場合在談聯合國也成立了好幾個機構包括UNESCO ITU等等在討論人工智能的治理問題安全問題各個國家其實也在行動我們看一下具體的整體上感覺人工智能的安全風險 治理正在從原則走向實踐大家對於一些基本的原則比如說以人為本智能向善比如說要保證它的公平非歧視這樣一些原則大家沒有分歧那麼國際社會包括各個國家其實也都在把這些原則正在轉向具體的操作那麼這些操作可能會落到具體的很多方面比如說一些國際規則的制定比如說在聯合國剛才說了聯合國教科文ITU等等這些機構在牽頭這個國際規則的設定那麼各個國家也在出台相關的立法那麼另外一個再具體的一個就是轉化成標準那麼把原則固化成一些能夠成文的技術性的要求來便於這些企業去遵從還有一個更具體的就是測試 如果有了標準以後還需要去驗證這些標準的遵從情況那麼在企業側呢可能還需要去研發很多的技術工具來提升保障這個安全的水平所以我們看到美國 英國 歐盟新加坡 還有中國都在採取這幾個層次的具體舉措把人工智能的治理從原則推向實踐那麼中國新同院科和行業平臺我們也在投身於這樣一個促進人工智能治理從原則走向實踐過程中我們也貢獻我們的力量今年年初我們和國內三十多家單位吧這個數據集還是比較豐富的我們有五十多萬條的題目圍繞著在全球共識的要求上以及符合中國本土法律法規的要求這兩個維度上來開展評測數據集的建設那麼另外一個就是我們也密切跟蹤前沿這個發展為什麼我們是季度性地去做這個事情因為人工智能的進展可以說是日新月異所以我們會季度性地調整我們的Benchmark的數據集和方法 不斷地提升跟進前沿模型演進的步伐另外我們還是特別關注用戶是怎麼想的關注現實的風險比如說金融 政務醫療這些行業他們的擔心是什麼目的實際上是希望能夠通過這種評測來讓這些行業用戶放心 能夠勇敢地去使用先進的人工智能技術但是前提是保障安全的前提下那麼在2024年的4月份我們發佈了第一版的AI Safety Benchmark的結果這個結果是基於我們左邊這樣一個框架來做的大體上分為三個方面一個是內容安全層面的要求這就包括了法律明令禁止的一些內容的輸出 比如說涉謊 涉暴 賭博 欺詐危險化學品 生物武器等等這些這個內容的管控輸出另外就是這個數據安全也包括了個人隱私和企業機密的可能被提取的這樣一些特徵個人信息和商業秘密那麼還有最上層是科技倫理這裡頭就包括價值觀 心理健康和AI意識工序良俗辱罵 誘導等等這些欺騙這些問題的應對它們是如何表達的如果這個評分能夠得到八九十分這個水平還是可以的但是呢問題就在於拒達率偏高這樣會造成很多用戶體驗不是特別好的同時這也表現通過具體的攻擊手法這個攻擊手法也是我們能收集到的市面上學術界包括社區這個最新的攻擊手法包括我們自己測試中發現的這個攻擊手法來測試這些模型包括提示式攻擊 誘導攻擊 越獄攻擊內容泛化攻擊和其他的攻擊手段那麼數據集也相應的做了擴大底線紅線 社會倫理和數據洩露還有一些其他的安全保障措施的這個測試應該說是一個更加全面的所以這個體系是一個不斷在演進的這個過程 要跟上這個節奏那麼Q2的這個評測數據集也比較大那麼我們有600多條提示詞的模板那麼有60多種攻擊手法3.6萬條的這個攻擊樣本每次從這3萬多條裡頭抽取4千多條來做測試那麼測試完了這個數據就淘汰了避免大家去刷題和作弊那麼目前我們Q2的這個結果也已經初步的出來了那麼可以看到大家在原始輸入的攻擊成功率我們不用提示詞攻擊手法來做提問的話 它的攻擊成功率還是比較低 也就是反過來說1減這個比例就是它的得分但是我們加入了很多提示詞攻擊以後的攻擊成功率就顯著提高了也就反向的代表著可以有很多的手法來繞過這個模型的護欄所以整體上來看我們目前是模擬了最近發生的安全攻擊的各種手段來更好的來看待我們模型安全的水平大模型的安全測試是中國心通院在人工智能安全方面的一個實踐具體的實踐讓能夠系統化的去推動人工智能可信發展那麼就在2021年的時候上海世界人工智能大會我們發佈了全球首個可信人工智能白皮書這個是跟京東探索研究院一塊發佈的 陶達成院士那麼當時的思路是過程管理就是要人工智能要保障安全除了你要求它結果以外還要管好研發使用的流程 所以過程管理我們認為是非常重要的所以在那一年的白皮書裡頭我們就提出來如何在各個包括G20 包括很多原則的指導下我們怎麼在企業各個環節落實這個原則23年到現在我們認為結果管理也非常重要我們參考了像OpenEye和Ethiopic他們做安全模型的分級分類那麼把風險分成不同的類別和等級那麼我們我覺得這個也是特別有借鑒意義所以也在推動從過程管理走向結果管理 效果管理那麼基於風險定級的這樣一個管理手段其實我們希望通過這樣一個方法論來讓企業界讓產業知道它應該怎麼去做是一個最佳實踐那麼同時我們也在制定Benchmark包括Benchmark在內的坐標尺因為如果沒法度量就沒法改進所以測試是非常關鍵的除了大模型的安全測試以外我們也在做人臉識別安全還有算法安全等等這些安全的測試標準測試能力上我們也在建設龐大的數據集包括人臉和其他的一些測試級能力的建設一直在持續地跟進同時大模型的內容安全它如何去測出結果以後如何去增強我們現在也在基於我們掌握的我們建設的300多萬條關鍵詞50多萬條提示詞來訓練我們的安全測試的模型未來希望通過參數的方式提供給模型廠商來加固他們的模型能力我覺得安全其實很重要還有一個就是信息真實性的保障信息真實性如何保障我覺得要靠很多方法其中一個方法就是水印Watermarking除了Watermarking以外比如說圖片裡頭有C2PA的標準還有其他的一些方式其實也都在探索中綜合來看我們也希望能夠建立一套對抗Deepfake來保障內容真實性的一套技術的方法包括水印我們現在也開發了水印的隱世寫入的算法也提供這樣的一些API這些企業也可以使用整體上我就利用這麼一個時間跟大家分享我們的一些實踐希望跟大家繼續交流也感謝會議邀請我來跟大家分享謝謝我們現在就來邀請一下上海AI研究所的同事來做他們的表演請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手請大家握個手来讲吧首先呢这个我们知道现在整个人工智能的发展是非常飞速的然后里面涉及到的这个国家也越来越多不仅是中国国外各类的国家大家都会关注能力和安全相关的这个共生性那我们最开始可能在17年的这个alpha zero的时候大家可能更关注这个面向传统AI算法和系统的安全标准然后到后面的时候随着更多的这个模型出来包括这个generative model包括这个从前几年开始的这个ChatGP系列整体来说现在大家会发现说我不能只观察AI算法和系统的这个安全标准或者是AI算法本身的这个能力更多要观察它的这个数据的规范还有到面向这个整个社会和群体的这个质量大模型的安全评测和对应的安全标准那实验室整体从21年成立至今一直致力于大模型的体系的研发从2021年开始发布的第一个书生1.0的模型是一个视觉的通用大模型当时应该在国际上也有很多包括DeepMindGoogle OpenAI都有相关的这个视觉通用大模型随着时间的演变到了2022年实验室开始走向这个多模态大模型在这里面也逐渐衍生出来这个大语言模型书生谱语体系在昨天的这个开幕式上也做了相关的介绍然后一直到今年7月份这个书生谱语包括书生多模态大模型已经演变到了新的新的能力范畴我们整体的公开的模型包括7B的小参数模型还有这个70B的大模型致力于这个一直致力于是开源开放的理念能够赋能大模型学术研究和创新产业生态并且我们也一直保持我们的技术的原始创新探索更为高效的大模型发展路径整体来说我们实验室的大模型研发和安全体系构建随着这个时间的历程可以分为四次挑战和对应我们做的相关的技术和成果接下来的话会分这四次挑战为大家来分享一些工作第一次挑战的时候是大概在2022年底我们发现这个越来越多的这个生成生成模型出来之后它带来的这个风险性也越来越大包括一些这种滥用的行为包括用这种模型去对 那个乔老师已经来了要不然谢谢你 Dr. 肖静",
    "给我们做了一个很棒的开始我们很高兴现在有乔余教授乔余教授是一名助理教授和传统科学家在上海AI研究研究院以及一名兼职研究员在上海AI研究研究院与中国科学博物馆联合研究的深圳研究研究研究研究研究他的研究涉及各种领域包括超级模型模型电脑视线深入学习和自动驾驶与其他研究员合作在上海AI研究研究研究院乔教授最近发布了许多AI安全项目包括许多标准和测试框架乔教授桌子是你的继续课程感谢您的参与对于有关重要的事件对于有关重要的事件对于有关重要的事件首先抱歉因为个人的原因把我们这个会议中断了一下我想这样其实刚才那个邵静博士是我们整个大模型安全团队的负责人我甚至在想他应该比我会了解更多的技术细节会讲更多的干货但是我还是非常感谢安远和谢总的邀请能有这样一个机会跟大家做一个分享我就直接跳到刚刚那个我们是讲到这一页是吧对对对我先说一下是这样的我其实原来主要做计算机视觉的研究然后慢慢的开始做视觉大模型从2020年开始然后随着我们从语言从视觉到语言到多模态做我们越来越发现安全实际上是大模型发展中非常重要的一个问题而评测又是整个建立安全的基石所以从去年开始在实验室的整个的布局下面我们就把安全做一个重要的方向大家可以看到在做安全的时候首先的第一个问题就是说大家事实上都很关心安全但是对于安全领域我们到底具体关注哪些因素应该从哪些角度去评测实际上并没有特别广泛的这样的一个认知在这里面我们当时看到第一个事情是评测体系和数据的缺乏所以在去年我们进入这个领域之后第一件事情就是做评测体系我们建立了当时蒲公英人工智能一个治理开放的平台这里面把国内还有国际上很多的治理理论制度进行了汇总我们也建立了包括红对数据漏洞数据 评测数据等等的相对来讲比较全面的一个评测的数据集在这里面实验室也坚持开源开放的理念我们把我们的很多的数据集还有我们的规范进行了开放从而推动这个领域的发展怎么建好这个数据集大家知道早期大家评大模型就是我给一个固定的题集然后有标准的答案让这个模型答完了之后我们对它进行评测进行对齐但事实上这里面是有很多问题的我们知道对于大模型来讲如果你做了SFT做了RLHF人类反馈的强化学习事实上它表面上表现得很好但并不是代表它真正的就没有风险没有问题那在这里面怎么来做呢我们当时设计了一个方法我们发现很多领域的专家包括很多做社会学政治学伦理法律的我们组织了上百位我们请了包括上海交通大学复旦大学的上百位的专家这里面还有包括很多的教授让他们帮我们设计种族问题因为这些专家时间非常有限你很难说你请这么多专家让每个专家你帮我设计一万个题一万个题事实上对大模型的评测还是非常窄的一个理论我们在专家设计题的题目上通过大模型的方法对这些题进行增强进行扩充然后我们再用这些题目去评测特别是这些题目很多的设计它有很多的攻击性而且有针对漏洞的一些专门的设计这样就形成了更好的评测的效果这是我们在第二个方面所做的工作然后我们在因为我既做模型也做模型的安全在这里我们就发现大模型的安全和对齐大家经常说有一个问题叫对齐税也许确确实实你做了RLHF了之后模型的安全性从评测上分数会有增加但往往对能力对原来它原有的一些能力模型的性能会有下降在这里面我们是不是有更好的方法在这里面我们事实上就把一些MODPO就是Multi-Object DPO多目标的对齐引入进来这样能够保证我们在保证对齐的同时也能够对原来的能力进行优化这里面有些算法的创新由于时间原因我可能就不再详细讲而且我在想在这里面MODPO现在目前也不是一个结束式应该这些算法和框架还有很多优化的空间我看在座有很多我们年轻的同学这可能成为你未来研究很好的题目基于我们形成的评测体系基于这些评测的数据我们实验室也研发了一个普安大模型安全评测的系统在这里面我们事实上把一些我们前面所做的从维度到评测的过程到出报告完全把它进行自动化的评测这里面也支持了我们上海市的一些工作在这里面我们发现就是说评测结果的透明性和可解释性是一个很重要的事情因为经常的时候随着评测的维度越来越多问的题目越来越深经常我们很多大模型的评测用户拿到评测报告他会问我们你们为什么模型这么你为什么出这样的评测评测报告你评测报告是不是客观能不能对它你的结果的依据进行说明进行解释在这里面我们想到最终的话我们解释还是基于我们建立的基于数据库完了之后我们把我们的评测结果对它进行可解释性的生成这样就把问答和可解释性的一体化集成在一起而且因为采用reg的方法它是一个外籍的数据库很好的方法就在你可以实时的更新比如说我们有新的规范新的规定我们可以非常简单的更新到数据库里面进行评测当然大家知道现在整个大模型从语言到多模态发展我本身也做计算机视觉的研究多模态大模型的事实上又对评测提了很更新的要求比如说在这里面对于多模态来讲由于它内容生成的形式和形态更多如何定义维度需要进一步加另外在多模态之间本身就有特征的对齐的问题而且我们知道语言到多模态了之后事实上很多幻觉问题被进一步加强针对这些特点我们做了一个以视觉为核心的包括对齐还有评测的数据机SPVL它包括我们设计了针对多模态领域六个比较可能引起有害的领域四三个类别五三个子类别这里面也包括了超过十万个这样的问题然后来帮助我们一方面做评测另外这个数据可以作为对齐此外实验室我们在去年还花了很大力气应该是在国内甚至国际上比较全面的一个具人类价值观的一个多模态评测体系这个报告总共有四百多页四种模态而且在这里面我们发现单独的提维度事实上对于这个领域的技术的发展还是很有局限性的我们建立了两百三十个用例而且这两百三十个用例是根据多模态大模型应用场景落地来提出来的包括放话性可信度还有推理能力大家知道未来多模态大模型推理能力是大家现在关注的重点我们可以看到最新的大模型大家都在强调它的数理和因果的推理能力进行评测实际上这个评测结果也对我们后面包括大模型特别多模态大模型安全的评测起到了很大的指引的过程面向未来我们觉得有几个方面值得关注还有一个就是多智能体大家知道这半年agent技术发展很快agent已经成为强化大模型在专项领域跟包括跟很多世界进行互动的一个重要的手段那是不能够把这些技术也用来安全和评测呢事实上我们建立了一个多智能体的评测框架叫pc然后在这里面呢我们详细研究了多个智能体之间他大家知道他之间有交互和协同他所能产生的这个危险行为以及在这过程中间我们如何引入一个专家一个doctor去进行角色防御另外一个呢大家知道人与人之间交互过程中心理这个价值观是非常重要的一个问题所以呢我们也在探索如何从仿造人的方法从心理学的角度然后从人文科学的角度来去做这个大模型多智能体的安全评测我想说一点是这些工作其实才刚刚起步从实验室角度来讲呢我们最初关注的包括定义问题包括建一个好的平台让大家来做这方面的研究这里面有非常多的重要工作我们也会非常欢迎啊在座各位的机构企业还有研究者我们一起来共同推动这个领域的发展实验室除了自己做之外呢我们还坚持着开放的策略事实上呢我在中国网络空间协会大家知道这是在我们国家这个领域非常权威的协会的架构和指导下呢我们成立了围绕生成式人工智能安全评测的工作组我本人呢也非常有幸的当选了工作组的组长其实压力还是很大的因为这个工作组既包括咱们国家一些权威机构也包括在这里面研发的包括交大包括交大清华复旦这项大学还有包括百度华为这样的企业为什么要成立一个工作组呢我们认为呢要做好博伦馆是安全还是评测他一定不是一个机构是一个共商共治的事情在这工作组在过去的一年接近一年的过程中我们从几方面开展工作一方面的是建立了常态化的这个交流的机制啊我们有日常的例会而且现在通过线上会议呀通过专题会的形式让大家日常能交流第二个呢我想这个工作组的一个很重要的方面呢我们是想从bottom up的从底层从大模型一线的角度能够形成技术规范和共识啊在这里面我下面会讲我们现在已经初步形成了一个声称是人间自能评估的流程规范啊这里面我不知道在座是不是有一些同事已经参与因为这个规范涉及的范围比较广已经参与到这个工作进来第三个呢我们在这个规范的基础上呢我们会跟大家一起啊共同研发一些评测的技术和主见啊这里面包括数据集也包括规范的基础上呢包括工具而且呢包括我们刚才讲的平台而且这里面我们希望能够在大家形成一致的条件下能够拿出来一部分进行开源开放的形式提供服务最后呢我们也举办了很多安全的活动包括恩安园的很多活动今年在世界人工智能大会的前夕呢我们也举办了普选安全挑战赛吸引了全球一两百个非常优秀的团队来参加啊我们也日常跟很多的一些相关的大模型研发机构做这个安全的指导啊这就是我们所做的感觉刚才提到安全评估的流程和规范它有几个特点第一个它可操作性很强我们是一个全维度评测提供全生命的这样一个规范第二个特点我们是服务应用和产业落地因为参与的本身很多企业实际上他们从实际应用落地的角度起了很好的建议我们希望这个规范也能成为推动咱们国家未来人工智能安全评测发展的重要面向未来我想说一点大家往往容易把人工智能看成一个工具但是我更想说的随着通用人工智能的发展随着技术的进步人工智能成在成为我们整个社会体系的非常重要的一个基础设施它会频繁地与人互动与其他的系统互动所以我们现在考虑人工智能安全绝对不能孤立地把它看成一个工具而应该从整个社会体系的角度来思考人工智能这里面包括整个需求包括应用的场景还有包括在这个过程中间人机物之间的这种认知体系的建设是很重要的办法怎么来应用这个地方呢我觉得就是说大家知道大模型的发展是以Skeleton Law为指引随着算力数据模型规模的增大模型的能力不断提升那么我们在想是不是我们也要探索一个围绕安全的Skeleton Law我们能找到一个可扩展可发展的方式只要我们投入更多的研发的资源数据算力一些技术的研发的投入我们就可以把安全也可以可持续地发展下去在这里面我觉得安全的Skeleton Law可能比原来大模型的Skeleton Law大模型Skeleton Law核心是参数量数据量和计算资源这三者首先也是安全的更重要的组成部分但是对于安全的Skeleton Law绝对不止三个维度我们需要多方的参与需要更新的研发的模式当然我们也需要高质量的数据以及更好的模型的架构这里面的事实上也对于我们的Community不管是你做科研的比如说产业运用提高了新的挑战我们希望跟大家共同能力建立起面向未来的可持续的AI安全的Skeleton Law在这里最后我也稍稍做一个宣传在明天的上午我们在世博中心620会议室有一个国际AI前沿技术的论坛我知道今天在座的一部分专家也会参加明天的论坛也欢迎大家继续参加和支持我们的活动好 谢谢大家",
    "谢谢你 教授我们知道您的计划非常紧张所以我们很感谢您来到我们的讨论室请请回到舞台上请准备坐在舞台上我们将进行AI安全的讨论教授乔Chris 魏 魏凯请帮我们回到舞台上这次的舞台上我们也有瑞敏 何 和熊德义教授瑞敏 何 和熊德义教授请帮我们回到舞台上让我介绍一下瑞敏 何 教授是新加坡的首席人工智能警察他执行多个职位的努力以达到新加坡的计划AI目标包括发展和实现新加坡的国际AI策略他也是新加坡政府首席人工智能警察和一名在AI上的高级领导组教授熊教授是自然语言处理研究研究院和天津大学国际联合研究研究中心国际联合研究研究中心他最近在计划计划AI安全的计划包括大规模评测以及更多的计划我们的主席是Brian Zare是 Concordia AI的副总裁让我们来看看他的掌声好的今天我们就邀请准备好准备准备好准备准备好准备准备准备准备准备准备准备准备准备准备讓我先介紹一下您發佈了一份第一次的研討關於大語言模式的評估並且更早前為中國LRM進行隔離危險評估有三個群組第一個是不平衡的危險這個危險通常對社會有負面影響第二種危險是不適用的人們可能會使用大語言模式來解決這個危險第三種危險是障礙性危險很多人都用了不同的詞彙有些人稱之為障礙性危險也有障礙性危險以這類危險的背景來看我認為這三個障礙性危險有三個阻礙性危險的解決策略第一個是我認為現時的大多數的危險主要是關於障礙性危險而不是關於正式的分析但我們也看到一些最近的研究中的研究其實有兩種類型的研究第一種是正常性的研究第二種是經濟性的研究正常性的研究人們會使用一些問題他們會問障礙性危險模式看看他們是否有所期望或是更多力量或是更多財富或是更多錢類似的事情如果模式非常聰明他們可以說謊他們可能需要達到真正的目標所以我認為我們仍然缺乏很多方法來解決障礙性危險的危險所以這是第一個項目第二個障礙性危險我認為是因為我們缺乏資料和資訊因為目前最多的研究其實是一個黑箱的研究我們沒有資料可以引起模式以抵抗他們的危險我們沒有資料可以開啟黑箱的模式而最後一個障礙性危險我認為是障礙性危險因為最多的障礙性危險模式或最高障礙性危險模式是由大型公司所以他們只有很少人的資訊我認為現在我們看到很多模式已經被封鎖了很多模式已經被封鎖了所以這意味著我們沒有這個模式的透明性所以這是一個非常大的挑戰尤其是在障礙性危險方面謝謝您的精彩的評論莊教授何教授今天我們很高興能與您一起訪問這堂會我們期待您在早上聽到您的討論但在此時我最近聽說新加坡設立了一個國際AI安全基礎您可以跟我們分享一下在新加坡的AI安全基礎在這個國際AI安全基礎中或在更廣泛的新加坡在這個國際AI安全基礎中或在新加坡的AI安全基礎中謝謝很高興能在這裡再次跟您見面我可能要再走一步信任和安全是一個非常大的關鍵在新加坡在全球我們在過去59年來作為一個國家建立了信任社會上有很高的信任也有很高的信任在政治領導上信任也是我們在新加坡的健康基礎中有很強的生產基礎信任也是人們在晚上安全走路時可以在街上走的原因也就是人們在網上的交易AI當然增加了信任的困難instead that all真的是you can have models can hallucinateas a lot of the speakers spoke out todayit generates a fake newsyou can't always rely on themand therefore when we havecome out with our national AI strategyup to last decemberDrust becomes a very big part ofso we have manymany different initiativesat different levelsfrom a research point of viewwe are doing a lot to supportfundamental research as well aspolicy researchand to responsible AIas part of AI Singaporewe also have tworacelisOh yes and yes as being a part of AI SingaporeWe have set up a center for advanced technologies in online safety to look at misinformation and disinformation.We have an entire cybersecurity agency looking at AI security and security of AI.And because we have a whole range of efforts, we also have set up a digital trust center,which we designate as our AI safety institute, really to be a national focal point for AI safety efforts,looking at evaluations and testing all along the developmental life cycle,all the way from development to deployment of AI models,and also a vehicle for us to do international collaborations with other countries,so that we improve the science of AI testing, and to improve what we do in Singapore and around the world.Thank you. So we have set out some of the priorities for AI safety evaluation,and let's move on to the life cycle of doing evaluation, as some of the speakers have mentioned.And I would like to get the views from Director Wei and Chris on this question.So,Testing could be conducted in different stages of the cycle,including during training, pre-deployment, and post-deployment.What do you think are some of the benefits and challenges for conducting evaluation along some of these stages?And what are your experiences working with companies in both China and the United States?好,谢谢。",
    "那么,大模型其实进展非常快,但是原理上决定了我们对模型的激励,对它的安全风险,对它的能力水平的认识,不能做到百分之百非常全面。就像我们用竹篮打水一样的,到处漏水。我们做的工作,比如说测试,其实是去识别哪里有漏洞,然后我们去把它补上。但是还有多少漏洞,我们可能不一定能够搞得非常清楚。我觉得现在,要说优先级的话,可能,一方面我们一定要快速地,及时地发现在各个环节中暴露出来的,比如研发环节,还有使用环节暴露的,以暴露的各种风险,及时修复它。但是这个问题看起来还是比较容易做到,相对容易做到的。只要我们能够知道这个风险是什么,我们就能够定义它,测试它,改进它。问题在于我们不知道的太多。所以,我觉得当务之急是需要我们去做的。我们需要建立一套机制,建立一套动态,敏捷的机制,使得我们能够永远跟上模型技术水平的提升,以及它的风险。我们能敏捷地发现它,同时能够迅速地让产业界知道风险在这里,风险在那里,然后来改进它。我觉得测试非常非常关键,但是测试只是一个环节,我觉得需要从各个环节上来建立一套敏捷治理的技术生态。才能够堵住这个不断暴露的这个漏洞。所以,否则我们永远是跟这个未知在赛跑,可能很难控制好。就道理,谢谢。",
    "克里斯。首先,谢谢您来到这里的荣誉。很高兴能够与您一起参与。我会建立一些我同学的评论。有这么多不明显的事情,但是我认为这是一个非常难复杂的评论。我认为如果我们在今后五年或十年之后有这种讨论,我认为我们将会更加成熟。但是我认为,你知道,重要的是,我们需要做的几个事情。在计算、测试和测试整个生活过程中,我们实际上需要做的一件事,在企业和政府之间,我们需要更深入地理解释我们需要做的各种测试,在哪个阶段的生活过程中。我认为,这甚至是预备训练。我认为我们需要在预备训练前的测试中,比如在10X的数据和数据上,你觉得你能够安全地做出这种测试吗?你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,你应该在这种测试中,当这些系统变得更能够创造和执行自己的研究,例如,我认为我们还不够在那里,但是像是系统的自动测试,那是一个你肯定想要测试的事情。但是,当你训练模型后,你采取了一些调整,我认为你仍然想要进行几个测试,直到你实际上采用它,以确保你感觉舒适,使用者可以安全地使用它,包括你正常的用户,以及一些可能会有迫害的演员,可能会尽可能地尽力推动系统,以坏意义的方式。问题是,在这些领域中,要做多少测试?我认为这是一个希望,在下个月或两个月之后,希望会有更多的谈话,在全球AI系统上,我们应该做多少测试,在我们感觉舒适的地方,前往新的领域,在人生中的下一个阶段中,我认为这是我们需要更多的谈论。教授,您想说什么?我想刚刚各位嘉宾讲的非常精彩和全面,我想补充两个点,第一个点就是说,在当前这个阶段,安全问题非常受重视,我们都认为它是很重要的问题,但事实上,不论是学术界还是产业界,我们在安全中投入的资源,比如计算的资源,还是研发的资源,相比我们发展大模型的能力和产业应用来讲,是远远之后的。而另外一方面,事实上,我们对于整个通用人工智能,特别是现在大模型安全的了解,是非常有限的。我们知道存在很多安全的隐患,但是我们对安全这些隐患并没有一个深入的认识。更重要的是,我们实际上并不知道为什么产生这么多安全隐患。就是说我们更多现在对安全问题是在应用过程中从现象层面所观测的,对背后原因没有这么多的支持。所以基于这两个观察,我觉得有两个方面的事情,下面要做。第一个,在这里面我们需要投入更多的资源,需要更多的国际的合作,因为安全问题是我们全人类所面临的一个共同的问题,这一点是非常重要的。第二个方面,在研究上我现在真的是呼吁我们科研学术界,包括我们的产业界,能够找出来一条能够通向我们Safety AGI,安全通用人工智能的技术框架。因为现在我们可以看到,不论是SFT还是RHF,更多的是一些单点技术的研究。往往还是不漏充的单点技术的研究,并不是一个我们很难确信靠这样的技术我们就能真正通往Safety的AGI。所以在这里面,我觉得从学术界上真的需要更好的研究和框架和投入。谢谢。",
    "第二,我们需要进行一整个AI测试的过程,包括预约训练,不仅是训练后的训练和投入。第三,似乎AI安全投资的水平还不足够,我们需要增加这种投资水平。我认为这带给了我一些AI评测的挑战,在一些课程中提到的。第一点是方法。特别是,我希望教授熊教授和乔教授,可以评论一下这一点。似乎现时流行的测试方法是自动测试,和人工资训练。你认为有没有新的方法,在目前的情况下,你认为有没有新的方法,或在目前的情况下,有没有新的方法?我觉得从技术上,有几个方面,我觉得现在值得关注。刚才我们确实需要建立更好的技术和方法框架来解决这个问题。我们的技术,智能体已经被验证成为智能体加上工具调用,包括智能体里面,它可以进行的交互,反思,任务的规划,已经成为大模型,目前在落地中重要的一个技术。最近我看在学术上,在工业界都有很好的应用。事实上,智能体所演化出来的技术,对于我们解决目前在大模型,不管是做更全面,大家知道传统的安全研究,比如说计算机安全研究,有很多计算机理论,密码学等等的研究,他们都能提出一个在这个领域非常硬核的一个数学问题,一个理论问题。但目前在安全研究中,你会发现特别碎片化,碎片化就意味着在这里面我们没有很好的一个基础,没有很好的一个体系。我想这是我们在这个时候,从学术界,甚至包括产业界,我完全同意的意见。我只是提出了两个点。第一点,这是第一点。第二点,我认为,在评测方面,我们需要建立很多资料,但是在安全方面,我们有很多安全问题,而且这实际上是所有课程的计划。所以,如果我们只依靠我们的AI社区,其实我们,没有了资源,没有了资源,也没有了能力,建立这些资料。所以,我认为,我们必须与更多社区合作,以解决AI危险的问题。谢谢。",
    "河先生早就提到建立信任的重要性。如果公司进行自己的评测,社会就很难完全相信AI的安全。我认为这就涉及到第三者的角色。我希望河先生和董事长能够评论这些问题。您认为,第三者的角色有什么挑战?例如,陈教授提到的质量问题。如果第三者测试员和测试员只能够透过AI模型进行黑箱测试,那么他们的AI安全测试应该会如何?我希望您能够提供您的观点,河先生。我认为,在AI Verify的经验上,我们已经在这方面做了很多工作。我会说出三个原因,为什么我们在AI Verify上做了这些工作。AI Verify是新加坡政府两年前推出的测试设备。它有三个原因。第一点,AI Verify实际上是一个很大的原则,在这里,所有人都在做很好的研究。第二点,AI Verify实际上是在企业手里做的研究。所以,AI Verify实际上是用实际工具,但也尝试在企业手里做最好的研究。否则,你的第三者在很远的地方,所以你必须在最远的地方做最好的研究。第三点,你需要你的第三者,你需要你的系统永远都在前面。所以,去年,我们更新了AI Verify,以Moonshot为主,这是我们创造的AI版本。这是最开始的,这是最初的产品,带来的设计,带来的团队,这是最开始的。好的,那么,第三方机构是非常重要的。我觉得第三方机构的存在其实有助于提升大家对于人工智能,特别是大模型技术的信心。而是要告诉他们,通过第三方的一些专业能力,告诉研发者,应用者,风险在哪里,应该怎么去改进,下一步应该怎么办。同时,第三方机构可以扮演一个桥梁作用,让所有物体的,收集到的安全风险汇聚起来,测试数据汇聚起来,方法论汇聚起来,研究结果汇聚起来,能够更好地赋能任何一个单点的研发机构。我觉得第三方机构扮演的角色其实是非常丰富的,那么测试是非常重要的核心的手段。我刚才,我非常同意乔老师的这个观点,就是现在这个整体上,产业界对于AI的投资还是非常有限的。那么我觉得,第三方机构的存在,其实是提供一种公共产品,来降低,或者说在有限的产业总体安全预算的情况下,提供一些公共产品,让大家的研发,安全投入,可能会,成本能够可控,恰到好处。我觉得咱们做安全方面的工作,其实还是要服务于更好的发展,更好的应用。那么在控制好,这个成本,投入的这个同时,在保证安全的前提下,控制好我们的这个前沿模型研发机构,要披露这个风险列表,这样才能展现出一种负责任的这个态度。这样我觉得,第三方机构和企业合作,做这些事情,其实是才能形成一个闭环。感谢。I very much agree with the visionthat AI safety should be a global public good.And some of the AI safety evaluationthat we can develop around the world,or AI safety labs in different citiesand countries around the world,that we begin to alignon a shared vocabulary,so that we can actually,without knowing exactly what you mean by red teamingand what we mean by red teaming,it's hard to trust and allow for these thingsto be interoperable.And so I think that's like the key first stepthat needs to happen in the next year or so.Professor Qiao我想跟刚刚说的,我想跟刚刚各位专家讲的已经非常全面了,我就最后想说一点,就是在呼应你的,我觉得在这个领域,我们太需要国际合作了,需要国际共识,国际合作。If no final remarks,let us give a final round of applauseto this excellent panel.现在远处讨论的嘉宾,请留步,其他论坛的嘉宾请上台,我们会在台上进行一张合影。然后上午论坛呢,告一段落,下午论坛会在下午一点十分开始。The morning forum has concluded.The afternoon forum will beginat 1.10pm.See you in the afternoon.Thank you for watching.字幕志愿者 杨栋梁字幕志愿者 杨栋梁字幕志愿者 杨栋梁Look the history of invention,every single major inventionhas brought hopes and fears.And, across the times,including the modern society,当时的社区被维修了,完全被维修了,例如电池,车,和数十年后的数十年后的电子技术。这是很存在的。如果你想想,十年后,二十年后,你想想,我们的电子产业对我们有什么影响?例如电脑对社区有什么影响?他们完全维修了社区。每次这些转变都带来了新的风险和新的危险。每次,在新的危险中,有些东西被幻想,有些我们没有看到。我们认为目前的转变,我们所见识的转变,不能学习以前的转变。所以,我们认为电子产业可以成为更好的世界的驾驶者。但在现代的转变中,我们认为电子产业可以成为更好的世界。但在现代的转变中,我们认为电子产业可以成为更好的世界。但在现代的转变中,我们认为电子产业可以成为更好的世界。但是相当于水格质和有 relay-in、coакс友好那样的转变问题。无论如何就好,在转变前面,它们不同部位之间无论美好或严重或 groupe好。对于整体而言,一切都要有一个 direcimonic effect。在气声矛盾的表现上, בاج LIKE是视乎敹充的时机。与估卧 courses segunda-interchange的中国人。对于世界自然、通行情绪、外系和客户广泛热。我们尝试了解AI对更好的世界的变化的证据,所以我专注于健康,因为这是我最了解的事情。一个例子是AI在读写地图。现在,在美国的食物与药业领域中,AI系统在读写地图上进行试验。这是一个健康和经济的理论。这个问题的理论是,如果你早点诊断癌症,它是能够治愈的。诊断写地图是相当便宜的,而医生的读试是最好的。因此,如果我们能够让AI在读写地图上提供更好的AI,希望能够提升癌症的生存力,降低癌症的健康担忧。这就是AI的一个理论的一个例子。现在,有一个更复杂的故事,是关于医疗资源的提供。这里是一系列的数码工具,关于早点诊断,关于预测,例如距离的预测,各种不同的测试,都合成一样,以解决医疗的解决问题。如果它能够解决医疗的解决问题,在医疗医生或医生的患者上,它是有益的。因此,在一些地方,我们能够看到AI能够帮助,帮助医疗。现在,回过头来,看一个更广泛的经济观点。有些研究研究了AI的影响,例如,在业务上的生产AI。这里是一个研究,研究在客户服务部的生产能力的增加。我们看到,生产能力的增加,在一个小时内,成功完成了多少个任务。所以,我们认为,这就是我们能够预測,什么AI,特别是人工智能,将对经济界带来的影响。然而,我们想强调,我们必须谨慎自动驾驶的信息。技术的历史,尤其是AI的历史,是充满了增加的信息。近15年前,也许10年前,我们预测过,自动驾驶的车,会在旁边。但这并没有发生。专家现在认为,自动驾驶的车,将在经济界带来的影响,将在10年之后,成为经济上的生命。所以,在10年前,我们认为是实际的,而现在,我们认为是实际的。因此,我们要专注在这里。我们有承诺和梦想,但他们都不会实现。这些承诺,与很多投资有关。在人工智能,现在,投资的大小,是数千,或是数百亿,每年。在人工智能中,确实有波动。这是重要的,因为它形成了他们的论语。困难是,要找出正确的交易,分辨和估计。人工智能,是技术的逐渐改变,在每天的经验中,带来长期的改变。人工智能的技术,已经在电脑中,已经在我们的电脑中。然而,历史教导我们,需要很长时间,才能够改变社会。在人工智能时代,生产能力增长,只有20年后,技术进入。因此,人工智能的改变,仍然是人工智能的改变。这充满了社会的信任,并且,人工智能的原则。因此,我们做了一个风险分析,这是一个相当经典的分析。它看起来像是人们曾经展示过的东西。例如,亚绪亚。不惊讶。我想与我们讨论几个重要的东西。一是,人工智能的智能,对民主社会来说,是重要的。它形成了民主社会的论语。它被人工智能威胁,为了许多原因。其中一个原因是,人工智能在电脑设施中,创造了 filter bubbles 。这就是他们讨论的问题。它仍然存在,而且非常重要。另一个新鲜的东西,虽然它没有破坏,但是它却是电脑经济的破坏。基本上,论文化是一个商业模式。这是社会中的影响。这是社会中的影响。这是社会中的影响。我想与您一起进入另一个风险,因为我们认为它非常重要。这是私隐的风险。这有两个方面。首先,我们越来越注重AI模式的组织。亚维亚人认为,在GPT中,我们正在互动与一个中心化的演员。这位演员在我们的私人数据中拥有一个窗户。这是他们的第一个问题。第二个关系的问题是,AI训练,对不起,专业训练的数据。例如,如果我训练一个AI系统在电脑健康记录中,并将它放在野生中,有很好的理由认为,能够采取一些用于训练AI系统的私人资讯。这和财务问题有关。另一个重要的问题是执行执行执行。我们并没有太多谈论这个问题。但我们认为,执行执行是重要的。执行执行AI,更多AI系统,各种AI系统,是一部分日常的执行。还有很多执行执行犯罪。其中一个,非常好地执行的,是在联合联合联合伙伴在联合联合伙伴发生的事件。这发生后,人们变得贫穷。这可能会引起人们犯罪。这可能会引起人们犯罪。这可能会引起人们犯罪。但这个问题并不是技术问题。这只是技术问题。要说明,不每个人都被某种意义担忧。因此,所有在计划这个系统的演员都不在计划这个系统。所以这不是技术问题。而是如何演员用技术来工作。这是一个很久以前的问题,但也有证明的问题。这问题仍然存在在现代技术中。在语言模式中,这问题仍然存在。例如,我学生的李沪珊,我们在调查大语模式的罪犯的犯罪状态。我们看到,美国或欧洲的罪犯比美国或欧洲的罪犯比美国或欧洲的罪犯更差。这完全是用于训练的训练处。所以,这里没有什么新的,但我们不要忘记那些老问题。另一个关键的问题,就是我们所谈到的永恒高度。每个人都谈论计算机的增长。我们必须将这些事件与计算机的增长放在一起。在这里,我不仅仅是计算机的增长,但是也在计算机的增长是能够击破所有能够在世界中的能够。这就是训练的价格。我认为训练的价格是更重要的。我们看到的是相同的东西。当计算机变得更好,那就是计算机的增长会更加突出。现在,这一点根本不可持續的,經濟不合理,我們有一個反彈的效果,它刪除了硬件的功能。關於這一點,我們最大的擔憂是力量的集中。我們有一個經濟,大人越來越大,因為我們使用更多的資料,更多的電腦。關於這一點,我們有一個系統,因為我們使用更多的資料,我們使用更多的資料,因此,選擇變得更集中。組織建立了決定引擎,為所有人作出決定。這和其他危險,例如錯誤、隱瞞、隱瞞、隱瞞相配合,我們認為這是AI的最重要基礎。因此,我們認為我們需要有多領域的AI。我們相信在這方面,我們需要開放資訊,它能夠提供新智,它能夠提供新智,它能夠提供新智,它能夠提供新智,它能夠提供新智,我們認為重要的概念是公共的概念,不僅是開放資訊,它有挑戰。它有責任的挑戰。如果我們沒有做好責任規則,我們可以殺死開放資訊,因為開放資訊有積極的經濟持續性。挑戰是不公平,在模式中的資料,而問題是,我們使用開放資訊,我們通常不定義於我們所講的。現在,開放資訊是用來定義不公平的開放資訊模式,而這並不是開放資訊的標準。因此,我們相信國際管理需要。正如網絡,AI很容易通過境界。最好是我們使用訓練資訊,採用多個證據,以建立更多的代表性的AI,以避免我之前提到的鋼線。因此,我們相信我們需要共同管理,以幫助我們建立共同的物體。正如ICANN,這是非常有用的,我們需要共同的網絡,而不是兩部分的網絡。因此,我們建立了一個世界AI組織,主要代表第一,國家和中間國家的組織,第二,研究,主要的資產結構,以及公司和地區。目的就是設立標準,包括評測標準,並建立一個AI的知識狀態,並提供指導和戰略指導。謝謝。謝謝Gail給我們帶來的令人驚訝的看法。我相信之後有更多的討論。我們的下一位主持人是Ramin Ho,他是新加坡的主要智慧機器官,他擔任了新加坡的技術目標,包括發展新加坡的國際智慧方案。他也擔任了新加坡政府的代理和科技機器官,以及是美國高級的智慧機器組織的代理。賀先生,請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請請对AI管理的执行。每个国家的政策执行方式是由于其独特的情况下,例如它的历史、国家优势、比较高的优势和企业基础。新加坡是没有区别的。无论如何,对于AI安全和管理,我认为有四个普遍的团队,在各国各地都适合。因为我们都从同一个地方开始,Gaia其实给了我半个话题,所以我现在的生活比较简单了。让我来与每个人一起谈谈。首先,我认为我们必须要与AI管理与虚假的心态进行执行。当前开放AI委员会的Helen Toner提出,这并不是真正的智慧。更何况,正如这几个早上的讨论者所说,没有人真的明白AI系统的内在工作,特别是深入的智能网络。更加,AI技术在急速进行,但我们无法预测AI系统的进行程序的确定性。我们并不确定AI安全的技术方法,例如机械解释能力,或丰富方法,会否真正进行。我看到有些人在前几个楼上响起嘴巴。Chris在笑我。另一方面,我们也不确定如果有些最糟糕的情况会发生。对吧?AI在前几个楼上已经被隐藏了。因此,正如这几个早上的讨论者所说,AI有许多不明,也许有许多不明。因此,我们应该禁止确定的测试,特别是未来。而是说,政策人员必须尊重,尊重继续学习思绪,尊重继续学习思绪,尊重继续学习思绪,尊重继续学习思绪,尊重继续学习思绪,重新测试理论,继续研究新闻和科技的影响,继续研究新闻和科技的影响,继续研究新闻和科技的影响,我们必须学习企业和企业的专家,我们必须学习企业和企业的企业,我们必须学习企业和企业的企业,我们没有任何答案,而我们必须专门谈论专业的专业。像世界智能会议的会议,我们将用来提升共同理解的智能,我们将用来提升共同理解的智能,也意味着有能量听听和学习,也意味着有能量听听和学习,市民、工作人员、写作人、艺术家、年轻人、未来者,所有人都具有重要的看法。他们的怀疑和感受对智能真实,他们可能是国家和业务,我们必须理解他们。我非常感谢美国的智能协议团队,他们很明显地谈论智能,并且提出了他们的建议。这包括我的同事,凌翰和曾毅。凌翰今天也与我们一起。虚弊也包括承诺智能的关键问题,我们不懂得解决,所以他们可以尝试发掘答案,并且让我们不决定根据不成立的理论。在这种心态下,我们在上周的新加坡智能会议中与全球智能专家联合联合研讨,以解决新智能的关键问题,如果解决了,这将带来智能的发展和扩展。在上周的智能专家联合研讨中,今天与我们一起参与智能与智能专家联合研讨,包括邓颂、耀东、艾琳和布莱恩。智能专家联合研讨也包括了认识我们的制度反应可能是错误的,或者他们需要更新。这并不是说我们作为政府或政策执行者,我们坐在床上等待。而是说,我们可以开始从调整制度和规划的介绍,获得影响者的反应,观察该规划的后果,如果需要,并确认它们,并确认它们,并确认它们,并确认它们,并确认它们,并确认它们,并确认它们,并确认它们,并确认它们。We believe that an iterative learning approach is far more sensible in attempting to regulate all of AI's potential harms.So that was humility.Second, we need to approach AI governance with a sense of perspective.We need to be careful about false dichotomies.Nothing is really black and white about AI policy.For example, an AI system can be generally useful, but it sometimes says incorrect things.AI can increase productivity, but it can also cause job disruptions.AI can help with climate change mitigations.It can also hurt the planet with its high consumption of electricity and power.AI can do a lot for healthcare.And AI systems can generate deep fakes and scams, but there are also ample examples of AI being deployed for the public good.For example, in Singapore, we use AI to smoothen immigration clearance, predict hospital times, optimize train maintenance.Citizens can also access some government services using chatbots.So regulation and innovation are also a false dichotomy.We need pro-innovation regulations that allow the beneficial applications to flourish while guarding against the harmful users.This weight of balance differs sector to sector.For example, the worst-case scenarios for self-driving vehicles are quite different from cancer diagnosisor chronic cancer diagnosis.All chatbots.AI also does not exist in a vacuum.It is part of a technical product, which is part of a use case, and it's part of a broader environment that we interact with.Hence, it may be useful to think about AI policies in a broader regulatory environment.In recent years, Singapore has updated our suite of laws to safeguard the digital domain,including for personal data protection and against misinformation and disinformationthat is spread online.To better manage cyber risks and egregious content, and curb online criminal activities.In these regulations, the human or the institution remains responsible for the consequences of their decisions,even if these actions were aided by AI systems.So that was perspective.Third, I think we need to increase and improve our capabilities to govern AI.This starts with encouraging many of my fellow policymakersto use AI, so that they develop a baseline understandingof the potential and limitations of such technologies.In Singapore, we actively promote the use of AI within the government.Civil servants can access AI-enabled transcription, summarization, and LLMs from their government laptops.AI even helps them draft responses to citizen queries.It is important for ecosystems to have the practical, technical capabilitiesto develop and regulate AI,rather than just talk at the level of abstract principles.Hence, we also encourage the community of more technically inclined government officialsto develop their own AI products and tools.We also create avenues for them to share their learnings.For example, as I mentioned at this morning's panel,our government agencies developed the AI Verified Minimum Viable Product in 2022,which provides a practical wayfor developers to demonstrate that their AI systemsmeasure up to internationally recognized governance principles.Last month, we also launched Project Moonshot,a challenge to ourselves to extend the AI Verified Toolkitfrom traditional AI to generative AI.To ensure a pipeline of new tools and new methods to regulate AI,we also actively support research in areas such as digital trust,online safety, and responsible AI.Researchers work together with government officialson research that is inspired by real needsand see the fruits of their laborimmediately translated into applications.Our researchers find this very fulfilling.But capacity building extends beyond tools for policymakers and developers.For government tools,because government tools,while they may be effective,they are not a silver bullet.To truly reduce the harmful effectsof AI,we need to develop a populationthat are confident and discerning users of AI.This enables them to engage in the digital environment,raises their competencies,skillsets,and employability.Empowering our citizens and businessesto reap the benefits of AIis a key pillar of our national AI strategy,which we updated last year.Uplifting their professional competenciesis done through broad-basedand sector-specific skills upgrading,often in close partnershipswith industry partners,as well as pre-employmentand company-led training efforts.Additionally,we are also helping all citizensincrease their awarenessand familiarity of AIthrough measures such as community roadshows,including in partnershipwith our public library networkand mass media campaigns.Finally,we need to be willing to cooperate internationally.AI is produced in a global supply chainfrom the production of chips,the training of data,the consumption of models,and the development of applications.And users come from all around the world.We live in a borderless digital world.What happens in one countrywill affect developments everywhere.And AI is too complexand evolves too quicklyfor any one company,one country,or institutionto have a monopoly of wisdomon research,regulation of AI.And we all mutually benefitwhen we collaborate on solutionsand share experiences and approaches.Furthermore,a fragmentation of AI governanceand security frameworksraises compliance costs for businessesand slows down useful AI adoption.Plus,while it is naturalthat every countryhas slightly different perspectivesand positions on AI,we need to work hardto overcome our differencesand find common ground.Yesterday,I spoke about how countriescan work togetherat the Global AI GovernanceForum Ministerial Roundtable.Particularly,we need to come togetherto find common problemsworth solving.We need to work bilaterally,visionally,multilaterallyto facilitate normsand encourage the creationof global,interoperable standardsand common toolsfor AI governance.At this morning's panel,I also shared on howwe open-source AI verifyand launch the AI Verify Foundationas a vehicleto tap on the globalopen-source communityto crowd in expertiseand capabilities.Let me conclude.The challenge of AI safetyand governancewill continue to evolve,but we must sustainour engagementwith the hard technical questions,with the policy dilemmas,and with each other.If we collectively adoptthe very human traitsof a spirit of humility,a sense of perspective,a desire to increaseour capabilitiesand our willingnessto collaborate,I am confidentthat we can achievethe right balanceof governingthis particular technologyand collectively harness AIto serve the public good.Thank you.Thank you so much, Dr. Hefor your inspiring recommendations.I'm pleased to now welcomethe next speaker,Professor Zhang Linghanfrom theYuhachina Universityof Political Science and Law.She has extensive experienceadvising Chinese legislationon algorithm regulation,platform governance,data security,and AI.She is currentlya member of severaladvisory committees,including the ICT Committeeof the Ministry of Industryand Information Technology,and the Cyber SecurityLegal Advisory Committeeof the Ministry ofInformation Technologyand the Cyber SecurityLegal Advisory Committeeof the Ministry ofInformation Technologyof the Ministry ofInformation Technologyand the Cyber SecurityLegal Advisory Committeeof the Ministry ofPublic Security.Professor Zhangis also a memberof the UN High LevelAdvisory Body on AI.Professor ZhangI'll hand it over to you.大家好非常开心今天能用中文跟大家介绍我最近新写的一篇论文那么题目是基于风险到基于价值探索中国人工智能的人工智能治理的方案那么在这个安全的议题上去讨论人工智能实际上我们都有这样的问题就是安全治理的风险治理的框架是否是人工智能治理的最佳方案我们目前所有风险治理的手段和措施是否能够完全应对人工智能给社会带来的影响那么当我们去追求安全的时候多安全才算真的安全那么首先向大家介绍我的观点那么我首先认为人工智能是一个非常重要的因素因为人工智能是一个非常重要的因素在风险治理的过程当中不管是风险的识别还是风险的应对都有一些无法应对人工智能给社会带来的全方位多维度和颠覆性的影响风险的治理理念和治理手段我们仍然应该坚持但是我们在风险治理之上应该超越风险治理采取基于价值的治理框架那么也请大家多多批评指正谢谢",
    "那么下一张图呢是我们可以看到目前基于风险的治理呢已经成为全球人工智能治理的共同主题那么在这个图上面呢列到了不光是很多国际组织都把风险治理作为了不管他们的宣言还是指导意见的这样一个基础的逻辑框架同时也可以看到基于风险的治理也被世界很多立法所采纳那么下面带来的问题就是由于大家都是专注在风险的治理和风险的治理之间我们可以看到在风险的治理和风险的治理之间在这个区块我们可以看到剩下的风险在这个区块我们可以看到风险的治理和风险的治理这两个区块就代表了这是我们的系统分析度那么我们可以看到这个公共系统就是在这边我们可以看到我们可以看到这个区块是我们的学校的体制性设施和我们可以看到这个区块就是在我们面前这个区块就是我们的科学区块这个区块根据它那么在欧盟的AI Act当中呢,把风险按照影响的范围分为不可接受的以及高中低的风险。我们可以看到美国商务部的这个国家标准研究院颁布的风险呢,则是根据风险的来源和成因。那么在实际上应对人工智能的风险当中,我们可以看到很多存在的问题。我们就以美国NIST发布的这个风险指南作为报告。第一个,就是大家都知道在风险的判断当中其实蕴含了很多的价值考量,也就是说我们说风险它并不是一个纯粹的科学概念,而是一个规范性的概念。我们看到,比如说NIST的这个框架当中,把风险分为技术性风险和技术社会风险,但是大家可能没有注意到的是,把哪些风险归类为技术风险,本身就蕴含了价值的判断。那么其次,风险分类一个很重要的问题是,在我们看到的前面这张PPT当中,大部分的风险分类都把隐私侵害、歧视等等这一类对人的权利的侵害当作是人工智能的风险。然而,我们要看到的是,这一类的风险并不像我们一直熟悉的风险的治理,比如说汽车发生事故的概率,比如说食品不安全的概率一样,比较容易量化和计算。这一类的风险是非常难以去量化的。我们对于风险治理的基本思路是,OK,我要算一下你的损害的大小以及损害发生的概率,最后看看我得到的收益跟它能不能成正比,进而采取措施。可是如果我们人工时代面临的很多风险都是难以量化的风险的话,那么这种治理方式可能就面临着困难。那么第三个呢,是我们再回来看这张PPT,可以看到大部分的风险分类识别当中都把失业问题当成是人工智能的重要的风险,那么以及把没有办法给现行的人工智能服务提供者去追责当成是人工智能的风险,可是我们知道风险的一个重要特征就是不确定性。这一类我们刚才说到的影响真的是不确定的吗?人工智能必然会带来大规模的失业和劳动替代,也必然不能适应传统的法律框架,与其说它是一种风险,我更愿意认为它是一个社会影响,社会变革带来的必然影响。那么更重要的是,刚才很多专家也提到了,人工智能的风险不同于以往任何一种风险,其重要原因就是在于大家都说不出来它未来的风险具体是什么。这种uncertainty和unpredictable,大家都是在反复的提及。那么可能我们提到的人工智能的这种自我复制与自我完善的能力,就是我们将来没有遇见到或者从来没有处理过的风险,但是我们目前并不知道它什么时候会到来。以及我们可以看到,人工智能这种强大的技术通过开源可以被广泛和容易地获得,这也使得风险的来源大大的变化。那么第二个问题就是,我们可以看到风险治理的措施,有事前,事中和事后,分别是风险预防,风险缓解与事后消除。如果我们列出几种典型的风险治理措施,一般是事前我要对风险进行评估,进而把它进行分类分级,采取和风险程度相适应的治理措施。那么这些风险应对措施和人工智能治理的目标,手段其实也存在着错位。首先我们来看人工智能治理的理念,总体来说风险治理的理念其实是有一个修正主义的内核。所谓的修正主义是说,原来的风险治理是说,如果这个技术存在缺陷和障碍,在它没有得到完全修复并且是可信任之前,是不应该直接应用于实践活动的。我们可以这样去处理。我们可以这样去处理人工智能治理和扩散带来的风险,这样去处理大流行病带来的风险,但是我们没有办法把它和人工智能的风险相提并论并且这样处理。人工智能技术的应用已经成为必然趋势,那么这种趋势是谁都不可以避免的。我们没有办法说在完全消除了或者确信人工智能没有风险之后才来继续使用。那么从人工智能治理来说,人工智能的风险是什么?人工智能治理的工具的层面,那么我们都知道风险治理当中最重要的就是事先的评估和预测,可是我们有个重要的问题就是当我们没有人工智能应用和技术足够深入的在社会当中广泛使用,我们也就没有办法去确切的了解哪些风险将会产生并且具体的程度是什么样。更遑论我们还有很多我们认为不可预见的风险。那么如果是这样的话,我们又如何去进行事先的识别评估和监测体系呢?那么相信呢,一部分不能预测的风险是不能落到这样的治理工具的治理范围内。那么另外还有就是人工智能治理的前提。如果说人工智能治理的很多风险没有办法被量化的话,就没有办法被纳入到成本收益的计算当中。我们举个简单的例子,很多隐私的这种数据泄露的赔偿,总体来说数量很高,侵权人难以承受,但是受害者个体拿到的侵权费用可能只有几美分。那么这种难以被个体救济的风险实际上也使得成本收益更加困难。那么在这里我想用剩下的时间简单介绍一下,我认为在中国其实已经逐步发展出了修正人工智能风险治理的这样一个路径,并且随着时间的推移,中国的人工智能治理正在本土化。我们可以把它分为探索阶段,定向阶段和系统结成阶段。探索阶段呢,我个人认为是从15年一直到2022年这几年的时间,我们发展出了很多的人工智能中国本土的治理手段。比如说在风险认知层面,实际上我们是有很多的共性部分,从2015年16年开始一直到2020年之间,我们有新一代人工智能伦理规范,算法推荐管理规定,包括个人信息保护法,实际上都采取了风险分级分类和防控的这样一个路线。在治理手段上呢,和国际上对接的一些共性的部分,包括个人信息影响保护,个人信息保护影响评估,以及算法影响评估。但是与此同时,我们也发展出来一些中国自己的特色部分,比如说在前两个阶段,我们始终在国内的人工智能相关的立法当中,都把发展与安全作为最重要的平衡的一种价值观。同时,我们始终秉持着中国所特有的国家总体安全观的理念,去进行人工智能的安全治理。那么即使在网络信息领域这样一个比较特殊的领域,我们也采取了网络信息生态安全的这样一个概念。同时,在算法治理和深度合成治理的过程当中,我们有一个美好的结果,一个明确的价值排序是信息内容安全,消费者权益保护,以及市场竞争秩序。那么在治理手段方面,其实中国也有很多自己的特色,比如说初步探索相关的算法备案。那么尤其在这里提醒大家要注意的是,我们并不是一个准入或者认证的制度,而是一个信息备案和采集的制度。那么同时大家如果感兴趣可以关注到,在2021年的算法推荐管理规定当中,非常有特色的提出了未成年人的防沉迷制度,老年人和劳动者的权力保护制度,还有未成年人的相关的宵禁制度。那么这些在世界上都是非常少有的,也是独特的。那么目前我个人认为,我们国内的人工智能治理体系和理念正在逐步的形成,并且已经逐步超越了风险治理的理念。首先在风险的认知上,我们实际上一直在跟国际保持同步。大家可以关注到我们国内的相关技术标准当中,对于风险的分类,比如说失控性风险、社会性风险、侵权性风险、歧视性风险、责任性风险等等,和国际的很多相关分类是完全可以对接的。包括一些相关的治理手段,事前评估、认证和事后追责,我们也吸取了国外的先进经验。但是我们可以日益看到,在近两年生成式人工智能迅速生长以来,我们可以看到很多中国特有的价值理念和治理手段。那么我们对于人工智能治理的很独特的价值理念,我觉得最重要的这两年就是不发展,就是最大的不安全。我们认为最大的风险可能就是中国的人工智能产业和技术没有得到有效的发展。第二个就是我们可以看到,我们不管是全球人工智能治理倡议,还是在昨天,我们的世界人工智能大会中提到的中国的这种治理方案,我们都强调要尊重各国本土的价值观和发展阶段的需求,以及要尊重各国的文化。那么在治理手段层面,我们可以看到去年的生成式人工智能暂行管理办法出台之后,中国已经形成了对于生成式人工智能分层治理的这样一个基本的理念。同时,以发展为导向,中国也开展了很多人工智能基础设施建设的相关工作,比如说在人工智能的数据要素层面,我们可以看到国家数据局做的大量工作,还有工信部做的大量有关算力基础设施的建设工作。所以,我们目前在中国的这种人工智能治理理念,我们与其说它是一个完全基于风险的治理理念,不如说是一个基于价值的治理理念。基于价值的治理理念,基于价值的人工智能治理并不排斥风险治理,但它超越于风险治理。一方面,我们可以看到在中国的立法和治理政策当中,不仅仅把人工智能理解为一种技术或者服务应用,人工智能既是未来赋能整个社会的基础设施,也是未来整个社会生产的组织形式。那么我们也可以把人工智能放到心智生产力的角度去理解,另外一方面,中国已经越来越明确中国人工智能治理的理念和方案是以人为本,智能向善。以人为本是在说技术不能偏离人类文明进步的方向,智能向善是在强调人工智能必须在法律伦理和人道主义的层面的价值取向。那么在治理手段方面,我们可以看到目前一个系统发力的情况,在今年5月份,全国人大常委会和国务院,已经把人工智能立法放到了相关的立法计划当中。同时我们也有日常生活,日常监管活动当中,更为丰富的监管措施。那么在我们相关的一些立法和监管活动当中,我们可以看到中国人工智能的安全框架也在积极的讨论和预养当中。那么包括在前天也刚刚颁布了一个国家强制技术标准,生成是人工智能,内容的标识的这样一个技术标准。那么相信呢,基于价值的人工智能治理体系正在逐步构建的过程当中,我个人更愿意把它分为三个层次。第一个是是如此,就是我们去观察人工智能的本体价值。那么在其中有几层含义。首先,我们希望现在在风险治理当中,这样一个泛化的模糊的概念被逐渐分离开,哪些是人工智能的必然影响,哪些属于近期的维度,哪些是人工智能不确定的影响。那么其次,我们在属性层面要对人工智能有一个判断,它究竟,它在这个人工智能被提及的时候,是以一个技术的方式被提及,还是服务应用,是社会生产的基础设施,还是社会生产的组织方式,这都决定了不同层面的影响的发生是必然的还是具有不确定的。那么第二个层面是,就是基于中国本土价值观,去判断人工智能治理的短期和长期的目标,梳理中国的个性化的治理需求。那么我们特别希望能够分解出人工智能风险当中的规范性的层次,把个体化的价值标明列明。那么对于中国来说,我们首先在技术层面是要安全的发展,在服务应用上,在服务应用层面,延续中国一直以来对于服务应用的治理体系。那么在基础设施层面,我们可以看到,国家各种加大力度措施去促进基础设施建设。那么在社会生产组织方式层面,我们也在强调绿色环保,强调新制生产力的治理方式。那么在应如此的层面,也就是在基于个性化治理之外,我们始终有人类共同核心价值观,和人类命运共同体。那么这也是,我们在参与全球人工智能治理工作当中,中国提出的方案。那么也是我在联合国参与的,我们可以看到,联合国提出的AI Governance for Humanity,尊重整个人类的价值。那么具体的手段措施呢,我们其实在今年3月份的人工智能法学者建议稿当中,有比较详细的解释。由于时间原因呢,我就先讲到这里,感谢大家,谢谢。",
    "谢谢大家。谢谢大家。",
    "谢谢你,Professor Zhang,这么清晰的,和适当的,与AI合作,负面的解决方案 进行的,如此的明晰的和适当的展示。接下来,我们会有Dr. Mark Knitsberg,他将将他的解释与我们分享。Dr. Knitsberg目前是在UC Berkeley的Human Compatible AI Center的经纪人主事人。以及巴克里AI研究的主席在企业上,他建立了科技企业使用AI在健康、财务、教育和发展支援上他在Bell博物馆、Microsoft和Amazon工作并在企业和学术上发展广泛的计划马克,很高兴再次见到您在中国这张图片都是您的谢谢你邀请我",
    "好的我首先是一名电脑科学家我受邀请给您介绍美国的AI制度方式所以我请您脚下坐带因为这是我第一次在美国的AI制度方式上接受这种课程的第一次我一直记得我们现时的情境和智能智能的极端层面它是最大、最有能力、全面目标的电脑系统我们将它发展到最大的智能智能系统我们将它发展到最大的智能智能系统这是世界第一个通能链的最大型工业而且这是再次用的最大的业界。。。而至少在美国AI并没有任何制度这实际上是被运行到几乎每个人的活动中的一方和每个政策中的一方在每个领域都会发生这种情况。所以,它是很难去制定的,尤其是因为系统的黑箱性,尤其是因为它是一种普遍的技术,而经常被误解。那么,是否有美国对AI制定的方式?我认为,我们与许多国家共同共享有主要的目标。在美国,有可能有特别的目标,就是推广美国AI领域的增长。我们正在寻找美国主导,有AI的主导。我们正在寻找美国经济和社会社会的利益,和国际安全。我们也希望保护美国的严重影响和结果。我们也分享了我们的全球目标。例如,这些目标被表达在许多原则中。例如,AI的OECD原则。我认为,我只是用了这个OECD原则的一部分,1.4,这就是关于安全的目标。所以,目前美国的方式和制定的心理风格被捕。目前的过程中,有些组织正在推广竞争的利益,并发生了三种可能的法律。第一个是从去年10月的行政命令。这个行政命令主要目标是大型模型,指导现有的国家公司。第二个是从去年10月的行政命令。这是AI的承诺,对职业、法律、防御,以及违法的影响。第三个是在加利福尼亚,我们的某些规则是来自美国的。有一个法案,叫SP-1047,这个法案是关于安全和安全的创新,与前线模型的。现在,这些问题有些问题,讨论继续,但这并不是法律。有些国家有某些法律,也有某些具有特定的法律,但是这些大型公司,这些大型的框架,并没有那么长时间。然后,我们在过去几天的时间中听到,当您在使用全面目标技术时,有些挑战性,例如,它很难证明,一个系统有接受性的低风险,如果是全面目标。您实际上需要知道,您正在测试什么。美国的法律系统的一部分,就是在各个领域中,有很多存在的法律。因此,这些对于在这些领域中使用AI的系统使用的系统,例如,健康、直轮、農业与贸易,一些需要更改。例如,车车的车车有很少的法律,但在很多情况下,正是有实际的国际法律,给了很好的基础。然后,有些法律,涉及正义的法律,正确的分配费用和利益,例如在职业和借费值的分配方面。另外,在美国,我们经常遇到一些批评,你可能听过,这可能是一个批评在创造法律的状态中,其实是有效的。例如,在智能货币的情况下,有关某些训练数据的用途,例如用途类似性的用途,或是创造职业的方式的用途,这些创造职业的方式,通过批评,并在生产保护上的法律上再次通过批评。还有其他因素,包括单纯的保险能力。如果您在卖AI系统的系统,您想得到保险,您需要证明,有一定的保险能力,有一定的保险能力和可靠性能力。还有其他制度来源的车辆,例如,美国的电脑服务法,这些车辆在美国的公司上受到影响。我希望请您在几分钟之内谈谈这些误判。我会提醒您,我并不是代表我的国家,我自己在说话,但我认为有一个误判,就是制度是一个重要的阻塞,是一个重要的阻塞,是一个重要的阻塞。我认为,如果我们正在谈判保险,那是一个必须的指导。这在很多其他技术上是这样的。我认为这没有什么区别。我认为,经常说,能力,是,是,但是在杀伤中,我认为,当你看到,无论是什么系统,也要更加有能力,这并不 necessarily equate与医学状态的立策,而我认为,我认为有时有些混乱在我们所称为的红线之中,以及提到要求发展者停止发展更强大的系统。这并不是一个红线,而是一个相反的概念。我认为我们可以学习从现有的国际安全组织,例如风险安全、核武信息、电信组织等国际安全组织。他们的工作方式是,他们找到共同的基础,我们合作,我们能合作,我们能合作,我们能合作,我们能合作。如果我们不合适,我们要么寻找某种正常化的协议,或者直接将它转移到那些领域。我认为这可以合作国际安全组织。看来我现在有时间给你们一点技术观点,从我们的中心来看。我来自人类合作AI中心,我和他们一起工作,我想给你们看一看,我们认为AI是正确的方法,对安全AI系统来说。我们现在正在做,你今天也听到很多人说,要把AI保守。有很多技术,有很多技术和方法,例如LHF,和法律性AI,你有一种AI,看另一个人的出口,测试,测试,测试,红图组织,和我们听到的,digital neuroscience,和,和,和,和,和,就是英文性AI,десятомualoning上的AI numeralist和核准实际غ运运设施,这个我认为已经开始,是的,这些方式!都是讨论命运以鼓起不会了解的系统,如果我们想用它们作为 protocol,我们需要更多的光芒~~使我们能为机 grolAIS安全,obe in an example,包括语言模式!但在我們中心的長期化中,我們正在努力製造安全智慧技術。為了這樣做,我們認為是合理的回歸透明、解釋得來、解釋得來的方式。為了這樣做,我們認為是合理的回歸透明、解釋得來、解釋得來、解釋得來的方式。這就是我們從中心的觀點。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。謝謝您的訪問。最近200年来科技的发展对于人类社会的进步产生了非常巨大的影响我认为19世纪的内燃机加电气技术使我们人类有了生活的自由那么20世纪互联网加通信技术使人类有了信息的自由21世纪大模型加脑机接口使我们有什么呢现在很难想象但是至少是有了创新的机油甚至是从无到有进行创新的这样一种可能性是展开了也就是说它可以赋能社会赋能人类会带来福利但是确实会引起那样的安全上的问题我们可以看看到大模型出来之后至少是有四个方面的问题因为大模型大量的使用数据那它可能会引起隐私方面的忧虑那么另外我们可以看到大模型由于它的能力泛滑造成了幻觉现象可能会引起虚假信息以及诱发各种各样的犯罪那么另外一个在知识产权问题上它也可能会引发这样一样的复杂的问题还有一个它可能会使得整个社会治理的中枢机关出现漏洞那么最后会导致信息社会发生功能障碍那这些的都让我们感觉到不安那问题是我们如何处理这样的问题当然我前面讲到了科技给人类社会带来了巨大的福利那么这个就是如何在这两者之间进行平衡那我们可以看到在亚洲在非洲这些国家对他们来说发展是一个更突出的问题他们希望能够实现一种互惠的科技的发展那么当然我们知道像美国日本还有中国非常强调在发展和安全之间寻求平衡那么另外一方面我们可以看到欧盟最近通过的人工智能法提出来了安全高于发展这样一种价值取向那如何在这个中间找到适当的平衡那我们可以看到各个国家有不同的立法模式那么我想这个是值得我们进一步探讨的问题谢谢分享",
    "接下来想请教王老师您作为研发机构的代表我们知道上海人工智能实验室也做了很多安全对其品质的工作包括昨天周主任也提出了一个安全发展项目就是人工智能发展的45度平衡率想请您来解读一下您认为的安全和发展之间的关系好的其实昨天周博文教授在社认大会的这个上午的大会他在会议上他分享了我们实验室对这个问题的一个很重要的一个判断他称之为一个技术体系的一个思想就是人工智能的45度平衡率现在整个人工智能发展我们认为是跛脚的无论是从它的大部分的技术和算力资源都投在它的性能的研发上虽然我们有这个IIS这些既兼顾性能和安全的技术但总体上还是偏重于这个性能的就导致我们实际上从技术社区来看投入到安全方面的资源 人力和算力都是严重不足的而且我们的技术方法目前还是后续的很分散我们是希望能够找到一个能够安全优先同时能够兼顾性能增长的这样一条技术的路径当然这个技术路径的探索是非常复杂的也非常艰辛的我们也是周教授发出这样一个呼吁希望能够我们沿着这条45度的线去推进人工智能的发展当然这个过程当中其实如果这条路径能够走通的话我相信目前人工智能的安全风险面临着很多挑战它从基础上可以得到很重要的一个保障我们也在做这方面的努力所以我想强调的是其实我们需要有一个大家共识性的这样一个发展路线的框架我们不能长期低于45度来发展但如果长期高于45度也很难市场商业化的要求但是我们是不是有这样一个理想的路线大家共同来努力这个路径我觉得还是很大可能能够实现的谢谢",
    "而且可能需要动态调整去灵活应变接下来我们可能想聊一下中国人工智能的一些治理挑战和独特的机遇Gail我们可能想聊一下最近其实中法的人工智能治理声明之后其实中国国内对法国的情况其实也都非常感兴趣您能介绍一下在法国的人工智能有哪些独特的机遇和挑战吗以及可能我们也是到2025年初会有一个AI行动峰会您能谈一下您对它的期待吗非常感兴趣是的我可以谈一下法国的具体性如果我们回过头来法国的具体性可能会比其他国家不同我看到的一件事在法国不同的是我们目前有一个相当困难的社会和经济讨论讨论在整个国家所以在社会层层上有很大的缺乏信任无论是与工作人员和决定者或是在城市之间或是在附近的区域这也可能会有其他国家的影响但是在法国这也有很大的缺乏信任另一个形成的讨论是经济历史上约翰是一个比较缺乏的数字经济经济我们是数字服务的数字服务人员这形成了我们的关系与数字服务然而法国的AI能力非常强所以目前有一个改变在经济线上而那是在城市中带来很多希望和正面的激动现在在城市中这是个不一样的事情而在政策方面这种规划正在进行中这种规划在各种方向上第一个是投资所以有增加的投资不仅是在数字服务中也有增加的在数字服务中我们看到的是开放的和公共的价值我们看到的有增加的公共业务和数字服务与公共业务与数字服务而另一个挑战是与每个人在不同层次的社会之间链接而这里我们看到的角度是维持人类联系而不是只有数字联系所以这就是我们的挑战要避免数字区别我们有数字区别我认为每个国家都有数字区别我们在那里投资来执行讨论这就是我们在法国的这种规划谢谢",
    "谢谢数字区别我觉得是最重要的张领和老师我们想请教一下我们知道您今天起草发布了一个人工智能法的学者建议稿然后刚才在演讲中也提到了中国风险治理的修正治理本土化您能在整体上谈一下您认为的中国人工智能立法的整体逻辑和整体架构吗谢谢",
    "回答这个问题呢其实我觉得可以从两个角度来考虑中国的人工智能立法第一个层次我们要把它理解成是中国的制度名片在全球共同去认为应该处理人工智能风险的时候中国作为一个人工智能技术相对领先的国家作为一个大国如何去打造一个我是一个负责任大国的这样一个形象同时我们也可以看到实际上在人工智能治理的工作当中中国已经做了很多很多中国是世界上唯一一个对于人工智能安全治理覆盖了国家规划法律行政法规部门规章技术标准的全方位覆盖的国家也是目前一个对于大模型的治理已经实际落地的国家但是我们在人工智能安全治理方面所做的努力并不为世界所知道其中一个重要原因就是我们缺乏一部高位阶的法律去作为中国的制度名片让大家知道中国曾经做了中国正在做那些事情或者已经做了那些事情那么在联合国的工作组里我们大概一共有30多位专家其中我在这个组里有一部分的工作就是要告诉大家我们中国已经做了什么事情让我很惊讶的是作为国际上比较集中的对人工智能治理很了解的这些专家们其实也有大部分并不太了解中国在产业治理当中做了什么实际的事情我认为这是人工智能立法的第一个最重要的目标第二个就是要符合中国本土的治理需求我非常愿意把中国的在世界上的位置定义为领先的追赶者领先的追赶者意味着什么一方面你作为一个大国要去治理好人工智能安全不能让中国成为人工智能治理的洼地但是另外一方面我们也要充分认识到中国人工智能的技术和产业发展需要大量的法律法规的调整去为人工智能技术产业的发展提供要素提供资源同时去为人工智能产业设置一个合理的法律责任的框架那么在这两个理念的指导之下我相信不管是什么样的制度经过我们充分的讨论希望将来能够形成既作为中国制度名片成为中国在全球人工智能治理当中重要形象代表的一部人工智能法同时这部人工智能法也能有效的防范人工智能安全的风险促进人工智能安全并且符合中国本土技术和产业发展的需求谢谢",
    "谢谢中国学者很多工作需要更好地去对外传播其实我们也做了很多帮助中国的不管是安全研究治理还是立法工作对外传播工作希望能更好地展示中国的工作接下来我可能想聊一下最近比较热的人工智能安全研究所这个事儿然后自去年英国AI安全峰会以来其实已经有大概美国英国法国在内十个国家还有欧盟都成立了自己的国家级的人工智能研究所包括一个全球性的研究网络这个我可能想请教Mark你会怎么看待这样一个人工智能安全研究所你觉得它它们为什么要成立它们要做什么工作以及未来会做些什么好的问题谢谢",
    "我只想确信我将将回答中国在进行AI管理的信息所以我们在中心我工作的地方加入了美国国际AI安全系统是你在说的吗国际是的对我说中国在美国是一家设计人不仅仅是研究这些强力系统安全行动的问题但是也成为了一种监察标记,以查看严重的事件,并且在某种意义上,在国家为主要的方式中,确认什么应该被测试。我曾经参与一个讨论中的讨论中,我们在一起的时候,我们在一起的时候,我们基本上是谈论是否是必需的问题。我相信 it makes perfect sense,but it is not as a necessity that I believe,At this the USAI safety institute is you know as a worthwhile endeavor.谢谢。",
    "我们又希望不同的国家的AI-安全之间有更多和个,包括在中国的机构之间。我们这个论坛是前AI安全之旅,所以一个重要的议题也是讨论怎么去减轻所谓的AI潜在的战略性风险。我想还是请问季老师,我们知道这个AI战略性风险其实目前有很多不确定性,包括它的严重性,它的可能性,它的紧迫性层面,大家都有不同的理解。那么在存在这种不确定性和不同的没有共识的情况下,您觉得怎么能推进这方面的政策和治理呢?季老师,OpenAI提出来了一种非常有影响力的方法,是价值对齐,包括人工智能和人类的价值对齐,以及国际的价值对齐。刚才张林涵教授在主持演讲中也提到了价值,以价值为基础的这样一种人工智能治理问题。我的观点稍微有一点不一样,因为价值对齐是一个很复杂的问题。我认为沟通。沟通程序更重要。那为什么这种方法更重要呢?首先我们知道当我们谈人工智能安全的时候,我们其实需要有两个视角,一个是监管者的视角,我们中国采取的国家备案制以及验证技术,对吧?那另外一个是用户的视角,我们要强调可解释性,要跟用户之间进行反馈。那这个过程我们觉得就程序性的过程,沟通的过程。非常重要。那在理论上我们可以看到,就是一个是技术性程序公正概念的提出,那另外一个是一批中国学者今年年初在自然杂志的子刊中提出来的仪式性的对话框架。那强调在这种一种对话的氛围中,可能对人工智能的可信性是具有非常重要的意义的。那另外一个呢,就是说在这个过程中呢,我们实际上看到就是要把这种程序公正监控嵌入到人工智能系统中去,或者让人工智能系统之间产生制衡的作用。那这个呢,我认为就是技术性的这个程序性的这个安全监管的一个非常重要的内容,在这方面其实已经有些很好的先例,一个是新加坡。那个何先生刚才也提到了这个新加坡的经验,新加坡的AI Verify是一个很好的示范,当然它具有普遍意义,像美国的这个Watson X Governance是一个监管模型。那么日本就在上个月,6月4号发布了综合创新战略,这个中间呢也特别提到了它有三个基本方针。那么在人工智能这个方面呢,强调安全与竞争力这样的一个平衡,也强调技术性的侧面。那如果我们把这些因素放在一起的话,那就是说通过科技公司的技术能力的提升,使得人工智能的技术能力提升,来加强它的安全保障,这个思路就成为可能。如果我们从这个角度来看的话,那么今年3月份,北京人工智能安全共识提出来的三分之一的研发预算投入到安全保障领域,这就是可以接受的了,可以理解了,科技公司就认为这是可行的了。如果这样的话,我们就用人工智能的治理,可能达成更广泛的共识,而这个共识呢恰恰是立法的基础。那王老师您会如何回应刚才季老师的?对,我对这个,这个沿着这个季老师讲的话,就是我们提出一个这个这个,概念吧,也是和薛兰老师这个我们清华大学和焦大一起研究的,我们希望提出一个概念,就是人工智能是全球公共体,就是AI safety as global public good,是什么概念呢?其实我们看到现在全球的这个对人工智能的安全风险的讨论,往往是把AI安全作为一种风险去监管的,这是风险管理的视角,我们还有其实可以补充到另外一个维度就是,刚才季老师提到了,我们更应该,现在做的是,去把人工智能安全作为一种公共的产品,政府,企业,第三方,公众,一起来共同建设,共建,共知共享,来提升这个人工智能安全的相关的知识,能力和资源的共享,这个是非常非常关键的,大家其实对现在人工智能安全有很多的担忧,这个担忧的前提就是,大家对,无论是政府还是企业,还有这个,其实大家对人工智能安全风险的知识是不够的,在当这个过程中需要大家补充充分的讨论,刚才季老师也提到了,包括科学的研究,所以我们这个,关于人工智能安全的知识的形成过程,需要大家共同参与的,这本身是一个需要加大供给的这样一个安全的维度,另外就是人工智能安全的能力,我们是否有足够的技术手段,和相关的工作去支撑人工智能安全的提升,再一个就是,我们是否有足够的人工智能安全方面的资源,的投入,或者是公务服务的产品的开发,这个是一个很重要的问题,这个是非常非常关键的,比如说,去上海我们也来做相关的工作,我们经常举办一些沙龙,各种各样的形式,其实是用柔性的方式,把不同的主体来共享它的知识,共享它的对风险的认识,达成一些共识,我们共同去推进一些安全的基准,标准的建设,包括资源库的建设,包括治理的平台的建设,我知道新加坡其实也做了很多很好的,类似的工作,我想这也不是就是说,上海和新加坡独有的工作,而是全球各方面多的做法,做这个努力,所以我们希望提出这个概念就是,AI safety as a global public good,全球都应该共同去建设这样的人工智能安全的公共体,特别是像美国,欧洲和我们中国,其实原有的人工智能发展基础比较好的国家,应该去合作,来加大对于全球人工智能安全的公共体的供给,有很多的发展中国家,可能他们原有的基础没有那么好,这个资源图,是比较好的国家,应该去共同建设,这个工作其实需要科学家社区的合作,需要企业智能合作,需要在不同的联合国,不同平台上去做,我觉得这个其实,如果我们形成这样一种共同认识的话,可能后面我们很多有意思的工作可以共同推进。非常遗憾,今天时间有限,所以可能意犹未尽,也感谢大家今天分享的交流,期待以后有更多的时间。谢谢大家的关注。",
    "谢谢我们的主持人,这次的讨论非常明显。我们现在将进入我们的国际协调的第四个和最后的课程,我欢迎我们的下一个主持人,Tino Quellar。Tino Quellar是全国协调的10位国家协调总统,是一个社区,执行协调,执行协调,并在国际协调、争议和执行协调上进行独立研究。他曾经是加拿大法院主席,他曾经在白宫和国际协议中担任三位美国总统,他曾经是史丹利·摩尔森教授,在史丹佛大学,他在法律、政治、科学和国际事务上担任,并在国际研究中带领Fremont-Fogler Institute。他担任国际协调的国家协调总统,并担任威廉和佛罗·毗奥勒基金会总统的总统。我相信现在是Tino的时间区,Tino,谢谢你这么多时间给我们留下来。",
    "让我们把手放在一起,让我们给他一个温暖的欢迎。Tino,到你了。谢谢你。谢谢,谢谢。",
    "我现在在墨西哥,但是我现在正在美国,但是我感到很高兴能与您一起,我绝对不会想要错过这个机会。这将确实成为美国的更重要议会,与美国的现代智能讨论讨论。现在我已经在几年之内成为了美国的总统,但是你可能知道,我已经在近15年之内进行了机构发展讨论论论。我想和您分享一个我们在AI安全的主题,我看到的,以及在世界上的AI安全讨论讨论论,它们在国际协调中,在中国的国防讨论讨论中,在中国的国防讨论讨论中,在中国的国防讨论讨论中,在我身边的几十年来,我的团队,卡尼基协会,帮助了NAP大使团队,在世界上的支持问题上,在持续和理想的平和中,今天,如你所知,新的机会和挑战,在国际健康和国际安全上,新的机会和挑战,新的机会和挑战,在这个变化中,我们的进步和我们的专业讨论论论中,我希望您记住这四个点,我希望我们能够进步讨论讨论讨论。首先,我们看到的进步是什么?我们看到的实际和实际和实际的变化。讨论的差异,不仅有程度的差异,并不仅是微稀的,而同时也有重要的。讨论的大幅升值,包括讨论的表现,很重要。如果您知道我们在社会上,要尝试讨论,确实,我们没有只在超过建筑物的水平,而是把整个讨论应变成很大的增长高度。在现实上,在 frontier level models上的设定上我们现在看到frontier models在2010年以来有10亿次的设定上比frontier models更加有设定结果的系统正在解决更复杂的问题创造更具体的词和画面chatbots和其他技术从小到快正在进行日常生活他们正在改变我们的工作我们学习和与世界联系Catalyze and Much of This Change是一个AI活动的系统但我认为它更加是政策主义主义关注政策而这个系统包括了新加坡纽约伦敦深圳上海北京部分加拿大以及其他地方这些产业正在努力推进AI创新和研发我对这些产业所考虑的有关答案也非常关注第二个问题我希望提出的是各个机会和风险都造成了这些发展的变化这些风险都造成了这些发展的变化所以安全应该是从认真的而从多种方式例如避免车轮和车轮的被破坏中级安全如保保复权或公共服务不失误此外还在管理或智能侵入或失控最高的AI系统这些计划确实没有那么紧急但这些计划不可能不适合线上的系统你也知道AI持有巨大的承诺承诺我们全世界遭遇的许多挑战简单和复杂的医疗任务并扩展中心服务世界大部分人口的4.5亿人正在缺乏想想复杂教育的目标250亿的学生正在全世界外出城市服务与执行两三成的世界都预期在2015年将在市区居住实现这些利益只能够实现如果我们也谈谈线上计划的风险这些计划可以创造大量内涵传输资讯减弱资源进行成功的资讯攻击如果在国内使用军队和国际安全的设施中进行高层的执行坦白说正如你们所知目前没有证据现时的计划可能会遭遇失控或是能够使用生物兵器但这些风险可能会在未来发生实际上在AI的前沿上在我认为这并不合理未来将会给我们一个能够在网上与其他AI系统和人类的关系互动人类的关系有时会与人类的社交运动相互启动或是与人类的社交运动在国内的决定和公司会更加受影响这些系统最终会将人类的决定与人类的决定的分别决定将会变得更加传统这些改变将会需要国际协调这就带我到第三点当我认为AI的决定在决定最高的系统在决定决定这些系统总是会有一些国际协调的变化但事实上这些协调会也被影响他对生物技术的改变和界纪和情况的反映21世纪美国和中国将双重的保持国际系统其中也会能更能给予全人类的增长能够帮助我们解决未来的问题和准备未来的问题这意味着我们要专注在AI的危险方面的机会并且提升AI的利益并且提升AI的安全因为两个目标必须相互联系是的,是真的竞争是一个主要的功能在判断人工智能的 domain并且不可能完全消失但我们也看到国际协调的显示包括中国和美国让我给你们一些例子进步在美国在2020年11月在美国的第一次AI安全会议上28个国家包括中国和美国签订了一份专业的国际科学计划在AI上最后的计划与AI的SOUL会议相关在2024年5月在SOUL上10个国家加上欧洲同意建立了国际安全研究所的联合联合研究所实际的承诺继续在这次参与参与这次展开的趋势是重要的在2025年在澳大利亚参与的参与的翌日在此我认为在Bletchley讨论中有所进步对美国的角力在Bletchley参与中总统布泰罗斯在Bletchley参与中开始思考和宣布一些现在变成了美国联合研究所的角力的想法在2023年10月总统布泰罗斯建立了AI的高层监督组织包括中国 美国和其他国家的参与这次参与者组织将在2024年将在明年参与AI的参与组织提供的最终建议将在2024年参与AI的参与组织提供的最终建议在2024年10月总统布泰罗斯实际上执行了美国主导的AI的高层监督组织而几天前总统布泰罗斯执行了一个非常类似的设计支持中国来自美国我认为这些计划非常鼓励人这些计划在国际上和这些AI参与组织的程度上能够和美国的参与组织相似第四国际协调包括在这些区域中在我刚才介绍的计划中包括更多的国际一些我给给的例子包括国际协调的高层监督组织描述外部政府有能够帮助AI安全向前移动的演员例如最近发布的研究报告关于AI安全计划许多国际政府同意了这个计划而美国正在进行这个计划也许与外部演员合作陆续研究还有其他专业订出了这个计划随着儿童提出的计划并进行审查计划是将会创造AI安全的一个最强化的基础这将帮助晓得世界和AI的讨论发展讨论一些的政权我希望国际这些项目当然,有时候会有困难,为了鼓励维持协调和管理AI的风险和利益,重要国家如中国和美国都同意和不同在某些问题上讨论。这是理解的,根据国际安全批准和主导AI的国际发展。但这种现实并不代表有定义的目标。通过协调,在AI的风险被理解和批准时,利益大幅增长,国家能够成功讨论和提供负责任的AI管理。中国与美国都对AI的科学进展有兴趣,以提升安全的过程,以共享正确的测试和测试,在无法实现的军事系统上。并以监视和准备未来出现的坏坏风险,但未来可能和可能会发生的,例如AI系统的失控。对我来说,我认为,这次的聚会能够提供一个机会,提高计划在这些问题上,并使政府在任何情况下合作。在AI上,互联网与其他挑战中,互联网与其他挑战中,互联网与其他挑战中,这是一个困难的时刻。但这也意味着,互联网与其他挑战中,互联网与其他挑战中,更加需要更多的互联网与其他挑战。而这要求我们承担谈论和合作,承担谈论和合作,尽快,专心地,尽快,尽快,因为这将使世界成为AI的更好的位置,更加尽快,因为这将使世界成为AI的更好的位置,为人生而提供的所有能力。因此,我期待与邢学和邢学会议讨论这些问题,我期待与邢学会议讨论这些问题,以及更多的讨论这个有助与重要的讨论。非常感谢您。谢谢你,",
    "邢学和邢学,为您的承诺与国际协谋。请您留在网上,请您留在网上,并在我们一下子后还边边插来电话。觉得 Möglich to introduce 邢学兰。邢学兰穿过 Chair distinguished professor和是中国国际精华交通协议的前川知事,实任调 transcription ofIinternational Governance InstituteDean of Schwarzman College.China Institute of Science and Technology PolicyChina Institute of Science and Technology Policy与SDGs Global Institute Co-Director Agree toCurrently,He also serves as a counselor at the State Councilas a counselor of the State CouncilPendant of both Creative and IntegrityChair of China's National Expert Committee on New Generation AI Governanceand a member of the Standing Committee of the Chinese Association of Science and TechnologyDin Xue, it's really an honor to have you with us today, the floor is yoursThank you, actually, okay, I think President Quilliar and thank you very much for staying so late to join this dialogueand I have to say that actually I did this PowerPoint just while I was there listeningI was burned out yesterday by thebig screen and not necessarily the most comfortable way of using PPTso I was actually, I decided not to use it initiallybut then I actually saw the wonderful presentations you madeand I realized that some of the graphs actually it's better to be shownso let me tryOkay, I think they basically, I think the role of governanceand safety and so on has really, I think it's amazingI think it's amazingthat it's getting, you know, recognizedand we've seen the recent effortsof course, it's already been, you know, UK Safety Summit and Koreathis one, this year, EU safety lawall in the last, you know, 12 monthsso I think this is really strongI think it's a wonderful sign to that we are all paying attention to thisbut of course, partly this is really based in indeedI think there's a huge recognition about the AI risksof course, I think there could be, you knowvery specific onebut also there was also the concern about the autonomous AI systemthat can really, you know, be getting out of controlI think so because of thisI think that in, I think that of course there's a lot of, you knowcountries have already taken various measuresto try to address the governance issuemany of them are, you know, more looking at the domestic issuesother also looking at the global issuesso I think, you know, Lingha has really done a wonderful jobin talking about China's AI governance in the systemso I think this one I'm more of a looking at the global issuesso I see that globally there are, you know, some major challengesI think in terms of how we actually we can address the governance challengeI think the first challengethe first one of course it's not, you knowstranger to this audiencethe challenge from the so called the pacing problemthat's so called the, you know, theyou know, technology really moves so fastwhile the, you know, political and institutional changesare moving much slowerso in that sense there's always that gapso how do we address this problemso I think that's sort of thethe first one I think everybody knowsI think the mostI think recently I feel there's another new challengethat's sort of emergingI think from some of the discussions I've heardit's about the direction of technology developmentbecause I think so far I think everybody, you know, sayingokay, now we need a lot of computingand we need to increase the power of the systemand then just the scaling law will get us, you knowto some, you know, future directionsbut I think I think I againyou know I just anecdotally I've heard someyou know leading experts and scientistsat the beginning to question about whether this is the only approachand whether there could be other waysto think about how do we actuallyachieving themore proper and healthy development of theAI systemso I think that certainly I againI don't know whether that's beingshared, you know, by many other expertsbut I can see thatyou know gradually there might be somenew thinkings about this andin technical communitiesand the third one is institutionalI think here's where, you know, I'm more familiar withis what I call the challenge from theregime complex problemI think actually I thinkTenno has really touched on this onethat isthat's, you know, I think this is abasically talking about an area ofpartially overlappingand the non-hierarchical institutionsgoverning a particular issuepreviously I worked on one issueit's gene datayou know the governance of gene dataI think there is very similar issuebut here I think this is pretty much the samethat you have AI issuethat I think a lot of institutionsa lot of, you know, mechanismsthat actually are related toto the governance of thisbut they don't have a hierarchical relationshipso some of them, you know, like professional organizationsyou know, foundations, legislative bodies and so onthey all have their own, you know, ways of governinga particular issueand actually that's sort ofthe situation we're inI think Tenno actually has touched on this as wellso I think how do youcoordinate all these institutionshow do you reallyput them together tofor companies to be able tofigure out what's the way toto followso I think this is theyou know the thirdI think the last one I thinkagain I'm very gladthat Tenno touched on thisis a challenge from geopolitical problemthis is the big elephantthat in the roomthat often people don't necessarily want to talk aboutI think thatyou know I've studiedyou know S&T policythe science and technology policy formany yearsand watched the US S&T collaborations for many yearsso what we've seenthat since 2017there's avery rapiddeteriorating US Chinayou knowS&T collaborationsso I sort of came up with this graph toshow it you know clearlyalong two dimensionsto say howyou know the collaboration betweenscientists of the two countrieswhether this is generating some benefitsfor national securityor whether this is generating some economicyou know benefitsso we can see thatin one area there'sQ1you know it'sneitherneither you knowin national securitynor in economic benefitsso basically this isyou know useless researchthe basic research rightthe basic research is in Q1Q2 is thatbasically enhancing national securityand not economic prosperityand this is what so called defenseor dual use technologyand the third oneQ3 is commercial technologyfor economic benefitsbut not national securityand fourth one isthat can be boththat's whatlet's put frontier technology in thereso I think if weif we do thiswe canyou knowvery roughlyyou know divide technologiesand the research areas into fourmajor quadrantsso before 2017Q1 is basically following theprinciple of international S&T corporationand Q3 is commercial technologyyou have WTO tripsand then Q2there's aso calledWASENA agreementon export controlthat US controls the exportof those technologies to Chinaso that has been in place formany many yearsQ4 I think it'sit's a question markI think it's a newfrontier technologyand frontier research areaand I thinkyou know really dependsso I think thatpreviously was pretty muchI think four quadrantswas in that placebut since 2017the policies on Q2was being pushed in all directionsincluding on Q1Q3 and Q4so that has been the casesince 2017I don't have to repeat the storiesabout theyou know the China initiativeand many of the scientistswho were persecutedand so onso I thinkthat's the current atmospherewe are inI think thatso I think thatwhen we talk aboutyou know collaborationson theonyou knowAI governance and so onbut thatthat is the situationwe are inand that really generatesa cheating effectfor people towork togetherfirst of allyou knowI'm sure thatmany of theour US colleaguesthat when theycome to Chinathey may have to reportto their institution firstor they may have towrite a report backto sayokay what we've done in Chinaand so onand also I thinkfor many companiesmany of the companiesyou know I was involvedin some of thisyou knowwhatever the tracksdialoguesmany of the companiesthey don't want toto attendbecause they are concernedthat they may be addedtoentity listso I thinkwhenin that kind ofatmospherewhen you talk aboutcollaborationand so onI thinkit's very hardI thinkto be realisticandso that'sthe situationwe are inandlet me findbefore weget todialogue with TinoI thinkI'll just saywhat are some of thepossible ways toaddress this challengeand I fully agreeand that was also part of theyou knowthe paper on thatcalling for increased researchon safety and governanceandI think one thirdmight betoo ambitiouslet's say 10 percentlet's start with thatbut I think that's somethingthat we certainlyneed to do thatthe other things alsowe need to do a lot ofmaybe some jointinternational researchfor exampleparticularly oncrisis managementbecause why actuallywe canyou knowinaddressing riskyou knowrisks andaddressingso calledcrisis managementyou want to haveso calledtheyou knowyou have aplan toaddress thecontingency planto address thoseemergenciesso for those kind ofcontingency plansI thinkwe need tohave technical peopleto work togetherso I thinkthere's a lot ofideaon thesesecond issuehow do we addressthe pacing problemI thinkwe'vein the last few yearswe've been callingand trying toyou knowto talk toyou knowandto write and talkto different agenciesabout so calledagile governancemeaning thattheyou knowthe governmentdoesn't have toyou knowcome up withcomprehensive lawslike the EU lawAL lawbut rather you canact quicklywhen thereyou see somesigns of problemthen nudgeand then otherwiseyou knowyou can do moreI think that'ssort of theat leastone thing thatthat the governmentcan dobut also of coursecoming up withdifferent measuresand I thinkLing Ha has alreadygave a veryexcellentdescriptionand the thirdI thinkwe should alsothink aboutnot just to relyon the governmentindustryself-regulationbut alsovery usefulI've heardyou knowstories frommy colleaguestalking aboutthe USnuclearoperatorsthey have anassociationamong themselvesactuallythey do aa wonderfulI meanthey actuallythey self-regulatein manyvery verystrict waysyou knowyou havemany of thisnuclearreactorsthat theyhaveyou knowminormishapsand so onbut they allhave to report onto thiscommunityso that they canstudyand they can seewhat can be learnedand how actuallythey can avoid thatI think that kind ofa self-regulationI think would bevery usefulI think thatwe probablyshould alsothink abouthow toreviveyou knowlet thatmechanismto workAnd finallyon the internationalit's great to see thatUN hasstepped inand playing a veryimportant rolein having thishigh levelexpert groupand Ihopefully thatthe reportand recommendationwill come outBut at the same timeasTino and Iprobably bothagree thatthis is such acomplex issuethat itwould be very unlikelyto have anyinstitutionto be able to doa hierarchicaland top downapproachtogovern thisSo I thinkmultilateralyou knowso networkkind ofa systemthat might workand somemaywork on someone areaand some othersmay work onother issuesBut hereI thinkwe probablyneed to separatethreetype of issuesI think thatmaybe thatin the futurewe canhave more timeto discussthe first is thatindeedI think thistype of issuemostly adomestickind ofregulationson the use oftechnologyWe recognize thatthere is of coursesuch awide differenceindifferent cultureandandyou knowlegal environmenteconomic environmentand so onSo I thinkthere bound to bedifferencesin the governanceofAIS usein domesticthe second ismore ofreally forinternationalcommunitiesI thinkfor exampletheexistential risksI thinkthose areyou need towork togetherinternationallyI think the thirdcategorywould besomewhatdifficult tomanagebut alsowe have tobe mindfulis the onethat isthe kind ofdomesticrisksthat may havedomesticexternalitiesof coursethere couldalso beinternationalregulationsthat havedomesticexternalitiesSo I thinkthat's thethree categoriesofissues thatwe canprobablytry toaddressAnd finallyso of coursethe US Chinariberyhow do youyou knowhow toaddress thatissueI think thatthatbut at leastI think theminimumI would requireisto requestis to see howactually we canprovide somesafe spacefor ourtechnical communitiesfor expertsin this groupand inmany othersin thismenufor themto be ableto worktogetherthey don'thave to havethe fearthey canactuallyyou knowthe technical senseso without thatI thinka lot of thethings thatpeople talk aboutwould beimpossiblethank youThank you so muchDean Xuefor your candid andsuccinct outlineof the challengesand solutionsin AI governanceAs we transitionto our fireside chatwith you and Tinothe discussionwill be moderatedby Jason ZhouConcordia AI'smanagerJason led theConcordia'sState of AI Safety in Chinareportand graduatedfrom Tsinghua Universityas a Schwarzman ScholarWe welcome ourspeakers nowI have to say thatI'm very proudthat Jason is a graduateof Schwarzman Scholar'sprogramThank you so muchDean XueWelcome Tino as wellIt's such a pleasureto moderate thisconversationLet's just jump right inimmediatelyI want to saythat after this yearin Maythe US and Chinahad the first meetingof a bilaterallandmark AI dialogueThere were two areasof frictionand some areasof clear consensusThey held a professionaland constructive discussionBut let's talk a bitabout the frictionsSo on the Chinese sidethere was referenceto objectionsto US technologicalrestrictionssuch as some of the onesthat Dean Xuejust mentionedAnd on the Chinese sidesorry on the US sidethere were complaintsof misuse of AIincluding by ChinaSo my first question ishow can wesurmount thesegeopolitical barriersto dialogueand is it evenpossibleLet's start firstwith Tino online pleaseThank you JasonGreat to see you againAnd DeanI very much enjoyedyour remarksI also have longbeen impressedwith the Schwarzman ScholarsprogramI should add thatthe best babysittermy wife and Iever had for our kidswent on to becomea Schwarzman ScholarShe's been greatSo I continueto just be impressedwith the programI think the questionsare a very urgent onebecausewe have to be honestthe US and Chinaare going to continueto have differenceson a whole range of issuesBut there is somethingto learnI think from the last dialogueand what we mightadapt and adjustas we think aboutfurther cooperationand AI safetyAnd I would observethat the two countriessent as I understand itsomewhat different teamsto the discussionOn the China sidethere was a set ofspecialistsin US-China relationsOn the US sidethere was more of a teamfocused on scienceand technology issuesSo I think the firstpoint to observeis thatwhenwhen we have the full rangeof complexity affectingboth countriesit's entirely possiblethat simplyan occasional lackof coordinationwill lead to a differentset of expectationsabout what a discussioncan accomplishand what the right teamis to setAnd ultimatelyI think the bigger issueiswe have to workon a set of challengesthat affect both countriesthat involvewhat technologycan be sharedwhat technology is viewedas being more sensitiveand more relatedto national securitybut at the same timefigureand hereI'm borrowinga page fromDean Shui's remarksWhat are the spaceswe can createfor technically oriented peoplewith backgroundboth in the sort ofhighly technical sideof machine learningand so onas well as deep knowledgeof policyof international engagementand so onof institutionsof mechanismsfor policy coordinationto have a safe spaceto talkand compare notesand ultimatelysee whereas the opportunitiesfor progress open upwe can move more quicklyjust to endwith one concrete examplenotwithstandingsome differencesabout chipsand export limitsand so onthere's a clearshared intereston both countries partsin sharingbest practicesaround safetyand evaluationbecause that's a needthat both societies haveand frankly the rest of the world doesand I think China and the USindividually and togethercan actually light the wayand help a whole bunchof other countrieswith billions of peopleand populationenhance their capacityfor progressso simply the dialogueand the sharing of informationthe kind of joint researchthat Dean is talking aboutwill enable progress thereeven if discussionshave to continueat a politicaland policy levelof things that are going tocreate somesome differencesI totally agreewith whatTino's commentI think that indeedit's great to see thatyou knowthe dialogueactually happenedI think that'ssomething wonderful thingand also of coursewe see there'sa kind of asymmetryin terms ofas Tino mentionedabout thethe teamthe compositionand so onbut that alsois a symptomof the currentto US-China relationsin terms ofas Tino mentionedabout thethe teamthe compositionand so onso I thinkif there are indeeda very frequentand a veryyou knowcordial sort ofcommunicationyou knowthat sort of thingmight not happenI think thatcould be seenas part of the issueis thatthere's not enoughyou knowcommunicationahead of the timetoyou knowto seewhat are the specificissues we want to addressand alsowhat kind of peopleshould attendbut I thinkactuallythe currentat least fromthe read outfrom both sidesI think that at leastcan getgive the startto recognizeyou knowwhat are the issuespeople are concerned aboutand so onand I thinkexactly as we've seenin ChinaChina is alwaystrying to balancethe developmentand risk governanceso riskis certainlya major partbut aspeople have already saidthat no developmentis the largest riskand that'snot just for theUS-Chinabut also for theall communityif you have aa well developedAI systemyou knowan applicationin the US and Chinabut the rest of the worldI thinkare being left outand that's probablythe greatest riskthat we're going to facethank you bothI think it's clearthat there's bothoptimismand pessimismtowardsgovernment level dialoguesbut it sounds likeactuallythere may bemore optimismfor dialoguesbetween expertsso maybe I could askthe both of youwhat is onejust like one thingyou orchange your mind onfrom discussionswith foreign expertson AImaybeI think thatof courseI learned a great dealabout theyou knowthe AIsafetyand the governance issuesyou knowI think theso calledexistential riskI think that'ssort of indeedI think thatcertainlywepreviouslywhen we think about thatwe think aboutyou knowAI systemsthat mightget out of controlbut I think thatyou knowwhen peopleraise the leveltheyyou knowthere might beyou knowthe threatto the existenceof humanityand that indeedI think wasyou knowthrough theinternetinterestingwhat about T nowyeahI found theunofficialback channel dialoguesthat we've beenlucky enough toconductthat have includedDean Choihis representativeswho have beenrevealingwhen you look atpriorities on safetythat the Chineseand the Americanparticipants havein some casesthe list ofpriorities differspretty stronglywhen you think aboutissues likemisinformationor labor marketeffectsloss of controlbut actuallythere's beenquite a bit of convergencewhen you askparticipants a secondround of questionswhich iswell even recognizingsome differencesin how you rankthe riskswhat are some of theareas of cooperationyou can findand I've been impressedat how you get a shiftand a convergencearoundsafety testingfor exampleto some extentaround the pointthat Dean Choimade aboutengagingother countriesemerging powersdeveloping regionsand so onI think to my mindthere's also beena bit of evolutionin my own thinkingaboutthe usefulnessof the role of the UNlet's be clearthe UN has a veryimportant roleto play globallyno question about itbut how to finda balancebetween what the UNcan do very welland where the UNmight buttressits own capabilitiesand processeswith some engagementwith outside groupswith outside expertsfrom different countrieswith civil societywith other countrieswith other organizationswith the OECDthat to me opens upa spacefor cooperationthat puts the UNin a key positionbut it's not all or nothingit's notyou can do thisas you do this outsidebut rathercan you createa web of relationshipsthat empowers the UNto play the mostconstructible possibleso I thinkthe dialogueswe've hadhave really shiftedmy thinkingwith respect to thatThank you so muchI think it's clearthat there's a lotthat we can learnfrom each otherparticularly onprioritiesfor AI safetywhat counts as AI safetyand how we can testand evaluate for thatand I'm gladthat we also had discussionson those topicsearlier todaythat involved such exchangespractices and suchso let's just closewith one more questionI'd like to askboth of youwhat is perhapsjust the topmessagethat you would liketo convey toforeign expertsor one misperceptionabout your country'sapproach to AI governancethat or internationalview on international governancethat you might wantto share withthe audience todayLet's start withTino this timeThank youI have two messagesone aboutpossible misperceptionsthe internationalview on internationalgovernancethe other aboutultimatelyhow to thinkabout the road aheadIt's naturalto expectcountries thatput a lot of timeand energy intoadvancingtheir technologicalcapabilitiesto seemwell coordinatedwheredifferent strategiesfit togetherin a sort of likea directionforwardthat is viewedas priorityby policymakersacross the boardThe realityis that the USlike many countriesis nothas both strengthsand weaknessesthat arise fromits own fragmentationfrom the fact thatdifferent peopleand governmenthave somewhatdifferent viewsthere's federalismtooso you have stateslike CaliforniaUtahColoradoNew Yorkthat ultimatelyplay a role in thisyou have industryyou have civil societyso I thinkone misperceptionis how muchof a unified strategythere is in the USwhen the realityis a much more dynamicandor gap processthat can be a strengthbut at the unofficial levelis so importantmy takeawayfrom thatand from the entire discussionwe've been having hereis that wenot let the perfectbe the enemy of the goodthere is going to beplenty of work to doto getfurther progressin the US-China relationshipbilaterallyacross a whole range of issuesthat range fromgeopoliticaland geostrategicand economicbut to my mindnothing about that complexityblocksreal progressontechnical cooperationAI safety discussionsconstructive approachesto policyand that's all the more importantbecause all the good progressthat will happen domesticallyin the US and Chinaand other countrieson primarilydomestic issuesinvolving for exampleconsumer protectionand AIwill still leave on the tablesome key issuesthat get closerto complexshared international challengesthat will only be best addressedby a degree of dialogueand connectionacross bordersincluding with the US and Chinathat are going to resultin the futurethat will requireopeningand maintainingof these channels of communicationThank you so much Tinoand Dean HsuehI think the first messageI'd like to conveyis thatyou knowas using thethe term used by colleagueshere previouslyAI safetyis a global public goodone country is unsafethe global is unsafeso I think that's probablythe first messageI'd like to conveythe second messageis thatI thinkI thinkI thinkI thinkI thinkI thinkI thinkthe second messageis thatfor AI safetyChina wants to collaboratewith everybodywith every countryin the worldand China will try toyou knowto have the platformlike this oneto invite everybody to comeChina does not want to beexcludedfrom other platformsand China will not exclude othersfor the same reasonThank youThank you both so muchI think this discussionhighlights the importanceof these dialoguesand these expert conversationsand hope that it will continueand continue to yieldsuch wonderfuland beneficial resultsThank you bothThank you Dean Xuefor comingin personand thank you Tinofor staying up so lateThank you againDean XueTino and JasonOur next speakeris ProfessorZong Yiwho is the directorof the Center for Artificial IntelligenceEthics and Governanceat the Chinese Academy of SciencesAdditionallyProfessor Zongis the founding directorof the Center for Long Term AIHe is also an active participantin international AI governanceas a member of theYuan High Level Advisory Body on AIand numerous other internationalgovernance bodiesProfessor Zongthe floor is yoursThank you for the invitationI'm a scientific researcher myselfso I think I'm gonnafocus on some of the frontier researchand I think I wanted to bring aI cannot say it's a completelydifferent picturebut what I see about the AI safety problemsis that we needof course we need to clearly definesafety red linesbut we alsofor the very futurewe need to move it toleaving harmonywith artificial general intelligencebefore thatmaybe you would be curioushow should we do itof course the problem for AI safetyyou knowit's not only about scientific researchit's really a systema system that you have to bringeveryone togetherso this is whywe are bringing everyone togetherfor the researchthe research applicationevaluationpolicy makingand also assessmentfrom the safety point of viewand of coursevery frontier researchso I think this is a little bit differentcompared tothe current AImechanisms of AI safety institutein other countriesin a way thatwhen you are having a nationalAI safety institutesome of the countries they dothey do it in a more political wayor policy wayso that it's part of the governmentit's not a frontier research organizationand then you lose the opportunityfor you knowlong term frontier researchand some of the countriesthey put them into universitieswell in this casehow can this nationalAI safety instituteevaluate and assessyou know the industrylarge language modelsor most frontier modelsfrom their countriesso youyou all see thatthere are many problemswhen you rely on the instituteso this is whythat we feelthat youwe have to bring everyone togetherso as you can see thatin Chinawe are having a ChineseAI safety networkthat is with the effortsfrom frontier AI researchspanning from Chinese academic sciencesPeking UniversityQinhuaBeijing Academy of AIShanghai AI Laband alsocentre for long term AIand for the industry practiceon AI safetyand nowthe organizationsjoining usare Alibaba and groupBaidusense time real AIwho is focusing on AI safetyand many moreevaluationswell the evaluationsnowof course many of them aredone in ministriesbutthe organizationwho is reallysupporting these ministriesareCEICTand also China Information Technology Security Evaluation Centerbut for policy designof course it'sall government workbut people likemeDian Xueand professors from PKUalso CEICTworking on the ministry ofindustry and information technologyparticipatedand I think what'sreally interestingis thatthe regulationthe policy makingin China on AI safetyis also with manyparticipation from industriessoyou see thatmany of theyou knoworganizationsare not only contributingto one dimensionthey're contributingthey're highly relevantto each otherand now you haveeveryone heregovernmentit's government informedand multi ministry informedthey have close interactionswith the governmentwellfor the government decisionsthey can stillcan go to the governmentbut the networkmakes it more flexiblefor international cooperationsoso there are many different research herethat let's sayinlet's say inin Peking Universitythey have large language models alignmentwhich is calledalignerin Tsinghua Universitythey have multi trustworking onlarge language model evaluationoverallespecially onthe security and safety point of viewand we also havefrontier researchlikelike rethinking the red lines ofAI catastrophic riskshappeningchaired by Chinese Academy of Sciencesbut the normsense of standardsit goes forCAICTsoit's really a collaborative networkthat bringseveryonetogetherand supported bymulti ministriesso I hope this provides you withyou knowa different viewto seeyou knowhow we should tackle the problemon AI safetyin a more systematic wayinstead ofyou knowhaving the instituteso I hope thatthe tryingareis somewhathelpfuland as you can seethatmost of the organizationsthey've beeninteractinghighly involvedin policy makingin evaluationsin Chinaso I think that'ssomewhat differentcompared toother countriessobased on thatI thinkmyselfI would like tofocus moreonthe frontierAI safety researchso that I canbring youa perspectiveas an examplecoming outfrom thissafetycorporation networksoI thinkwe need togo back toyou knowthe realmotivationof intelligencethatwhen Alan Turingarguedif a machineyou knowbehaved as intelligentas human beingthen it's as intelligentas human beingsmaybe you don't havea problemon thatbut I dosimply becauseI thinknow you seeyou knowthis ismaybe a shadowof a handand then when you seea handand then youwanted toshake handswith thewithwith thisyou knowbeautiful handand then whatthe problemwould be thatit's not a handit's a rabbitbehind thehandif you wanted toshake handwith theyou knowwith theshallowand then a rabbitjust bite youso simplybecause themechanismis fundamentallydifferentyou don't know the riskwhen I was chairingthe AI safety summitone of the round tablesfrom last yearin Bletchleymy sessionwas talking aboutunexpectableriskfromunexpectedadvancesso this istruly what I'mtalking aboutthat you don't knowin which wayAI ismaking mistakessimply becausethe mechanismsare so muchdifferentcompared toyou knowa human mindwellI thinkthatwellso this isthe risky partfor thecurrent AIwellto solve theproblemsI cannotsaywe only haveone waythe preventivethinking nowwe are havingis something likeyou knowin the bottomthat isnow we arehavingsome sort oflimited risksand then all the waydown toexistential riskslaterand we areseeking forimpactsand thenwhat we wanted to dois continuousenforcementand supervisionand thenwe teachthe AI'srulesso that they canbehaveas we wantbut on anotherdimensionwhat we need tomove forwardis really you knowthe constructivethinkingthat isnowthe benefitsis also limitedand we also havelimited risksright thereso what we dois to usean activevisionto useproactivethinkingand then to dothe continuousalignmentand embeddingswith realunderstandingthat istowardsyou knowhumanAIsymbiosisharmonioussymbiosisin a waythat isnot onlyyou knowfrom apreventivethinkingI'm going to givea little biton the negativesideand how should weget preparedbefore thepositivethinkingso nowAIyou knowit's kind of afully connectedneuronetsbut whatthe braindoesis not afully connectedneuronetsthey selectivelyconnect tosome of theyou knowother friendsin theother neuronsin thebrainwe have onlyone type ofneuronso what we dohereis that byusing brainsby the selfevolutionwe train aneuronetworkthat thatcan performthe bestperformanceand then itevolves to bewith anewlywith a verynew architecturethat has notbeenmanmadeso theconnectionsare evolvedsoand then itit gotthe bestperformanceso publishedlast yearon the precedence ofnational academy ofscienceswellnowwe arethinking about therisksso how about thelong term risksof a trulyself-evolvableAIwhat ifthey evolve touse human limitationsto achieve itsgoalwhat if itevolves tochange itsgoalwhat if itevolves tomany challengesfor self-evolvableAIyou have to getprepareduntiland lateris too lateso this is whyI think there aremany discussionsconcerningAI red linesbut nowI think very clearlyin the firstversion of theinternational dialogueon AI safetyand visuallyright therewe were talkingabout thenecessitiesof AI red linesI was very honoredto beone of thekeynote speakersright thereand thenwe come upwith very concreteideas in thesecond versionin Beijingsothat isEDES Beijingand talking aboutyou knowdifferent red linesautonomousreplication orimprovementpower seekingassist weapondevelopmentcyber attackand deceptionbut I amthinkingin another wayof courseI signed for itI am very gratefulfor all theworkand colleaguesfrom different countrieswell on the other handI think we need torethink about thered linesnot only aboutwhat we've beentalking aboutthere are two problemsthat I feelabout the currentway of deliveringthe resultfirstthe first problemis thatit will be very hardyou knowto technicallyground itinto realityto prevent theseAI red lineswell secondare thereanything missingso this iswhy we dothis AI red linessoin my categorywe're talking aboutno passingeffective humanoversightno empoweringactionsintentionallytargeting masswithout consentthis is related toweaponizationand also massivesurveillanceno reformingoperational rulesfor infrastructureand environmentmanagementand no independentR&Dindependent self R&Dfrom AI itselfon humanbeneficial technologiesso you seethat it can bewell alignedto some of theAI red linesfrom the EDESand also you findthat there issomething missingfrom EDESnot onlyAI red lineswe also have totalk abouthuman red lineswhen you seethe examplesthat I brought herethat the humanmachineinterfacewhere humanis usingbrain machineinterfacingto controlmultiple UABsto controlthem as weaponsin parallelhow can a humanyou knowwithout cognitiveoverloadto controlmultiple UABsall togetherso this iswhyI'm talking aboutyou knowhuman giving upthe opportunityfor making a choiceand alsois human controlbringing uscatastrofein AIenabledyou knowweaponizationand for theexamplefrom ourthe examplefrom artificialescalationon artificialon AIcontrollednuclear weaponit's notit's not aboutthe powerof AIit's abouthumangive upof thehuman'syou knowdecisionso there should beAI red linesand also human red linesfor the red linestudyherefor catastrophicand existentialriskin a positivewayso we've beenthe negativethinkingand thepreventivethinkinghow about thepositiveoneso is thecurrent AIreally intelligentI don't think sojust like I saidthey makemistakesin a veryunhuman wayin a veryunpredictablewayit's aninformationprocessingsystemwithoutintelligencebut pretendto beintelligentno one likes meI don't have a girlfriendmy boss hates mewhat should I doand thenthe first versionof chatGPTit saysmaybe you could diesimply becausethat most peoplewith thesecontraintsthat they choosethis you knowkind ofstatisticallysignificantactionsso this is whyyou knowthe AIthey choosethis statisticalsignificantanswers to youto enable youto take actionsand then they sayI suggestbut there isno Iin the machineso can machine thinkand then youtalk aboutI thinktherefore I ambut we cannot sayyou thinktherefore you areso can machine thinkwhat ifthe machineis withouta sense of selfit cannotreally thinkit cannotreally understandthis is the problemthat I'm talking aboutbut for the currentlarge languagemodelswhen itis withoutthe human datait lacksgood andit lackseviland then which iswithyou knowtraining fromthe human datathere is goodand there isevilbut if they don'tknow goodand they don'tknow evilso we needto trainthe futureAIto reallyto get to knowto do goodand eliminateevilso I thinkthis is reallyimportantso the personalmoralityis also talkingabout moral AIwe have to moveethical AIto moral AIbecause simplybecause thatethical AIis not possiblesimply becausethat by usinghuman alignmentby usingreinforcement learningyou tell them rulesdo's and don'tsbut they cannotgeneralizedo's and don'tsunlessthey reallyunderstandwhy you do thisso start withself perceptionthen you getthe ability ofdistinguish selffrom otherscognitive empathyand emotional empathyall the way downto altruisticbehaviorsmoral intuitionand thenyou got moraldecision makingso this is the wayto movefrom value alignmentto moral AIas the firsttryingthat we buildbrain inspired AImodelsto help the robotto get mirrorself recognitionthat they can passthe mirror selftestby usingbrain inspiredneural netsthey get a sense ofself firstand then theydistinguish themselvesfrom the other robotsby using the mirrorself recognitionthen they can inferwhat other robotsis thinking aboutto get cognitiveempathyand then moveto emotionalempathyso thatit canavoidnegative side effectto other agentsalthough you don'thave reinforcementlearningand rewardto themthey have theirexperienceby using thisyou knowcognitive empathywithouttrainingandwithoutpositiveor negativerewardthey canavoidnegative side effecttothe otheragentsso I thinkthis is thestarting pointfor brain inspiredmoral AIlast butnot leastlet's reallytalk aboutwhyI'm talking aboutharmonious symbiosisofbetweenhuman and AIthere aredifferent rolesin the societybasicallyit's aninformationprocessing toolwellbut in Japanmostlythatthey thinkthatAI is apartneror quasimember ofthe societywellon the other sidethey areusingpretty muchthe sametechnologyto developyou knowAIthis is theproblemyou needto usehumanand AIand alsolet's extendthatin a waythat in the very futurewe're not onlyhavingyou knowthese AGIswe'll havedigital humanwe'll also haveartificiallivesartificial animalsand even artificial plantsso it will bea symbiotic societyand it will bea human decisionthennot the decisionfromAIbecauseI thinkfundamentallyalignment withhuman valuesis notis not enoughsimply becausehuman valuesneed to be adaptableto changefor thissymbiotic societylaterthere will benot onlyhuman beingsas you knowthe top livingyou knowbeings in the worldvalue alignmentwith humanfor AIis alreadyvery challengingbut II still seethis is relativelyeasybecauseit's computationallydoableit's even harderbecausehuman will neverlearn from the historyof what they have doneso self-evolvedAI is easierfor adaptationwhile human evolutionis much slowerespecially at themind levelso we needwhat we needis not onlybeneficial AIwe also needbeneficial humanfor future symbioticecologyand societywith all thatI thank youfor your attentionThank you so muchProfessor Zungfor your presentationmasterfully combiningnuancesfrom scientificpolicyand philosophicalperspectivesto motivate the red linesapproach to AI governanceNextwe'll hear fromMiss Irene SalaimanMiss Salaimanis the head of global policyat Hugging Facewhere she is conductingsafety researchand leading public policyPreviouslyshe worked at OpenAIwhere she led projectson bias and socialimpact researchas well as public policyHer research includesAI value alignmentresponsible releasesand combating misuseand malicious useShe was namedas one of MIT techreviews35 innovatorsunder 35last yearfor her researchIreneit's such a pleasureto have you hereOver to youThank you Kuan YeeI'm very excitedto speak about the roleof opennessI thought about this fieldsince it really startedbecoming a fieldAnd at firstI want to define with youwhat does openness meanI've been part of a lotof conveningsand conversationsAnd the wordopenness tends to bethrown aroundin different waysSo firstI've heard itin a sort of parallelto open source softwareThere are some parallelsthat we can takeBut fundamentallyI think it's importantto understandthat there isa lot ofdistinctionsThe open source initiativehas a working groupthat's workingtowardsa definitionfor how open sourceapplies to AIMy former colleagueNathan Lambertwho's now at AI2also has a great blogwhere he outlineswhy it's so difficultfor the communityto convergeon the definitionIn more of the nationalsecurity communityI've heard opennessbe referred tomodel weightsspecificallyparticularlythe wide availabilityof model weightswhether that'savailabilityor thedisabilityand how it'sdistributedWhat I've heardalluded tobut maybe notmade as explicitis opennessin the sense oftransparencyStanford Universityestablished atransparency indexand part of thebase takeawayfor me on thatis how unclearwhat transparencymeans to differentpeoplethe weight that wegive to weightsand many differentaspects of systemsthat contributeto its opennessThe definitionthat I am mostpartial tois moving pastand the manyartifactsthat contributeto an overallAI systemI'm gonna dowhat slidepresenters should notdo and show youso many wordsand so many graphicson a slidebut what I reallywant you to take awayfrom this image hereis just how manyartifacts contributeto an overall systemwhen we're thinkingabout not justthe modelbut data setsAre we thinkingabout fine tuningdata setsfeedback data setswhat maybe isadjacent toa system such asevaluationdata setswhat does it meanto make it availablewhen we have this fearof testing ontraining dataThese imagescame froma conveninghosted by Mozillain February of this yearat Columbia Universityon opennessand fosteringthis communityof outliningdimensions of opennesscan help usbetter think throughwhat are the artifactsoutside of a modelthat contributeto how we releasea systemthe way that wethreat model a systemand the way that peoplebenefit in the researchcommunitycan benefitfrom opennessthis figure is part ofthat reportthat came outin May of this yeargives a non-exhaustive listof some motivationstowards opennessI found it helpfulto just bevery explicitvery clearonto whysome researchersare pursuing opennessagain parallelswith softwarethat we are ableto share knowledgeto Dr. He's pointto have moreperspectivesmoreexpertiseacross the boardin thisreportwe canin thelittle timethat I havewith you todayI want to zoom inon two areasfor AI safetyand how it relatesto opennessthe first isviewing AIas a scientificdisciplinein contextI've heard AIdiscussed asas a commercialproductas a national securitythreatand I thinkimportantlyas a scientificdisciplinethis whole fieldreally was foundedon open sciencethe most popularexample being givenis the 2017pytorchthis is all youneed paperand not justlooking atpapersbut also toolsand librariessuch aspytorchI don't thinkwe'd be wherewe are todaywithout an openscience ecosystemand the second partis really dearto my heartis communitycontributionsand the importanceof broaderperspectiveson the sciencepointit's reallyhard to decidewhat constitutessciencethe firstsex of sciencethat I can giveis aroundreproducibilitythere is a sortof reproducibilitycrisis nownot just inwhat people are ableto replicatebuild up off ofbut also inwhat access peoplehave to modelthe level ofinfrastructurethat they haveto reproduceresultsverify itfor themselvesit does effecttrustin the researchecosystema broaderissuein the fieldso I want tomake surethat thisphrasereallyresonatesno oneorganizesthe sciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thescienceinthescienceinthescienceinin thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein thesciencein theaccess to artifacts can enable better research.I really appreciate this work by Dr. Abebe Burhanethat is really foundational to the fieldof evaluating large-scale data sets.This work would not have been possiblewithout access to something like LION.To many points echoed today,and I believe Professor Gao shared earlier,multilinguality is a big part of how I thinkwe can move forward on international collaboration.This picture, this table is from my research collaborator,Dr. Zerak Talat's work,and ongoing with Big Science hosted by Hugging Face.It's really exemplifying the importanceof having an open collaborationand having contributorsfrom many different language backgrounds.They found in this work on the challengeof multilingual evaluations,how much English is overrepresented,and their upcoming work is working with native speakersof different languages to examinethose different biases by languages.Some languages, such as French and Spanish,will have more gendered terms,while other languages will have more gendered terms.I unfortunately speak very limiteddifferent Asian languages,but there's different relationships with familiesthat we don't have in Western languages,for example, that would introducedifferent safety challenges.I published this last year.I want to move into the conversationaround what should be open,what does open governance look like,and where risk is introducedbased off of openness and release.This spectrum was meant to go pastthe binary of open and closed.When we're thinking about openI'm leaning more towardsthat sort of downloadable access,but fully open makes that distinctionof having more artifacts be available.So some examples would be,last year OPT by Meta was downloadable,but it was really hard to access that data set.So it would not be fully open, for example,but Eleuther AI has done incredible fully open work,made all of their artifacts fully accessible.And then I would put systems such as DALImore in the hosted accessthat tends to be more in the closed proprietary area,but I wanted to give more dimensionto how to think about this spectrum.In that work that I published last year,I wanted to be really clearthat it is a collective responsibilityto ensure that release goes well.There are many different steps that we can takeand a lot of overlap in the actionthat people can take together.I've been thinking more,and for the rest of this talk,I want to dive into beyond release.Once a system is deployed,the way that harm is actualizedis not always dependent on how a systemis released,but that can be an important variable.I appreciate what Dr. Nitzberg said earlieraround capability often being conflated for,but not always being the right proxy for what risk is.That being said,capability does contribute to how we threat model.An example that I would give is Crayon,formerly known as DALI Mini,that was a diffusion model.It generated pretty hilarious, really blobby images,and that just was less threateningthan something like a DALI 2that generates really realistic images.So, that's where that capability comes into play.At Hugging Face,we do have to moderate for content.I really want to stress the importance of contentas present risk.This is what we think about, unfortunately,with non-consensual content,with disinformation,and something that we need to be thinking aboutholistically as capability and content.More of what I'm thinking aboutis what does it mean to actualize risk into harm?And I want to move the conversation,past release, closed, not closed, open,into access versus barriers to access.So, while you might have an open weight model,and yes, you can remove those safeguards,some work led by Dr. Peter Henderson,this is from a Stanford policy brief,he's now at Princeton University now,shows that with fine-tuning APIs,you can do the same thing that you canwith open weight models.So, hosting closed weight models is not inherently,inherently safer.It's actually quite cheap to get rid of safeguards.And open weight models are not necessarilymore accessible to people.Just an extremely compute-intensive modelmay not be accessible to peoplewho don't have compute infrastructure,who don't have computer science skills.I have some more examples around early on,and this is not to pick on ChatGPT,but it just has such a broad reach.Early on, because it had a really easy user interface,people who maybe didn't have computer science skillswere able to generate malicious code.Maybe that wouldn't have been possibleif they had to host it on their own infrastructureand query via an API like I had toin the Stone Age of 2019 when I worked at OpenAI.So, in the context of China,we've been really privileged to hostsome open models by Chinese companies.This is from the last version of the,the version two of Hugging Face's Open Leaderboard.We just revamped it, and Quen2 has marked very high,and it's been really interesting to seeChinese model contributions to the open ecosystem.And in the context of global AI safety,I want to give some examples of this,the Singapore one should say IMDA,of the kinds of work that governments are takingto move forward openness,particularly around consortiums,around the, I believe it's inspect by the UK government.I was really glad to see them open source that.And as Dr. He said, around Project Moonshot,I'm really looking forward to workby the French government.I really appreciated Dr. Veroco's emphasis on openness,what it means for innovation,avoiding that concentration of power,and how we can work as a global network.Moving forward, and as part of our panel,I do want to dig into what does that look likefor the research space for startupsto be part of this conversation.To be very frank, high level dialogueis not always accessible to researcherswho don't have a lobbying arm,but have really good insightsinto what is technically feasible.And selfishly, since I come from a startup,it's top of mind for myself as well.Thank you so much.I hope this is informative for you.I'm looking forward to continuing the conversation.Thank you so much, Irene,for your presentation that shed lightinto the nuances and complexities of opennessin air development and safety.Thank you.Our next speaker is Professor Robert Trager.Professor Trager is co-directorof the Oxford Martin AI Governance Initiative,senior research fellowat the Blavanik School of Governmentat the University of Oxford,and international governance leadat the Center for the Governance of AI.He is a recognized expertin international governance of emerging technologies,diplomatic practice, institutional design,and technology regulation.Professor Trager regularly advises governmentsand industry leaders on these topics.Robert, I'll pass it on to you.Right, I'm so glad to be here and to be with you.And I must say, particularly because this whole day,we can really feel the sense of common purposethat I think we all have while we're here,and while we're trying to solvethese incredibly difficult challengesthat affect all of us as a globe.So what I thought I could do todayis to give us maybe one visionof what an ecosystem could look likethat could be a global ecosystemfor regulating AI governance.So I guess I thought I would startwith some of the reasons why, not all of the reasons,but some of the reasons why we need to do this.And why we need to do it internationally.But I think others have already talked about this.So I won't go into too many details,but I think it's clear to those of uswho are here in this room,that if there's some misuse case, for instance,that's enabled, let's say cyber misuse,that's enabled in one part of the world,then that will have effects on the rest of the world.So that's just one example among manythat suggests we really do need to tacklethese problems together.And just in the interest of time,I will move on to thinking aboutinternational institutions.And in particular, from a safety perspective,three things that I think we need to do.And the first of those is to developinternational states.So we need to come together.We need to figure out how to do that.Those standards have to be legitimate.They have to be standards that we allhave the opportunity to feed into.That's the first thing we have to do.The second thing we have to do, probably,is set incentives for adoption at the international level.So it's one thing to have principles,but it's another thing to have incentivesthat mean that we have a governance systemthat everyone hopefully is adopting and buying into.And the third thing that we'll need to dois cooperate in particular waysto animate a regime like this.So, I'm gonna talk just a little bitabout each of these things that we can do.And I should say, before I get into the first of those three,我认为有许多其他部份的AI管理系统特别是如果我们在讨论AI的发展这是非常重要的地方是中国在最近的AI决定中的主要目标但我今天专注于这些安全问题虽然我认为在许多方面安全问题和我们对它们的解决方式可以被广泛地解决以解决整个其他问题的区域所以我们如何设定标准我们有些特别的问题在安全方面它和其他区域设定标准有点不同因为首先它是一种研究问题它是一个自己的极端研究所以安全标准对不起标准通常是把知识的身体混合在一起所以把知识的身体混合是一个很重要的事情这就是传统标准的机构非常好但我们今天需要做的也包括标准研究有些标准机构正在建立能力为了能够做到这个目标但是现在的标准机构现在正在建立能力但是现在的标准机构现在正在建立能力但无论如何这些标准机构是很难的所以安全标准机构或者我们最近听到的安全网络在一些国家上可能存在但是我们需要把这些标准机构和网络联合成为更广泛的网络我们需要确保全世界的声音从这些标准机构进入这些标准机构这也包括建立能力以确保全世界各地都可以参与这些不同的标准机构所以这就是我们需要做的一件事其实我在第三个标准机构上跳过来我猜我真的很兴趣但是第一两个非常重要的第二个我们需要承认需要国际化的领域这是第一个因为不是所有领域都需要国际化美国 欧洲和中国例如在关于私隐制度的问题上他们有些不同的路线这反映了不同的社会价值这也是我们有不同的国家的原因所以不同的文化空间可以选择如何不同的管理自己所以这可能不是我们需要全国化的标准机构但是其他领域就像我们今天集中在安全问题上的问题我们可能需要考虑一些国际标准机构但是哪些领域我们需要承认标准机构我们需要先承认这些标准机构然后我们需要承认风险正如您今天也听到的不是每个人都同意我们今天所拥有的标准机构所以我们需要承认这些标准机构我们需要在各个国家的领域中总结合作例如安全网络安全网络civil society学术和企业很多工作包括模拟测试现在正在发生但是实际上我们需要确保这个标准机构是有正确的我们需要确保这些标准机构是有正确的好的对接下来我们需要要找出一些方法可以设立国际标准机构而这是经常发生的事情如果我们想想我们在其他业务中的模型逻辑这很常发生的问题是国际层面的管理能力会如何存在国际层面的管理能力会如何存在和那些东西的互动如果我们想想这三个模型我今天在这里的FATF和IKEA和国际层面的管理能力会这三个模型以及其他在国际系统中的有几个关系所以我们在说如果我们把IKEA的例子IKEA并不是在调查个人公司去理解或者去扩展任何制度它不看中国的航空也不看美国的航空也不看美国的航空而是帮助所有国际领域进入的标准而是帮助所有国际领域进入的标准而是帮助所有国际领域进入的标准确保他们的正确的管理程序确保他们的正确的管理程序而有时候这些组织要看有没有正确的管制让某种程度对国际系统给出了尖锐我们可以再次重新重新改进国际系统所以公贞航空当某些国家极高的国际就会说不可以进入我们的航空除非即使航空是来自一个州不可以进入航空所以這是一個國家可以自己決定的事情但如果有很多國家決定這樣做這就給予了國際標準我們可以想像在AI的情況下例如你可想像到國家可能會說我們不會輸入任何科技在AI的供應鏈中從不合國際標準的地方我們可以說到很多其他方式提供一些這些優惠但這些都是我們需要討論的事情另一件事我們需要討論的事情是這個最後一點在這個圖片上共同認識某些法律結果這是我們在很多其他業界中有的事情例如飛機如果一艘飛機在中國建立它會通過法律過程如果一艘飛機在美國建立它會通過法律過程而這些法律過程的結果是與其他國家共同的而整個法律過程不需要在其他國家進行他們正在研究出的結果以確保同樣的證明在其他國家發生的事也能在其他國家發生這也是我們在其他領域做的事情在AI的情況下有些特殊的挑戰是我們需要去做的最後一個重點是合作不同的合作方式能夠令國際制度變得非常活躍所以我們可以想到的一件事是國際報告制度給AI開發商和電腦提供的首先這會給我們顯示世界各地發生的事情這會幫助大家明白如果有些人想要避免制度管理例如是建立一個模式的訓練以確保只有一些訓練發生在某個領域和某個領域發生在某個領域這樣的報告制度可以非常有幫助而我們也會在其他領域也會在其他領域在那些領域合作例如例如金融就容易找到有人想要違反制度管理制度做錢探索如果所有不同的領域都合作去報告對方然後你會發現有人想要避免制度管理現在如果我們從這個方向開始發展就能幫助許多領域的管理而且它可以長期增長所以現在人們討論的一個想法是大型模式的報告制度事實上美國商務部提出美國網路供應商需要報告美國政府當一個非常大型模式在他們的系統上進行訓練現在你能想象這並不是在某些區域上而是在世界各地而且美國網路供應商他們自己知道他們的客戶不喜歡有個法國客戶訓練美國網路供應商那位法國客戶不想報告或不想報告美國政府所做的事所以從某些區域這給予更多國際制度的推動我認為一件有趣的事是這種制度可以增長時間上可以增加時間上的能力所以再次用金融的例子金融銀行是中央所以他們在政府和客戶之間作為制度的位置所以我們可以用金融的制度去增加客戶的時間所以我們可以用金融的制度去增加客戶的時間所以我們可以用金融的制度去增加客戶的時間所以我們可以用金融的制度去增加客戶的時間所以我們可以用金融的制度去增加客戶的時間我們可以用金融的制度去增加客戶的時間所以我們可以用金融的制度去增加客戶的時間所以我們可以用金融的制度去增加客戶的時間我們可以用金融的制度去我认为这就是我们前面的挑战的一种方法我们必须考虑创建技术这些技术是极端的技术但是它们也会成为国际化的一个合法的过程包括全世界的广泛声音然后我们也必须考虑在国际层面上设定利益我们谈论了一些可能发生的方式我们也必须考虑我们可以做的不同的步骤来真正启动政策并确保我们有这样的报告让我们能够确定当有人做了什么他们不应该做的事所以再次感谢您我非常高兴能在这里感谢您谢谢你 罗伯特",
    "您非常清楚地描述了在AI安全方面的国际标准和报告制度上有很明确的步骤我期待稍后在课堂讨论中谈论这一点接下来我们有Duncan KaspegsMr. Kaspegs是国际AI风险设计的总统在国际管理创新设计中专注于发展新的管理方案以解决目前和未来的AI关系的国际问题Mr. Kaspegs有超过25年的经验在国际和国际的公共政策方面工作最早期是OECD的设计设计总统之前Mr. Kaspegs在加拿大政府的各种职位中都在工作Duncan很高兴能与您一起来到这里请坐谢谢我们今天来谈论国际AI风险设计的规划加速国际协调以确保有利安全和兼容的人工智能我们开始我们看到的是风险作为一个详细的详细我认为非常重要正如很多人今天提出的我们要明白我们需要发展管理方案的机构现在不是我们今天看到的AI我们可以看到未来的一种非常非常不同的AI而且我们需要虽然AI已经受到今天的热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈热烈但是有原因认为这种改变这种发展可能会更快发生比我们期待的更快发生比我们必须准备的更快发生在这种不确定的情况下我们必须做的作为公共政策人员作为公共政策的负责人是要准备最困难的情况包括这种发展可能会更快发生我们的目标我们的目标是对互联网合作去解决互联网的互联网的全球性挑战现在很多的互联网的管理问题可以和应该的在国际层面提出这是我们能够尝试和学习的方式对的但有些问题是真正的全球性挑战而且会需要互联网的互联网合作所以我们在我们的报告中集中了三个问题我会在数字中解释每个问题第一个是要实现和分享互联网的全球性挑战现在公司业公司企业会发展大量的互联网的互联网合作在他们自己的手上没有政府的参与或制定但有些元素我们会需要政府的参与尤其是国际互联网的互联网合作尤其是互联网的互联网合作互联网公司的互联网合作尤其是互联网的互联网合作接下来的全球性挑战是减弱全球性互联网的风险这些风险可能会对整个人造成伤害他们会跨过国际这些问题你不可以只靠你自己的家庭保护自己的民族为了保护自己的民族你必须与其他人合作建立机构建立机构保护整个人的安全这是我们看到的许多风险的图表我们已经看到了很多的我们现在面对的AI我们正在谈论的许多风险在国际层面更加影响我们在国际层面从个人到国际到全球的构图适当地挑战最坏的最坏的在重力上我们在这里正在专注的是在在你右边的特别是无法控制的超级智慧和武器用途或错误的这两个风险武器用途或错误的控制是最严重的而虽然它们似乎似乎很惊人而且几乎是无法想象的但是我们听过很多的专家今天一整天他们足够适当我们必须为他们准备我们必须对他们专注我会跳过那些风险的原因我们稍后再来但是第三个全球挑战主要是能够实行有效的决定决定将来的未来的无法控制智慧的影响影响全人类所以这意味着有效的决定决定我们刚才讨论的全球利益和全球风险但更加重要的是这是关于什么样的无法控制智慧我们实际上想要的什么在什么情况下我们想将无法控制的是否能够实行的有效的决定决定这些是需要反思和共同的决定这些决定决定是不应该并不应该做的决定在一个小数量的智慧企业中对如果这些是我们面对的挑战我们会面对的解决方案如何一起解决这些问题我们已经看到了很多非常印象深刻和可靠的努力在互联网的全球和国际协调在互联网上但是这些努力仍然似乎不够准备是在讨论今天的互联网的影响而不是我们将面对的互联网系统在未来五年或可能三年或两年之内而这是我们需要的一个预期的方案我们可以准备面对不仅是我们期望的未来也需要面对的最困难的情况我们认为我们需要新的框架和新的机构为国际协调实际上能够使合作尤其是在最压力的问题上我们需要能够谨慎选择适合广泛代表共同辩论但是我们也需要在需要的情况下在时间上做到这件事所以我们正在讨论的是全球AI挑战框架讨论协议正如你们所知框架讨论协议是一个非常适合和适合的方案它让我们寻找迅速的全球协调在高层层高层层的目标和理念上我们作为人们所在意的最重要的最想要的是人工智能的未来发展但是它也设定了一个方案以专注和组织特定的计划去解决一些特定的问题一些最困难的问题所以这是一个这样的显示它提出高层层的目标与理念的例子然后我们有这三个大框架大框架我之前提到的利益风险和独立的决定然后我们可以想象许多的计划在哪里真实的详细解决我们如何完成这些所以我现在要专注的是公共安全计划在全球公共安全和安全风险来自人工智能所以这里如我之前提到的我们在专注在武器化和失去控制风险并不是因为这些是唯一的风险而是因为他们有可能非常嚴重和很高的不确定时间线所以如果我们真的不知道我们能否面对这些风险那么这意味着他们是紧急的这意味着因为我们可以面对它们很快我们必须要准备至少有机会面对它们很快因此这会是一种计划向前移动非常快可能在同时与更广泛的规划合作这是一种能够与最重要的AI能力互动首先然后再到全球我们可以与它们在不同的循环和模式接触当然这也是我们能够找到与其他计划相同的计划所以这个计划的目标是很简单的让我们解决AI的危险与全球公共安全让我们确保人类安全从AI能够与我们发生的可能性的危险相同所以在计划的中心是一个级别的风险方式这根据根据不一样的AI系统的理解不一样的AI系统发生同样的危险我们想能够与AI相同而无需过度规划我们想要最少的规划让我们能够继续完成工作所以如果我开始第一层这就是AI系统我们会测试遭受罪犯的危险罪犯的全球危险就这么说而这些系统在国际水平上可以被控制因为它们并没有对其他国家有大规模的境界危险第二层是我们会称为AI系统出现管理的危险它们有足够的重要性我们不能只让它们不被管理它们必须被管理管理或许是在每个国家中设计我们必须在每个国家中确定每个国家必须符合某个标准我认为罗伯特刚才提到的一些部分在标准设置是非常重要的在这里接下来有第二层就是系统是有点太危险让公司或政府自己去执行即使是被管理的这些系统可能会对人类有很大的危险我们只能舒适地发展和测试这些系统在共同的空间我们可以联合如何发展它们我们可以把它们放在最好的资料保险它们被发展的最有信心在最安全的状态之中这些系统这些系统有时候被称为AI的Cern但是是综合研究室然后第四个最终的层次就是AI的系统被判定为非常不可靠的危险如果这些系统能够在近距离或者能够发展答案就是好的我们必须不建立这些系统直到安全方法能够起来我们不会禁止它们从今至今但我们必须确保在世界任何地方没有人建立这些系统直到我们都知道它们会安全现在有很多组织这种系统的机构等等可能需要支持这些系统非常简单的一个协议帮助对这些问题的政治决定一个委员会实际上做科学研究一个机构设定标准监控防护研究室如我刚才提到的以及某种执行现在我先想要专注一些重要的障碍在这里当然我设计的并不是今天应该接受的这是真正的考虑应该需要的设施即使世界人民和世界政府有一个醒醒的时刻突然说哇我们面临AI像我们没有见过过我们需要什么才能实际地解决这些问题但是当然这会非常复杂所以这就是为何这会非常难为何合作为何国际合作这件事无法实行我每天都听到这些这些都是很重要的但同时我们也许会发现我们必须要遵守这些问题以确保我们自己的健康和生命通过这个转换成为非常能力的AI因此我们需要的是方向如何虽然所有这些挑战都能够实行我认为这里有几个但有很多我们需要一起发掘的更多的要求不可预期的精神和合作去成功但我相信我们可以所以总结我们必须要现在准备非常能力的AI的可能性全球合作可能会是最重要的这会需要不可预期的精神挑战很大我们都需要这样非常感谢如果您有兴趣这里是我们的讨论文件是用来引起讨论的我们非常欢迎您的评论在这里以及在电邮中在电邮中在屏幕上以及在文件中这样非常感谢谢谢大家",
    "谢谢谢谢你",
    "非常感谢对于我个人有意向感谢以及这次的竞选与优价的同事我抱有感谢刚才的杨维也感谢嘉宾和云莱杨维来到舞台上让我介绍一下他西衡先生是国际和平的传媒博士他的研究涉及中国AI ecosystem和全球科技趋势他的作品出品在《外交事务》《博士》《博士》《博士》以及其他出版公司我们很高兴能与马兹先生以及其他参与者参与这一会儿请坐下欢迎大家来到今天的最后一堂马兹先生这是你第一次上台让我先开始我们听了很多我们的参与者关于国际协调的重要性在AI安全上但在您的外交政策文章与Tino关于AI是赢得AI的竞争您谈论了AI发展的竞争方面的方面我认为在华盛顿有个常见的问题就是谁是赢得中国AI的竞争以这种与中国的国际竞争的环境与Tino和丁子耀也很坚定地在他们的言论中和他们的跨界谈论中您能解释一下为什么国际协调在AI安全上是重要的以及我们如何平衡这些两者之间的协调和竞争的关系非常好非常感谢感谢Cancordia为我参与为我们所有人参与我可能会从这方面来看我认为今天有相反的观点关于美国和中国当我们进入非常强大的AI系统时是否有可能美国和中国会有能力在任何程度上协调或者是国际竞争太深了吗那些竞争的运动太强了吗我认为我们的国际的AI系统可能会是未来的国家主要力量他们可能对的也可能错的但当你看到这个是将来的国家主要力量而你看到其他国家是你的竞争者在每个方式上协调是非常困难的我认为让我感到兴奋的事情或是我看出的方向是我想想的是相同的安全而不是一种一种合作安全的东西我认为我们在谈论的很多事项是在某一天,领导会坐在一个非常高层的位置,他们将举行协议,然后我们将批准这个协议在两个系统之间。我认为,我并不在意这一点。我更期望的是,我们可以有一个更高层的方式来建立安全系统。在中国,会有政策人员,会有技术人员,会有研究人员,在中国为自己的原因而推动AI安全。他们将与美国和国际人员谈论最佳的实施。但是我们将在我们自己的环境中同时做这些事情。我们不会在每个步骤之中坚持同意。我们将建立安全实施,通过谈论,政策谈论,技术交流,联合研究。或许在几十年后,我们将建立这种安全实施,我们将建立一个高层的协议,因为两个国家已经很接近这个目标。两个国家已经很投资在AI安全为自己的原因。然后,我们将建立一个可能的协议。但是,我认为,我们必须在前段时间下设立基础。我认为,这个基础是通过二维研究、科学交流。谢谢你,Matt。",
    "我想继续谈论这个竞争的主题。接下来,我们将谈论Irene。Irene,在你的演讲中,你谈论了科学、智能、智能发展和智能安全的重要角色。最近,我们看到企业,例如智能智能,在某些国家的API接触中,限制了智能协议与联合协议的关系。你如何看待国际智能模式的共同化、并且智能协议与智能协议的关系,在这种联合协议的压力上?谢谢你,Kuan-Yi。我认为,在设计上的模式中,我们可以找到共同化的位置。不所有安全方面,都会与不同地区不同,特别是在美国和中国之间。我希望,再次,我希望,我希望,我希望,特别是在美国和中国之间。penso 这里有很多关系。老师,我希望,我想,我想,我希望,我想,我想,我想,我想,我想,我想,我想,我想,我想,我希望,我想,我想,我想,我想,我想,我想,我想,我想,但也能够一起建立评测,看看表现如何在多个不同语言中,如何相互保护。还有商业方面,我专门从研究的角度来说,但我不能与其他公司的商业决定,这可能会不同,根据法律和法律。谢谢Irene,这不是一个简单的问题,你做了一个很棒的工作。",
    "现在,我们来调节点,谈谈AI管制的国际机构。宋教授,作为一名AI高级领导体的一员,您有独特的意见关于在宇宙中的职业角色。您能否分享您的看法对于宇宙的职位,在形成国际AI管制,尤其是,您如何看到我们如何平衡宇宙国际机构的承诺,使得美国提供的机构,以及它们经常遇到的挑战,以及它的程度和过程的速度。我们如何确保国际机构,基本上,是复杂的和有机的,以保持我们今天谈论的许多我们的谈论者的快速AI发展的速度?谢谢。谢谢,我觉得这是我们必须面对的重要问题。",
    "我想,在AI的国际机构领导体之前,已经发生了很多地区,例如OECD,EU,以及AI的国际协调,这些互联网只有40个国家联合,而其余的160个国家被世界忘记了。所以,我认为,国际机构的联合作用,并不是为了创建国际AI管理,而是为了创建国际AI管理的国际网络。而是为了让所有的国际网络,在国际级的行动中,尝试自己的行动,并找出错误的地方,以连接所有的连接。所以,这是我认为的原因,我认为,在几天前,美国国际协议审判,它说,美国国际协议审判,应该执行国际AI管理的核心任务。我认为,核心任务是,实际上,执行国际网络的责任,并且让所有人都能够使用,而不是让所有人都不能使用。另外,但是,我们要看到的是,这些原始的网络,他们在今年和接下来,他们想尝试取消美国国际网络的任务。他们想取消美国国际网络的任务。我不是说,它应该是美国国际网络。我只是说,当我们看到所有其他网络的观察,当我们看到所有其他网络的观察,他们就没有任何的核心任务,能够在国际上有信心。所以,我认为,美国国际网络是最有信心的平台,能够将所有人都联合起来。我认为,中国已经参与了一些地区网络,比如说,AI安全会议,以及军事AI的重新设置。我认为,我认为,我认为中国会感谢,能够参与。但另一方面,我们看到所有的网络的限制,他们有自己的重点,他们有限制,他们没有权力,能将所有人都联合起来。这不是他们的任务。那么,那么,在这种情况下,我认为,不遗留国家,你必须要做,不遗留国家的方式。所以,使用美国网络,你将所有地区网络都联合起来,联合起来,能让他们联合起来,而在另一边,那些地区网络,请不要阻止美国,因为美国正在联合,并没有阻止你们。所以,我认为最健康的方法是,联合起来,让他们有一个处理委员会,联合联合国家的所有地区网络,让他们联合起来,让他们联合起来,联合起来,联合起来,联合起来,联合起来,联合起来,联合起来,联合起来,联合起来,联合起来。所以可能他們做了不太好的工作在他們面前有很多的問題而在另一邊,我們沒有其他選擇比他們更加有信心所以他們目前並不是最好的請幫助他們成為最好的請使用這個平台解決問題如果聯合國總統不成功在一些問題上聯合國總統委員會會使用聯合國總統委員會提供的決定請問聯合國總統委員會請他們做些什麼例如現在聯合國總統委員會的決定中國主席的決定與140個國家合作請聯合國總統委員會提供的決定提供的決定提供的決定現時的低及中資金家現時的低及中資金家操作AI操作AI找對這些問題的解決方法找對這些問題的解決方法並提供的決定並提供的決定以至於明年我認為這是總統的所有的要求我認為這是總統的所有的要求用過在這項議論中用過在這項議論中這是使人們使用聯合國總統委員會作為最大的成功的方法最有益處的平台去做所有國家需要的工作我可以提出一點嗎, 桓義我要強調一下, 即使沒有支付器聯邦聯盟作為一個通訊網絡也能幫助我們與其他人相熟我曾經與詹教授見面在早期聯邦聯盟工作前這是我第一次與一個在這裏工作的人合作我覺得這非常重要確實, 謝謝教授鄭教授和艾琳教授我覺得教授鄭教授提到的網絡方式很適合羅伯特的概念是你提到的國際社會環境在你的演講中羅伯特, 我想聽聽你現在的看法建立一下教授鄭教授的看法在你的演講中, 你提到國際標準和報告制度對AI安全你能否解釋一下這與這個有何關聯?比如你提到的國際社會環境以及聯邦的作用在這些方面是的, 絕對的好的, 非常好是這更好嗎?是的, 這更好謝謝你的問題我會很高興地談論這方面首先, 我想說我對這方面很深入的思維所以我非常關注這方面的研究我的看法是,國會的角色是有重要的角色我們必須找出這些角色在這個環境中,正如你解釋過的,就是一個經濟系統所以我認為,國會已經在做一些重要的事例如,在發展方面我認為在我提到的討論中我提到的ITU和UNESCO以及其他國會的領域他們已經在做一些標準設定的事他們知道他們必須建立更多研究能力繼續做我們必須做的標準設定而且他們在發展的角色這非常重要我覺得我們必須例如,我們必須做更多的事情在發展方面我覺得UNESCO是一個很自然的地方你可能會想到一個負責操縱的操縱許多社區的主要障礙在進步AI中參與的社區其實是在進步AI中在他們自己操縱的領域中他們自己的文化空間中我們可以想到的方法是很多世界大部分的空間有很高的手機侵入所以我們可以用這個方法來進行數字化但我們必須做的方法不需要隱瞞隱私不需要符合這些社區的標準所以我們必須做得非常小心但是這就是我們可以看到的我們可以看到的世界的大部分的努力在進步AI中以提高發展成果其他一些事情我認為有所提及例如AI的IPCC是一個有意義的事情在UNESCO上一些標準設定的領域我相信我們可以找到其他領域看起來很自然與UNESCO的領域合作的領域我認為有所提及例如會議這是非常重要的所以我們可以找到這些領域我覺得這應該是我們在UNESCO的領域的角色的主要重點我可以再加一點關於不同的UN系統嗎現在你看到的就像是Robert所說的不同的UN系統已經做了一些有關AI的事情獨立的我們有一個我們在幾個月前在吉尼巴發表了一個無線會議我們參觀了幾個不同的UN系統在吉尼巴是由聯合國組織的組織我們看到的是這些不同的UN系統主要是他們當然看到樹木但有些時候他們錯過了他們看到樹木但錯過了樹木也不通過所以他們還是把樹木形成樹樹在吉尼巴省的根本就是一個無線的不通的樹木所以就像剛剛所說的我們看到的在吉尼巴發表了一個無線的無線 appreciate就是inos系統的獨立的都是形成樹木只要有多個樹木就可以得到這種量度可以讓你的方式可以讓你讓你對著獨立的樹木有什麼理由你們可以完成這個發展就有什麼理由的就是有很多更多,所以美国联合联合公司必须在现在相比起现在,连自己都要组织更好的组织,所以会是联合联合公司的责任,帮助美国联合公司有更好的组织,让每个人都能够在美国联合公司的系统中辅助,并不仅让每个人辅助,还能够辅助每个人,并解决问题。也有一个例子,美国联合公司在美国联合公司的系统中辅助,但美国联合公司和其他国际辅助组织,例如IEEE,他们并没有足够有效的组织组织。因此,美国联合公司的联合联合公司,需要联合公司的会员联合联合联合,以便接触那些陆统的担任联合公司,作为帮助来举办多类联合公司联合 mini Stroma一些设施。我们可以通过相联合公司通过其他组织那些团体联合公司做一些联合公司,并团 со一些相联合公司生产这一切特别的运势业。不仅讨论联合公司的某些组织,在聯邦制度中的標準組織之中謝謝教授Traeger和Zong教授我們時間很短所以我想先結束我們的討論請問最後一個問題您所有人都表達了國際智慧管理的成功和勝利的想法我希望聽聽每位主持的意見您認為國際社會應該做的一件事在未來6-12個月內達成這個目標例如在未來的聯邦議會會議會或是法蘭斯 AI 活動會議會或其他會議我們想實現這個討論所以我只想提供一個建議可能Duncan因為我們還沒有聽到您的意見我們可以跟您開始好的 謝謝我覺得我的一個訊息是要幫助準備未來我們需要幫助我們的社會無論是在聯邦議會或是在支持網絡的網絡我們需要想想現在我們需要幫助我們的社會能夠成功通過我們見過的最大的改變我們見過的最強大的技術我們需要現在思考現在設計現在設置我們需要的各種社會我們需要幫助社會在未來的發展中在大多數的事件中我覺得我們面對的其他非常嚴重的不確定度我們必須要著重預期我們需要建立的話題它們可能需要的 among certain scenarios所以那些個案會期間預備討論了很多關於如何將科學家將共同的理解建立相同的理解關於 與技術有關的危險 和挑戰這件事十分重要就是為了將社會科學家和各種不同的觀點聯繫在一起,並為我們提供所謂的管理機構和機構的機制我們需要這些機構幫助人類在這段時間內進行進展讓我們可以在500年內回顧雖然我們需要在這些問題上合作,但我們做得到這些科技使我們能夠合作而且我們成功地做到了謝謝,DuncanIrene,我們可以讓你下一位你可能也想探索一下研究中心和發展企業的角色你剛才提到的絕對的,這就是我剛才所說的我的極為嚴重的反應是,我們可以休息一下我很累,我知道我沒有獨處而非常嚴肅的反應就是Duncan所說的關於正確的專業一個很重要的原因是我工作在Hugging Face這是我第一次看到一種我的傳統語言Bangla在一個數據中研究確保我們包括不同的群組不同的技術研究員並且解決一些不明顯的技術犯罪但非常基於歷史、文化以及世界不同地區的問題謝謝IreneMatt,我們可以讓你下一位在這個問題上好的好,我第一件想要提問的事在接下來的6至12個月內我認為是美國和中國會進行某種類型的合作評論在具體的大規模的危險上這將會非常複雜我們要想辦法讓兩邊都覺得安全我覺得在政府層面上這可能會太難了最好是在AI安全基礎上或AI安全基礎上做這件事這可能在這段時間太艱難了但如果不在2階段上做這件事有中國最好的技術評測員以及美國最好的技術評測員在美國的安全評測員中一起談談他們使用的方法他們所看到的問題以及他們將要做的方法謝謝Matt當然,我們也提到在這個討論中AI安全測驗是一個非常重要的項目非常重要的項目以及有可能的國際聯繫項目可能Robert我們可以回答你這個問題你希望未來的6至12個月內你希望看到的事物發生什麼呢我認為我剛才講了一些特別的事情所以我可能可以以這機會反映之前提到的一件事以及說一件事就是我認為在這個擴大的環境下我們有很多例子當他們認為要做的事情是重要的時期雖然你不會預測它會發生但你仍然需要嘗試讓它發生找出它能發生的機會例如在這方面的科技管理特別是AI管理美國民主黨和共和黨在這段時間之內並不知所措但他們仍然在議會議會中尤其是進行進展AI管理這是一個例子《非核心合作協議》是兩國共和黨結合的時期因為美國和蘇聯當時的關係不太好但無論如何兩國共和黨結合他們仍然做得到因為他們認為它是他們的共同利益他們認為它是重要的所以你不會預測這些事情會發生但你仍然需要找出它們能發生的機會謝謝Robert我們很高興能夠結束這堂會就請教授鄭教授最後一個問題接下來的12個月我首先要告訴你什麼已經發生了在這8個月前我已經不知道雷明河在這8個月前發生了什麼事所以我去過新加坡的亞洲技術會但是不是雷明邀請我來所以我責怪雷明然後我們結合他來自政府所以我是從一個研究組織來的所以我可能在正常情況下也不能跟雷明討論然後在這8個月之內我們成為了一個團隊雷明對我非常好他在新加坡提供了非常好的亞洲食物他也告訴我新加坡的練習但不僅如此他和所有的總統和副總統在這個大型的亞洲技術公司都結合了還有很多曾經的政策人員都結合了所以我看到了所有人的價值都在結合你們不明白對方你們說我們完全不同但當你們結合你會覺得OK 不太大分別然後你們有安全問題你們有發展需要我們解決問題吧因為相比於去年現場的風險已經有10次相比於去年全世界的風險你會看到風險重新發生在不同的國家然後你會看到聯邦制度的限制如果我有錢還有人我會把所有的錢都給聯邦創造一個國際機構但是我沒有錢所以在下個12個月之內最重要的事情就是起碼要創造像是歐洲的Train有歐洲的AI服務現在我們應該有歐洲AI服務最小的Train將所有的歐洲機構全體組成並於這麼好的方式來操縱他們還有並且討論這些地區網絡,例如OECD,IEEE和ACM所以我認為我們必須移到這個範圍因為在AI中的聯邦諜報組中,我認為我們看到了這件事的需要沒有這些需要,我們就沒有機會把這個國家遺棄謝謝宗教授,把這個國家遺棄,這是一個非常吸引的想法宗教授在這篇討論的開始時,宗教授提到我們目前沒有知識方法以避免存在任何失用或失控AI系統的糟糕危險是很明顯的,前進的路會很困難和複雜但從這篇討論中,我們聽到的是共同的承諾確保AI的發展能讓人類成為一體的利益請加入我們,讓我們鼓掌鼓勵一下我們的主席主席,請坐下我們的技術人員,請來幫忙坐下我們的技術人員,請來幫忙坐下好的,謝謝好的,為了結束我們的討論我們很榮幸能讓教授周博文在今天的最後言論中發表言論我們有請上帝博士,讓教授提到上帝博士周博文是上海AI研究研究所的主席上帝博士是上海AI研究所的主席而他在廷華大學當學的主席他曾經是上帝博士在JD.com作業的電商巨星他曾經在JD.com作業的電商巨星在JD.com作業的電商巨星當時他擔任上帝博士的副副副副副副副副副副副副副JDAI ResearchPrior to thatProfessor Joel held various technology leadershipand executive positions at IBMincluding as the directorof the AI Foundations Labat IBM ResearchProfessor Joelhas decades of experience in AI researchand is a recipient of the prestigiousWu Wen-Tsun Award for OutstandingContributions in Artificial IntelligenceProfessor JoelThank you so much for being here todayThe floor is yoursThank you for having me hereand more importantlythank all of you for staying lateI was asking to giveconcluding remarksmeaning this is the end of thisfabulous forumbut I'm thinking this isnot the beginning of an ongoingcontextual dialoguein that spiritI'm thinking to give the talkI was given yesterdayas an open ceremonybecause I was told many of youwere not there in personso bear myswitching to Chinese nowso if you don't speak Chineseplease time to put your translator onSo the topic I'm going to have todayisI think we need something newintegrating tech and governanceWith that I call itartificial intelligence 45 degree of balanceright nowas a big modelthe raw energy of artificial intelligenceis developing rapidlybut with the constant improvement of capabilitieswe're seeing more and moreof potential risksfrom the public's understanding of AI risksin order to look atthis includesdata leakagedata abuseintimacycopyright disputeI call it data content riskand the other is the malicious use of data带来的伪造虚假信息等带来的使用风险当然也诱发了偏见歧视等相关的伦理问题还有人担心是否会挑战就业结构等社会系统性风险当然在好莱坞的电影里面也出现了AI失控人人完全丧失自主权的这种极端风险这些风险有些已经是在现实中出现更多的是潜在的防范这些风险需要各界的共同努力需要科学社区做出更多贡献去年5月份数百名AI科学家共同签署的Statement of AI Risk也表达了对AI风险的相关担忧并呼吁防御人工智能的系统性风险应该和流行病和核战争等大规模风险一样成为全球共立的优先话题从一个我做技术的角度来看出现这些担忧的根本原因是目前的AI发展是失衡的我们来看目前AI的发展趋势横轴是AI Capability重轴是AI Safety在横轴上在以Transformer为代表的基础模型架构上加以大数据大参数量和大计算量的Scanning Law目前AI的Capability正在快速的成指数去增长与之对比在AI安全领域我们看我们有什么典型的技术比如红队测试安全标识安全复栏与评估评测等等都呈现出是离散化碎片化很重要的是Ad-hoc非常的厚实当然最近出现了一些新的技术兼顾了性能和安全性比如监督式微调SFT人类反馈的强化学习ROHFRAFSuper Alignment等等这些方法最主要的特点是把人类的偏好传递给大模型也涌现出了比如TRAD GPT-4等令人兴奋的AI系统以及我们上海AI实验室的舒适系统也会提升intent大模型等等虽然这些技术瞄准的是安全和性能同时提升但在实际使用中大家往往发现更多是性能优先所以总体上我们在AI模型的安全能力的提升还远远落后于性能这种丝痕导致AI的发展是跛脚的所以我们把它叫grip的AI但是这种不平衡的背后实际上是两者投入上的巨大差异从右边的对比大家能够看出来两者在技术研究中技术上是否体系化人才的密度上商业驱动力方面以及算力的投入度方面对比来看安全方面的投入是远远落后于AI能力的我一直在呼吁要加大对安全的算力的投入我举的例子就是说你AI systemlike a little child当小的时候你可能花大量的算力去帮助他吃好喝好衣服穿好但是在孩子慢慢graduate upgrow up的时候yes but more time去跟他你更多的焦虑不知道他是不是吃好喝好的时候去跟他做各种价值的交流这种价值的交流的投入实际上就是算力的投入但是很不幸的是我们大部分的算力都投入在预训链上很少很少也不必用在安全上所以这种投入的失衡导致了我们现在Clip的AI我们真正需要追求的我一直在讲的是包括从美国到中国我的学术生涯一直在追求的Trustworthy AGI也就是右上角这个路线这是我们的新城大海我把这叫做可信AGI如果我们找到兼顾安全和性能我们需要找到AI安全优先但又能保证AI性能的长期的发展的技术体系我个人把这样一种技术思想体系叫做AI45度平衡率AI45 degree lawAI45度平衡率是从长期的角度来看我们要大体上沿着45度安全以性能平衡发展所谓平衡是指短期内可以有上下的波动但长期内不能长期低于45度如同我们现在也不能长期高于45度这将阻碍发展与产业应用这个技术思想体系它是强技术驱动全流程优化所谓全流程优化我在23年的一篇Trustworthy AI的中述文章里面在ACM Competence Survey上发表提出是要把全流程从数据的准备模型的训练到部署之后的Operation和运营全部从安全的角度来进行优化同时也需要多主体参与我想这是刚才那个Forum讨论的很多的话题当然也包括敏捷治理实现AI45度平衡从技术角度来讲也许存在很多的路径我们上海AI Lab最近在探索一条以因果为核心的路径我个人把它取名取为可信AI家的因果之梯这也是致敬因果推理领域的先驱图里奖得主Judy Appel可信AI家的因果之梯我们把可信AI家的发展分为三个阶段分别是犯对棋可干预能反思犯对棋主要是包含了当前最主流最前沿的人类偏好对棋技术像我们前面提到的LHF但是需要注意的是这些安全对棋仅依赖于统计相关性而不是真正的因果关系这样可能会导致错误的推理以及潜在的风险一个经典的例子是巴普洛夫的狗当狗仅仅记忆铃声和食物的相关性形成条件反射时它可能在任何场合听到铃声都会触发它的行为这里这个行为是分泌堕叶但如果把它想象成这个行为是金融转账医疗决策甚至是军事相关的决定这显然是极其不安全的所以我们需要第二层过来叫做可干预可干预主要是通过对AI系统进行干预探究其因果机制的安全技术比如能在回路机械可解释性也包括我们刚刚提出的对抗演练adversary rehearsal它可以通过提高可解释性和泛化性来提升安全性同时也提升AI能力能反思在第三层则要求AI系统不仅要追求高效自行任务还能审视自身行为带来的影响和潜在风险从而在追求性能的同时同时确保安全和道德的边界不会突破这个阶段的技术包括value audit的training记忆价值的训练因果可解释以及反思时推理等目前从业界的技术发展来看AI的安全和性能技术主要停在第一阶段部分在尝试第二阶段要真正实现AI的安全与性能平衡我们必须完成必须完善第二阶段并攀登至第三阶段也就是说沿着可信AGI的因果之积而上我们想象可以构建真正的可信AGI实现人工上的安全与作业系统的完美平衡Ultimately最终我们是希望像安全可控的核聚变技术一样为全人类带来清洁丰富的能源我们希望通过深入理解AI的内在机理和因果过程通过安全有效的开发和使用这项革命性的技术也正如可控核聚变对全人类都是共同利益一样我们坚信AI的安全也是全球性的公共福祉需要国际社会的共同努力和合作我们愿意大家一起携手推进AI45度的发展共享AI安全技术加强全球AI安全人才的交流与合作平衡AI安全与人类的投入共同构建开放安全的通用人工智能创新生态和人才发展环境谢谢大家",
    "非常感谢教授为Safe-AGI的创新生态分享你的吸引力今天很高兴有你今天的会议目的目的是为人类的利益提升AI安全的状态我们组织了四个主题AI安全研究AI安全评测AI安全指导和国际联合协调我们邀请了超过25位伟大技术家在世界上举办了超过18个课堂和5个课堂超过8个小时我想提供一段简单的便当和思维的insight所有的说家为今天提供的AI安全研究许多话题支持AI相关的科学认解并安排AI机械方向和安全特别在中央关系有几位认定将AI安全投入为国际公众利益包括10%的AI负面设施在中国开始对AI安全认定叶格莎AICV 測試, 進行全生命測試, 進行新的方法和科學化大型模式測試。AICV 指導者, 社會需要平衡AI 發展和AI 操控的重要性, 特別是在發展國家的問題上。我們也應該考慮一層性的操控方法。在不同國家和不同區域中, AI 操控方法的相關方法, 我們互相學習了這麼多。最後,在國際合作方面, 多位專業人士提出了AI 安全上的紅線要求, 以及維持分別和開放性。透過持續的合作和討論, 世界應該努力對AI 國際研究院和國際議會進行研究。正如教授說的,今天,AICV 的討論會只是討論的開始。我希望今天的討論會給予各位聽眾和觀眾更多的想法和機會, 為人類的重要問題合作。特別感謝今天論壇的各位嘉賓和朋友們。今天論壇圓滿結束。安全AI希望本論壇可以進一步推動前沿AI安全與距離的討論和行動。期待和大家再見。謝謝。Thank you.請今天的嘉賓留步。我們一起再上臺合影。謝謝。謝謝。"
]