## “AI+领航 智绘未来” ——智能计算与强化学习论坛

### 引言

随着科技的发展，智能计算已成为推动各行业发展的核心力量。无论是电网、交通、金融，还是先进制造、供应链、工业，智能计算都在发挥着越来越重要的作用。此次论坛旨在探讨基于强化学习的智能算法的研发历史与最新进展，展望未来智能计算的发展方向及其应用前景。

![论坛封面](https://static.worldaic.com.cn/IMAGE2024/2024-06-20/40b82ee502d8424fb82f40f70c9f727f.jpg)


### 议程安排

| 时间       | 主题                                    | 演讲嘉宾 |
|------------|-----------------------------------------|----------|
| 14:00-14:05 | 论坛开幕                                |          |
| 14:05-14:35 | 求解超大规模数学规划问题的新进展        |          |
| 14:35-15:10 | 智能计算与大模型决策：理论与实践        |          |
| 15:10-15:50 | 生成式人工智能的机遇和挑战及在量化交易中的应用 |          |
| 15:50-16:25 | 生成式人工智能的应用：面向生成优化的引导扩散模型 |          |
| 16:25-16:55 | 人工智能和数据科学的经济展望            |          |
| 16:55-17:00 | 论坛闭幕                                |          |

### 嘉宾介绍

1. **叶荫宇**
   - 公司：斯坦福大学
   - 职位：管理科学与工程系及计算数学工程研究院李国鼎讲席教授

2. **葛冬冬**
   - 公司：上海交通大学
   - 职位：智能计算研究院院长

3. **王梦迪**
   - 公司：普林斯顿大学
   - 职位：电子计算机工程系和统计与机器学习中心终身教授

4. **苏炜杰**
   - 公司：宾夕法尼亚大学
   - 职位：沃顿商学院终身教授

5. **陈溪**
   - 公司：纽约大学
   - 职位：斯特恩商学院Andre Meyer冠名终身教授

---

欢迎大家光临2024世界人工智能大会智能计算与强化学习论坛的活动现场，非常荣幸能与大家共同见证智能计算与人工智能技术的新进展。本次论坛由世界人工智能大会组委会、上海交通大学智能计算研究院、山树科技联合举办。我是主持人曹依然，谨代表本次论坛的全体组委会对各位的到来表示热烈的欢迎和衷心的感谢。

当前，智能计算正成为时代的刚需。电网、交通、金融、先进制造、供应链、工业软件等重要行业对智能计算的需求剧增。本次论坛旨在深入探讨基于强化学习的智能算法的研发历史与进展，展望智能计算的未来，探讨如何与优化技术、GPU计算、强化学习、统计等方法相结合，以及其在全球范围内的领先应用。

接下来，请允许我隆重介绍出席本次活动的嘉宾。他们是斯坦福大学的李国鼎奖习教授；上海交通大学访问奖习教授叶英宇，欢迎您；上海交通大学智能计算研究院院长、山树科技联合创始人、首席科学家葛东东，欢迎您；纽约大学斯特恩商学院中生教授、计算机科学和数据科学中心教授陈希，欢迎您；普林斯顿大学中生教授、AI创新中心教授、机器学习研究中心主任王梦迪，欢迎您；宾夕法尼亚大学沃顿商学院中生教授、机器学习研究中心主任苏伟杰，欢迎您。让我们用掌声再次欢迎所有到场的嘉宾和观众朋友们。

接下来让我们进入论坛的主题分享环节。首先，

### AI与数学优化：求解超大规模数学规划模型的新进展

各位来宾，大家好！今天我们有幸邀请到斯坦福大学的李国鼎奖旗教授、弗诺依曼理论奖得主、优化运筹领域的领袖学者叶英宇先生，为我们带来主题演讲《AI与数学优化：求解超大规模数学规划模型的新进展》。让我们用热烈的掌声欢迎叶教授。

谢谢大家，我很高兴能在这里和大家分享我在AI和优化之间关系上的一些心得体会和研究成果。感谢这次大会给我这个机会。首先，介绍一下我自己。提到弗诺依曼，不知道大家是否了解，他是计算机领域的顶尖人物，计算机的创始人之一。实际上，弗诺依曼对AI也有很大贡献，特别是在数字经济和均衡理论方面，我们依然在引用他的思路。

今天我想跟大家介绍一下什么是数学模型和规划，有时也叫优化。提到数学规划，不得不提到几位重要人物：Coopman和Kantorovich。他们在上世纪30年代和40年代发明了线性规划或线性优化，并因此获得了1975年的诺贝尔经济学奖。他们得奖时还邀请了George Dantzig，他提出了单纯型法（Simplex Method），用于求解线性规划问题。Dantzig的贡献在于，他认识到数学模型如果没有算法支持，只是纸上谈兵。因此，一个数学模型或大模型不仅需要数据和模型，还需要算法。

一个数学规划通常包含三个要素：数据、变量和目标函数。目标函数可以是最大化或最小化，还有约束条件。例如，背包问题：给你一个5公斤的背包和5个物品，每个物品有一定重量和价值。我们需要决定放哪些物品进背包，以满足重量限制并最大化背包内的总价值。这种问题需要用数学优化的方法来解决。

近年来，随着AI的发展，数学优化的重要性更加突出。例如，在AI训练中，神经网络的参数调整就是一个优化问题。我们最近的研究表明，使用ADAM算法可以显著提高训练效率。通过优化，我们可以将原本需要几个月的训练时间缩短到一个月。

优化在各个领域都有重要应用，例如航空航天、载人飞船回收、电网调度等。优化求解器在这些领域发挥了巨大作用。例如，南方电网已经使用了优化求解器来进行电力调度。

过去，我们在优化求解器方面依赖国外技术，但近年来，我们在国内也取得了显著进展。例如，华为在2019年停止使用IBM的求解器后，我们开发了自己的求解器，并帮助华为解决了问题。我们的求解器不仅顶上了，而且性能超过了原来的IBM求解器。

总结来说，数学优化和AI是互补的关系。数学优化依赖科学和逻辑，而AI更多依赖知识和经验。通过结合两者的优势，我们可以解决更多复杂的问题，实现更高效的应用。

最后，我希望大家能够更加重视数学优化和AI的结合，共同推动这一领域的发展。谢谢大家！


### 智能计算与大模型决策理论与实践

谢谢叶教授的精彩分享，为我们展示了在求解复杂数学规划问题上的最新技术突破，为该领域的研究和应用提供了宝贵的思路。放眼全球，COPD求解器团队已经基于新的GPU架构开展了一系列开发与建设工作，并将求解器与大模型决策平台结合，形成了一套全新的运筹学教学与工具应用系统。接下来让我们用热烈的掌声欢迎上海交通大学智能计算研究院院长，山树科技联合创始人、首席科学家葛东东先生为我们带来主题演讲：“智能计算与大模型决策理论与实践”。

掌声有请葛院长。谢谢大家，非常荣幸有这个机会来跟大家分享。我这个演讲实际上是接着叶老师的话题继续讲。我们主要考虑两件事情，因为刚才叶老师也提到，实际上我们做人工智能（AI）最重要的事情就是找到落地场景。从十年前大模型（Large Language Model, LLM）兴起到现在，最重要的问题就是这些技术能解决哪些实际问题。

十年前，第一次大数据和AI兴起时，大家就在考虑这些技术到底在现实中能用在哪里。十年过去了，大家看到的主要应用还是集中在视觉识别等少数几个领域。现在大模型兴起后也面临同样的问题。今天早上我还看到很多大模型公司在下一轮融资中遇到了困难，原因就是缺乏好的适用场景。

但刚才叶老师也介绍了，其实在现实中有很多行业需要大量基于数据的复杂决策能力。这些行业直接使用简单的回归或预测模型无法解决问题，因为这些问题过于复杂。例如，物流问题、华为的生产问题等，这些问题的系统非常复杂，有几千万条约束相互制约，单纯依靠AI方法进行预测是不够的，因为这往往会破坏问题的可行性。

因此，我们需要将多种方法结合起来，特别是优化方法和AI方法的结合。近年来，GPU算力的发展也很快，我们需要考虑如何利用这些硬件优化方法。

另外，大模型在处理复杂决策问题方面能起多大作用也是我们在思考的问题。今天我主要汇报两件事情：一是建模，大模型能否帮助我们建模；二是计算，建完模型后如何求解这个复杂的系统问题。

我们在上海交通大学有一个智能计算研究院，主要解决这两件事情。我们开发了一个名为COPT的求解器，目前线性规划方面排名全球第一，非线性规划方面排名第二。这是一个相对成熟的软件，但所有的计算架构一直使用CPU，很多投资人问为什么不能用GPU。这是因为数学优化和高精度计算传统上使用CPU，GPU在这些复杂运算上不友好。

例如，一阶算法、二阶算法和整数规划，这些都不适合GPU。然而，去年底开始情况有所变化。我们成功地用GPU架构解决了线性规划问题，以前需要几万秒的问题现在几百秒就能解决。例如，一个问题从五万九千秒缩短到916秒。CPU加GPU的混合架构使得这种高精度高性能的计算发生了范式转变。

英伟达发布了一个名为QDAS的函数，可以实现矩阵分解，这是所有二阶算法的基础。过去GPU无法实现高速并行矩阵求逆，但英伟达现在做到了，我们测了性能，虽然还不完美，但这是一个很大的进步。

未来的计算趋势是从CPU向CPU加GPU的混合架构发展。英伟达发布的新架构如B200，将数据传输速度提升了30倍，这将大大减少CPU和GPU之间的交换时间，使得计算效率大幅提高。

我们团队在这方面做了很多研究，但目前所有东西都依赖于英伟达的芯片，国内还没有自主的双精度芯片，这也是一个限制。虽然我们在硬件上的差距不大，但在软件特别是底层的数学函数计算上，国内还有很大差距。这是我们智能计算研究院需要解决的问题之一。

我们智能计算研究院的任务是建设这些底层的数字计算算子，以弥补国产硬件的短板，使得软硬一体化。这是一个动态的过程，需要跟随硬件的发展不断调整算法设计。

例如，线性规划、半征定规划和二次图规划等问题，我们都在用新的方法重新设计这些算法，使其适应混合架构的特点。同时，我们也在探索如何用AI方法加速这些算法。例如，通过历史数据分析找到算法中的加速点，或者设计好的分布式算法。

举一个复杂的供应链网络的例子，这个网络有上百万条边，疫情和战争带来了很多挑战，例如如何设置安全库存，提升效率。以前解决两千个节点的问题需要1200小时，现在我们能在几分钟内解决上百万条边的问题。

我们还在研究智能决策中的建模问题。大模型在处理复杂决策时还存在很多问题，例如数据稀缺、问题复杂、数据质量低等。我们正在尝试用合成数据和新算法来克服这些瓶颈，以便更快地在各个行业中应用。

我们的智能计算研究院希望在新的基础计算架构体系中成为国际引领者，定义标准和架构。同时，我们也在研究如何利用大模型帮助建模，例如通过自然语言处理来自动生成模型。

总结来说，我们在计算和建模方面都做了很多工作，目标是解决复杂决策问题，使大模型更好地应用于实际场景。希望未来能与大家共同探讨这些问题，谢谢大家。


### 生成式人工智能的机遇和挑战以及在量化交易中的应用

让我们有请纽约大学斯特恩商学院的中生教授，同时也是计算机科学和数学科学中心教授的陈希先生，为我们带来关于生成式人工智能的机遇和挑战以及在量化交易中的应用的主题演讲。掌声有请陈教授！

谢谢大家的掌声，感谢大家在这么热的天气里前来参加。我今天的演讲会尽量轻松一些，主要是介绍一下生成式人工智能的挑战以及我最近做的一些研究工作，最后还会谈谈人工智能在量化交易中的应用。首先，作为背景介绍，我曾在亚马逊广告组担任首席科学家，因此会结合一些我的实际经历来讲解这些内容。

在开始之前，我想简单介绍一下我写的两本书。第一本是《Beyond AI》，这是一本面向MBA学生的科普书，讨论了AI在不同商业场景中的应用，目前正在翻译成中文。第二本书还未上市，预计今年下半年会出版，内容涉及区块链和Web3，希望成为一本Web3相关的MBA教材。这本书是我与哈佛大学商学院的朱峰教授和耶鲁大学计算机系的区块链专家张帆教授共同编写的。

2023年3月14日是一个值得纪念的日子，因为那天我们发布了GPT-4。这款大语言模型在性能上全面超越了之前的版本，甚至实现了多模态问答功能。比如，你可以给它一张图片并询问图片中有什么错误，它能识别并告诉你。自GPT-4诞生以来，生成式人工智能迅速掀起了一波浪潮，涵盖了语言、文本、多模态等多个领域。

今天我们不详细讨论这些，而是来看看生成式人工智能面临的挑战。首先是情感沟通和隐私保护的问题。举个例子，我经常问GPT能不能劝我多锻炼，GPT会告诉我锻炼的各种好处，但这样的回答并不能真正激励我去锻炼。我理想中的GPT应该能够进行更有情感和意图的沟通，而不仅仅是说教式的对话。这样的对话虽然可能会泄露一些隐私，但这不是我们今天讨论的重点。

第二个挑战是“幻觉”问题。GPT有时会自信地编造一些并不存在的东西，比如虚假的文献。这会对用户造成很大困扰。为了解决这个问题，有一种方法叫“检索增强生成”，它结合了搜索引擎和GPT。例如，当你问GPT一个问题时，它会先去网上搜索相关信息，再将这些信息与用户的问题整合起来生成答案。这种方法可以大大减少幻觉的发生。

第三个挑战是去中心化的验证技术。训练大模型的成本非常高，比如OpenAI在训练GPT-4时花费了7800万美元，Google在Gemini上的花费更是高达1.91亿美元。通过分布式计算，可以将不同地区的计算中心结合起来，降低成本，但这也带来了验证的问题。例如，你如何验证这些分布式计算是否真的完成了任务？区块链技术可以在这方面提供帮助，通过加入一些标志来验证训练过程的真实性。

最后，我想谈谈人工智能在量化金融中的应用。量化投资依靠大量的数据和AI进行交易，减少了人性的影响。通过AI，可以将传统的线性模型转变为非线性模型，并自动发现市场中的关键因素。此外，大语言模型的出现，使得我们对文本和图像数据有了更深刻的理解，进而开发出更好的策略。

举个例子，2015年成立的XTX Markets公司，通过深度学习进行高频外汇交易，交易量超过了九大银行的总和，成为世界第一名。这家公司拥有大量的显卡资源，利用AI进行了量化交易的革新。

AI在量化交易中的应用还包括对另类数据的分析。比如，使用大模型提取新闻中的上下文信息，以预测股票收益率。在疫情期间，通过卫星图像监控沃尔玛停车场的车辆数量，来估计沃尔玛的营收情况。此外，还有一种叫做“聚灾债券”的金融工具，通过对天气的精准预测来决定投资收益，这推动了气象学的发展。

总而言之，AI在金融和运筹学中的应用面临可解释性的问题，需要在性能和可解释性之间找到平衡。最后，AIGC（生成式人工智能内容）时代即将到来，这也契合了本次大会的主题。谢谢大家！


### 生成式人工智能的应用：面向生成优化的引导扩散模型

掌声有请王教授，大家好。今天我主要从大模型技术层面来讨论当前的一些重要问题，同时也想稍微回顾一下几十年前的历史。智能，或者说让机器完成智能任务，这一话题并不是从大模型或生理学习开始的，而是从控制论开始的。控制论是由美国数学家诺贝尔·维纳提出的。在这个过程中，他与香农共同探讨了信息传输以及从信息到决策的过程。控制论和信息论是同时代最重要的理论和方法论基础，直接引领了信息时代、互联网时代和人工智能时代。控制论出版于1948年。一个非常基本的概念是，控制论讨论的是智能，生物（人、动物）是智能的，机器也是智能的。它们的共同点是什么呢？这些系统都是控制系统。不论是生命体还是机械，都需要在复杂环境中根据收集到的信息做出反馈。这个计算可以通过物理和机械元件实现，也可以是数字计算，这就是控制论。更现代的控制论演变成了强化学习。强化学习首次广为人知是在2014年，AlphaGo的对抗式博弈通过自动学习来最大化利益，其核心依然源自最早的控制论。强化学习还应用于多个智能体的实时决策优化、机器人以及自动驾驶等领域。另外一个有趣的强化学习应用是2022年发表在《Nature》上的，通过强化学习控制核聚变反应堆中的等离子场。可控核聚变是控制领域最难的问题，因为核聚变系统本身是混沌系统，传统控制理论无法解决这一问题，但强化学习算法可以精准控制我们想要的形态。这些都是强化学习的应用。然而，强化学习在高维系统中找到最优策略的能力非常强，但其弱点在于不善于利用数据。

今天我们要讨论的不是强化学习，而是大模型。大模型是否可控？从事互联网和技术的朋友可能都知道凯文·凯利这位著名作者，他的书《失控》30年前预测了生命体和机器将变得越来越相似，机器越来越智能，生命体也越来越被工程化。这本书预测了深度学习、大规模神经网络以及涌现（Emergence）。涌现指的是，每个神经元单独是无意识的，但整合在一起时会自然而然地形成意识。书名叫《失控》是因为它预言了当智能体超过人类时会发生失控。这本书没有预测之后会发生什么。所以，当智能体超越人类时，我们面临的就是现实问题。去年7月，OpenAI提出了一个问题：超智能可能在未来几年内实现，如果我们悲观一点，可能在未来十几年或几十年内实现。当这件事发生时，AI系统比每个人都聪明，甚至比我们所有人加起来都聪明，我们如何保证它还能按照人类意志服务？这就是对齐（alignment）或超对齐（superalignment）的问题。目前所有的尝试都是初步的，但一种基本的超对齐方法是使用人工反馈的强化学习。假设我们有一个已经预训练好的大模型，相当于一个从未接触过世界的天才儿童。他知道很多信息，但从未灵活地整理过，也没有解决过任何问题。和这样的儿童聊天是痛苦的，所以要先教他如何与人相处，然后再教他解决复杂问题。我们要让大语言模型不断与人类用户交互，收集简单的人类反馈，比如对回答是否满意或更喜欢哪段回答。用这些数据训练奖励函数，再用强化学习方法微调大语言模型的生成过程。这个过程可以理解为token生成是一个控制过程。仅仅模仿无法学会推理，因此生成需要变成一个策略。我们说话时是有策略的，而不是鹦鹉学舌。这就是为什么要用控制和强化学习，但强化学习有很多不足。我们组去年用离线用户反馈数据对齐机器人时，发现数据分布不同会导致distribution shift，也就是数据不完全覆盖，产生reward hacking或over optimization等问题。为了解决这个问题，我们提出了一种双层强化学习算法，通过双层优化，逼近理论最优效果，取得了显著提升。最近，我们在大语言模型对齐方面做了很多工作，涉及具体场景和深入方向。比如，不同人类需求的精细对齐，用强化学习方法注意少数需求。我们发布了基于自我对抗的深度离线对齐算法和实时收集在线数据的在线对齐算法，比现有技术提高了30%到50%。

我们不仅可以微调大语言模型，还可以将小模型和大模型结合。小模型生成，大模型监督，保证生成结果与大模型一致，这可以通过强化学习实现。我们发布的spec decoding++实现了大模型推理2.26倍的加速。讨论对齐时，可以把对齐奖励函数变成数学证明或自动编程的对错。我们可以从已对齐模型出发，迁移到新奖励函数，优化效果提升25%到100%。这都是为了让大语言模型更好地为我们所用。

接下来，我们来谈可控大模型。生成式AI也是大模型。当我们想到生成式AI时，想到的是图片、小视频等，但生成式AI也可以生成蛋白质等。生成式AI的历史很短，最初是用变分自编码器（variational autoencoder）压缩高维数据，发现解码器可以把噪声变成图片。对抗生成网络（GAN）最早也不是为了生成，而是为了分类和辨别，但生成器非常有趣，于是生成变成了单独任务。生成式AI希望有一个模型或方法，把噪音变成设计、图像等。VAE和GAN生成器都是神经网络，一步把噪音变成数据点，但扩散模型改变了生成式AI逻辑。扩散模型控制一个过程，从噪音开始经过多次迭代生成图片。我们训练的神经网络起到控制器作用。扩散模型找到一个随机控制过程，从噪音生成图片很难，但把图片变成噪音很简单。扩散模型通过逆过程学习去噪声的控制过程和控制器。我们组去年理论上解释了为什么加噪声去噪声过程能逼近复杂数据点。扩散模型求解控制过程，可以用它做更多事，如优化问题。我们可以用生成式AI和求解器结合，生成复杂的新目标函数解。我们还做了一系列工作，自适应预训练好的扩散模型，在解码过程中加控制量，设计最有效平滑的解码过程，生成目标函数解。这说明我们可以指哪儿打哪儿设计。

最后，当我们用生成方式升级优化算法时，生成式模型可以用于机器人控制，生成新的蛋白质结构、DNA和RNA分子。我们组今年用生成式语言模型生成新的mRNA疫苗设计。去年，谷歌用生成式模型开发材料，生成了220万种新的晶体结构，满足数学拓扑结构。这些结构按人类搜索速度需要800年才能找到，但用生成式AI几乎瞬间完成。生成式AI能更精确地控制扩散模型，生成复杂设计，不止于小视频和美图，还能生成新材料、新电路设计、新基因序列、机器控制。让大模型可控，为我们所用，这是通往通用人工智能（AGI）的必经之路。谢谢大家。


### 人工智能和数据科学的经济展望

让我们掌声有请苏教授，谢谢！以前我做报告一般是先做好PPT，然后再起个标题，这个标题只要符合内容就行。这一次我先起了个标题，然后再考虑怎么做PPT，这给我增加了很大的难度。我今天上午才完成，特别难的是“展望”。什么是展望呢？因为这是预测未来的事情，这是最难的。因为我涉及到人工智能、数据科学和经济，尤其是经济，我并不是很懂。虽然我在商学院，但对经济并不很熟悉。所以今天的报告更多是启发性的，我只是抛砖引玉。

前面的几位老师已经做了很多铺垫。去年开始，生成式人工智能可以说是迎来了爆发。它在之前已经有了多年的积累，但去年突然大爆发。现在它已经成为了一种军备竞赛。最初需要几千块GPU，门槛逐渐提高到几万块，最近新闻报道称XAI，Elon Musk的公司，计划购买十万块H100来训练他们的模型。这种训练所需的能量可能需要国家级别的能力来驱动。

那么它的影响力是什么呢？从学术角度看，它使得以往公认的图灵测试(Turing Test)的重要性减弱了。图灵测试是计算机的一个假想实验，如果我们闭上眼睛与一个东西对话，发现它与人对话没有区别，那么我们认为它达到了人工智能的水平。现在大语言模型（LLM）在对话中给人一种接近人类的感觉，但我们还不能说它达到了通用人工智能（AGI）。包括叶老师提到的一个简单例子，有时它会出错，看起来很合理但总是有错误，这与人类不同。因此，图灵测试可能逐渐失去了它的客观性。大语言模型更擅长于编码、写作和知识整合，但它并不是完全新的知识的产生。

它已经产生了巨大的商业价值，但很多人抱怨这种价值都归于NVIDIA。很多公司在创造AI的价值，但要在垂直领域中实现价值还需要时间和精细的工作，因此目前只有NVIDIA赚到了钱。

从经济的角度看，生成式人工智能与经济有什么关系？早期就有一篇论文，在GPT-4刚发布时分析了它对就业市场的影响。结论是，它对一些高精尖工作影响更大，而对体力劳动的影响较小。这个结论与葛老师的观察一致。实际上，它首先替代的是我们这些人。看到GPT的出现，我第一个反应是，我以前引以为豪的一些技能其实被高估了。今年年初，DeepMind推出了一个名为Alpha Geometry的项目，解决了国际数学奥林匹克几何题目，这是我曾经自豪的技能，但发现现在这些东西已经完全自动化了。越是抽象高端的能力，AI解决得越快，而体力劳动则替代较慢。

从经济角度看，AI与经济的交融越来越深。数据成为了新的资源，那么如何给数据定价呢？这是一个问题。陈老师也提到过，定价是非常重要的，因为它能反映数据的实际价值。如何保证数据的公平性也是一个重要问题，我们不能只关注多数人的需求，也要关注少数人的需求。

从宏观经济的角度看，AI的发展对就业市场的扩散速度是不确定的，也是一个值得研究的问题。AI可能会替代人类，这取决于工作的复杂度和环境的互动程度。AI的发展可能导致两极化，甚至中产阶级消亡。中产阶级是最容易被替代的，而底层体力劳动暂时不会受到威胁，资本更不会受到威胁。因此，AI可能带来巨大的社会变革。

这是我对AI和经济的初步理解。首先，AI对经济的影响有两种可能性：一种是可预测的，另一种是不可预测的。我们需要考虑AI对工作的影响，引入信息完备性的概念。信息完备的工作如程序员、会计和作家较容易被替代，而顶尖的科学家和获奖者也可能最容易被替代。

关于AI对经济的设计，从经济角度看，我们需要考虑AI模型的经济考量。我们需要了解世界的分布，而不是只知道多数人的需求。OpenAI的训练数据中也体现了这一点，他们雇用了大量人工标注数据，这些数据中有百分之二三十是不一致的。这种不一致体现了一种自由度，也是一种经济考量。

我们近期的工作是通过强化学习和奖励模型，使大模型更好地符合人类预期。我们提出了一个正则项，通过加权函数来保持数据的分布，确保模型符合人类偏好。

第二部分，我们讲一下数据。数据是21世纪的石油。Elon Musk 购买 Twitter 是一个明智的决定，因为它提供了大量实时数据。XAI 可以直接使用 Twitter 的数据来训练模型。各国对数据保护的反应不同，比如日本基本放弃了数据保护，所以 OpenAI 在东京设立了分部。

不保护专利可能短期有利，但长期会影响创新能力。因为创新需要激励，如果 AI 学习了我们的作品并生成类似的作品，那么艺术家如何生存呢？这会导致数据质量下降，最终影响模型的效用。

我们提出一个方案，用经济手段解决版权问题。让有专利数据的公司或个人得到相应回报，同时确保 AI 模型能用最好的数据进行训练。这个平衡可以通过博弈论中的 Shapley 值来实现。每个数据提供者的价值可以通过 log 函数计算，然后分配相应的回报。

比如，有四位艺术家，我们可以用他们的数据训练模型，计算效用的提升，然后用 Shapley 值分配回报。这保证了每个艺术家的贡献得到合理回报，同时确保 AI 模型能用最好的数据进行训练。

总结一下，生成式 AI 对经济的影响是革命性的，可能会颠覆传统的经济学假设和机制。因此，经济学和 AI 领域的交叉研究是一个非常有前途的领域。AI 是一个强大的工具，但我相信人类有能力掌控它，不会失控。谢谢大家！
