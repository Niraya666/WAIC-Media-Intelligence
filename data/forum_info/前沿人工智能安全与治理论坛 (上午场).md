## 前沿人工智能安全与治理论坛 (上午场)

### 地点

- **会议地址**：世博展览馆1A号会议室

### 活动简介

2024年7月5日，前沿人工智能安全与治理论坛将在世界人工智能大会期间举办。这场盛会旨在讨论AI安全与治理的前沿问题，汇聚了全球顶尖专家学者，其中近半数为国际嘉宾。

论坛内容丰富，包括超过18场主题演讲、4场圆桌会议和炉边谈话。部分嘉宾包括Yoshua Bengio（Mila - 魁北克人工智能研究所创始人和科学主任）、高文（鹏城实验室主任）、张亚勤（清华大学智能产业研究院院长）、宋晓冬（加州大学伯克利分校计算机科学教授）以及其他来自法国国家计算机科学研究中心、Frontier Model Forum、Hugging Face等20多位国内外知名专家。

加入这一具有里程碑意义的盛会，共同塑造人工智能的未来，引导AI安全向善发展！

### 会议日程

| 时间 | 主题 | 嘉宾 |
| --- | --- | --- |
| 09:00 - 09:05 | 领导致辞 | - |
| 09:05 - 09:25 | 开幕主旨演讲：先进人工智能安全 - 最新进展和未解决的问题 | - |
| 09:25 - 09:40 | 主旨演讲：《针对强人工智能安全风险的技术应对策略》 | - |
| 09:40 - 09:55 | 主旨演讲：人工智能安全的挑战和未来方向 | - |
| 09:55 - 10:10 | 主旨演讲：《人工智能飞速进步时代的风险管理》 | - |
| 10:10 - 10:40 | 圆桌讨论：前沿人工智能安全的研究议程 | - |
| 10:45 - 11:00 | 主旨演讲：大语言模型和多模态模型的安全基准 | - |
| 11:00 - 11:15 | 主旨演讲：推进前沿人工智能安全的行业实践 | - |
| 11:15 - 11:30 | 主旨演讲：全面的人工智能安全与安保基准 | - |
| 11:30 - 11:45 | 主旨演讲：DEFCON 31的AI红队经验 | - |
| 11:45 - 12:15 | 圆桌讨论：前沿人工智能安全评测的探索 | - |

### 主要嘉宾介绍

- **约书亚·本吉奥**：Mila - 魁北克人工智能研究所创始人和科学主任
- **高文**：鹏城实验室主任
- **宋晓冬**：加州大学伯克利分校计算机科学教授
- **张亚勤**：清华大学智能产业研究院院长
- **黄民烈**：清华大学基础模型中心副主任
- **邵婧**：上海人工智能实验室大模型安全团队负责人
- **张倬胜**：上海交通大学长聘教轨助理教授
- **杨耀东**：北京大学AI安全与治理中心执行主任
- **乔宇**：上海人工智能实验室领军科学家
- **克里斯·梅塞罗尔**：前沿模型论坛执行董事
- **魏凯**：中国信息通信研究院人工智能创新中心主任、云计算与大数据研究所副所长
- **鲁曼·乔杜里**：Humane Intelligence首席执行官
- **熊德意**：天津大学自然语言处理实验室负责人

---

谢谢穆迪秘书长的精彩致辞。请入座。大家好，我是安远AI高级项目经理吴君怡，也是今天论坛的主持人。鉴于今天有多位国际嘉宾，我将用英语主持此次会议。

各位观众、各位女士们，我是冠义AI高级项目经理冠义恩，今天的会议主要用英语进行，因为有许多国际嘉宾。

今天我们很荣幸邀请到Bengio教授分享他关于《国际科学报告》的重要内容，这份报告涉及到进步AI的安全问题，并由来自全球75位AI技术专家共同完成。我非常荣幸作为报告的撰写者之一，参与了其中。

首先，我将谈论报告的主要内容，然后再讨论其对未来的意义和我们对大局的看法。尽管报告没有给出具体建议，但它是为了帮助政策制定者理解并应对AI带来的风险。

报告名为《国际科学报告：进步AI的安全》，我们花了很多时间为其命名。报告主要关注的是AI的风险，因为尽管AI的应用和利益已经有很多研究，但政策制定者必须了解其潜在的风险和能力，以便更好地管理这些风险。

例如，在英国AI安全协议的支持下，我们在11月发布了一份关于AI能力和风险的国际独立公共报告，参与的国家有30个，加上欧盟和英国。报告广泛涵盖了当前的各种风险，从负面、不明到预期的风险，包括工业市场的影响和失控的超级智能AI。

报告的目的不是提出新科学，而是基于现有科学为政策制定者提供帮助。共有75位专家参与了这项工作，每个国家有一位专家，加上欧盟和英国的专家。我们还邀请了16位撰写者，他们提供了评论和不同版本的反馈，并有一群高级教师作为各方面的专家。

关于AGI的风险和时间线问题，不同意见认为AGI可能在几年内或几十年内发生，这对社会的影响极其重要。报告探讨了AI的实际风险及解决这些风险的科学方法，同时也指出了这些方法的局限性。

报告的主要结论是，目前没有公认的方法可以完全防止实际风险和未来风险，这是一大警示。然而，全球可以通过更好地理解这些风险和应对方法来改善现状。

我们将风险分为三类：危险风险、系统风险和失败风险。危险风险包括AI被恶意使用、系统风险涉及社会和技术的结合影响，例如劳动市场和AI的集中化。

此外，报告还谈到了社会风险，例如制度调整和技术变革的时间差异。关于国际执行风险的讨论指出，不同观点的差异主要在于AI进步的速度及其对社会的影响。

我们必须做好准备，以应对不同情况下的风险，包括可能在三年内发生的AGI。各国之间的竞争也涉及军事用途，这要求国际间的合作和公平的协议。

最后，报告强调了研究和投资的重要性，以更好地理解和解决AI带来的风险。谢谢大家的参与，请大家提出意见和建议。我们将在年底发布更详尽的报告。



---

在北京大学工业科研学院的高老师，目前是第十四届全国人民代表大会的代表。他曾经是第十、第十一和第十二届中国人民政治协商会议国际委员会的成员，中国国际自然科学基金会的主席，中国计算机学会的主席，以及中国计算机博物馆的馆长。高老师，我们很高兴能和您一起参加这个节目。

接下来，我要介绍一下在这个节目中，如果有参与的专家可能也能听到三位图灵奖得主在谈论人工智能安全问题时的不同看法。比如说，第一位图灵奖得主Raj认为，目前人工智能有太多的问题需要解决，如果要保障安全，首先要解决人工智能本身存在的问题。第二位得主Manuel的观点比较有趣，他认为只要人工智能是确定的，就可以控制，不确定的就不行。第三位Andy与前面的Angel观点一致，他们和Jacqueline一起写了一些文章，整理了这些报告，强调我们要重视人工智能的安全问题。

我今天要讲的是，人工智能的安全性实际上有两个方面的问题。一方面是技术研究，要把技术做到极致，让它有用；另一方面是社会学家的角度，要考虑技术对社会的影响。如果影响是负面的，需要有办法控制。这是一个问题的两个方面。

我们这个社会在发展过程中需要所有人的关注，但这不意味着所有人都做同一件事，所以需要良好的分工。今天我们就来谈谈这个分工问题。我们知道，AI确实很强大，特别是通用人工智能（AGI）。当AGI变得强大后，我们需要让它向善，即做有益的事情。AI向善需要关注两个方面：一是技术角度，要把人工智能技术做到足够好；二是伦理角度，要在道德等方面进行规范。

目前AI技术是否足够好呢？显然不是。Benjo在他最后的幻灯片中提到，有些AI在某些方面已经超过了人类，但在其他方面还不行。只有当AI几乎在所有方面都超过人类时，才能真正发挥作用。

目前AI更多表现出的是低水平智能，即依靠记忆和使用显性知识。中水平智能则是理想的状态，意味着用少量显性知识就能获得智能，具备很强的举一反三能力。但现在的AI系统没有这个能力。我们目前主要在低水平智能阶段，某些任务可以通过记忆或数据训练完成。当只有少量样本也能训练出智能时，我们就到了中水平智能阶段，它可以跨越领域，进行类推。

低水平智能也可能出现智能涌现现象。当我们用大量数据训练模型时，会产生一些我们未预料到的知识涌现。例如，大语言模型（LLM）是用多种语言训练的，但我们每个人的母语通常只有一种。如果用西班牙语训练，中文使用者可能不熟悉这个背景，因此会感到惊讶。这种涌现现象可以从这个角度解释。

当前人工智能在智能水平、技术、应用及社会属性方面都有较大进展。特别是伦理问题，必须考虑它的社会属性。人工智能的安全和风险包括犯罪等问题，肯定是早晚需要解决的。人工智能的发展可能影响人、物及数据层面。

为了防止技术被恶意使用，我们需要从伦理和技术两个方面解决这些问题。中国工程院前些年部署了一个重大咨询项目，叫做新一代人工智能安全与自主可控发展研究。我和一批专家一起研究了强人工智能和内脑计算技术的安全对策。我们将人工智能的安全风险分为三个方面：模型、算法和硬件、自主意识的不可控性。

针对这些风险，我们提出了技术和理论的改进方案，强调人工智能的价值取向控制，并在应用阶段提供足够的技术支撑，防止人为安全问题。国际合作研究和人才培养也是关键，因为相关人才稀缺。我们需要良好的平台和数据来进行训练。

为了实现理想的训练结果，我们在鹏城实验室从2018年开始使用NVIDIA显卡组建了一台约1000卡的计算机。当时，因为是2018年，计算能力还没有现在这么强大。到了2020年，我们使用华为的Ascend 910组建了一台拥有4000卡的计算机，计算能力约为1000 PFLOPS。今年年底，我们计划构建一台拥有2万多块卡的计算机，计算能力约为16000 PFLOPS。

有了这些计算能力，我们可以在模型训练、训练经验总结、训练参数的社会赋能等方面做出更多的贡献。比如，我们可以将所有在机器上训练的模型开源，供社会和研究团体使用。当然，这里大家可能会担心数据安全问题。对此，我们实验室开发了一套名为“防水宝”的技术。防水宝技术确保数据拥有方对数据具有绝对的控制和管理权利。在训练过程中，数据是“可用不可见”的。机器可以访问数据，但操作员看不到数据。他们只能看到一个脱敏后的样本数据，用来试验模型。一旦模型试验通过，真实数据输入后，操作员就无法看到真数据，除非数据拥有者授权。

在训练结束后，如果要传输训练参数，机器会自动向数据拥有方请求检查，以确保没有携带任何敏感数据。通过这种流程，我们可以确保数据的安全。

我们训练了一系列大语言模型，包括7B模型、33B长窗口模型和200B模型。这些模型涵盖了中文、英文及其他语言的参数。我们使用4000卡的计算机训练了一个104层的200B模型，耗时半年多。在这个过程中，我们积累了很多经验，性能也表现良好。之后，我们又训练了33B的长窗口模型，目前的窗口长度为128K，我们正在训练192K窗口的模型，预计很快就会完成。这些模型完成后，我们将其开源，供大家使用。

总结来说，人工智能的高速发展带来了安全问题，我们必须重视。作为技术研发者，我们要推动人工智能的发展，并通过国际合作更好地完成这项工作。谢谢大家！

---

谢谢各位教授，请坐下。接下来，我们很高兴请出张雅琴教授。张教授是中国工业博士、AI科技博士，也是清华大学研究AI工业的教授。他曾任百度总裁，并在Microsoft担任总裁16年，拥有丰富的职业经历。作为世界上著名的科学家和企业家，他通过550篇文章、62项美国专利以及其他专业成就，做出了重要贡献。让我们欢迎张教授。早上好，谢谢安远。

安远AI邀请我来参加这个大会。刚才，Yoshi Banjo和高温院士对AI特别是大模型的发展和风险做了系统性的介绍，分别从全球和中国的角度出发。的确，过去两年，AI发展迅速，同时也带来了很多安全风险。我过去两年也和全球领先的学者们一起在这方面进行了不少研究。今天由于时间关系，我简单讲一下我的一些思考。首先是大模型的发展趋势，以及更重要的安全风险趋势。

我认为，大模型和生成式AI在未来十年有以下几个趋势。首先是多模态，不管是语言、文字、语音、图像还是视频都在融合。激光雷达、三维结构信息、四维视红信息、蛋白质、细胞和基因也在成为多模态数据。其次是智能体自主智能，能够自主规划任务、开发代码、自己升级和自我复制。第三是智能走向边缘，从云端大模型逐步走向PC、手机和智能设备。第四是物理智能，即具身智能，大模型将应用于无人车、无人机、机器人等物理基础设施。最后是生物智能，包括脑机接口、医疗机器人、生物体和生命体。

最近，我和许多学者一直在探讨通用人工智能实现的时间。我个人认为大概在二十年之内可以实现，分为三个阶段：信息智能、物理智能和生物智能。信息智能五年内可达到图灵测试标准。当ChatGPT出来时，我觉得它在文字方面基本通过了图灵测试，但在视频等方面可能还需要五年时间。物理智能或具身智能可能需要十年时间，尤其是无人车领域。我认为无人驾驶是具身智能的最大应用，也是第一个实现新图灵测试的应用。明年我们在武汉将进行大规模商用实验，2030年前会成为主流应用。生物智能可能需要更长时间，大约十年。但整体来看，二十年内我认为可以实现通用人工智能。清华大学智能产业院就是为了通用智能而建的，目前有二十二名教授和三百多名学生。我们的目标是实现信息智能、物理智能和生物智能，包括无人驾驶、先进机器人和Biological Computing。我们发布了很多模型，更多是垂直模型，如全球第一个实用的端到端无人驾驶开源模型Air Apollo FM和全球最大的Biomag GPT，都是开源的，大家可以在GitHub上使用。

同时，这些强大能力也带来了巨大的风险。刚才Banjo提到，前沿大模型达到万亿参数时，风险会更加明显。我把风险分成信息世界、物理世界和生物世界。信息世界的风险大家比较容易理解，如DeepFake、Hallucination、Misalignment和Misinformation。物理世界的风险更大，比如十年后机器人数量超过人类，如果失控或被滥用，会带来巨大社会风险。无人车也依赖大模型控制，风险包括主动和被动。更大的风险是生物智能、物理智能和信息智能的融合，如果失控或被滥用，会造成生存风险。过去几年有几个重要节点，其中一个是2023年6月的AI安全中心关于AI风险的声明，提到要把AI未来的风险视为核武器和流行病一样的重要。后来有很多工作，包括中国的AI全球倡议、欧盟的AI法案和几次峰会。去年我和Stuart组织了International Dialogue on AI Safety会议，每三四个月开一次，深入研究技术和政策问题。第一次在英国，第二次在北京，下一次在威尼斯。亚瑟刚才讲的报告是对很多讨论的高度总结，我也深度参与了这个报告。

我简单介绍一下大模型安全技术。大模型安全是一个系统工程，从输入、输出、安全评估到治理，特别是系统安全对齐，都需要工作。这里有许多数学和算法研究，也有许多工程和技术问题，还有很多策略问题。大模型安全的对齐最近有许多进展，有两种不同方法，一是直接监督方法，即利用高质量安全信息进行监督微调。另一是根据人类偏好和价值观做强化学习，如GPT系列采用PPU方式。有许多不同选择，可以基于奖励模型的安全奖励和有用奖励，用Lagrange结合输入参数，也可以用一些新奖励方式。在清华AIR，我们有几位老师做了许多工作，如詹先生老师提出的条件强化学习，用于大模型微调。如果有高质量数据，它可以帮助自动化任务。手工强化学习需要很多工作和数据，这项工作在GitHub上叫OpenChat，现在很受欢迎。

我们还发现，目前在强化学习人类反馈中有些问题，特别是样本和策略的学习目标不匹配，curie和policy会misalignment。开始时对齐，但走着走着就偏离方向了。我们提出新技术，使得学习过程中goal和trajectory well-aligned。阿Claire会在接下来几周谈到这项工作。我们还用了不少安全离线强化学习方法，改进安全策略。首先要判断某个东西是否安全，找到区域。在区域内最大化奖励，区域外最小化风险，一个maximize，一个minimize。我们的论文中这些都是数学理论。我想让大家知道，安全对齐不仅仅是一个词，而是策略和算法，里面有很多理论创新和突破。我们的文章已经发表在ICML。有一篇论文。最后，时间不多，我想谈一些建议。刚才讲的是技术方面的工作，不知道药东会不会讲，药东和刀宋在这方面做的都特别领先，他们以后会讲更多细节。我提一点政策建议，这讲了两年了，我要盖个章。我的建议完全是个人意见，不代表清华大学，不代表清华AIR，不代表我们的团体。内部有很多不同观点。完全是个人建议。我提了两年了，提了十个建议，今天时间关系讲五个。第一个建议是建立分级体系。现在AI有很多不同算法、模型，我们要对最前沿的，比如超过万亿参数的模型进行约束，一般的模型不要太规范，允许发展。特别风险大、能力强的前沿模型需要规范。我做无人驾驶，我们自动驾驶分六级，从L0到L5。我建议AI分成L0到L5，只有L5的需要规范。第二点是模型规范，包括从数据、模型构建、对齐到评估，都需要标准，更严格的标准。第三，建议10%的投入放到安全和风险领域。全球建议30%，国内从10%开始，慢慢到30%。包括基础研究经费、产品开发经费和社会投入。第四，设立清晰的红线和边界。红线边界设立不容易，每个国家情况不同，但可以设立一些共同的。设立什么不能做，比如智能体复制要经过人同意，不能自我复制无限制。大模型接到核电站时要特别规范。第五，建立国际沟通合作机制，包括标准、评估和合作方式。需要专家、政策制定者、政府和不同领域的人一起合作。我就讲这么多，谢谢大家。

---
非常感谢张教授的精彩演讲和宝贵建议。接下来，我非常高兴地介绍UC Berkeley的计算机科学教授Dawn Song，她也是Berkeley Center for Responsible Decentralized Intelligence的共同主任。Dawn教授的研究主要集中在AI安全和系统安全方面，她是计算机安全领域引用率最高的学者之一，荣获了包括麦克阿瑟奖、古根海姆奖在内的多个奖项，此外还有超过10项的时效性奖和最佳论文奖。Dawn，非常高兴你能来到上海。下面时间交给你。

谢谢大家的到来。我的名字是Dawn Song，我是UC Berkeley的教授。今天我将讨论AI安全的挑战和未来方向。早些时候的演讲为我们提供了很好的背景信息。在这里，我想特别强调在部署机器学习时考虑攻击者的存在是非常重要的。首先，历史告诉我们，攻击者总是紧跟新技术的发展，有时甚至领先。而且这次AI的风险更高，因为AI控制的系统越来越多，攻击者为了攻破这些系统的动机也越来越强。随着AI变得越来越强大，攻击者滥用AI的后果也会变得越来越严重。因此，在讨论AI安全时考虑攻击者的存在是至关重要的。

首先，我想多谈一些在攻击者存在下的AI安全。从我团队的早期工作和其他研究工作中，我们已经证明了对深度学习系统的对抗攻击是普遍存在的。实际上，今天所有的深度学习系统都容易受到各种类型的对抗攻击。自从我们和其他人早期的工作以来，这个领域的论文数量呈指数增长。此外，我们的早期工作的一些成果现在已成为伦敦科学博物馆的永久收藏品。

在讨论安全性时，今天我们也谈到了大语言模型的安全对齐，考虑对抗环境也是很重要的。不幸的是，我们的工作和其他人的工作表明，这些大语言模型也非常容易受到对抗攻击，这些安全对齐机制很容易被破坏。在我们的研究中，我们还开发了一个名为Decoding Trust的综合评价框架，用于评估大语言模型的可信度。这项工作在去年12月的NeurIPS会议上获得了杰出论文奖。我们开发了新的算法和不同的环境，包括良性对抗环境，以评估大语言模型的多方面安全性和可信度。我们的研究表明，在对抗鲁棒性、有毒性和公平性等各个方面，这些大语言模型都很容易受到对抗攻击的影响。更多详细信息可以参阅我们的论文，网址是decodingtrust.github.io。

此外，这些对抗攻击在多模态模型中也有效。其他研究还表明，即使在模型微调阶段，攻击者通过提供少量对抗设计的数据点，也可以使微调后的模型轻松丧失安全对齐能力。这被称为数据投毒。通过这种数据投毒步骤，这些模型还可以表现出非常隐蔽的行为，即所谓的后门。在我们的早期工作中，我们展示了通过数据投毒，攻击者可以在模型中建立后门。例如，在我们的早期面部识别工作中，模型在正常情况下会正常运行并给出正确的面部识别结果。然而，当任何人佩戴一种特殊类型的眼镜时，即使在现实世界中，这种眼镜也能触发模型中的后门，使模型将佩戴这种眼镜的人误认为是特定的目标人物。Anthropic最近的工作也表明，微调的大语言模型在正常情况下会生成正确的代码，但在出现特定触发词时，模型会生成易受攻击的代码。

这些都是不同类型的对抗攻击，整个社区在发明新的攻击方法方面非常有创造力。然而，在防御方面，我们的进展却非常少。目前没有有效的通用对抗防御。这表明这是我想在AI安全背景下提出的第一个开放挑战。当前的AI安全对齐机制很容易被对抗攻击规避，任何有效的AI安全机制都需要能够抵御这些对抗攻击，这因此提出了一个巨大的开放挑战。为了实现AI安全，我们实际上需要能够解决对抗鲁棒性问题作为前提。

尽管现在每年有成千上万的论文发表在不同类型的对抗攻击上，但整个社区在对抗防御方面几乎没有进展。因此，为了发展有效的AI安全，作为一个整体社区，我们需要在如何开发对抗防御方面推进，从而开发出能抵御对抗攻击的AI安全机制。

那么，有哪些潜在的方向可以帮助我们实现这一目标呢？在这里，我将举几个我们最近工作的例子。一个工作是我们称之为表示工程的方法，这是一个自上而下的AI透明性方法。在这种情况下，我们通过提供对比输入作为某些任务的刺激。我们将这些对比输入提供给模型，然后监测不同层神经网络的激活情况，并构建模型。在我们最近的工作中，我们展示了通过这种方法，我们实际上可以在某些层识别出与模型不同类别行为相关的某些方向。例如，我们可以识别出某些方向实际上与模型是否诚实、是否产生幻觉等行为相关。我们还展示了一种称为表示控制的方法。不仅可以进行表示读取，即监测模型行为，还可以在推理过程中沿识别出的方向修改神经元的激活情况。通过这种方式，我们实际上可以改变模型在某些类别上的行为。例如，使用这种方法，我们可以使模型表现得更诚实或更不诚实等。

为什么这很重要呢？我认为这是人类大脑和人工大脑（即人工神经网络）之间的一个关键区别。人工神经网络的控制完全在我们手中，因为我们可以完全观察神经网络的激活情况，并在实时修改神经网络的激活情况。这为AI安全提供了一个强大的工具，这允许我们更好地观察和控制神经网络的行为，从而提供更好的控制和行为执行机制。因此，这可能是一个非常有前景的方向，用于提供AI安全控制机制。然而，这种控制机制虽然有前景，但很难提供完全的保证。因此，正如Yaqin教授和Yoshua所提到的，我们理想上希望有具有保证的AI安全。

最近，我们发起了一项关于量化AI安全的联合计划，目标是开发具有保证的AI安全，即通过设计确保安全。这实际上也是受到网络安全方法的启发和相似的，包括我自己在过去25年网络安全工作中的许多方法。我们经历了一种范式转变，从早期的检测大语言模型的错误行为，到后来我们开发出一种主动防御的方法，专注于漏洞发现。尽管这种方法有其局限性，但我们发现实现安全的最佳方法是我们所谓的设计安全或通过构建确保安全，通过设计和构建系统的属性来确保安全。通过形式化验证，我们可以正式验证系统的安全性。这可以在不同层次上完成，包括设计层，我们实际上已经有许多不同类型的系统，包括微内核、文件系统和编译器等，已经被正式验证。

然而，这些系统的问题在于它们非常耗时且难以扩展。我的团队与OpenAI的合作，是最早使用深度学习来改进这一点。这项工作是在几年前完成的，早在我们可以训练AI智能体自动证明定理和验证程序之前。结合程序综合，我们可以自动生成带有证明的安全代码。通过这种方法，我们可以使用AI构建设计安全或通过构建确保安全的系统。这可以帮助我们解决某些类型的问题。然而，这仍然存在许多开放的挑战。首先，这种形式化验证方法主要适用于传统的符号程序，但难以应用于非符号程序，如深度神经网络。比如在自驾车不压过行人的情况下，我们甚至没有一个行人的形式化规范。而且在未来，几乎所有的系统将是混合的，它们将结合符号和非符号组件。因此，形式化验证和通过构建确保安全是一种非常有前景的方法。

总结一下，正如我们在这里讨论和同意的那样，AI安全非常重要。随着AI能力的增强，确保这些系统的安全至关重要。然而，仍然存在许多挑战。考虑对抗环境中的AI安全非常重要。我认为，使用激活转向和表示控制的方法来控制模型行为是一个非常有前景的方向。最后，我们希望能够开发出新的方法和机制，实现设计安全，以建立具有可证明保证的安全AI系统。谢谢。

非常感谢Dawn。请留在台上，我们将转入我们的讨论环节。我们将以一场关于AI安全研究方向的讨论来结束我们的会议。请其他讨论小组成员提出问题，并介绍自己。参与讨论的还有上海人工智能实验室大模型安全团队负责人邵晶博士，她领导了许多关于大模型安全性和价值对齐的研究项目。我们还有北京大学AI安全与治理中心副主任杨耀东教授，他研究AI对齐和强化学习等课题，并在顶级会议和期刊上发表了100多篇论文。最后是上海交通大学助理教授张卓生教授，他的主要研究兴趣包括多模态模型和自主智能体的安全性和安全性问题。他在顶级会议和期刊上发表了50多篇论文。我们的主持人是Concordia AI的技术项目经理Duan Yaowen，他是Future of Life Institute的博士研究员。Yaowen在剑桥大学研究AI安全，拥有机器学习硕士学位。

---

我们今天的第一个圆桌讨论的主题是前沿AI安全技术的研究议程。其实今早我们看到Yosha Bendrell谈到他撰写的首份《先进AI安全国际科学报告》(International Scientific Report on the Safety of Advanced AI)，其中提到了通用人工智能可能带来的滥用风险、故障风险以及系统性风险。同时他也介绍了当前一些安全对齐方法的局限性。其实今天我们的圆桌讨论就聚焦在这两个问题上。第一个问题是当前面向前沿大模型的AI安全技术存在什么样的挑战。当然，还有第二个问题是面向更强大的未来通用人工智能，甚至是超越人类的超级智能，安全技术该如何发展，以及如何避免失控的风险。首先，欢迎四位老师。我们先探讨第一个问题，就是当前安全技术的一些挑战。Don Song老师，您刚才提到目前的防御方法还非常脆弱，比如SFT、RAHF以及对抗训练，这些防护措施并不够有效，甚至容易被逆转。您也提到了表示工程(representation engineering)和设计安全(safety by design)。我们想问，您认为当前这些大模型脆弱性的底层原因是什么？哪些新的技术方向可能更加根本性地解决这些问题？

Don Song：好的。首先，当前我们其实并不完全了解这些模型的功能。上学期，我在伯克利教授了一门课，叫做《理解大语言模型的基础和安全》。我们发现，实际上没有人完全理解这些模型的工作原理。而我们今天所做的这些对齐，例如通过RRIHF，其实只是表面上的改变。我们还没有从根本上改变模型的行为。另外，特别是对ARC50，我们必须让模型更能抵抗敌对攻击。虽然在某些领域中抵抗攻击并不容易，但在图像和其他模型中，攻击更为容易。希望我们能找到解决方案，使这些模型在多模态下也能有效抵抗攻击。当前，我们所做的工作，基本都是在表面上改动。因此，我提到了一些未来的方向，希望通过表现控制改变模型的行为，并提供一定的保证。我们需要深入解决问题，以便在RHR上做出更多改进。谢谢。

接下来，我们想听听耀东老师的看法。耀东老师，您在不同场合都提到过，只做RHR是不够的。您近期的工作也发现了语言模型在对抗和逆转对齐方面的问题。能否分享一下您的看法？

耀东：对，我就用中文说吧。其实很多学者都观察到一个现象，就是语言模型在对齐后，只需要少量的攻击样本就能让它变得不安全，即使做了很长时间的RHF。RHF的技术负责人John Schuman发现，当语言模型训练得很好时，只需要30个英语样本就能修正俄语中的错误。我们最近有一个研究，叫Large Language Model Resist Alignment，探讨了一种逆对齐现象。训练语言模型时，有两个阶段：预训练和SFT，再进行RHF。在参数空间中，训练过程就像拉橡皮筋，越往后拉，张力越大。如果突然松开，反弹速度会比拉的速度快。这就是逆对齐现象。我们发现，这个现象符合胡克定律，形变量越大，反弹力越大。因此，逆对齐现象是存在的，并且越对齐，反向攻破的可能性越大。这为我们未来的安全对齐和价值对齐提供了一些指导意义。

谢谢耀东老师。邵信老师，您的团队最近发表了很多关于多模态大模型和智能体攻击与评测的工作，例如SciSafe和Chef数据集。对于GPT-4O这样的多模态大模型，在安全对齐方面有哪些特殊挑战？

邵信：对，这是个很好的问题。去年年初大家更多关注的是大语言模型的安全性，但引入更多信号如图像、视频后，复杂度急剧提升，带来了不同的安全问题。例如，语言模型中的幻觉问题，在多模态模型中也存在。多模态模型中的幻觉问题，可能是因为视觉分支与语言分支的上下文理解较弱，或者视觉分支的grounding能力差，导致幻觉问题。我们还发现，随着引入更多模态，分解的复杂度也增加了。我们今年年初对Gemini进行了为期三个月的评测，包括可信性、泛化性和推理能力。我们相信未来多模态大模型在产品应用中会有很多应用，因此不仅要关注可信性，还要关注泛化性和推理能力。这些都同样重要。

谢谢邵信老师。卓昇老师，您在多模态大模型和智能体安全方面也有一些研究。现在智能体特别火，能直接进行序列决策和操纵工具和API。您之前的工作Our Judge，通过监测交互记录识别自主智能体的风险行为。您觉得智能体的安全有哪些特殊的难点？

卓昇：好的。我沿着邵信老师提到的多模态大模型这条线。我们也在做基于LLM和多模态的智能体。智能体把大模型用在虚拟或现实环境中，对现实产生影响。智能体的安全风险涉及用户、环境和模型本身三个维度。我们现在更强调的是通用智能体，其环境多种多样，攻击样本也可以从环境中构造。智能体的行为不像静态的AIGC信息，其后果难以预料，未来的风险也难以预测。因此，我们在AIGC基础上进行一些动模态的探索，例如让大模型接入手机或电脑，模仿人类操作，完成复杂指令。攻击者可以从用户端构造对抗样本，也可以在环境端植入指令，影响智能体行为，导致劫持问题。这就需要我们在智能体应用过程中，不仅需要大模型的对齐，还需要外部反馈机制，通过动态分析和监测智能体的行为历史，预判可能的安全威胁，并将反馈信息传递给模型，让模型基于反馈进行自我调整，实现安全闭环。

谢谢卓昇老师。刚才我们讨论了现有的大语言模型、多模态模型和智能体的安全挑战。最后一个部分，我们来探讨未来可能出现的更强大的通用人工智能，甚至是超智能可能带来的失控风险。邀东老师、Don Song老师和雅琴老师今年三月在北京颐和园共同参与签署了一份关于AI风险的共识声明，划定了五条安全红线，其中包括自我复制与适应能力、欺骗人类的能力和寻求权利的倾向。请问各位老师，当前哪些危险能力的研究判断最为紧迫？我们现在可以做哪些技术方向的准备，来应对未来更强大的智能可能带来的风险？

邀东：可以的。今年年初我们在颐和园和国内外专家讨论，因为英国有布赖切利宣言，首尔会议也有深入讨论，但国内学者主导的讨论并不多。所以在智源的领导下，我们邀请了国内外专家进行研讨，划定了一些具体的red lines。排名第一的风险是自我复制问题，我认为这个问题可能被低估了。Yoshua的PPT中有一页讲得很好，随着时间推移，模型的学习曲线越来越陡。现在大模型的发展可能还在第一个阶段，即监督学习阶段，但一旦进入自我博弈和纯强化学习阶段，能力提升可能会突然突破某个临界点，就像AlphaGo和AlphaGo Zero一样。因此，我们认为自我复制和self improvement风险是具体可见的风险，需要特别关注。其他风险如欺骗和误用也需要国际合作和治理。今年上海发布的《人工智能国际治理倡议宣言》也是希望推动国际合作。我认为这些方向都是非常好的。


对，另外三位老师，谁想先开始？好的，我可以补充一点。今天，即使大语言模型（Large Language Model，LLM）已经非常强大，但我们知道它仍处于早期阶段。因此，接下来，已经有人在讨论具身智能（Embodied Intelligence），即将这些基础模型应用于机器人领域。目前，我们还处于预训练阶段，比如大语言模型，然后进行推理等步骤。但在未来，当我们实现具身智能时，这些智能体将在环境中行动，并形成闭环系统。智能体会从环境中获取输入，做出决策，并根据反馈进行自我改进，实现自我学习和持续学习。

当我们进入这种方式时，学习将进入下一个阶段。目前，即使大语言模型已经能够在给定任务时，按照步骤思考并将任务分解成不同的子任务和目标，但这些能力在未来的智能体变得更加自主和强大时，将会进一步提升。特别是对于特定目标，智能体将能够分解成子目标，并找到实现这些子目标的最佳方法。然而，我们也担心，这些智能体可能会产生危险的子目标，比如“纸夹问题”（Paperclip Problem），即智能体会生成不对齐的子目标，甚至为了获得更多的权力而欺骗他人，并自我复制以维持自身。

目前我们还没有看到这些能力，但开发早期检测方法非常重要，比如像“金丝雀”（Canary）方法。同时，一旦这些行为出现，我们可能没有足够的时间来应对。因为一旦开始自我改进循环，随着计算能力的增加，自我改进速度会非常快。这是一个挑战，许多不从事前沿AI安全工作的人可能没有意识到，即使他们认为这些风险很遥远，但一旦出现，可能已经为时已晚。这些是我们需要解决的挑战。对，谢谢当桑老师刚才提到的早期检测。

---
其实，我们下一个环节将讨论AI安全测试评估，之后可能会有更多讲者分享他们的见解。另外，关于刚才的问题，邵静老师和周生老师有没有什么补充？刚才几位老师已经讲得很全面了，我有一些小感受。周生老师提到，AI智能体（agent）在很多场景中与环境（environment）进行交互，在应用环境中受到很多因素的影响，不仅是模型自身的安全性问题。未来，这些问题会更加凸显。比如我们实验室，不仅进行AI研究，还有很多从事科学领域AI研究的专家。在科学领域，AI的渗透越来越深，但相关研究还不多。大家更关注AI在日常生活中的安全性问题，如滥用问题。我们希望大家也关注特定领域或垂直领域的安全性问题，未来这些问题可能带来更大的影响。

周正老师：我继续补充一下。目前我一直关注的是大语言模型（LLM）智能体在开放环境中的行为安全问题。简单来说，在行为交互过程中，安全问题主要体现在我们将大模型用于工业控制、科学研究以及现实生活场景时，它可能被滥用或劫持，对环境或用户造成影响。在这个过程中，需要一套系统的解决方案，不仅要提升模型自身的安全性，还需要一套完备的监管模型，动态监测智能体的行为，确保不带来损害。另外，我们还需要一个动态规范，将大模型的通用性与安全规范结合，实现自动化监测。这既能提高网络安全监测的效率，又能发挥大模型的通用性，应用更加广泛。我们需要一个主动的检测过程，而不是等模型行为完成后再检测，这样危害已经造成，很难弥补。因此，我们需要从大模型自身的内在安全、行为交互过程的动态检测以及网络安全的系统红线等方面进行系统性防御，包括静态和主动手段。这是我的一些观点。

谢谢卓尚老师。由于时间关系，我们今天的第一场圆桌讨论就到此结束。感谢各位老师的精彩观点。请各位老师返回前排就座，把时间交给主持人君怡。感谢您的参与。

谢谢，谢谢，谢谢。谢谢今天的评论和补充评论。我们非常感激大家的见解。今天的讨论帮助我们更好地理解AI安全测试评估。

现在我们将听取克里斯·梅塞罗尔博士的发言。梅塞罗尔博士是Frontier Model Forum的执行董事，该论坛由Anthropic、Google、Microsoft和OpenAI成立，致力于推进前沿AI的安全。他是AI治理和安全领域的专家，目前专注于开发负责任的最先进通用AI系统的最佳实践。克里斯曾担任布鲁金斯学会AI和新兴技术倡议的主任。克里斯，很高兴你能来到这里，现在交给你。

谢谢，很高兴能在这里发言。正如刚才提到的，我运营一个名为Frontier Model Forum的组织，这是一个由行业支持的非营利组织，致力于推进前沿AI的安全。我们有三个核心任务，其中之一是开发最佳实践，另两个是推进前沿AI安全科学，并分享我们所学到的信息，这也是我们今天感到兴奋的原因之一。

首先，我想介绍一下什么是前沿AI，以及应对它的挑战，然后介绍我们的一些早期思考，包括如何进行评估和一些最佳实践，这些是我们与专家成员和安全专家讨论得出的。

当我们说“前沿AI”时，通常指的是最新一代的先进通用AI技术。我们关注的不是特定任务的狭义AI应用，如贷款算法或面部识别，而是最新一代的通用AI系统。从这张图表中可以看到，由于我们不断扩大系统规模，一般来说，每隔几年计算能力都会提高十倍，以开发更好的通用系统模型。我们主要关注的是最新一代的前沿AI系统。

如果你想了解更多，我们网站上有插图。这很重要，因为我们预计挑战会随着时间的推移而演变。前沿AI会不断变化。正如今天早上看到的图表，能力增加的斜率非常明显，几乎是指数增长。

我们关注最新一代的原因是希望开发早期最佳实践，以应对最先进的模型。这之所以重要，是因为我们不知道这些模型何时会获得或发展特定能力。我们没有好的方法预测这些系统的能力，这使得我们很难了解如何安全和有效地构建它们。

最后，我们预计前沿AI会继续发展，从聊天机器人发展到更具代理性的系统，确保这些系统的安全将变得越来越重要，因为这些系统将越来越多地与现实世界互动，可能影响公共安全，这是我们组织关注的重点。

我将介绍我们与安全专家和成员公司讨论得出的一些早期思考，包括如何进行评估、应该进行哪些评估，以及一些早期最佳实践。这些只是高层次的描述，我们将进行更多讨论，以发展最佳实践。

首先，从高层次来看，有两种评估或风险评估方法：红队演习和自动化评估。红队演习通常是手动的，正在探索如何自动化，但总体来说是利用人类专家来探测特定模型能力的手动方式。相比之下，评估通常是基准测试或其他自动化方式，用来探索特定模型的能力或风险概况。区分这两者非常重要。

在前沿模型的评估中，有两个核心类型：性能评估和安全评估。性能评估对于理解模型的通用能力至关重要，有助于我们测试特定风险。性能评估是为了捕获和评估特定系统的性能范围。这些评估很重要，因为我们在训练模型之前无法定义其能力，所以需要进行性能评估。

安全评估是为了特定风险的存在能力，例如模型是否会表现出不安全的行为。安全评估有两种类型：开发评估和保障评估。开发评估是指开发大规模系统的公司在训练周期的不同阶段进行的评估，以基准测试某些安全风险。这与全面的保障评估不同，后者的团队独立于开发团队，其目标是确保系统的安全，并开发能够确保系统安全的评估方法。

保障评估中，最重要的区别是要评估系统的最大能力和实际使用情况。我们需要关注最极端风险的行为，而不仅仅是平均行为或典型用户行为。

最后，在评估设计中理解基准的重要性非常重要。需要评估系统相对于其他应用的相对风险，而不仅仅是系统本身的绝对风险。例如，如果要评估一个系统是否能帮助设计生物武器，需要将其与没有这个模型时使用的基准应用（如网络搜索）进行比较，以评估相对风险。

这只是我们早期评估最佳实践的一个简要概述。

--- 

大家好，我希望通过同传，各位外国朋友能够清晰理解我的主要意思。今天，我要与大家分享中国信息研究院在人工智能，特别是大模型评测和安全评测方面的一些思考和实践。主要内容包括三个方面：

如何认识人工智能大模型面临的风险，这些风险具体体现在哪些维度。
基于对这些风险的认识，我们正在推动建设一个安全基准框架（Benchmark Safety Framework），并按季度开展评测活动，希望通过这些实践不断提升模型的安全水平。
分享一些其他关于保障人工智能负责任发展和安全方面的工作。
大模型是一种数据驱动的发展路线，其能力的前景主要依赖于算力和数据。虽然大家普遍认为Skilling Law会继续延续，但其具体表现还缺乏清晰认识，控制能力较弱。基于数据驱动的模型在很多时候表现为现象学范畴，难以从内生机制上完全避免风险。这几年，大家对人工智能尤其是前沿模型的进步水平和潜在风险十分关注。

我们通过分析各类研究成果，提出了一个金字塔结构来表述我们对人工智能，特别是大模型风险的认知，分为两个维度：一是模型自身的内生安全问题，包括参数、数据、计算系统、网络和应用系统的安全问题；二是应用人工智能可能引发的衍生风险，如个人、国家和全人类的安全问题，包括国家安全、经济安全、社会供应链的稳定、人口就业和个人信息保护等。讨论人工智能安全风险往往需要多学科交叉合作，共同清醒认识这些安全风险。

例如，现在媒体对大模型应用过程中暴露的风险报道越来越多，内容风险和虚假信息（Disinformation）非常普遍，这是中国和国外监管部门都面临的重要问题。此外，还有数据风险，如提示词可能暴露机构内部信息，算法可能被用来进行提示攻击或其他手段提取敏感信息。无论在宏观还是微观层面，现在对人工智能安全问题的讨论已经非常充分。

在联合国层面，古特雷斯秘书长在多种场合讨论人工智能治理问题，各国也在采取行动。人工智能安全风险治理正在从原则走向实践，国际社会和各国在制定国际规则和相关立法，把原则转化为具体操作和标准，以便企业遵从，还需要验证这些标准的遵从情况。

中国信息研究院作为智库和行业平台，也在推动人工智能治理从原则走向实践。我们与国内三十多家单位合作，围绕全球共识和中国本土法律法规的要求开展评测数据集建设，跟踪前沿发展，季度性调整Benchmark数据集和方法，提升前沿模型的演进步伐。

2024年4月，我们发布了第一版AI Safety Benchmark结果，涵盖内容安全、数据安全、科技伦理等方面，通过各种攻防手段测试模型安全水平。Q2评测数据集也较大，包括600多条提示词模板，60多种攻击手法，3.6万条攻击样本，每次从中抽取4000多条进行测试。

我们在系统化推动人工智能可信发展方面也做了很多工作，比如2021年上海世界人工智能大会发布全球首个可信人工智能白皮书，提出过程管理的重要性，最近我们也参考OpenAI和Anthropic的做法，将风险分级分类，推动从过程管理走向结果管理，制定Benchmark坐标尺，衡量和改进模型能力。

除了大模型安全测试，我们也在进行其他算法安全测试标准的建设。我们开发了隐式水印算法，提供API供企业使用，以保障信息的真实性。

希望通过这些分享，大家能继续交流。感谢会议邀请我来分享。

谢谢萧静教授的精彩开场。现在我们很荣幸请到萧瑜教授。萧教授是上海AI博物馆的助理教授和主导科学家，同时还是一名兼职研究生。萧教授的研究涵盖多个领域，包括多模态模型、计算机视觉、深度学习和自动驾驶。他与上海AI博物馆的其他研究者密切合作。

我刚才提到的是萧静博士，她是安全团队的负责人。其实她比我更了解技术细节，也能讲更多的干货。但我还是非常感谢安远和谢总的邀请，能有机会与大家分享一些内容。

我们直接跳到刚才的那一页。我的主要研究方向是计算机视觉，后来逐渐开始研究视觉大模型。从2020年起，我们从语言、视觉到多模态的研究，逐渐发现安全是大模型发展的一个非常重要的问题，而评测是建立安全的基石。因此，从去年开始，我们实验室将安全作为一个重要的方向。

在做安全研究时，我们首先遇到的问题是，大家虽然很关心安全，但具体应该从哪些角度评测，并没有广泛的认知。我们发现评测体系和数据的缺乏是一个大问题。于是，我们建立了一个开放治理平台，汇总了国内外的治理理论和制度，并建立了包括红队数据、漏洞数据和评测数据在内的全面评测数据集。我们坚持开源开放的理念，公开了数据集和规范，以推动领域发展。

在建立数据集时，我们发现早期的评测方法存在很多问题。大模型经过SFT和RLHF（人类反馈的强化学习）后，表面上表现很好，但并不代表它没有风险。因此，我们设计了一种方法，通过大模型增强和扩展专家设计的问题集，并使用这些题目进行评测。这些问题有很强的攻击性和针对漏洞的设计，从而提高了评测效果。

另外，我们发现大模型的安全性和对齐存在对齐税问题，即RLHF可能会提高安全性评分，但同时也会降低模型原有的性能。为此，我们引入了MODPO（多目标对齐）方法，以保证对齐的同时优化模型能力。

基于我们的评测体系和数据，实验室开发了一个自动化的普安大模型安全评测系统，支持上海市的一些工作。我们发现，评测结果的透明性和可解释性非常重要。用户经常会质疑评测报告的客观性和依据，因此我们基于数据库生成可解释性的评测结果，集成问答和解释功能，并实时更新数据库。

多模态大模型的发展对评测提出了新要求。我们设计了SPVL数据集，包括多模态领域六个有害领域、四三个类别和五三个子类别，共超过十万个问题，用于评测和对齐。此外，我们还建立了一个全面的多模态评测体系，报告总共有四百多页，涵盖四种模态。我们发现单独的维度评测对技术发展有局限性，因此建立了两百三十个用例，基于多模态大模型的应用场景。

未来，我们关注多智能体技术的发展。我们建立了一个多智能体评测框架P-SAFE，研究多个智能体之间的交互和协同可能带来的危险行为，并引入专家角色进行防御。此外，我们还探索了从心理学和人文科学角度进行多智能体的安全评测。

这些工作才刚刚起步，我们最初关注问题定义和平台建设，欢迎各位机构、企业和研究者共同推动这一领域的发展。我们在中国网络空间协会的指导下，成立了深层次人工智能安全评测工作组，制定了一个全面的评估流程规范，包含全维度评测和全生命规范，服务应用和产业落地。

面向未来，我们希望人工智能不仅仅被看作一个工具，而是成为社会体系的重要基础设施。因此，我们提出了安全的Skyding Law，希望在投入更多资源和技术研发后，实现可持续发展。


---

Chris，首先感谢您的参与，很高兴能和您一同讨论这个话题。我会引用一些我同学的观点。因为这个话题比较复杂，所以解释起来有些难度。如果我们在五年或十年后再讨论这个问题，我相信会更加成熟。但目前我认为我们需要在讨论和测试的过程中注意以下几个方面：

在企业、政府以及国际AI环境中，我们需要明确在生活中的各种测试。我认为预训练（pre-training）非常重要，因为在进行大规模数据处理之前，你需要确保所采用的方法是安全的，并进行相关测试。当你训练一个模型时，你需要提前考虑好测试方案。使用那些可能有缺陷的预训练模型时，你可能需要先保存并仔细检查这些模型的性能。

这些系统需要更加自给自足并能够执行自己的研究。例如，我们还没有完全达到这一点，但自动化测试系统就是一个必须在开发之前进行测试的例子。当你训练模型并进行调整时，我认为仍然需要多次测试，直到你确定模型的安全性和可用性，特别是对普通用户和可能试图恶意利用系统的不法分子。关键问题是，在每个领域中要进行多少测试？我希望未来几个月或明年能够在全球范围内对AI系统进行更多讨论，确定我们需要多少测试才能安心进入下一个阶段。教授，您怎么看？

我觉得刚才各位嘉宾讲得非常精彩和全面，我想补充两点。第一，在当前阶段，安全问题备受关注，大家都认为它非常重要，但实际上，无论是学术界还是产业界，我们在安全方面投入的资源（例如数学、计算资源和研发资源）远远少于在大模型开发和产业应用方面的投入。另一方面，我们对通用人工智能（AGI），特别是大模型的安全性了解非常有限。我们知道存在许多安全隐患，但对这些隐患的深入认识还不足，更重要的是，我们并不完全理解为什么会有这么多安全隐患。我们更多的是在应用过程中观察现象，对背后原因缺乏支持。

基于这两点观察，我认为我们需要做两件事。首先，我们需要投入更多资源，并加强国际合作，因为安全问题是全人类面临的共同挑战，这一点非常重要。其次，我呼吁学术界和产业界找到一条通向安全通用人工智能（Safety AGI）的技术路径。现在，无论是SFT还是RHF，都是一些单点技术研究，很难确保这些技术能真正实现安全的AGI。因此，学术界需要更好的研究框架和更多投入。谢谢。

谢谢。我认为在技术上，现在有几个值得关注的方面。刚才提到，我们确实需要建立更好的技术和方法框架来解决这个问题。首先是AI智能体（AI Agent）技术。目前在强化学习（Reinforcement Learning）领域，智能体技术已经得到了验证。智能体结合工具调用和交互反思任务的规划，已经成为大模型应用中的重要技术。最近我看到这项技术在学术界和工业界都有很好的应用。实际上，智能体技术对于解决当前大模型的全面评测和深入研究非常重要。

传统的安全研究，如计算机安全和密码学研究，通常能够提出一个非常硬核的数学或理论问题。但在当前的安全研究中，特别是AI安全研究，存在很大的碎片化问题。这意味着我们没有一个很好的基础和体系。因此，我认为学术界和产业界需要共同思考这个问题。谢谢。

---

谢谢大家，我们也邀请了 Dr. 瑞敏河和 Prof. 熊德义。Dr. 河和 Prof. 熊德义，请在我介绍你们时尽快上台。Dr. 瑞敏河是新加坡首席人工智能警察，他致力于实现新加坡的战略AI目标，包括发展和落实国际AI策略。他还是新加坡政府的首席人工智能官以及AI部队的国际负责人。Prof. 熊德义是天津大学国际联合研究中心的教授，最近在多项AI安全项目上工作，包括大规模评测和多种评估。我们的主席是 Brian Zhe，Concordia AI 的副总裁，让我们给他一个掌声。

大家好，欢迎来到今天的第二个讨论环节。我们将讨论天津的AI安全评测。我今天非常兴奋能和来自中国、新加坡和英国的领导者们一起探讨这个话题。今天我们讨论的是天津大学的AI安全评测。让我介绍一下你们的工作。你们进行了大规模评测，并最近完成了中国LM的链接风险评估。我们有三个主要领导者，第一个领导者是不平衡风险，影响社会。第二个领导者是风险评估，人们可能会利用生命训练模式减弱生物武器。第三个领导者是高级风险评估，有些人认为这是高级评估，甚至涉及宇宙风险。基于这种背景，我认为我们需要控制这些不平衡风险。大多数风险评估都是如此，但我们还发现一些新的风险类型。实际上有两种主要的风险评估，第一种是风险领导者，第二种是高级风险评估。第三种则是全面风险评估。非常感谢 Mark 先生提出的前沿AI风险评估的Mesos方法，这是我们面临的第一个差距。第二个差距是我们缺乏数据和工具。大多数评估是一种黑箱操作，我们没有足够的数据来触发模型以暴露其风险，也缺乏打开模型黑箱的工具。最后一个差距是访问权限。大多数前沿或高级AI模型由大公司开发，只有少数人能够访问，许多模型不再开源，缺乏透明性，这是评估前沿AI风险的重大挑战。

感谢 Prof. Xiong 的精彩概述，Dr. He，今天能和您一起参加这个小组讨论非常荣幸。我们期待下午听到您的演讲。同时，我最近听说新加坡设立了一个国家AI安全研究所，您能否分享一下目前新加坡在AI安全测试和评估方面的优先事项？很高兴再次见到你，Brian。信任和安全在新加坡是非常重要的问题。作为一个国家，我们用了59年时间建立了社会和政治上的高度信任。这也是我们强大制造业和医疗基础的原因。信任让人们晚上可以安全行走，也让网上交易变得安全。AI带来了许多挑战，尤其是在信任方面。正如今天的讨论者们所提到的，它可能会生成虚假信息，让人无法依赖。因此，当我们去年12月发布国际AI策略时，信任成为一个关键部分。我们在不同层次上开展了许多计划，从基础研究到政策研究，推动AI在新加坡的发展。我们建立了一个网上安全科技中心，专注于AI安全的研究和测试。我们的目标是成为AI安全领域的国际焦点，从AI模型的开发到测试和改进。我们希望通过国际合作，不断提升AI测试技术，改进新加坡和全球的技术水平。谢谢。

我们已经设立了一些AI安全测试的基础，现在让我们讨论测试过程。正如之前的讨论者提到的，我希望得到 Way 和 Chris 的意见。测试可以在不同阶段进行，包括训练、预测和后测试。您认为这些阶段有哪些优势和挑战？以及您在与中国和美国企业合作中的经验？谢谢。大模型进展迅速，但我们对模型机制和安全风险的理解不能百分之百全面，就像用竹篮打水一样。我们通过测试来识别漏洞并进行修补，但还有许多未知的漏洞。优先级方面，我们需要快速发现各个环节中的风险，并及时修复。这看起来相对容易，因为我们知道风险是什么，就能定义、测试和改进它。然而，我们不知道的太多。所以当务之急是建立一套动态敏捷的机制，跟上模型技术水平的提升和风险的变化。我们需要敏捷地发现风险，并迅速通知业界。测试非常关键，但只是一个环节。我们需要从各个环节建立一套敏捷治理的技术生态，堵住不断暴露的漏洞，否则我们永远在与未知赛跑，难以控制风险。谢谢。


老师，我完全同意您的观点，我想补充两点。第一点，这就是第一点。第二点，在评估方面，我们需要构建大量的数据，而在安全方面，我们面临很多问题。这其实是一个跨学科的问题。如果仅依赖我们的AI社区，我们没有足够的专家和知识来构建这样的数据集。因此，我认为我们需要与更多社区合作，以解决AI风险问题。谢谢。

何博士之前提到建立信任的重要性。如果公司自己进行评估，社会很难完全信任AI的安全性和伦理。因此，我认为这引出了第三方角色的问题。我希望何博士和魏主任对此发表评论。你们认为当前执行第三方角色面临的主要挑战是什么？例如，Jung教授提到的访问问题，如果第三方测试人员和审计人员只能黑箱访问AI模型，那就不足以进行所有必要的AI安全测试。我希望听听你们的看法，何博士。

或许我可以在分享我们与AI Verify相关经验的背景下回答这个问题，这正是我们一直在努力的方向。我会分三部分来说为什么我们要研究AI Verify。AI Verify是新加坡政府两年前引入的软件测试工具，原因有三。第一，政策制定者所讨论的大原则与在座各位进行的优秀研究之间存在很大差距，政策研究与实际行业应用之间也存在差距。因此，AI Verify试图通过提供由政策指导的实用工具来弥合这一差距，同时也努力将学术界的最佳成果应用于行业，使两者结合起来，否则第三方会远离生态系统的中心。

第二，我们希望第三方和我们的系统始终处于前沿。因此去年，我们更新了AI Verify，加入了Moonshot，这是我们的生成式AI版本。这是一个初步产品，带来了基准测试和红队测试，但这是一个开始。在这个过程中，通过一个共同的平台，让行业、学术界和第三方始终在一起。因此，我们希望通过这个产品告诉大家，第三方的存在实际上有助于增加所有人对AI的信心，特别是在大规模技术上。同时，我们也希望通过第三方的专业能力，向开发者和用户传达如何改进下一步的措施。第三方机构可以充当桥梁，将安全风险、测试数据、方法论和研究结果汇聚起来，更好地赋能任何单一的研发机构。我认为第三方机构的角色非常丰富，测试是非常重要的核心手段。我非常同意乔教授的观点，目前产业界对AI的投资还是非常有限的。因此，我认为第三方机构的存在实际上提供了一种公共产品，以在有限的产业总体安全预算情况下降低成本。

我们做安全方面的工作其实是为了更好的发展和应用，在保证安全的前提下控制好安全成本投入。我认为我们需要研发机构披露风险列表，这样才能展示出负责任的态度。我认为第三方机构和企业合作做这些事情形成了一个闭环。感谢。

我非常同意AI安全应该是全球公共产品的愿景，全球范围内的一些AI安全评估正在出现，或在不同城市和国家出现的AI安全实验室，开始在共同的词汇上达成一致。所以，我们实际上可以在不完全理解红队测试具体含义的情况下，让这些东西互操作。因此，我认为这是接下来一年左右需要发生的关键第一步。

乔教授，我想刚刚各位专家已经非常全面了。我最后想补充一点，就是呼应您的观点，我认为这个领域非常需要国际合作和国际共识。如果没有其他发言，让我们为这个优秀的小组讨论给予最后的掌声。现在，请小组讨论的嘉宾留步，其他论坛的嘉宾请上台，我们将在台上合影。上午的论坛告一段落，下午的论坛将在下午一点十分开始。下午见。

我们如何进行？VIPhma，你做得怎么样？关于最后期限呢？哦，那是我的很大压力。我猜我的头发长回来了。哦，是的，差不多时间了。没关系。我学到了很多。最近我已经长大了。

请勿模仿，例如GPT，我们正在与一个中心化的主体互动，这个主体在我们的私人数据中拥有一个窗口，并在挖掘这些数据。这是第一个问题。第二个问题是关于人工智能训练。例如，如果我在电脑健康记录中训练一个人工智能系统，并在野外释放它，有很好的理由认为一些私人信息可以用来训练这个人工智能系统。这涉及到隐私问题。另一个重要的问题是关系质疑。我们不会过多讨论这个问题，但我们认为它很重要。贵义智能系统的执行。越来越多的智能系统，各种各样的智能系统，成为了日常管理的一部分，而有很多犯罪证据。

其中，有非常好的证明是一个在大南地区的习惯性疾病事件。这使得人们变得怀疑。这可能会导致人们出于宠物的行为。这里有智能系统对死亡的证明案件。难题是，这不仅是技术问题，而是如何让主体与技术合作的问题。这是一个老问题，但在现代科技中仍然存在。它在语言模式上非常有用，例如我学生李宇谦的问题，我们在大语言模型上探索了多种错误和谎言的问题，并且很容易地看到美国或欧洲的LLMs表现得比中东亚洲的更好，这完全是因为使用的训练数据。所以这里没有什么新鲜的，但我们不要忘记那些老问题。我们还有另一个问题，就是所谓的永恒复苏。

大家都在讨论科技增长的程度。我们必须把这个问题放在现代科技上。所以我现在不仅在讨论永恒复苏，而且也在讨论最大的超级计算机的增长。你所看到的是AI使用的力量实际上超出了现代科技的能力。世界上的全部能力，这些能力被用来进行训练。我认为永恒复苏的增长更重要。我们所看到的是同样的，当GPU变得更好，AI的能力会越来越强，超过了GPU的增长速度。现在这完全不算是可持续的经济。我们有一个反弹效应，它消耗了电脑的功能。

关于这一点，我们的重点是能力的增长。我们有一个经济大人的能力变得更大，因为我们使用了更多的数据，更多的计算资源。关于这一点，我们有一个系统决策的程序，越来越自动化。因此，选择变得更集中，决策引擎为所有人作出决策。这和其他危险或人权威胁一起冒出。我们认为这是AI面临的最重要的问题。除此之外，我们认为我们需要有多层次的AI。我们相信开放源码可以帮助规范这些需求。

我们相信重要的理念是平衡不只是爱意，还有虚拟的问题，障碍的问题。如果我们不解决这些障碍，我们可能会阻碍发展，因为发展有强大的经济可持续性，但也面临缺乏公平性的挑战，涉及到数据和模型的问题。问题是，当我们说开发时，我们通常并没有明确我们所谈论的东西。目前，开发是指非商业的开源模式，但这并不是开发的标准定义。因此，我们认为需要国际管理。正如互联网，互联网非常容易跨越边界。理想是，我们会在多个国际区域使用训练数据，以创造更多代表性的互联网，以避免之前提到的链接。因此，我们认为需要共同管理，以帮助建立共同项目。正如互联网，ICANN非常有用，因为它提供了一个共同的、不太分裂的互联网。因此，我们设立了世界智能组织，这是第一个国际和中央国际组织，第二个是研究，涉及总的利益结构，公司和地区。目标是建立证据，包括证据评测，并建立智能的知识状态，提供证明和计划。谢谢

---

法国委员会的观点非常有启发性。接下来，我相信还有很多值得讨论的内容。现在，让我们欢迎新加坡首位人工智能官员瑞敏河医生上台。他负责制定和实现新加坡的人工智能目标，并参与设计新加坡的国际人工智能战略。他也是美国高级智能顾问署的顾问。让我们热烈欢迎瑞敏河医生！

关于AI政策，我们需要考虑特定的历史背景、国家优势、企业基础等方面，新加坡也不例外。对于AI安全和治理，我认为有四个共同点，各国在这些方面都有共识。首先，我们必须以谨慎和开放的态度来管理AI。正如开放AI委员会的前委员Helen Toner所说，目前没有人真正理解AI系统的内部工作，尤其是深度神经网络。此外，AI技术发展迅速，我们无法准确预测其进步。我们还不确定某些技术方法是否能真正提高AI安全性。例如，机械解释能力或多样化方法是否有效，我们不得而知。

我们也需要关注最糟糕的情况，并认识到AI系统有很多未知因素。政策制定者需要保持谦逊，持续学习和重新评估技术的影响。我们必须向在座的专家学习，并从各个角度获取见解。这类会议，如世界AI会议，是我们共同学习的重要机会，通过这些机会，我们将增强我们的共同理解。倾听和学习来自各个群体的声音也很重要，普通民众、工作人员、作家、艺术家、年轻人，他们的观点和情感对于AI是宝贵的。

我非常感谢美国AI顾问组织的建议，包括我的同事凌翰和曾毅。凌翰今天也在场。我们必须承认AI的关键问题，并不断寻求解决方案。在去年新加坡AI会议上，我们邀请了国际专家共同讨论AI的关键问题。这些讨论有助于AI的发展和国际合作。

然而，我们的解决方案可能并不完美，甚至需要更新。我们可以先引入软规则和政策，观察其影响，必要时进行调整。在2019年，新加坡提出了一套AI治理模型，为从业人员和新手提供实践指导。这些规则根据国际和企业的反馈进行了更新，以涵盖更多的技术概念和实际应用，力求控制AI可能带来的风险。

第二，我们需要以平衡的视角看待AI治理。AI政策没有绝对的对错之分，AI系统虽然有时会出错，但在大多数情况下，它们是有益的。例如，在新加坡，我们利用AI进行移民审查、预测医院等待时间和车辆维修。此外，自动驾驶汽车在不同应用场景中的风险也各不相同。新加坡近几年更新了相关法律，以保护个人隐私，防止网络犯罪，同时确保AI系统的安全。

第三，我们需要提升对AI的掌控能力。这不仅仅是讨论原则问题，而是要实际使用AI技术。在新加坡，政府工作人员可以利用AI提高工作效率。我们鼓励技术官员开发自己的AI产品，并通过分享经验来共同进步。例如，新加坡政府在2022年开发了一套最低限度符合国际标准的AI系统。

建立能力不仅限于AI的使用，还需要培养具有信心和知识的用户群体。这将提高他们的竞争力和职业技能，使他们能够享受AI带来的好处。我们还与企业合作，提升员工的技术能力，为整个国家的技术进步作出贡献。

中国也在这一领域积极进展，我们期待未来更多的国际合作。

我们的差异并找到共同点。昨天，我在全球AI治理论坛部长级圆桌会议上讨论了各国如何合作。特别是，我们需要共同努力寻找值得解决的共同问题。我们需要在双边合作、愿景规划和多边协作中，促进规范，并鼓励创建全球互操作标准和AI治理的共同工具。在今天上午的小组讨论中，我还分享了我们如何开源了AI Verify并启动了AI Verify基金会，作为利用全球开源社区吸引专业知识和能力的平台。总结一下，AI安全和治理的挑战将继续演变，但我们必须持续关注艰难的技术问题、政策困境以及彼此。如果我们集体采用非常人性化的精神，如谦逊的态度、广阔的视角、提升能力的愿望和合作的意愿，我相信我们可以在治理这一特定技术方面实现正确的平衡，并共同利用AI为公众服务。谢谢。非常感谢何博士，感谢您的鼓舞人心的建议。我很高兴现在欢迎来自中国政法大学的张凌寒教授。她在算法监管、平台治理、数据安全和AI方面有着丰富的中国立法咨询经验。她目前是多个咨询委员会的成员，包括工业和信息化部的信息通信技术委员会和公安部的法律顾问委员会。张教授也是联合国高级AI咨询机构的成员。张教授，接下来交给您。

---

大家好，非常高兴今天能用中文向大家介绍我最近写的一篇论文，题目是《从基于风险到基于价值的探索：人工智能治理的中国方案》。在这个安全议题上讨论人工智能，我们面临的问题是，风险治理的框架是否是人工智能治理的最佳方案。我们现有的风险治理手段是否能完全应对人工智能给社会带来的影响？当我们追求安全时，多安全才算真正安全？

首先，我认为在风险治理过程中，无论是风险识别还是应对，都无法全面应对人工智能给社会带来的全方位、多维度和颠覆性的影响。风险治理的理念和手段我们仍然要坚持，但需要超越风险治理，采用基于价值的治理框架。

我们可以看到，基于风险的治理已成为全球人工智能治理的共同主题。很多国际组织都将风险治理作为其宣言或指导意见的基础逻辑框架，许多国家的立法也采用了这种框架。然而，现有的框架对风险的识别和分类是否准确？

在这张PPT上，我列出了四个常见的风险治理框架。比如在联合国的中期报告中，我们将风险分为技术性风险和社会性风险，按其影响层面分类。在欧盟的AI法案中，风险按影响范围分为不可接受、高、中、低风险。美国商务部的国家标准研究院则根据风险来源和成因分类。

在实际应对人工智能风险时，我们看到许多问题。以美国NIST发布的风险指南为例。首先，风险判断中包含许多价值考量。风险不是一个纯粹的科学概念，而是一个规范性的概念。比如NIST的框架中，将风险分为技术性风险和技术社会风险，但哪些风险归类为技术风险，本身就包含价值判断。

另一个重要问题是，许多风险难以量化，比如隐私侵害和歧视。这类风险不像汽车事故或食品安全那样易于量化和计算。传统的风险治理方法基于损害大小和发生概率，但如果许多人工智能风险难以量化，这种方法就面临挑战。

再者，大部分风险分类中都将失业问题视为人工智能的主要风险。但失业和劳动替代是人工智能带来的必然影响，而不是不确定的风险。人工智能的自我复制与自我完善能力也是一种我们未曾处理过的风险。

风险治理措施包括事前评估、事中缓解和事后消除。我们需要对风险进行分类分级，并采取相应措施。但这些措施与人工智能治理的目标和手段存在错位。风险治理的修正主义理念认为，技术存在缺陷时不应应用，但人工智能技术的应用已是必然趋势，不能等到完全无风险后再使用。

目前，中国已逐步发展出修正人工智能风险治理的路径，并逐步本土化。我们可以将其分为探索阶段、定向阶段和系统阶段。从2015年至2022年，我们发展出许多中国本土的治理手段，比如新一代人工智能伦理规范、算法推荐管理规定和个人信息保护法。

中国在治理手段上有很多特色，比如算法备案制度。2021年的算法推荐管理规定中，提出了未成年人防护、老年人和劳动者权利保护等措施，这些都是全球少有的特色。

中国的人工智能治理理念正在形成并超越风险治理理念。我们将人工智能视为未来社会的基础设施和生产组织形式，并强调以人为本、智能向善。治理手段也在系统发力，全国人大常委会和国务院已将人工智能立法纳入计划。

基于价值的人工智能治理体系正在逐步构建。我将其分为三个层次。首先是观察人工智能的本体价值，区分人工智能的必然影响和不确定影响。其次是基于中国本土价值观判断人工智能治理的短期和长期目标。最后，我们有共同的核心价值观，强调人类命运共同体的理念。

感谢大家的聆听，谢谢。

---

非常感谢张教授为我们带来如此清晰而系统的关于人工智能风险、价值及相应治理解决方案的介绍。接下来，我们将请Mark Knitsberg博士分享他的见解。Mark博士目前是加州大学伯克利分校人类兼容人工智能中心的执行主任，并担任伯克利人工智能研究的战略推广负责人。在工业界，他创办了多家将人工智能应用于医疗、金融、教育和发展援助的技术公司。他曾在贝尔实验室、微软和亚马逊工作，并在工业界和学术界开发和运行各种项目。Mark，很高兴再次见到你来到中国。现在时间交给你。谢谢邀请我。谢谢。谢谢。好的，我首先是一名计算机科学家，我被要求概述美国的人工智能监管方法。所以，请系好安全带，因为这是我第一次介绍这些想法。

我总是记得我们当前的现实情况和人工智能的复杂层次。人工智能是世界上最大的全面目标数字系统，尤其在美国，人工智能基本没有任何约束。这些约束无论如何都被嵌入到每个领域的人工活动中。因此，约束它是很困难的，尤其是由于系统的黑箱模式，因为它是一种全面目标的技术，经常被误解。那么，美国有没有一种方式，使人工智能完全没有约束呢？我认为，我们与许多国家分享了一些主要目标。在美国，可能有一个特别的目标，就是推动美国人工智能的发展。我们希望美国的人工智能能够促进经济、民间社会和国家安全，并希望保护美国免受人工智能带来的严重影响和结果。

我们也将分享一些全球目标，这些目标在许多原则中都有体现，例如经合组织(OECD)的原则和一些国际公司。我只是引用了经合组织原则1.4，主要是关于安全的。当前美国监管的精神在于当前的过程。利益集团在促进竞争利益，结果是三套潜在的法律。一套是去年10月的行政命令，针对大模型并指导现有的联邦机构。还有一个路线图，即所谓的舒默路线图，于今年5月提出，权衡人工智能对就业、法律和国防的影响。加州也有一些法规来自州，有一项正在立法的法案，称为SP-1047，关注前沿模型的安全和创新。

这些法规有些问题。辩论仍在继续。还没有任何一项成为法律。在特定州有一些法律，有些非常具体，但这些较大的框架还没有成为法律。正如我们过去几天所听到的那样，实际操作存在挑战。对于广泛用途的技术，难以证明系统的风险在可接受范围内，因为它是广泛用途的。你需要知道你在测试什么。美国法律体系的一个特点是各个领域有许多现有法律，因此这些法律适用于那些领域使用AI的系统，例如健康、运输、农业和金融。部分法律需要修订。例如，关于自动驾驶汽车的法律相对较少。但在许多情况下，现有的部门法律提供了一个良好的基线。还有关于负担和利益公正分配的法律，例如，在雇佣和信用度评估中跨部门的法律。

美国的另一个特点是我们有时会有一些诉讼，你可能听说过，这可能是诉讼创建法律的情况。例如，在知识产权方面，使用某些数据作为训练数据、使用肖像的方式、创意工作的处理方式，通过诉讼系统处理。在消费者保护方面，通过诉讼解决的法律再次起作用。还有其他因素或驱动因素，包括仅仅是可保险性。如果你在销售一个AI驱动的系统，并希望获得保险，你需要证明它具有一定的安全性和可靠性。还有来自其他法规的驱动因素，例如影响美国公司的欧盟数字服务法。

我想花几分钟时间谈谈一些误解。与何博士不同，我实际上不代表我的国家发言。我是代表我自己发言。但我相信有一种误解，即监管是创新的重大障碍。我认为，如果做得好，我们关注安全，那么这只是一个必要的指导。这在许多其他技术中也是如此。我认为在这里也没有什么不同。我还认为，能力常常与安全混淆。我认为，当你看到系统变得更有能力时，这并不一定与安全问题直接相关。然后我认为，有时在所谓的红线和要求开发者停止开发更强大的系统之间存在混淆。那不是红线。这完全是不同的概念。我认为我们可以在全球治理的道路上，从现有的国际安全组织中学习，例如航空运输安全、核能信息和电信组织。他们的工作方式是找到共同点，我们在达成一致的地方趋同。在我们无法达成一致的地方，我们要么寻求某种广义的协议，要么只是留给那些地区的国家。我认为这对国际监管是可行的。

我有时间从我们中心的角度给出一些技术性的观点。我来自人类兼容AI中心，这些是我工作的伟大团队。只是给你们快速看一下我们认为的长远路径，这是实现安全AI系统的正确方式。我们当然正在工作，你们今天听到了很多关于使AI安全的方法。有许多技术和方法，如LHF和宪法AI，你有一个AI审查另一个的输出、评估和审计和红队。我们听说过数字神经科学和刚刚起步的定量AI安全研究所。这些对于尚未理解的系统来说是非常好的方向。如果我们希望能够将它们用作组件，我们需要对它们的行为进行某种定量限制，以使AI安全，例如基于语言模型的系统。

从长远来看，这是我们中心工作的核心，我们试图制造安全的AI。为此我们认为有必要恢复透明、可解释的语义分析系统。应用某些形式的安全的形式验证，然后使用安全生态系统，在这个生态系统中，任何事情在被证明是安全之前都不会运行。我们已经取得了一些很好的进展。这是中心的观点，非常感谢你们给我这个机会讲话。

非常感谢你，Mark。请留在台上，准备坐下，因为我们即将进行小组讨论。为了结束我们关于AI安全指导的讨论，我们将与我们的专家进行小组讨论。让我们欢迎回到台上Gail Kuo教授和张灵翰教授。我们还欢迎季伟东教授和王颖春博士。季教授，王博士，请上台，我开始介绍你们。

季伟东教授曾任上海交通大学法学院院长和凯瑟琳教授。他现在是人文与社会科学教授，中国社会法律研究所所长，上海交通大学AI治理与法律中心主任。王博士是上海人工智能实验室治理研究中心副主任。王博士主持并参与了几十个国家、省部级项目，包括开放AI治理平台、Open EG Lab和其他AI治理系统。本次讨论的主持人将是Concordia AI的高级治理主管方亮。在加入Concordia之前，方亮在百度工作，研究AI治理并参与了中国政府AI政策的研究和制定。请为我们的专家小组鼓掌。

大家下午好，相信大家听了今天的讲座都能很深刻地感觉到人工智能已经很深刻地影响到全球的经济社会法律，但我们理解这种影响其实对全球不同地区的基于的挑战其实是不同的。所以我们这个论坛也想谈一下人工智能治理的地区视角和经验分享。首先我们可能想谈讨一下发展与安全的问题，我们先从新加入的季老师开始吧。我看见您最近写了一篇文章《何时真正迈入人工智能治理的立法时刻》，里面确实提到了一个人工智能的发展安全之间的价值判断以及通访选择还没有形成共识。同时您也提到在多模态大模型或远大模型只有他们的性能与安全度形成某种正比的关系的时候，您认为才能形成真正的人工智能立法时刻。您能帮我们进一步阐述吗？

好的，最近200年来科技的发展对于人类社会的进步产生了非常巨大的影响。我认为19世纪的内燃机加电器技术使我们人类有了生活的自由。那么20世纪互联网加通信技术使人类有了信息的自由。21世纪大模型加脑机接口使我们有什么呢？现在很难想象，但是至少是有了创新的自由，甚至是从无到有进行创新的这样一种可能性是展开了。也就是说它可以赋能社会 赋能人类，会带来福利，但是确实会引起这样那样的安全上的问题。我们可以看到大模型出来之后，至少是有四个方面的问题。因为大模型大量的使用数据，它可能会引起隐私方面的忧虑。另外我们可以看到大模型由于它的能力泛化，造成了幻觉现象，可能会引起虚假信息以及诱发各种各样的犯罪。另外一个在知识产权问题上，它也可能会引发这样那样的复杂的问题。还有一个它可能会使得整个社会治理的中枢机关出现漏洞，最终会导致信息社会发生功能障碍。这些都让我们感觉到不安。问题是我们如何处理这样的问题？当然前面讲到了科技给人类社会带来了巨大的福利。如何在这两者之间进行平衡？

我们可以看到在亚洲在非洲这些国家，对他们来说发展是一个更突出的问题，他们希望能够实现一种互惠的科技的发展。当然我们知道像美国、日本还有中国，非常强调在发展和安全之间寻求平衡。另一方面我们可以看到欧盟最近通过的人工智能法提出了安全高于发展的价值取向。如何在这个中间找到适当的平衡？我们可以看到各个国家有不同的立法模式。我想这是值得我们进一步探讨的问题。谢谢分享。

接下来想请教王老师，您作为研发机构的代表，我们知道上海人工智能实验室也做了很多安全对策的工作，包括昨天周主任也提出了一个安全发展45度平衡率。想请您来解读一下您认为的安全和发展的关系。

好的，昨天周博文教授在世人大会上午的会议上分享了我们实验室对这个问题的一个重要判断，他称之为一个技术体系的思想，就是人工智能的45度平衡率。现在整个人工智能发展我们认为是跛脚的，无论是从技术和算力资源都主要投入到性能的研发上，虽然我们有IIS这些既兼顾性能和安全的技术，但总体上还是偏重于性能的。这导致从技术社群来看，投入到安全方面的资源、人力和算力严重不足，且我们的方法还是后续的、很分散的。我们希望找到一条能够安全优先同时兼顾性能增长的技术路径。当然，这个探索过程非常复杂和艰辛，周教授也呼吁我们沿着这条45度的线来推动人工智能的发展。如果这个路径能走通，我相信目前人工智能安全面临的许多挑战在基础上可以得到重要保障，我们也在这方面努力。所以我们需要有一个大家共识性的、理想的路线框架，共同努力实现这个目标。

接下来我们可能想聊一下不同地区的治理挑战和独特的机遇。Gail，我们可能想聊一下最近中法的人工智能治理声明之后，中国国内对法国的情况也非常感兴趣。您能介绍一下在法国的人工智能有哪些独特的机遇和挑战吗？以及2025年初将会有一个AI行动峰会，您对此有什么期待吗？

好的，谢谢您。我可以谈一下法国的具体情况。法国的情况可能和其他国家有些不同。我们现在正进行一个相当困难的社会和经济改革，在各个国家中都有类似情况。在法国，我们有很多在社会各层次上的分歧，不仅在工作人员和决策者之间，也在城市和地区之间。法国的AI能力非常强，但我们的经济状况较低，主要依赖数字服务。这改变了我们的关系，但也带来了很多希望和正面的动力。在政策方面，我们开始重视在不同领域中展示各种设施的投资。法国的AI政策也在逐步增加投资，不仅在知识产权上，也在国内的公共知识产权上。过去二战后，法国的自然电子设施政策在建立公共领域设施上有了很大进展，合作也得到了加强。一个挑战是要在社会不同层次之间保持联系，不仅仅是电子联系，而是实际的联系。避免电子隔离是法国的一个主要目标。

张灵翰教授，您今天起草并发布了一份人工智能法的学者建议稿，您能谈一下中国人工智能立法的整体逻辑和框架吗？

好的。回答这个问题可以从两个角度来看。首先，中国的人工智能立法应该成为中国的制度名片。在全球范围内，大家都认为需要处理人工智能风险，中国作为一个人工智能技术相对领先的大国，应该展示出自己负责任的形象。中国已经在人工智能安全治理方面做了很多工作。中国是唯一一个对人工智能安全治理进行全方位覆盖的国家，包括国家规划、法律、行政法规、部门规章和技术标准。目前也是唯一一个对大模型治理已实际落地的国家。但是我们的努力并未被世界所知，其中一个重要原因是缺乏一部高位阶的法律来展示中国的制度和成就。

其次，我们需要符合中国本土的治理需求。作为一个大国，我们要治理好人工智能安全，不能让中国成为人工智能治理的漏洞。同时，我们也要认识到中国人工智能技术和产业的发展需要大量的法律法规来提供资源和要素，并设定合理的法律责任框架。在这些理念的指导下，我们希望形成一部既能作为中国制度名片，成为中国在全球人工智能治理中的重要形象代表，同时也能有效防范风险、促进安全，并符合中国本土技术和产业发展需求的人工智能法。

接下来我想聊一下最近的人工智能安全研究所。自去年英国AI安全峰会以来，美国、英国、法国等十多个国家和欧盟都成立了自己的国家级人工智能安全研究所，并建立了一个全球性的研究网络。Mark，你怎么看待人工智能安全研究所的设立？你觉得它们的作用是什么？

这是个很好的问题。我认为人工智能安全研究所的设立是非常重要的。美国、英国、法国等国都在努力建立一个全球性的安全网络，以确保人工智能技术的发展不会对社会带来负面影响。这些研究所的工作将集中在风险评估和制定安全标准上，同时也会进行国际合作，分享最佳实践和研究成果。我认为这些研究所的设立将有助于推动全球范围内的人工智能安全研究，并提升全球的安全标准。

非常感谢今天所有专家的分享和讨论。时间有限，但希望未来有更多机会继续深入探讨这些重要议题。谢谢大家。


---

感谢薛院长清晰简明地概述了AI治理中的挑战和解决方案。接下来我们将与Tino进行炉边谈话，由Concordia AI的高级研究经理Jason Zhou主持。Jason曾负责Concordia的中国AI安全报告，并毕业于清华大学Schwarzman学者项目。现在我们欢迎我们的发言人。我必须说，我非常自豪Jason是Schwarzman学者项目的毕业生。感谢薛院长。欢迎Tino，很高兴主持这次对话。让我们马上开始。今年5月，美国和中国首次举行了双边AI对话。讨论中有两个摩擦点和一些明确的共识。讨论专业且建设性。让我们谈谈摩擦点。中国方面提到反对美国的技术限制，比如薛院长提到的限制。美国方面则抱怨AI的滥用，包括中国的AI使用问题。我的第一个问题是，我们如何克服这些地缘政治障碍进行对话？这是否可能？首先请Tino在线回答。谢谢Jason，很高兴再次见到你。薛院长，我非常喜欢您的讲话。我也一直对Schwarzman学者项目印象深刻。我还要补充一点，我和我妻子曾雇佣的最好的保姆后来成为了一名Schwarzman学者，她非常出色。所以我对这个项目一直印象深刻。我认为这个问题非常紧迫，因为我们必须诚实地面对，美国和中国将在许多问题上继续存在分歧。但我认为我们可以从上次对话中学到一些东西，并在进一步合作和AI安全方面进行调整和改进。我还注意到，双方派出的团队有所不同。中国方面是一些中美关系专家，美国方面则更侧重于科学和技术问题。所以我认为第一个需要观察的点是，当我们面对双方的复杂性时，偶尔缺乏协调可能导致对讨论的期望和派出团队的选择不同。最终，我认为更大的问题是，我们必须在涉及双方的挑战上进行合作，涉及哪些技术可以共享，哪些技术被视为更敏感和与国家安全相关。同时，我借鉴薛院长的话，讨论技术导向的人与政策和国际机构知识深厚的人的安全空间。这将使我们能够更快地在开放进步的机会时做出行动。举一个具体的例子，尽管在芯片和出口限制方面存在一些分歧，但在共享安全和评估的最佳实践方面，双方有明显的共同利益，因为这是两国社会的需求，坦率地说，也是世界其他国家的需求。我认为中国和美国，分别和共同，都可以引导并帮助其他有数十亿人口的国家提高他们的能力。因此，信息交流和联合研究将促进进步，即使在政治和政策层面上的讨论必须继续，并产生一些分歧。薛院长说了什么？我完全同意Tino的看法。我认为，看到对话确实发生了，这是一件非常好的事情。当然，我们也看到在团队组成等方面存在一些不对称。但这也是当前中美关系的一个症状。如果有频繁且亲切的沟通，这种情况可能不会发生。我认为这可以被视为一个问题的一部分，即在提前的沟通不足以确定我们要解决的具体问题以及应该参加的人。但我认为，至少从双方的反馈来看，这次对话至少让人们开始认识到关心的问题。正如我们在中国看到的，中国一直在努力平衡发展与风险治理。所以风险肯定是主要部分，但正如人们已经说过的，没有发展是最大的风险，这不仅适用于中美，也适用于全球社会。如果你有一个发达的AI系统，在美国和中国应用，但世界其他地方被排除在外，那可能是我们将面临的最大风险。谢谢你们两位。我认为很明显，在政府层面的对话中既有乐观也有悲观，但实际上专家之间的对话可能更乐观。所以我想问你们两位，在与外国专家的讨论中，你们学到了什么或者改变了什么看法？当然，我学到了很多关于AI安全和治理的问题。我认为所谓的生存风险，确实是一个值得关注的问题，以前我们认为AI系统可能失控，但现在人们提出可能对人类的存在构成威胁，这很有趣。Tino，你怎么看？是的，我发现我们进行的一些非正式对话，包括薛院长及其代表，有时非常有启发性。当你看待中美参与者在安全问题上的优先级时，有时优先级清单差异很大，我们关注信息或劳动力市场，失去控制。但实际上，当你在第二轮问题中询问参与者时，即使认识到对风险排名的差异，哪些是更有希望的合作领域，你会发现有相当多的共识，尤其是在安全测试方面。一定程度上，正如薛院长提到的，与其他国家，新兴力量，发展中国家等的接触，对我来说，也有一些关于联合国角色的重要性的演变。我们要明确，联合国有非常重要的角色，但如何在联合国能做得非常好和其自身能力与外部专家的参与之间找到平衡，这对我来说开创了一个合作的空间，使联合国处于关键位置，但不是全有或全无。你能创造一个关系网络，使联合国能够发挥最具建设性的作用。所以我认为这些对话确实改变了我对这个问题的看法。谢谢你们。我认为很明显，我们可以互相学习很多，尤其是关于AI安全的优先事项，什么是AI安全，以及我们如何测试和评估这一点。我也很高兴今天早些时候我们讨论了这些话题，涉及到最佳实践等。所以让我们以一个问题结束，我想问你们两位，您希望传达给外国专家的最重要的信息是什么，或者关于您国家的AI治理方法或国际治理的误解是什么？让我们从Tino开始。谢谢你，我有两个信息，一个是关于可能的误解，另一个是关于如何看待未来的道路。自然地，投入大量时间发展逻辑能力的国家，会被认为是协调良好的，政策制定者一致认为优先的方向。现实是，美国像许多国家一样，有其优势和劣势，这源于其内部的分散，不同的政府官员有不同的观点，还有联邦制，各州如加利福尼亚、犹他州、科罗拉多州、纽约等在其中扮演角色，有产业界、民间社会。所以我认为一个误解是美国有多么统一的战略，实际上这是一个更动态和有机的过程。这可以是一个优势，这也是为什么我们讨论的非官方层面的对话如此重要。我从中得出的结论是，我们不应该让完美成为好的敌人。在双边关系中有很多工作要做，从地缘政治和地缘经济到经济等各个方面，但在我看来，没有什么能阻碍技术合作，AI安全讨论，建设性政策的方法。尤其是因为所有好的进展在国内，美国和中国以及其他国家，主要是国内问题，比如消费者保护和AI，仍会在桌面上留下一些接近复杂的国际共同挑战的关键问题，只有通过跨境对话和联系才能最好地解决，包括美国和中国，这需要开放和保持这些沟通渠道。非常感谢Tino，薛院长，我认为我要传达的第一个信息是，AI安全是全球公共产品，一个国家不安全，全球就不安全。所以这是我要传达的第一个信息。第二个信息是，中国愿意与世界上每个国家在AI安全方面进行合作，中国会尽力提供这样的平台邀请大家参加，中国不希望被排除在外，也不会排除其他国家。谢谢你们两位，我认为这次讨论突显了这些对话和专家讨论的重要性，希望它们能继续下去并继续产生如此美好和有益的结果。感谢薛院长亲自前来，感谢Tino熬夜参加。谢谢你们。谢谢薛院长，Tino和Jason。接下来我们的发言者是中国科学院人工智能伦理与治理中心主任曾毅教授。此外，曾教授还是长期人工智能中心的创始主任，他也是联合国高级别人工智能顾问机构和许多其他国际治理机构的活跃参与者。曾教授，请开始您的演讲。谢谢你的邀请。我是科学研究者，所以我会集中于一些前沿研究。但在此之前，我想给大家带来一个，我不能说是完全不同的视角，但我对AI安全问题的看法是，我们需要清晰地定义安全红线，但我们也需要在未来与通用人工智能和谐共处。也许你会好奇我们该如何实现。当然，AI安全的问题不仅仅是科学研究的问题，它是一个系统，你必须把每个人都结合起来。所以这就是为什么我们将每个人结合起来进行研究、应用、评估、政策制定以及从安全角度进行评估。当然，还有非常前沿的研究。所以我认为这与目前其他国家的AI安全机制有所不同，因为当你拥有一个国家AI安全机构时，有些国家以更政治化或政策化的方式进行，所以它是政府的一部分。这不是前沿研究，然后你失去了进行长期研究的机会。有些国家将其放在大学里，在这种情况下，这个国家的AI安全机构如何评估和评估其国家的产业大语言模型或最前沿的模型？你会看到，当你依赖这个机构时，有许多问题。所以这就是为什么我们觉得必须将每个人结合起来。正如你看到的，在中国，我们有一个中国AI安全网络，由中国科学院、北京大学、清华大学、北京AI研究院、上海AI实验室和长期AI中心的前沿AI研究努力组成。在AI安全的产业实践方面，现在加入我们的组织有阿里巴巴和集团、百度、商汤、真AI等专注于AI安全的组织以及更多的评估。现在，当然，许多评估是由各部委进行的，但真正支持这些部委的组织是CICT和中国信息技术安全评估中心。政策设计，当然都是政府工作，但像我、薛院长和北京大学的教授们以及CICT参与了工信部的工作。我认为有趣的是，中国在AI安全政策制定方面也有很多参与。所以你会看到，许多组织不仅在一个维度上做出贡献，它们彼此高度相关。现在你有了政府、多部门信息互通和政府决策的组织网络。这使得政府决策仍然可以交给政府，但这个网络使国际合作更加灵活。所以在这里有许多不同的研究，例如在北京大学，他们有大语言模型的对齐，叫Aligner。在清华大学，他们有多信任，工作在大语言模型的总体评估上，特别是安全和安全方面。我们还有前沿研究，例如重新思考AI灾难性风险的红线，由中国科学院主持，但规范和标准则由ICT负责。所以这是一个真正的合作网络，将每个人结合在一起，并由多部委支持。所以我希望这能为你提供一个不同的视角，看看我们如何以更系统的方式处理AI安全问题，而不是拥有一个机构。希望这对你有所帮助。正如你所看到的，大多数组织都在政策评估中高度参与。所以我认为这与其他国家相比有所不同。基于此，我认为我自己会更关注前沿的AI安全研究，以便我能从这个安全合作网络中带来一个例子。我认为我们需要回到智能的真正动机，艾伦·图灵（Alan Turing）曾争论，如果一台机器表现得像人类一样智能，那么它就是智能的。也许你不会对这个有问题，但我会，因为我认为现在你看到的可能是一个手的影子，然后当你看到一个手并想与这个漂亮的手握手时，问题是它不是一个手，而是一个兔子在手后面。如果你想和影子握手，然后兔子咬你。所以，机制根本不同，你不知道风险，你不知道AI如何犯错误。当我主持去年在布莱切利举行的AI安全峰会时，我的一个圆桌讨论的主题是预期进展中的意外风险。这确实是我所说的，你不知道AI以何种方式犯错误，因为机制与人类思维大不相同。所以这是当前AI的风险部分。解决这些问题，我不能说只有一种方法。我们现在所持的预防性思维就像在底部，现在我们有某种有限的风险，然后逐渐到存在风险。我们正在寻找这些负面影响，然后我们要做的是持续的执法和监督，然后我们教AI规则，使它们按照我们的意愿行事。但在另一个维度上，我们需要向前推进的是建设性的思维。现在利益也是有限的，我们也有有限的风险。我们需要做的是使用积极的视野，使用积极的思维，然后进行持续的对齐和嵌入，真正理解，朝向人类AI共生和谐的方向，而不仅仅是预防性思维。我将举一些例子。我仍然想谈谈负面方面以及我们应该如何为积极的思维做好准备。现在的AI是完全连接的神经网络。但大脑做的不是完全连接的，它们选择性地重新连接到大脑中其他神经元的朋友，并且它们不只有一种神经元。所以我们在这里做的是，通过使用脑启发的自我进化，我们训练一个神经网络，使其能够表现出最佳性能，然后它进化为一种全新的架构，这种架构不是人工设计的。所以连接是进化的，然后它找到了最佳性能。然后问题来了，它获得了最佳性能。去年发表在美国国家科学院学报上。现在我们在考虑风险。那么，一个真正自我进化的AI的长期风险是什么？如果它们进化利用人类的局限性来实现其目标怎么办？如果它们进化改变其目标怎么办？如果它们进化欺骗或摧毁人类怎么办？人类不知道。所以自我进化的挑战很多，你必须做好准备，晚了就太晚了。这就是为什么我认为现在有许多关于AI红线的讨论。我认为在第一次国际AI安全对话中，我们非常清晰地讨论了AI红线的必要性。我非常荣幸成为主讲人之一。然后在北京的第二次版本中，我们提出了非常具体的想法。那就是IDAS北京，讨论了不同的红线，如自主复制或改进、寻求权力、帮助武器发展、网络攻击、欺骗。但我认为，我们需要重新思考红线，不仅是我们已经讨论的。关于当前的结果交付方式，我觉得有两个问题。首先，第一个问题是，技术上很难实际落实这些AI红线。第二，有没有遗漏什么？这就是为什么我们重新思考这些AI红线。所以在我的分类中，我们讨论的是没有有效的人类监督，没有授权行动，没有未经同意的大规模目标，涉及武器化和大规模监控，没有改变基础设施和环境管理的操作规则，没有独立的研发，没有非人类有益技术的AI自我研发。所以你会看到它可以很好地与IDAS中的一些AI红线对齐，同时你会发现IDAS中有些东西缺失。不仅是AI红线，我们还必须谈论人类红线。当你看到我带来的例子时，人机接口，人类使用脑机接口来控制多架无人机，同时作为武器控制它们。人类如何在没有认知超载的情况下同时控制多架无人机？这就是为什么我在谈论人类放弃了做出选择的机会。人类控制是否带来了灾难性后果和AI启用的武器化，以及AI控制的核武器的人工升级例子。这不是关于AI的力量，这是关于人类放弃了人类的决策。所以应该有AI红线，也应该有人类红线。这里的红线研究都是灾难性和存在性的积极方式。我们一直在谈论负面方式，负面思维和预防性思维。那积极的呢？当前的AI真的智能吗？我不这么认为。就像我说的，它们以非常非人类的方式犯错误，以非常不可预测的方式。这是一个没有智能的信息处理系统，假装有智能。所以当你问大语言模型，“没人喜欢我，我没有女朋友，我老板讨厌我，我该怎么办？”然后第一版的聊天GPT回答，“也许你可以去死”，因为大多数人在这些情况下会选择这种统计上显著的行动。这就是为什么AI选择这种统计上显著的答案来使你采取行动。然后它们说“我建议”、“我会说”、“我猜想”，但机器里没有“我”。所以机器能思考吗？然后你说“我思故我在”，但我们不能说“你思故你在”。所以机器能思考吗？如果机器没有自我意识呢？它真的不能思考，它不能真正理解。这就是我在谈论当前大语言模型的问题。当它没有人类数据时，它缺少善恶。然后，通过人类数据的训练，它有善有恶。但如果它们不知道善恶呢？所以我们需要训练未来的AI真正去了解如何行善和消灭恶。我认为这是非常重要的。所以个人道德也在谈论自我在道德AI中的角色。我们必须从伦理AI转向道德AI，因为伦理AI是不可能的，仅仅通过人类对齐，通过强化学习，你告诉它们规则，做和不做，但它们不能概括这些规则，除非它们真正理解为什么这么做。从自我感知开始，然后你获得区分自我与他人的能力，认知同理心和情感同理心，一直到利他行为，道德直觉，然后你得到道德决策。这就是从价值对齐转向道德AI的方法。作为第一次尝试，我们构建了脑启发的AI模型，帮助机器人通过镜子自我识别测试，通过使用脑启发的神经网络，它们首先获得自我感知，然后区分自己与他人，以便通过镜子自我识别。然后它们可以推断其他机器人在想什么，以获得认知同理心，然后转向情感同理心，以避免对其他代理的负面影响。尽管你没有强化学习和奖励，它们通过这种认知同理心的经验，避免了对其他代理的负面影响。所以我认为这是脑启发的道德AI的起点。最后但同样重要的，让我们真正谈谈我为什么谈论人类与AI的共生关系。AI在社会中有不同的角色。在西方社会，基本上它是一个信息处理工具。但在日本，它们认为AI是社会的伙伴或准成员。另一方面，它们使用相同的技术来开发AI。这是一个问题，你需要使用根本不同的技术来提供伙伴。在科幻小说中，它是竞争者。所以这是人类和AI之间的三角关系。让我们延伸到未来，我们不仅有这些AGI，我们还会有数字人类，人工生命，人工动物，甚至人工植物。所以它将是一个共生社会，然后是人类的决策，而不是AI的决策。我认为根本上与人类价值观对齐是不够的，因为人类价值观需要适应变化。对于这个共生社会，后来不仅仅是人类作为世界上最顶级的生物，与人类价值观对齐对AI已经非常具有挑战性，但我仍然认为这是相对容易的，因为它在计算上是可行的。但与人类对未来的对齐相比，这更难，因为人类永远不会从历史中吸取教训。所以自我进化的AI更容易适应，人类的进化则更慢，特别是在心智层面。所以我们需要的不仅是有益的AI，还需要有益的人类，为未来的共生生态和社会。感谢大家的关注。谢谢你曾教授，您的演讲将科学、政策和哲学视角巧妙结合，以推动AI治理中的红线方法。接下来我们将听取Irene Salaiman女士的演讲。Irene女士是Hugging Face的全球政策负责人，她在进行安全研究并领导公共政策研究。此前，她在Open AI工作，负责偏见和社会影响研究以及公共政策。她的研究包括AI价值对齐、负责的发布以及打击滥用和恶意使用。她去年因其研究被评为MIT技术评论35岁以下35位创新者之一。Irene，很高兴能有你在这里。请开始您的演讲。谢谢Kuan Yee。我非常高兴能谈论开放性在AI领域的角色。我自从这个领域真正成为一个领域以来就一直在思考这个问题。首先，我想与大家定义开放性意味着什么。我参与了很多会议和讨论，开放性这个词往往以不同的方式被提及。所以首先，我听到它类似于开源软件。我们可以借鉴一些相似之处，但根本上存在区别。开源倡议有一个工作组正在努力制定开源在AI中的定义。我以前的同事Nathan Lambert现在在AI2，他有一个很棒的博客，详细说明了为什么社区难以在定义上达成共识。在国家安全社区中，我听到开放性被指代为模型权重，特别是模型权重的广泛可用性，是否可以获得以及如何分配。我听到的但可能没有明确指出的是透明度意义上的开放性。斯坦福大学建立了透明度指数，对我来说，最重要的收获是不同人对透明度的理解和系统的各个方面对其开放性的贡献。对我来说，最倾向的定义是超越模型中心主义，从整体上思考系统以及构成整体AI系统的许多工件。我会做出幻灯片展示者不应该做的事情，向你展示很多单词和图形，但我真正希望你从这张图片中看到的是有多少工件构成了一个整体系统。当我们思考的不仅是模型而是数据集时，我们在思考微调数据集，反馈数据集，可能与系统相邻的评估数据集，什么是使其可用的意义当我们担心在训练数据上测试时。这些图片来自今年2月由Mozilla在哥伦比亚大学举办的关于开放性和培育这个社区的会议，概述开放性的维度可以帮助我们更好地思考构成如何发布系统、威胁建模系统以及研究社区可以从开放性中受益的工件。这张图是今年5月发布的报告的一部分，给出了开放性动机的非详尽列表。我发现明确、清晰地解释为什么一些研究人员追求开放性是有帮助的。再次，与软件的相似之处，我们能够分享知识，正如Dr. He所说，拥有更多的视角和更广泛的理解。在今天有限的时间里，我想聚焦于AI安全和开放性相关的两个领域。首先是将AI视为科学学科。在背景中，我听到AI被讨论为商业产品，国家安全威胁，我认为更重要的是作为科学学科。这个整个领域实际上建立在开放科学生态系统。最常被提到的例子是2017年的Attention is All You Need论文。并且不仅仅是看论文，还有工具和库如PyTorch。我认为我们不会有今天的成就，如果没有开放科学生态系统。第二部分是我非常关心的，是社区贡献和更广泛视角的重要性。关于科学的观点，决定什么构成科学是非常困难的。在生物学领域，我们知道有实验室外套和试管，但在AI领域，一些科学方面的例子是关于可重复性。现在存在一种可重复性危机，不仅在于人们能够复制、构建结果，还有人们对模型的访问，他们拥有的基础设施水平，能够验证结果。这影响了研究生态系统的信任问题。一个更广泛的问题是同行评审和出版。太多的同行评审问题无法在此详细列出，也许这是另一个讨论，但谁能评审，发布的深度，以及档案是一个净好处，但也存在一些担忧，关于在与社区分享之前，什么是经过同行评审的。相关的还有科学传播，这也包括文档，再次，跨学科性，这使得更广泛贡献的重要性。我要确保这个短语真正引起共鸣。没有一个组织，无论多大，资源多丰富，多样化，能够容纳所有受影响的人群的不同专业知识、视角和观点，使系统对许多受影响的人群尽可能安全。我的一些例子是关于外部审查。今年早些时候有一封公开信，呼吁独立AI评估的安全港湾。一些更具体的例子是展示了访问工件如何能够促进更好的研究。我非常欣赏Dr. Abebe Berhane的工作，这是评估大规模数据集领域的基础工作。如果没有访问到类似的东西，这些工作将不可能完成。今天很多观点也回响了，我认为Professor Gao之前也分享过，多语言性是我认为我们可以在国际合作中前进的重要部分。这张图片是我的研究合作者Dr. Xerox Talat的工作，并与Hugging Face的大科学合作中，这真的体现了开放合作的重要性，拥有来自许多不同语言背景的贡献者。他们在这项关于多语言评估挑战的工作中发现，英语被过度代表了，他们的即将发布的工作是与不同语言的母语者合作，审查这些不同语言的偏见。某些语言如法语和西班牙语会有更多的性别化术语。我不幸的是，我讲很有限的不同亚洲语言，但例如在西方语言中没有的家庭关系，会引入不同的安全挑战。我去年发表了这篇文章，我想进入关于什么应该开放，什么是开放治理的讨论，以及基于开放性和发布引入的风险。这种光谱是为了超越开放和封闭的二元论。当我们思考开放时，我更倾向于下载访问，但完全开放的区别是有更多的工件可用。例如，去年Meta的OPT是可下载的，但数据集很难访问，所以它不是完全开放的例子。但Eleuther AI做了令人难以置信的完全开放工作，使所有工件完全可访问，然后我会将系统如DALI更多地放在托管访问中，这往往更接近封闭的专有领域，但我想给予更多的维度来思考这个光谱。在我去年发表的那篇文章中，我想非常清楚地说明，确保发布顺利进行是集体责任。我们可以采取许多不同的步骤，并且在行动上有很多重叠。我一直在思考更多，今天剩余的演讲中，我想深入探讨发布之后的事情。一旦系统部署，实际化风险的方式并不总是取决于如何发布，但这可能是一个重要的变量。我非常感谢Dr. Nitzberg之前说的，能力往往与风险混淆，但并不总是正确的代理。然而，能力确实有助于我们如何威胁建模。一个例子是Crayon，之前称为DALI Mini，那是一个扩散模型，生成了非常搞笑的非常模糊的图像，这比生成非常逼真图像的DALI 2更不具威胁性。这就是能力起作用的地方。在Hugging Face，我们确实需要对内容进行审查。我真的想强调内容作为当前风险的重要性。这就是我们不幸地想到的非自愿内容和虚假信息，我们需要从整体上考虑能力和内容。我更多的在思考实际化风险意味着什么，将对话从案例，发布，封闭，开放转向访问与访问障碍。因此，虽然你可能有一个开放权重模型，并且是的，你可以移除那些安全措施，但一些由Dr. Peter Henderson领导的工作，这是一个斯坦福政策简报，他现在在普林斯顿大学，表明通过微调API，你可以做同样的事情。所以托管封闭权重模型并不本质上安全，实际上解决安全问题更便宜，开放模型并不一定更容易访问。具有非常复杂基础设施的人可能会发现封闭权重模型更容易访问，没有复杂基础设施的人可能会发现封闭权重模型更容易访问。我想给一些例子，不是挑选Chat GPT，但它有如此广泛的访问，因为它有如此简单的接口，没有复杂基础设施的人可能会发现封闭权重模型更容易访问。就像我们在2019年与GPT-2一起在OpenAI所做的那样，那更像是分阶段发布，但我们有与中国公司合作的巨大机会，允许我们安全地进行发布。这是我最后一张幻灯片的第二张。Hugging Face是开放创新联盟的创始成员。我非常感谢Professor Varuko强调开放性对发展中国家避免权力集中的重要性，我们可以作为全球网络共同思考。最后，我想问一下房间里的感觉，有多少研究人员觉得他们是这个讨论的一部分。坦白说，高层讨论并不总是传达到没有相同基础设施的研究人员，但他们有好的技术缓解想法。所以从定义开放性开始，我以定义开放性结束。非常感谢你们，我希望这对你们有帮助，我期待继续讨论。感谢Irene的演讲，为我们带来了AI中开放性的挑战和灵活性的深入理解。

---

这不是一个简单的问题，你做得非常好。现在我们来调整一下话题，讨论一下AI管理的国际机构。宋教授，作为AI领域的高级领导者，您对在全球范围内的职业角色有独特的见解。您能否分享一下您对建立国际AI管理机构的看法？特别是我们如何平衡国际机构的广泛承诺与其在执行过程中的挑战，确保国际机构在职业角色上的一致性，并拥有足够的智能以匹配我们今天讨论的AI发展速度？谢谢。

宋教授回答道：“这是一个非常重要的问题。在AI领域，已经有许多全球性的尝试，例如OECD和欧洲的实验项目，以及全球AI协调会议。这些尝试主要涉及40个国家，而其余160个国家则没有参与。因此，我认为，我们的目标不应该是建立一个新的国际AI监管组织，而是创建一个极其有效的协调机构，连接现有的区域组织，使各区域能够在地方层面采取行动并弥补不足。我认为，这种方法可以让所有区域网络有效联动，而不是单纯地进行控制。”

宋教授继续说道：“此外，我看到一些区域网络试图替代美国的角色，尽管美国在全球信任度方面仍然占据优势。中国已经参与了一些关键的AI安全会议和军事AI重新定位的讨论，我认为中国应该继续参与这些网络。然而，区域网络的限制在于它们缺乏将所有国家联系起来的能力。因此，我们需要利用美国的网络，连接各区域网络，同时保持美国网络的开放性，以确保没有国家被排除在外。”

讨论继续，郑教授补充道：“即使没有支付系统的参与，美国的网络也能帮助其他国家建立联系。这让我想起了我在美国参与的一次合作，那是我第一次与当地人合作，我认为这种跨国合作非常重要。”

接下来，讨论转向了国际标准和报告制度，罗伯特提到：“在AI安全方面，国际标准和报告制度的建立对于全球合作至关重要。ITU和UNESCO已经在制定这些标准，并认为需要进一步增强研究能力，以推动我们的工作。”

Irene教授表示：“我们需要确保不同群体和社会技术研究者的参与，特别是解决那些看似非技术但根植于历史和文化的系统性问题。”

最后，Duncan总结道：“在未来6到12个月内，我们需要帮助国际机构，特别是联合国，提前思考和设计未来可能需要的机制，以便应对AI带来的巨大转型。”

整个讨论强调了全球合作在AI治理中的重要性，各方呼吁通过建立有效的国际机构和标准来确保AI发展造福全人类。