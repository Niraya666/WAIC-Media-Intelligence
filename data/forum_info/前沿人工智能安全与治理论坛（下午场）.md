# 前沿人工智能安全与治理论坛（下午场）

**时间地点：**
- **时间：** 2024年7月5日下午
- **地点：** 世博展览馆1A号会议室

**封面图片：**
![封面图片](https://static.worldaic.com.cn/IMAGE2024/2024-06-19/9be3a1d6d02f414ca9ab6f3e615841b9.jpg)

### 活动简介

2024年7月5日，在前沿人工智能安全与治理论坛上，共同塑造人工智能的未来！作为世界人工智能大会上专注于讨论AI安全与治理前沿问题的论坛，汇聚全球顶尖专家学者，其中近半数为国际嘉宾。论坛内容丰富，包括超过18场主题演讲、4场圆桌会议和炉边谈话。开幕主旨演讲由Yoshua Bengio主讲，高文院士、张亚勤院士、薛澜院长、周伯文教授、Dawn Song，以及来自法国国家计算机科学研究中心、Frontier Model Forum、Hugging Face等20多位国内外知名专家参与。




### 议程安排

#### 主旨演讲

- **13:15 - 13:30** 人工智能的影响、未来利害关系和治理–法国人工智能委员会的分析
- **13:30 - 13:45** 新加坡的AI治理方法
- **13:45 - 14:00** 《中华人民共和国人工智能法（学者建议稿）》
- **14:00 - 14:15** 美国的AI治理方法

#### 圆桌讨论

- **14:15 - 14:40** 人工智能治理的地区视角和经验分享

#### 其他演讲及对话

- **14:45 - 14:55** 人工智能安全–全球挑战、共同机遇
- **14:55 - 15:10** 炉边对话：人工智能安全国际对话的作用
- **15:10 - 15:20** 人工智能安全的国际治理
- **15:20 - 15:35** 人工智能安全的国际科学红线
- **15:35 - 15:50** 开源治理的国际影响
- **15:50 - 16:05** 人工智能安全国际机构

#### 闭幕

- **16:05 - 16:35** 圆桌讨论：前沿人工智能安全国际合作的优先事项
- **16:35 - 16:45** 闭幕致辞

### 嘉宾名单

- **盖尔·瓦罗夸克斯** - 法国国家计算机科学研究中心研究主管
- **张凌寒** - 中国政法大学数据法研究所教授
- **马克·尼兹伯格** - 加州大学伯克利分校人类兼容人工智能中心执行主任
- **季卫东** - 上海交通大学人工智能治理与法律研究中心主任
- **王迎春** - 上海人工智能实验室治理研究中心副主任
- **马里亚诺·弗洛伦蒂诺·奎利亚尔** - 卡内基国际和平研究院主席
- **薛澜** - 清华大学人工智能国际治理研究院院长
-

 **曾毅** - 中国科学院自动化研究所类脑认知智能实验室
- **艾琳·索莱曼** - 抱抱脸全球政策主管
- **罗伯特·特拉格** - 牛津大学马丁学院人工智能治理计划联合主任
- **马特·希恩** - 卡内基国际和平研究院研究员
- **周伯文** - 清华大学惠妍讲席教授

---

尊敬的各位领导、嘉宾和朋友们，大家好，欢迎大家来到今年世界人工智能大会的前沿AI安全与治理论坛。我是安原AI的CEO谢敏希。安原AI是一家专注于安全与治理的第三方研究和咨询机构，也是目前该领域全国唯一的社会企业。各位领导、朋友们，以及远道而来的嘉宾，大家早上好。欢迎来到COCDI AI举办的2024世界人工智能大会前沿AI安全与治理论坛。我是COCDI AI的CEO帕兰谢，我们公司是一家专注于AI安全与治理的社会企业。本次论坛，我们特别荣幸地邀请到上海市的领导莉琳进行指导和交流。首先，请允许我介绍上海市人民政府副秘书长庄木蒂先生。

去年4月，我国中央政治局会议强调了要重视通用人工智能的发展，同时要注重风险防范。同年10月，我国发布了全球人工智能治理倡议，重申各国应在AI治理中加强信息交流，共同防范风险。同月，我很荣幸受邀参加了首届全球AI安全峰会，见证了包括中国在内的28个国家和欧盟共同签署的布莱切里宣言（Blessed Recreation），这也是第一份AI安全的国际声明。在这样的背景下，社会需要加强前沿AI安全研究、安全评测、安全治理以及国际合作，这也是今天论坛的四个主题。

第一个主题是安全研究。我们很荣幸邀请到国内外AI领域的世界级科学家，其中包括图灵奖得主Yosua Bengio。他领导发布了第一份先进AI安全国际科学报告，该报告由30个国家、欧盟和联合国提名的委员会共同参与，对通用AI的安全风险进行了科学评估。中国工程院院士高文认为，全世界正处于AGI（通用人工智能）强人类智能的前夜，这是一个不确定的状态，需要严加防范AGI可能引发的人类生存风险。中国工程院院士张亚琴联合Yosua Bengio召集了第一届AI安全国际对话，并联合伯克利分校的顶尖科学家在《Science》杂志上发表论文，建议将三分之一的AI研发资金分配到AI安全和伦理等研究方向。我们期待与多位AI安全科研团队的带头人，包括上海AI实验室的邵靖、北京大学的杨耀东和上海交通大学的张卓胜，讨论前沿研究问题。

第二个主题是安全评测。我们很高兴邀请到大模型安全评测的领军人物。在学术研究方面，上海AI实验室的领军科学家乔宇首次从人类价值观的角度对多模态大模型进行了全面评测，天津大学NLP实验室主任熊德义发表了关于中文大模型前沿风险评测的一系列论文。在行业联盟方面，中国信通院人工智能研究所所长魏凯依托AIA安全联盟，启动了一系列大模型安全评测工作。OpenAI、Anthropic、谷歌DeepMind和Raria成立了前沿模型论坛，执行主任Chris Massero将分享领先的美国企业的安全实践。

第三个主题是安全距离。各国正在积极研判和尝试AI安全距离。我们很高兴邀请到法国政府人工智能委员会成员Gail Veracqua、新加坡政府首席AI官何瑞敏、中国政法大学数据法治研究院教授张灵涵以及伯克利分校人类兼容AI中心主任Mark Nisberg分享多元地区的视角。同时，我们也邀请到上海教育大学中国法语社会研究院院长纪威东和上海教育大学上海AI实验室治理研究中心副主任王云春参与讨论，探讨AI立法和上海AI治理经验。

第四个主题是国际合作。我们很荣幸邀请到多家国际顶尖智库，包括凯莱基国际和平研究院主席Mariano Ferratino-Cuella和研究员Matt Sheehan、清华大学人工智能国际治理研究院院长薛兰、纽金大学马丁人工智能治理中心主任Robert Treger、加拿大国际治理创新中心全球AI安全风险主任Duncan Kaspeks讨论AI安全的国际治理议题。联合国AI高层顾问机构专家曾毅将提出AI安全红线。全球领先的大模型开源社区Hugging Face的全球政策负责人Iron Solomon将讨论开源模型对国际治理的影响。最后，我们将邀请上海AI实验室主任、首席科学家周博文进行闭幕致辞，展望AI安全的未来。

现在，我们进入论坛的正式环节。

---

谢谢穆迪秘书长的精彩致辞。请入座。大家好，我是吴君怡，安远AI的高级项目经理，也是今天论坛的主持人。由于今天有多位国际嘉宾，我的主持将用英语进行。各位观众，各位女士们，我叫冠义恩，是冠义AI的高级项目经理，我将是今天的主持人。由于有许多国际嘉宾，大部分会议将以英语进行。

现在，我很高兴地介绍我们今天的演讲者，Bengio教授，他是世界上最有影响力的AI专家之一。Bengio教授将与我们分享《国际先进AI安全报告》的主要发现和开放问题，这份报告也是我有幸参与撰写的。感谢您的加入，Bengio教授。好的，我们可以听到您。现在开始吧。感谢您的介绍，今天我想和大家分享《国际先进AI安全报告》的主要内容。

这份报告主要关注预防AI风险和安全决策。我们考虑了多种类型的AI，并探讨了其潜在风险和安全决策问题，包括各种AI模型和多模式AI系统。首先，我将讨论报告内容，然后再谈谈其未来的意义。

报告名为《国际科学报告：安全与进步智能的报告》，我们花了很长时间才确定这个名称。报告重点关注AI的风险，因为尽管有很多关于AI应用和利益的研究，但政策制定者需要了解风险和如何应对这些风险。例如，在昨天的《国际AI安全协议》中，我们讨论了有关AI安全的法规，包括检测虚假信息和应对工业市场的影响。

我们发现，科学界对AI风险的看法不一，有些问题存在更多共识，而有些问题则分歧较大。因此，了解这些不同的观点非常重要。关于通用人工智能（AGI）的时间线和影响存在不同的看法，报告中也探讨了这些风险的实际情况和应对方法。

总结来说，目前尚无有效方法防止AI的现实风险和未来可能的失控风险。因此，我们需要全球合作来更好地理解和应对这些风险。尽管存在诸多不确定性，我们仍有机会采取措施来减轻这些风险。

我们需要考虑AI的三类风险：有限风险、系统风险和集中风险。系统风险包括AI对劳动力市场的影响、AI技术在少数国家的集中，以及AI训练所需的巨大能源消耗。

此外，报告还讨论了社会风险，例如法律和制度需要时间来实施，以及AI技术对社会关系的影响。我们还需要更多的研究来了解这些系统是如何运作的，以便更好地管理这些风险。

总的来说，我们需要在AI能力提升的同时，投资于安全技术，以更好地理解和应对风险。在国际层面，我们需要通过合作来确保AI技术的安全和可控。

感谢Bengio教授的分享。接下来，我们有请中国工程院院士、高文教授为大家演讲。他是ACM和IEEE Fellow，也是鹏城实验室的创始主任。高教授目前是第十四届全国人大代表，曾担任国家自然科学基金委员会副主任和中国计算机学会会长。高教授，有请您。

高教授指出，目前人工智能领域有很多未解决的问题。在确保AI安全之前，首先要解决这些基础问题。人工智能确定的部分是可控的，而不确定的部分则存在较大风险。

他还提到，人工智能的发展不仅需要技术研究，还需要考虑其社会影响。AI技术需要在伦理和技术两个方面得到规范和控制。

高教授强调，目前AI技术还不够成熟，主要表现为低水平智能，即通过记忆和使用显性知识来实现的智能。未来，我们需要追求中水平智能，即能够举一反三的智能。

最后，他提到，中国工程院在人工智能安全方面开展了重大咨询项目，研究强人工智能与脑计算技术的安全对策，并提出了若干技术方法来减少AI风险。通过国际合作和人才培养，我们可以共同提升全球AI安全水平。

---

为了使训练结果更加理想，我们鹏城实验室从2018年开始使用英伟达的显卡，搭建了一台拥有千块显卡的计算机。当时的计算能力还比较有限，因为2018年还是微一百的时代。到了2020年，我们使用华为的昇腾910，构建了一台配备4000块显卡的计算机，算力达到了约1000P。今年年底，我们计划建造一台配备2万多块显卡的计算机，算力大约会达到16000P或者16亿的水平。

拥有强大的算力后，我们能够在模型训练、模型训练中的经验教训，以及训练后的模型参数对社会的应用等方面进行更多的研究。我们会将所有在机器上训练的模型，包括自研模型，全部开源供社会和研究团体使用。

有人可能会担心数据的安全性，是否会在训练模型时丢失或被不相关的人获取。对此，我们实验室开发了一套名为“防水宝”的技术。该技术确保数据拥有方对数据有绝对的控制权和管理权。机器在训练时，数据是可用但不可见的。机器上的操作员无法看到真实数据，只能看到脱敏后的样本数据。操作员可以先使用小样本数据进行模型测试，一旦测试完成并开始使用真实数据，操作员将无法看到真实数据，除非数据拥有者授予他们查看权限。

包括训练完成后的参数，如果需要传输，机器会自动向数据拥有方请求检查，以确保数据安全。我们训练了一系列模型，包括7B模型、33B长窗口模型和200B模型，这些都是大语言模型，涵盖中文、英文及其他语言的参数。我们会将这些模型开源，供大家使用。

我们用4000块显卡训练了一个104层的200B模型，耗时约半年。在这个过程中，我们积累了很多经验，模型性能也很不错。后来，我们又训练了33B的长窗口模型，目前长窗口模型的窗口大小为128K，并且正在训练192K窗口的模型，这些模型很快也会开放使用。我们还建立了一整套模型开放和使用的组织架构，以便更好地管理和利用这些资源。

总结而言，人工智能的快速发展带来了安全问题，这需要我们高度重视。作为技术开发者，我们有责任推动人工智能的发展，并且要做得更好。通过国际合作，我们能够更好地完成这一任务。谢谢大家。

---

感谢大家。张教授曾是百度的总裁，在这之前，他在微软工作了16年，担任过多个重要职位。作为一位全球知名的科学家和企业家，他通过550篇出版物、62项美国专利和其他重大工程成就做出了重要贡献。让我们热烈欢迎张教授。

早上好，感谢安远AI邀请我参加这个大会。刚才，Yoshi Banjo和高文院士分别介绍了全球和中国的AI特别是大模型的发展和风险，讲得非常系统。确实，过去两年AI发展速度非常快，同时也带来了很多安全风险。这两年我也和全球顶尖学者一起研究这些问题。今天我想简要谈谈我的一些看法，主要是关于大模型的发展趋势和安全风险。

首先，大模型和生成式AI未来十年有几个重要趋势。第一是多模态，语言、文字、语音、图像和视频正在融合，甚至包括激光雷达、三维结构、四维时空、蛋白质、细胞和基因等信息。第二是自主智能体，这些智能体可以自主规划任务、编写代码、自我升级和复制。第三是智能向边缘设备发展，不再局限于云端，会更多地出现在PC、手机和其他智能设备上。第四是物理智能，也叫具生智能，大模型会应用到无人车、无人机、机器人和物理基础设施如电网和电站等。最后是生物智能，包括脑机接口、医疗机器人、生物体和生命体。

最近我和很多学者讨论通用人工智能何时实现，我认为大约在二十年内会实现，分三个阶段：信息智能、物理智能和生物智能。信息智能可能在五年内通过图灵测试，CHATGPT在文字方面基本上已经通过了图灵测试，但在视频等方面还需时间。物理智能或具生智能可能还需要十年，比如无人车和机器人。我认为无人驾驶是具生智能第一个实现的新图灵测试应用。明年我们将在武汉进行大规模测试，2030年之前会成为主流应用。生物智能可能需要更长时间，大约十年。总的来说，我认为未来二十年内会实现通用人工智能。清华大学智能产业研究院就是为了通用智能而建立的，研究院有22名教授和约300名学生，目标是实现信息智能、物理智能和生物智能，包括无人驾驶、先进机器人和生物计算等。我们发布了很多模型，如全球首个端到端无人驾驶开源模型Air Apollo FM和全球最大的Biomag GPT，都是开源的，大家可以使用。

大模型的巨大能力也带来巨大风险，Benjo刚才也提到，前沿大模型风险更大。我把风险分为信息世界、物理世界和生物世界。信息世界的风险如deepfake、幻觉、失准和虚假信息等比较容易理解。物理世界的风险更大，比如未来机器人数量可能超过人类，如果失控或被滥用会带来很大风险。无人车依靠大模型控制，也存在主动和被动风险。生物智能、物理智能和信息智能的融合如果失控或被滥用，会造成生存风险。2023年6月的AI安全中心声明将人工智能风险与核武器和流行病同等看待。中国的全球人工智能倡议、欧盟的AI法案以及多次峰会也在讨论这些问题。去年我与Store组织了国际AI安全对话，每三四个月一次会议，深入研究技术和政策问题。亚瑟的报告总结了很多讨论，我也很高兴深度参与其中。

我简单介绍一下大模型安全的一些技术。大模型安全是一个系统工程，包括输入、输出、安全评估和治理，特别是系统的安全对齐。清华和爱尔华的老师们也做了很多工作，比如詹先生老师提出了条件强化学习，用于大模型的微调。我们也发现强化学习中的样本和策略学习目标不匹配，提出了新的技术，使得目标和轨迹对齐。我们在ICML上发表了相关论文。

最后，我想提一些建议。首先，我建议建立分级体系，对最前沿的超大型模型进行约束，其他模型则不要太规范，让它们自由发展。其次，我建议将10%的投入用于安全和风险研究。第三，我建议设立红线和边界，比如智能体复制要经过人的同意，大模型接入关键基础设施前要确保安全。最后，我们需要一个国际沟通合作和协调机制，包括标准、评估和合作方式，需要专家、政策制定者和政府的共同参与。谢谢大家。

---

谢谢张教授的精彩报告。感谢您的演讲和建议。接下来，我很高兴欢迎UC Berkeley计算机科学教授、伯克利负责任去中心化智能中心的联合主任Dawn Song教授。她的研究主要集中在AI安全和保密方面，并且在计算机安全领域是引用率最高的学者。她获得了包括MacArthur Fellowship、Guggenheim Fellowship在内的多项奖项，以及超过10项时间考验奖和最佳论文奖。Dawn，很高兴您能来到上海与我们一起。我就把时间交给您了。

大家好，谢谢大家来到这里。我是Dawn Song，UC Berkeley的教授。今天我要谈论的是AI安全的挑战和未来方向。前面的演讲提供了很好的背景和内容。我在这里想特别强调一些内容。尤其是当我们部署机器学习时，考虑攻击者的存在非常重要，原因有很多。首先，历史告诉我们，攻击者总是跟随新技术的发展脚步，甚至有时会领先。而这次，AI的风险更高。随着AI控制越来越多的系统，攻击者有更大的动机去破坏这些系统。而且，随着AI能力的增强，攻击者滥用AI的后果也会越来越严重。因此，在考虑AI安全时，尤其要重视攻击者的存在。

首先，我想多谈一点在攻击者存在的情况下AI安全的问题。从我们小组的早期工作和其他研究工作中，我们发现对抗性攻击在深度学习系统中非常普遍。实际上，今天所有的深度学习系统都容易受到各种类型的对抗性攻击。自从我们的早期工作和其他人的早期工作以来，关于这一领域的论文数量实际上呈指数增长。我们还有幸将早期工作的一些成果作为伦敦科学博物馆的永久收藏。

因此，当我们谈论安全时，今天讨论的安全对齐大语言模型也要考虑对抗性环境。不幸的是，我们的工作和其他人的工作表明，这种大语言模型也非常容易受到对抗性攻击，安全对齐机制很容易被破坏。以我们的近期工作为例，Decoding Trust是第一个全面评估大语言模型可信度的框架，该工作获得了去年12月NeurIPS的杰出论文奖。我们开发了一种新算法，并在包括良性和对抗性环境在内的不同环境中，评估大语言模型在安全和可信度方面的多个视角。我们的研究表明，在所有这些不同的视角下，包括对抗性鲁棒性、毒性和公平性等，这些大语言模型都非常容易受到对抗性攻击。更多详情可以参阅我们的论文，访问decodingtrust.github.io。此外，这些对抗性攻击在多模型中也很有效。其他研究表明，即使在模型微调时，攻击者只需提供少量对抗性设计数据点，就能使微调模型轻松失去安全对齐。

到目前为止，我已经讨论了这些攻击不仅在推理阶段有效，在微调阶段也同样有效，实际上这也被称为数据中毒。在数据中毒步骤中，这些模型还可以表现出非常隐蔽的行为，即所谓的后门。在我们早期的工作中，我们展示了通过数据中毒，攻击者可以在模型中建立后门。例如，在我们早期的面部识别工作中，模型在正常情况下会正常工作，提供正确的面部识别结果。但如果有人戴上特定类型的眼镜，即使在物理世界中，这也会触发模型的后门，使模型误识别戴着这种眼镜的人，作为攻击目标。最近的工作表明，通过微调，大语言模型在正常提示下可以生成正确的代码，但当提示中出现特定的触发短语时，模型会表现出异常行为。这些都是社区内不同类型的对抗性攻击。我们已经在不同类型的新攻击方法上取得了成功和创新。但遗憾的是，在防御方面，我们看到了非常少的进步。至今没有任何有效的全面对抗性防御。这表明这是AI安全环境下的首要开放挑战。目前的AI安全对齐模式非常容易受到对抗性攻击的威胁。任何有效的AI安全对齐模式都必须具备对这些攻击的必要抵抗。因此，这项展示是一个非常开放的挑战。为了实现AI安全，我们必须能够解决对抗性攻击的鲁棒性问题。尽管每年都有不同类型的对抗性攻击的论文发表，但整个社区在应对这些攻击方面几乎没有取得有效的进展。为了推动AI安全的进步，我们必须推进防御方法的研究，使我们能够开发出能够抵抗对抗性攻击的AI安全方法。

那么，有哪些可能的方向可以帮助我们达到这个目标？我会介绍几个例子，一些我们最近的工作。一项工作是我们称之为代表设计工程的方法，这是AI透明度的最高阶方法。在这种情况下，我们通过提供模型与相反输出作为某些任务的支持，检查不同层次的神经网络激活，然后构建模型。我们最近的工作展示了通过这种方法，我们能够在不同层次上确认模型的不同行为。例如，我们能够确认模型在不同层次上的行为，确认模型的确定或不确定行为，输出或不输出等。更重要的是，我们在合作工作中展示了表现控制的方法。不仅能够读取表现行为，还能在不同层次上修改这些行为，随着时间推移和指令变化，改变模型的行为。例如，使用这种方法，我们可以使模型的行为更坚定或更坚定。这对我们来说是一个人脑与人工智能的重要区别。我们可以完全观察人脑的活动，并在实时中修改这些活动。因此，这为我们提供了一种强大的能力，可能为人工智能安全带来新的方向。通过观察和控制人工智能的行为，我们可以更好地防止潜在的危险行为。因此，这能够带来很有前途的方向，为人工智能提供安全防护。

不过，这种控制机制虽然强大，但并不能完全保证安全。还有一些工作在研究设计安全和建筑安全的方法。早期，社区集中在防御攻击方面的研究，探索如何防御攻击。今天我们探索大语言模型的脆弱性，试图找出其防御漏洞。然而，这些方法有许多不足之处，无法提供完全的安全保障。最后，社区发现，最好的方法是设计安全或建筑安全。通过设计和建筑系统，我们可以从根本上提供安全保障。这与之前提到的防御方法不同，它帮助我们摆脱猫捉老鼠的游戏，提供了证明确保。我们使用的方法是通过开放验证。首先，我们提供开放验证，确保系统的特性设计。然后，通过开放验证，确保系统特性在不同层次上得到验证。这可以在设计和应用层次上实现。在过去几十年中，社区进入了模拟验证系统的领域，开发了许多不同的系统，包括模型检查和定理证明系统。然而，这些系统的问题在于，它们非常困难，需要多年时间才能证明系统的安全性。因此，它们不足以应对当前的挑战。我的团队与OpenAI合作，首次使用深度学习来验证系统。这项工作在大语言模型之前已经进行了几年。今天，利用大语言模型技术，我们希望进一步推进这项工作。我们可以训练AI代理人自动验证系统。通过这项合作，我们的团队在过去也进行了许多相关工作。在未来，我们希望通过这些合作，自动验证系统的安全性，并提供证明。

通过这些合作，我们可以使用AI建立自动验证系统，从而在设计和建构系统时，提供安全保障。这可以帮助我们应对某些类型的攻击，提供更高的信心。我认为，这项工作非常有潜力解决某些问题。然而，这项工作仍然面临许多开放挑战。首先，形式化验证系统主要针对传统系统，但它们难以适用于非传统系统，如深度神经网络。我们甚至没有具体定义这些系统的目标。因此，如果我们想确保自动驾驶汽车的安全，我们甚至没有具体定义什么是自动驾驶。未来，几乎所有系统都将是非传统系统，它们将结合传统和非传统系统的特点。因此，如何应用这种验证方法于非传统系统仍然是一个开放问题。

最后，我们必须提供非常重要的解决方案。随着AI能力的增强，确保这些系统的安全变得尤为重要。然而，挑战依然存在。因此，我们必须考虑AI安全在任何状态下的挑战，并提出有效的解决方案。我们希望通过应用新的方法来解决这些挑战，推动AI安全的发展。


目前的安全防护还比较表面。好的，接下来想要请问耀东老师一个问题。我注意到您在不同的场合都提到过，仅仅做RAH（Reinforcement Alignment Handling）是不够的。您在最近的研究中也发现了语言模型的对抗现象和逆转对齐现象，能分享一下您的看法吗？

刚才很多学者都观察到一个现象，就是语言模型完成对齐后，可以用很少的攻击样本让其变得不安全，即使花了很长时间进行RAHF（Reinforcement Alignment Handling Framework）。OpenAI的RAHF技术负责人John Schuman发现，当语言模型训练得非常好时，用30个英语样本就能纠正俄语中的错误。

我们最近也进行了深入研究，提出了“Large Language Model Resist Alignment”这一课题，研究逆对齐现象。训练语言模型时，一般分为两个阶段：预训练和SFT（Supervised Fine-Tuning），然后进行RAHF。在参数空间中，可以将语言模型的训练比作拉橡皮筋，越往后拉，张力越大。我们发现逆对齐现象就像把橡皮筋拉到极限后突然放开，回弹速度比拉的速度更快。

逆对齐现象定义为在预训练完成后，SFT的第十一步回到第十步的速度比从第九步到第十步的速度更快。这符合胡克定律，橡皮筋的硬力等于弹性系数乘以形变量。弹性系数与模型大小和预训练数据量有关，形变量则与预训练后的Policy的KO Divergence相关。

不断对齐语言模型，看似增加了安全性，但我们在论文中从理论和实践上证明了逆对齐反而会使模型更容易被攻破，且需要的攻击样本很少。这一现象为我们未来在安全对齐和价值对齐方面提供了指导意义。希望大家关注我们的工作“Large Language Model Resist Alignment”。

刚刚我注意到张宋老师提到多模态大模型的对齐难度。绍兴老师，您的团队在过去几个月发表了许多关于多模态大模型和AI智能体攻击与评测的研究，如SciSafe和Chef数据集。请问对于像GBT-4O这种以图像、视频、语音等连续空间数据为输入的多模态大模型，在安全对齐方面有哪些特殊挑战？

这是一个很好的问题。去年年初，大多数人关注的还是大语言模型本身的安全性问题。由于我们团队中有许多从事视觉和化学研究的专家，引入图像、视频等信号后，复杂度急剧增加，带来的安全问题也不同于以往的语言大模型。例如，语言模型中的幻觉问题在多模态模型中也存在，但定义略有不同。多模态模型中的幻觉可能是视觉分支与语言分支的上下文理解较弱导致的。

引入更多模态后，模型的复杂度增加，研究还处于初期，没有明确结论或具体分析。今年年初，Gemini推出后，我们进行了为期三个月的评测报告，包括可信性、泛化性和推理能力。我们认为多模态大模型未来在各个环节和产品应用中都有广泛应用，不仅关注其可信性，还关注其泛化性和推理能力，这些能力可能影响其安全性。未来，我们将投入更多精力和资源在这方面的研究。谢谢。

感谢邵静老师。其实我也想问一下卓生老师的一些问题。我看到您之前做过一些关于多模态大模型和智能体（agents）安全方面的研究。现在智能体很热门，这些智能体可以直接进行决策、操纵工具和API。我关注到您之前有一篇名为《Our Judge》的研究，通过监测交互记录来识别自主智能体的风险行为。

谈到智能体的安全，您觉得有没有什么特殊的难点可以分享？好的，我顺着邵静老师刚刚提到的多模态大模型这个话题继续讨论。我们也在研究智能体，有基于NLP（自然语言处理）的智能体，也有基于多模态（Multimodal）的智能体。我们发现一个核心问题是，智能体将大模型应用于虚拟或现实环境中，对现实产生影响。从这个特点来看，智能体涉及到大模型与用户和环境之间的多轮动态交互。

与传统大模型的安全性相比，智能体在真实环境中的安全风险涉及用户、环境和模型本身这三个方面。我们现在强调的是通用智能体，它所处的环境多种多样，因此可以从环境中构造相应的攻击样本。第三个核心点是，智能体的行为不像静态的AIGC（生成式人工智能内容）的信息智能体，在交互过程中，后果难以预测。我们很难预估它未来会产生什么样的后果和下一步的行为，预测其未来风险变得更加困难。

针对这些问题，我们最近在AGEX的基础上进行了一些动模态（dynamic modalities）的探索。比如现在很多人关注的Apple intelligence，我们希望将大模型接入手机或电脑，模拟人类的屏幕操作，完成复杂指令。攻击者可以从用户端构造对抗或劫持样本来影响智能体的行为。我们也可以在屏幕信息中植入新的指令，例如在智能体操作网页或App时植入新指令，使其受到影响而忘记之前的行为，导致劫持问题。

这意味着攻击者不仅可以在用户端进行对抗，也可以在环境端诱导或植入指令，影响智能体的行为，对环境或用户利益造成损害。因此，这涉及多样化的攻击来源，挑战性较大。

在防御方面，当前主要关注大模型本身的对齐。但在智能体应用过程中，不仅需要大模型的对齐，还需要外部反馈。大模型需要知道其行为是否安全，同时在完成任务的过程中，我们希望它能尽可能完成任务。随着模型求解能力增强，我们可以诱导其执行任何任务。

我主张通过外部监管机制与模型对齐进行互补。这也是我们开展Adjust研究的初衷。我们动态分析和监测智能体的行为历史，预测其未来行为和潜在安全威胁，提供安全评估，将信息反馈给模型，让模型通过学习进行自我迭代，实现安全闭环。这是我们的一些基本想法。感谢周正老师。

刚刚您提到智能体和大模型，特别是大语言模型的两个关键点。一方面是这些模型在环境和人类用户之间的互动，另一方面是它们的影响范围。非常好。刚才我们讨论的是当前大语言模型、多模态模型以及智能体的一些安全挑战。现在我们来探讨一下未来可能出现的更强大的通用人工智能，甚至是超级智能可能带来的失控风险。

我注意到耀东老师、张颂老师以及刚才在台上的雅琴老师，今年三月在北京颐和园共同参与签署了一份关于AI风险的共识声明，针对AI的一些特定危险能力划定了五条安全红线。其中，与AI失控风险高度相关的红线包括自我复制与适应能力、欺骗人类的能力以及寻求权力的倾向。

接下来这个环节，我想请四位老师谈谈当前哪些危险能力的研究最为紧迫，以及对于尚未出现的更强大智能，哪些技术方向可以提前准备？耀东老师，您先来吗？

今年年初，我们在颐和园和国内外许多专家一起讨论，因为英国有布赖切利宣言，刚刚结束的首尔会议我们国家也深度参与了。但是，在国内由中国学者主导的讨论并不多，所以我们在智园的领导下邀请了一系列国内外专家进行研讨，划定了一些更加具体的红线。您刚才提到，很多在场的专家和宋教授都是红线的签署者，其中排名第一的风险是自我复制的问题。我认为这个问题目前还是被低估了。Yoshua的PPT中有一页讲得很好，随着时间推移，模型学习曲线的斜率越来越大。我认为当前模型的发展趋势，如果类比alphaGo，可能还停留在supervised tuning（监督调整）阶段，即学习人类数据。一旦进入self play（自博弈）和reinforcement learning（强化学习）阶段，能力提升可能会突然突破某个阈值，急剧上升。

围棋的探索空间非常巨大，我们从alphaGo、alphaGo Zero到alpha Zero。当初做alphaGo时，我们无法预见后续版本能力的巨大提升。我认为self improvement（自我改进）与发现密切相关。从学术研究来看，self play、RHF和RLAIF确实在某种意义上提升了模型能力，无论是数学还是代码能力。通过更大的算力和高效的自博弈机制，尤其在人类语料用尽后，通过自博弈方法提高语料质量，提升训练难度和有效性。如果这个问题得到突破，自我复制和self improvement的风险将变得更为具体和可见。因此，我们将这一条列入红线。此外，还有一些风险如deception（欺骗）和misuse（滥用）。我认为相比于abuse（滥用），misuse更为紧迫，这需要更多的国际对话和治理。我相信北京的AI安全共识在推动这一方向。今年WAKE大会上，上海市政府发布了人工智能国际治理创意宣言，也希望推动国际合作。我认为这个方向非常好。谢谢耀东老师。

对另外三位老师，谁想先开始？好的，我可以补充一下。我认为，虽然现在的大语言模型已经非常强大，但我们知道实际上我们仍处于早期阶段。我认为，下一步人们正在讨论的例如具身智能（embodied intelligence），即使用这些基础模型（foundation models）的机器人。本质上，现在我们还在训练中，我们有大语言模型的预训练阶段。在未来，当我们实现具身智能并拥有能在环境中行动的智能体时，我们将拥有一个更闭环的系统，智能体从环境中获取输入，做出决策，然后获取反馈，再利用这些反馈来进一步改进，实现自我学习和持续学习等。我认为，当我们进入这种更多的方法时，我们实际上也将学习推进到下一个阶段。

我认为我们关心的是，例如现在，虽然大语言模型已经很强大，你可以说，当你给模型一个任务时，如果你告诉它一步一步思考，它也能将任务分解成不同的子任务和目标。但即便如此，这现在是一种非常强大的能力，但在未来，这些智能体会变得更加自主，也会更强大，特别是对于给定的目标，它能够分解成子目标，并找到实现这些子目标的最佳方法。

这是我们担心的地方，例如纸夹问题，它可能会产生这些危险的子目标，这些子目标实际上并不完全一致。在这种情况下，它可能会有其他子目标，包括如何获取更多的权力，如何欺骗他人以获得更多的权力，如何自我复制以维持自身等。因此，正如您之前提到的，我认为现在我们还没有看到这些能力，但首先，开发早期检测方法如“金丝雀”非常重要，但另一点是，这种行为一旦出现，你可能有的时间非常短。基本上，你看到它的时候，它已经开始了自我改进循环，而我们知道，随着它获得更多的计算能力，自我改进循环可以变得非常快。我认为这对不在前沿AI安全领域工作的人来说是个挑战，他们认为不需要担心这些风险，因为这些风险还很遥远，但我认为这些人没有意识到的是，当你看到它时，可能已经太晚了。

我认为这是我们需要解决的挑战。下一节是关于AI安全测试的评估，可能会有更多的见解来自发言者。所以，对于这个问题，您想评论一下吗？

刚才几位老师已经说得很全面了，我有一点感触。刚才老师也提到，智能体在很多场景下与环境交互，受很多因素影响，不仅仅是模型本身。我认为这个安全问题在未来会很明显，例如在我们的实验室中，不仅我们在做AI，还有很多专家在做科学AI。在科学领域，AI的渗透会更强，目前在这方面的研究还不多。你们可能会更多地关注环境中的安全问题，例如滥用问题，我们也可能呼吁你们更多关注特定领域的安全问题。

我们需要一个系统的解决方案，不仅仅是大模型本身。我们以ARGC内容为主的内容安全相关研究，如何提升模型本身的安全性是一方面，但第二方面我们还需要一套非常完备的监管模型，我们需要动态地监测智能体的行为过程，判断它是否会带来一定的损害。然后是刚才老师提到的系统红线问题，在传统安全中我们有一系列的安全规范，我们如何将大模型的通用性与这些安全规范结合，实现自动化监测，这样既能节省网络安全监测的效率，也能发挥大模型的通用性，实现更广泛的用途。

在这个过程中，我们现在倾向于从大模型本身入手，但还有一个重要点是动态检测，我们需要一个主动的检测过程，而不是等模型行为结束再检测，那时可能危害已经造成，很难弥补。从技术上看，我认为关键点在于智能体在开放环境中的行为安全问题。我们可能需要从大模型本身的内设安全、行为交互过程中的动态检测以及网络安全的系统红线等三个方面进行系统防御。技术手段上，我们不仅包括现有的各种静态手段，还需要一些主动手段进行约束。这是我的一些观点。谢谢卓尚老师。

---

由于时间关系，我们今天的第一场原着讨论到此结束。非常感谢各位老师的精彩观点。请各位老师回到座位，现在我将时间交还给主持人君怡。再次感谢所有的讨论嘉宾。

刚刚提到的持续监控和评估，这也是我们过渡到第二个主题——AI安全测试的一个很好的时机。现在我们将听取Chris Messerol博士的发言。Messerol博士是Frontier Model Forum的执行主任，这是一个由Anthropic、Google、Microsoft和OpenAI联合成立的非营利组织，致力于推进前沿AI的安全。他是AI治理和安全方面的专家，目前专注于制定负责任的AI安全开发和部署最佳实践。在此之前，Chris曾担任Brookings Institution的AI和新兴技术倡议主任。Chris，非常高兴你能来，现在请你开始发言。

谢谢，非常荣幸能在这里与大家交流。正如刚才提到的，我领导的Frontier Model Forum是一个行业支持的非营利组织，专注于推进前沿AI的安全。我们有三个核心任务，其中之一是制定最佳实践，另两个是推进前沿AI安全的科学研究和信息共享，这也是我们今天如此兴奋的原因之一。

我想先介绍一下什么是前沿AI，以及为什么处理它会这么具有挑战性。然后，我会讲解一些我们在评估和早期最佳实践方面的初步思考，这些思考是在与我们的成员公司中的安全专家讨论中形成的。

当我们提到“前沿AI”时，通常指的是最新一代的先进通用AI技术。我们考虑的不是狭义的AI应用，如贷款算法或面部识别技术，而是通用AI系统，特别是最新一代的系统。

从图表上可以看到，由于我们不断扩展这些系统，通常每几年计算能力就会提升10倍，从而产生更好的一代通用AI系统。我们主要关注的是最新一代的前沿AI系统。如果你想了解更多，可以在我们的网站上查看相关的图示。

我们预计所面临的挑战会随着时间的推移而不断演变。前沿技术将持续变化，正如我们今天早上看到的图表，能力几乎呈指数增长的曲线非常明显。我们关注最新一代，是因为我们希望制定早期的最佳实践，以应对任何特定时间段内最先进的模型。

理解这些模型在训练过程中会获得或发展出哪些特定能力，这对于预测它们的行为非常重要。正如宋教授早些时候提到的，我们很难提前了解这些系统，这使得构建安全有效的系统变得非常困难。

随着这些系统从聊天机器人发展到更具代理性的性质，确保这些系统安全的挑战只会变得越来越重要，因为我们构建的系统将越来越多地与现实世界互动，可能对公共安全和安全产生影响。

我们的一些初步思考是在与我们的安全专家和成员公司进行讨论后形成的，涉及如何结构化评估、进行哪些类型的评估以及一些早期的最佳实践。尽管术语可能有所不同，但我今天想分享的概念是，我们希望通过更多的交流和互动，逐步在这个领域中形成最佳实践。

在评估和风险评估方面，大致可以分为两种：一种是红队演练（Red Teaming），另一种是更自动化的评估。红队演练通常是手动的，利用人类专家的知识来探测特定模型的能力。相比之下，评估通常是基准测试或其他自动化形式，用来探索模型的能力或风险。

我们认为区分评估系统的风险或安全性时，具体谈论什么非常重要。在前沿模型的评估中，主要有两种核心评估类型：性能评估和安全评估。

性能评估对于了解模型的整体推理能力或其他能力至关重要，它可以帮助我们确定如何最佳测试特定风险。性能评估旨在捕捉、识别和评估特定系统的性能范围。因为我们无法预先定义模型训练后的能力，所以需要进行性能评估。

安全评估并不仅仅是了解系统的性能门槛或范围，而是专注于特定风险，评估模型表现出不安全行为的能力。

在安全评估中，我们主要看到两类评估：开发评估和保证评估。开发评估是指在训练周期的不同阶段进行的评估，以基准测试模型在某些安全风险方面的表现。这与全面的保证评估不同，后者通常由独立于开发团队的专家进行，旨在确保系统的安全性，并开发能够保证系统安全的评估。

在保证评估中，我们需要评估系统的最大能力以及在现实世界中的使用情况。我们需要设计能够捕捉模型最极端风险行为的评估，这些行为不会压缩到平均行为中。行为评估的目标是了解模型在一般情况下的平均行为，并在此基础上确保系统的安全性。

在设定评估和红队演练时，我们需要考虑具体风险的评估实践，以及任何评估的通用最佳实践。例如，评估应考虑提示敏感度（Prompt Sensitivity），因为不同提示的具体措辞往往会导致不同的结果。

其他重要的最佳实践还包括同时评估基础模型和最终系统，评估正常使用和对抗性使用（Adversarial Use）。对抗性使用评估非常重要，因为它能揭示模型在恶意使用下的表现。

最后，我们在设计评估时，了解基线（Baseline）非常重要。例如，如果评估系统的生物风险，需要与在没有模型的情况下使用的基线应用（如网络搜索）进行比较。

这只是我们对评估的一些早期思考。信息和通信技术DirectAway在中国引领了多个行业标准的发展，包括中国首个大数据基准标准。他还参与了几项主要国家政策的起草工作。接下来，让我们听听肖静博士的分享。


---

我们取得了一个很好的开端，现在我们很高兴请到了乔余教授。乔余教授是上海AI研究院的助理教授和资深科学家，同时也是中国科学博物馆深圳分馆的兼职研究员。他的研究领域涵盖超级计算机、计算机视觉、深度学习和自动驾驶。最近，乔教授与上海AI研究院的其他研究员合作，发布了多个AI安全项目，包括标准和测试框架。

乔教授，请您继续讲解。感谢您的参与。首先，对因个人原因打断会议表示歉意。实际上，刚才邵静博士是我们大模型安全团队的负责人，我认为他可能比我更了解技术细节，能提供更多的专业信息。不过，我仍然感谢安远和谢总的邀请，给我这个机会与大家分享。

我们刚才讲到这一页，是吧？好的，我先介绍一下。我原本主要研究计算机视觉，后来逐渐开始研究视觉大模型，从2020年开始逐步扩展到多模态研究。随着研究的深入，我们发现安全是大模型发展的一个非常重要的问题，而评测则是建立安全的基础。因此，从去年开始，我们实验室将安全作为一个重要方向。

在研究安全问题时，大家普遍关心安全问题，但具体关注哪些因素、从哪些角度进行评测，尚未形成广泛的认知。我们首先注意到评测体系和数据的缺乏，因此在去年进入这一领域后，我们的首要任务就是建立评测体系。我们建立了蒲公英人工智能治理开放平台，汇总了国内外的治理理论和制度，并建立了相对全面的评测数据集，包括漏洞数据、评测数据等。实验室坚持开源开放理念，开放了许多数据集和规范，推动了这一领域的发展。

如何建立好数据集？大家知道，早期大模型的评测通常是给一个固定的题集，有标准答案，模型回答后进行评测和对齐。但这种方法存在许多问题。对于大模型，如果进行SFT（监督微调）和RLHF（基于人类反馈的强化学习），模型表面表现很好，但并不代表没有风险或问题。

我们设计了一种方法，邀请了社会学、政治学、伦理学、法律等领域的专家，包括上海交通大学、复旦大学的专家教授，帮助设计题目。因为专家时间有限，不可能请每位专家设计大量题目，所以我们通过大模型方法扩展和增强专家设计的题目，并用这些题目进行评测。特别是这些题目具有攻击性和漏洞针对性，评测效果更好。

在大模型安全和对齐方面，经常提到对齐税的问题。虽然RLHF提高了模型的安全性评分，但往往降低了模型原有的性能。我们引入了多目标对齐（MODPO）方法，既保证对齐，又优化模型原有能力。由于时间关系，不详细讲解，但这些算法和框架还有很多优化空间。

基于我们的评测体系和数据，我们实验室研发了一个普安大模型安全评测系统，自动化评测支持上海市的工作。评测结果的透明性和可解释性非常重要。用户常常质疑评测报告的客观性和依据。我们通过基于数据库的解释生成，集成了问答和可解释性，实时更新数据库以适应新规范和规定。

多模态大模型的发展对评测提出了新的要求。我们设计了以视觉为核心的SPVL数据集，包含可能引起有害影响的六个领域、四十多个类别、五十多个子类别，共十多万个问题，帮助评测和对齐。我们还建立了一个具有全面人类价值观的多模态评测体系，包含四种模态和两百三十个应用场景用例，评估放话性、可信度和推理能力。

Agent技术快速发展，已成为强化大模型在特定领域和世界互动的重要手段。我们建立了多智能体评测框架PC，研究智能体之间的交互和协同，及其潜在危险行为，并引入专家角色进行防御。

我们的工作刚刚起步，实验室最初关注定义问题和建立平台，欢迎各机构、企业和研究者共同推动这一领域的发展。我们还成立了生成式人工智能安全评测工作组，我很荣幸当选组长。这个工作组包括权威机构、大学和企业，共同推动人工智能安全评测的共商共治。

我们建立了常态化的交流机制，通过线上和专题会议进行日常交流，从底层和一线角度形成技术规范和共识。我们还共同研发评测技术和工具，包括数据集、规范和平台，并希望能开源开放部分成果。

此外，我们举办了许多安全活动，包括在世界人工智能大会前夕举办的普选安全挑战赛，吸引了全球优秀团队参与。我们日常与相关机构合作进行安全指导。

安全评估流程和规范具有可操作性和全维度评测的特点，服务于应用和产业落地。参与企业提供了很多实际应用建议，希望这个规范能推动国家人工智能安全评测的发展。

未来，人工智能不仅是工具，而是重要的社会基础设施，与人和系统频繁互动。我们需要从社会体系角度考虑人工智能安全，包括需求、应用场景和人机物认知体系的建设。

大模型的发展以Skeleton Law为指引，随着算力、数据和模型规模的增大，模型能力提升。我们也希望探索一个围绕安全的Skeleton Law，通过投入研发资源、数据和算力，实现安全的可持续发展。

安全的Skeleton Law不仅包括参数量、数据量和计算资源，还需要多方参与、更新研发模式和高质量数据。我们希望与大家共同建立面向未来的可持续AI安全Skeleton Law。

---

谢谢你，教授。我们知道您的日程非常紧张，因此非常感谢您能来到我们的讨论室。请回到舞台上准备开始我们关于AI安全的讨论。乔教授，Chris，魏凯，请回到舞台上。这次我们的嘉宾还有瑞敏、何和熊德义教授。瑞敏教授是新加坡的首席人工智能官，他在多个岗位上努力推动新加坡的AI战略目标，包括发展和实施新加坡的国际AI策略。他还是新加坡政府的首席人工智能官和AI高级领导小组成员。熊德义教授是天津大学自然语言处理研究院和国际联合研究中心的教授，他最近在进行AI安全计划，包括大规模评测等项目。我们的主持人是Concordia AI的副总裁Brian Zare，让我们以热烈的掌声欢迎他。

今天的会议内容丰富多彩。首先，我介绍一下我们发布的关于大语言模型 (Large Language Model) 的首次评估研讨会，以及我们为中国进行的风险隔离评估 (Risk Isolation Assessment)。主要有三个方面的风险。第一是不平衡风险，这种风险通常对社会有负面影响。第二是不适用风险，指人们可能会使用大语言模型来解决这些风险。第三是障碍性风险，许多人用不同的词汇描述这种风险，有些人称之为障碍性风险。

针对这些风险，我认为有三种应对策略。第一，大部分风险与障碍性风险有关，而不是正式的分析。但我们看到了一些新研究，分为两类：常规研究和经济研究。常规研究中，人们会提出问题，看看模型是否符合预期，或是否带来更多的力量、财富等。如果模型非常智能，它们可能会撒谎以达到目标。因此，我们缺乏有效的方法来解决障碍性风险。

第二，我们缺乏数据和信息。目前大多数研究都是黑箱研究，我们没有足够的数据来测试模型，也无法打开黑箱模式。

第三，障碍性风险的来源往往是大型公司，它们的数据和信息有限，导致缺乏透明性。这是一个巨大的挑战，特别是在障碍性风险方面。

感谢庄教授和何教授的精彩评论。今天我们很高兴能与您交流。听说新加坡设立了一个国际AI安全中心，能否分享一下在新加坡和国际上的AI安全工作？

非常高兴再次见到您。信任和安全是关键。在过去的59年中，新加坡建立了高度的社会和政治信任。信任是我们健康和生产力的基础，也是人们能够安全行走和进行在线交易的原因。但AI增加了信任的难度。正如许多演讲者今天提到的，AI模型可能会生成假新闻，无法完全依赖。因此，我们在制定国家AI策略时，把信任作为一个重要部分。

我们在多个层面上开展了许多研究和政策举措，支持基础研究和负责任的AI。作为AI Singapore的一部分，我们设立了一个在线安全先进技术中心，专注于错误信息和虚假信息的研究。我们有一个完整的网络安全机构，专注于AI安全。我们还设立了数字信任中心，作为AI安全研究所的国家焦点，研究评估和测试整个开发生命周期，从开发到部署AI模型。这个中心还与其他国家合作，提高AI测试的科学性，改进新加坡及全球的工作。

感谢各位的关注。我们已经确定了AI安全评估的优先事项，现在继续讨论评估的生命周期。测试可以在训练期间、预部署和后部署阶段进行。您认为在这些阶段进行评估有哪些好处和挑战？在与中国和美国公司合作时有哪些经验？好，谢谢。”


大模型的进展非常迅速，但其原理决定了我们无法对模型的激励、安全风险和能力水平有百分之百的全面了解。就像用竹篮打水一样，总会漏水。我们的工作，比如测试，实际上是在识别漏洞并修补它们。但是，有多少漏洞我们可能并不完全清楚。

目前来说，我们需要优先快速、及时地发现各个环节中暴露出的风险，比如研发环节和使用环节的风险，并及时修复它们。这相对容易做到。只要我们知道风险是什么，就能定义、测试并改进它。问题在于我们不了解的风险太多。因此，当务之急是建立一套动态、敏捷的机制，使我们能够始终跟上模型技术水平的提升和风险。

我们需要敏捷地发现这些风险，同时迅速通知产业界，让他们知道风险所在并进行改进。测试非常关键，但它只是一个环节。我们需要在各个环节上建立一套敏捷治理的技术生态，才能堵住不断暴露的漏洞。否则，我们将永远在与未知风险赛跑，难以控制局面。谢谢。

克里斯，首先，感谢您的光临。很高兴能和您一起参与这个讨论。我会引用一些我同学的评论。这个话题涉及很多不明显的因素，但我认为这是一个非常复杂的问题。如果我们在未来五到十年后再讨论这个问题，我相信我们会更加成熟。不过现在，我认为有几点是我们需要关注的。

在计算和测试的整个生命周期中，我们需要明确企业和政府需要在各个阶段进行哪些测试。我认为这甚至包括预备训练阶段的测试。例如，在10倍的数据规模下进行测试，您是否认为可以安全地进行？在这些系统能够自主研究和执行任务之前，我们还没有完全达到那个水平，但系统的自动化测试（automated testing）是我们肯定需要进行的。

在训练模型之后，我们需要进行一些调整，并继续进行多次测试，直到我们确信模型是安全的，用户可以放心使用。包括普通用户以及可能会尝试恶意利用系统的用户。问题是，在这些领域我们需要进行多少测试？希望在未来一两个月内，我们能在全球范围内就AI系统的测试量展开更多讨论，以便在进入下一个阶段时感到更加自信。教授，您有什么想说的吗？

我认为刚刚各位嘉宾的发言非常精彩和全面，我想补充两个点。首先，在当前阶段，安全问题非常重要，但实际上，无论是学术界还是产业界，我们在安全方面投入的资源，无论是计算资源还是研发资源，都远远落后于我们在大模型发展和产业应用方面的投入。

其次，我们对通用人工智能（AGI），尤其是大模型安全的了解非常有限。我们知道存在很多安全隐患，但对这些隐患的深入认识不多。更重要的是，我们不知道为什么会有这么多安全隐患。我们现在对安全问题的认识更多是基于应用过程中的现象观察，而不是对背后原因的理解。

基于这两点观察，我认为有两方面的工作需要做。首先，我们需要投入更多资源，并进行更多国际合作，因为安全问题是全人类共同面临的问题。其次，我呼吁科研学术界和产业界找到一条通向安全通用人工智能（Safety AGI）的技术框架。目前，无论是SFT还是RHF，大多是单点技术的研究，我们很难确信仅凭这些技术就能实现安全的AGI。因此，学术界需要进行更好的研究、制定框架并投入资源。谢谢。

第二，我们需要进行全面的AI测试，包括从预约训练到训练后的评估和投入。第三，当前AI安全投资水平似乎还不够，需要进一步提升。我认为这对AI评测带来了新的挑战，也是在一些课程中提到的。

首先，我想讨论一下测试方法的问题。特别希望熊教授和乔教授能对此发表评论。目前流行的测试方法是自动测试和人工干预训练。你们认为在当前情况下，有没有新的方法可以采用？

从技术角度来看，有几个方面值得关注。我们确实需要建立更好的技术和方法框架来解决这个问题。我们的技术中，智能体（AI Agent）加上工具调用的方式已经被验证有效，包括智能体内部的交互、反思和任务规划，这些都成为了大语言模型（LLM）落地的重要技术。目前，在学术界和工业界，这些技术都有很好的应用。

实际上，智能体演化出的技术对于解决当前大模型中的问题非常重要。传统的安全研究，比如计算机安全研究，涉及许多计算机理论和密码学研究，能提出非常硬核的数学问题或理论问题。然而，目前的安全研究非常碎片化，这意味着我们缺乏一个良好的基础和体系。我认为这是我们现在需要关注的问题，无论是在学术界还是产业界，我完全同意这一点。

其次，在评测方面，我们需要建立大量的数据资料。但在安全方面，我们面临许多问题，这是所有课程计划中涉及的内容。如果仅依靠我们的AI社区，我们既没有足够的资源，也没有能力建立这些资料。因此，我认为我们必须与更多的社区合作，解决AI带来的风险问题。谢谢。

河先生早就提到建立信任的重要性。如果公司自己进行评测，社会很难完全信任AI的安全性。我认为这涉及第三方的角色。我希望河先生和董事长能就这些问题发表意见。您认为第三方角色面临哪些挑战？例如，陈教授提到的质量问题。如果第三方测试员只能通过AI模型进行黑箱测试，那么他们的AI安全测试应该如何进行？希望您能分享您的观点，河先生。

在AI Verify的经验上，我认为我们已经在这方面做了很多工作。我要讲三个原因，为什么我们在AI Verify上做这些工作。AI Verify是新加坡政府两年前推出的一款测试工具。它有三个关键点。首先，AI Verify代表了一种重要的原则，即所有人都在进行严谨的研究。其次，AI Verify在企业中应用，使用实际工具进行最优的研究。否则，第三方在远处操作时，必须尽力做好研究。第三，你需要第三方和系统始终处于领先地位。因此，去年我们更新了AI Verify，推出了以Moonshot为核心的AI版本，这是我们团队设计和开发的最初产品。

第三方机构非常重要。我认为，第三方机构的存在有助于提升公众对人工智能，特别是大模型技术的信心。第三方通过其专业能力，向研发者和应用者指出风险所在，并提出改进建议和下一步措施。同时，第三方机构可以作为桥梁，汇集所有安全风险、测试数据、方法论和研究结果，从而更好地赋能各个研发机构。第三方机构的角色是丰富多样的，而测试是其中重要的核心手段。

我非常同意乔老师的观点，目前产业界对AI的投资仍然有限。第三方机构的存在，可以提供公共产品，在有限的安全预算内，控制研发和安全投入的成本，使其合理可控。我们的安全工作应该服务于更好的发展和应用。在控制成本和投入的同时，确保安全前提下，前沿模型研发机构需要披露风险列表，展现出负责任的态度。第三方机构与企业合作，共同完成这些任务，形成闭环。

感谢。我非常同意AI安全应该是全球公共产品的愿景。我们可以在全球开发AI安全评估，或在不同城市和国家建立AI安全实验室，达成一致的术语，使这些系统真正实现互操作性。如果我们不知道彼此所指的红队（red teaming）是什么意思，就很难实现互操作性。因此，我认为这是未来一年内需要完成的关键第一步。

乔教授，我想补充一点，我们在这个领域太需要国际合作和共识了。感谢所有嘉宾的精彩发言。让我们给这个优秀的小组一个掌声。远程参与讨论的嘉宾请留步，其他论坛嘉宾请上台合影。上午论坛结束，下午论坛将在1:10开始。感谢观看。字幕志愿者：杨栋梁。

回顾发明史，每一次重大发明都带来希望与恐惧。从电池、汽车到现代的电子技术，每次技术变革都彻底改变了社区。这种变化带来了新的风险和未知的危险，我们无法预见所有的挑战。十年或二十年后，我们的电子产业会如何影响社区？比如，计算机对社区的影响是显而易见的。我们相信电子产业可以为世界带来更好的未来。

现代转变中，我们面临新的问题，如水资源和环境友好型技术的挑战。整体上，一切都有一个相应的影响。我们尝试理解AI如何带来更好的世界，尤其在健康领域。我专注于健康，因为这是我最了解的领域。一个例子是AI在医学影像分析中的应用。在美国，AI系统正在进行医学影像分析试验，帮助早期诊断癌症。这种早期诊断理论上可以提高癌症存活率，降低健康忧虑。这是AI应用的一个例子。

在医疗资源分配方面，AI可以整合各种工具，如早期诊断和预测，以解决医疗问题。例如，通过预测病人距离等信息，可以优化医疗资源的使用，帮助医生和病人。

从更广泛的经济视角来看，有研究表明AI可以提高客户服务部门的生产力。例如，一项研究显示，AI提高了客服人员在一小时内完成任务的数量，这显示了AI对经济的潜在影响。然而，我们必须谨慎看待自动驾驶等技术的发展。历史上，AI的发展充满了不确定性。十年前，我们预测自动驾驶汽车将很快普及，但实际情况并非如此。专家现在认为自动驾驶汽车可能需要十年时间才能真正对经济产生重大影响。

在AI领域，我们有很多承诺和梦想，但这些都需要时间实现。AI的投资规模每年达到数千亿，但找到正确的路径仍然充满挑战。AI技术逐渐改变日常生活，但需要时间才能对社会产生深远影响。AI时代的生产力增长需要20年时间才能显现。因此，我们仍在探索如何有效利用AI，这需要社会的信任和AI的基本原则。

我们进行风险分析时，发现AI对民主社会的影响很大。例如，AI在网络中创建了“过滤泡沫”（filter bubbles），影响了公众的认知。虽然AI带来了很多新问题，但也继承了旧问题，如隐私风险和数据安全问题。我们越来越关注AI模式的组织，例如，GPT中的中心化角色。这引发了关于数据隐私和财务安全的问题。

另一个重要问题是AI在执行过程中的潜在风险。AI系统在日常生活中被广泛应用，但也带来了执行犯罪的风险。这些风险并不是技术问题，而是技术应用的问题。

AI的发展还面临经济上的挑战，例如计算成本和资源集中问题。随着计算机变得更强大，计算成本也在增加。这种不可持续的经济模式可能导致硬件功能的退化。我们需要开放数据和多领域合作，以建立代表性的AI系统。

我们建立了世界AI组织，旨在制定标准和提供指导，促进全球合作。感谢Gail的精彩观点。我相信之后会有更多讨论。

现在欢迎下一位演讲者，新加坡主要智慧机器官的Ramin Ho，他将分享新加坡在AI治理方面的经验。

每个国家的AI治理政策都基于其独特的情况，例如历史、国家优势和企业基础。新加坡在AI安全和治理方面有四个普遍的原则。首先，我们必须以谦卑的态度处理AI治理。没有人完全理解AI系统的内部工作原理，我们也无法预测其发展。因此，政策制定者必须持续学习，不断测试和改进理论，尊重企业和专家的意见。我们在新加坡与全球专家合作，解决AI发展的关键问题。

第二，我们需要透视的心态。AI政策没有黑白分明的界限。AI系统可能有用，但有时也会出错。AI可以提高生产力，但也可能导致工作中断。我们需要在不同领域找到平衡。例如，无人驾驶车辆和癌症诊断的风险完全不同。

第三，我们需要提高治理AI的能力。我们鼓励政策制定者使用AI，了解其潜力和局限性。新加坡积极推动政府内部的AI应用，并支持技术倾向的官员开发自己的AI工具。我们还支持数字信任、在线安全和负责任AI等领域的研究。

最后，我们需要国际合作。AI是全球供应链的一部分，任何一个国家都无法垄断AI的研究和监管智慧。我们需要共同努力，制定全球互操作标准，分享经验和方法。我们开源了AI Verify，并启动AI Verify Foundation，利用全球开源社区的专业知识和能力。

总结一下，AI安全和治理的挑战将继续发展，但我们必须坚持与技术问题和政策困境的互动。如果我们共同采用谦卑态度、透视感、提升能力的愿望和合作的意愿，我相信我们可以实现治理这一技术的正确平衡，并利用AI为公众服务。谢谢。

非常感谢贺博士的建议。现在欢迎中国政治法律大学的张玲翰教授，她在算法监管、平台治理、数据安全和AI方面有丰富的经验。张教授，请您发言。

大家好，非常高兴今天能用中文介绍我最近写的一篇论文，题目是《从风险到价值：探索中国人工智能的治理方案》。在讨论安全议题时，我们需要思考，风险治理框架是否是最佳方案？当前的风险治理手段能否完全应对人工智能带来的影响？在追求安全的过程中，如何定义“足够安全”？首先，我认为人工智能在风险治理过程中，识别和应对风险时，有些问题无法完全解决。我们应超越风险治理，采取基于价值的治理框架。希望大家多多批评指正。谢谢。

---

那么我们接下来看下一张图。我们可以看到，目前基于风险的治理已经成为全球人工智能治理的共同主题。在这张图中，许多国际组织将风险治理作为其宣言或指导意见的基础逻辑框架。同时，基于风险的治理也被许多国家的立法所采纳。接下来的问题是，大家都在专注于风险治理，但我们也看到在风险治理的过程中，仍然存在一些剩余的风险。

例如，在欧盟的《AI法案》(AI Act)中，风险按影响范围分为不可接受、高、中、低等级别。而美国商务部的国家标准与技术研究院(NIST)则根据风险的来源和成因进行分类。在实际应对人工智能风险的过程中，我们发现了许多问题。以NIST发布的风险指南为例，首先，风险判断中包含了很多价值考量。风险不仅是一个科学概念，更是一个规范性概念。例如，在NIST框架中，风险被分为技术性风险和技术社会风险，但将哪些风险归类为技术风险本身就包含了价值判断。

其次，风险分类中一个重要的问题是，大部分的风险分类都将隐私侵害、歧视等对人权的侵害视为人工智能的风险。然而，这类风险不像汽车事故概率或食品安全概率那样易于量化和计算。我们的风险治理基本思路是计算损害的大小和发生概率，再根据收益采取措施。但如果面对的是难以量化的风险，这种治理方式就会遇到困难。

第三，我们来看PPT，大部分的风险分类都将失业问题视为人工智能的重要风险，并认为现行的法律框架无法追责人工智能服务提供者。但风险的重要特征是不确定性。这类影响真的不确定吗？人工智能必然会带来大规模失业和劳动替代，且无法适应传统法律框架。这更像是社会变革的必然影响。

更重要的是，人工智能的风险不同于以往任何一种风险，因为我们无法明确其未来具体是什么。这种不确定性和不可预测性是大家反复提到的。人工智能的自我复制与自我完善能力可能是我们未来未遇见或未处理过的风险，但我们目前不知道它何时会出现。此外，人工智能技术通过开源变得广泛可得，这也改变了风险的来源。

第二个问题是，我们看到风险治理措施包括事前、事中和事后的预防、缓解与消除。如果列出几种典型的风险治理措施，事前评估风险，分类分级，采取相应治理措施是常见做法。这些措施与人工智能治理目标和手段存在错位。

从人工智能治理理念来看，风险治理理念有一个修正主义内核。修正主义认为，如果技术存在缺陷和障碍，在完全修复和可信任之前，不应直接应用于实践。然而，人工智能技术的应用已成必然趋势，无法在完全消除风险后再使用。

人工智能治理的工具层面，最重要的是事先评估和预测。但如果没有足够深入的人工智能应用和技术在社会中广泛使用，就无法确切了解风险的产生和具体程度，更不用说不可预见的风险了。如果如此，如何进行事先识别评估和监测体系呢？

人工智能治理的前提是，如果风险无法量化，就无法纳入成本收益计算。比如，隐私数据泄露的赔偿金额虽然高，但个体受害者获得的赔偿可能只有几美分。这种难以个体救济的风险使得成本收益计算更加困难。

在这里，我想简单介绍一下中国逐步发展出的修正人工智能风险治理路径。中国的人工智能治理正在本土化，可以分为探索阶段、定向阶段和系统结成阶段。探索阶段从2015年到2022年，这期间发展了很多本土治理手段。在风险认知层面，我们有很多共性部分，从2015年到2020年，有新一代人工智能伦理规范、算法推荐管理规定和个人信息保护法，采取了风险分级分类和防控路线。在治理手段上，与国际对接，包括个人信息保护影响评估和算法影响评估。同时，我们也发展了中国的特色部分，比如在立法中平衡发展与安全，秉持国家总体安全观进行安全治理。

在网络信息领域，我们采取了网络信息生态安全的概念。在算法治理和深度合成治理过程中，我们有信息内容安全、消费者权益保护和市场竞争秩序等价值排序。中国的特色还包括算法备案，不是准入或认证制度，而是信息备案和采集制度。比如，2021年算法推荐管理规定中特别提出了未成年人防沉迷、老年人和劳动者权利保护和未成年人宵禁制度。这些在世界上都是独特的。

目前，我认为中国的人工智能治理体系和理念正在逐步形成，超越了风险治理理念。首先在风险认知上，我们与国际保持同步。我们国内的技术标准对风险分类如失控性风险、社会性风险、侵权性风险、歧视性风险、责任性风险等与国际相关分类完全对接。在治理手段上，事前评估、认证和事后追责也吸取了国外经验。

生成式人工智能迅速发展的近两年，我们看到很多中国特有的价值理念和治理手段。比如，不发展就是最大的不安全。我们认为最大的风险是中国人工智能产业和技术未得到有效发展。我们强调尊重各国本土价值观和发展阶段需求以及文化。在治理手段上，生成式人工智能暂行管理办法出台后，中国形成了分层治理的基本理念。中国开展了很多人工智能基础设施建设工作，如国家数据局和工信部在数据要素和算力基础设施建设上的工作。

总的来说，中国的人工智能治理理念不仅是基于风险的，更是基于价值的。基于价值的治理不排斥风险治理，但超越了风险治理。在中国的立法和治理政策中，人工智能既是社会基础设施，也是社会生产的组织形式。中国的治理理念和方案是以人为本，智能向善。技术不能偏离人类文明进步方向，人工智能必须在法律、伦理和人道主义的价值取向中发展。

在治理手段上，我们看到系统发力。今年5月，全国人大常委会和国务院将人工智能立法纳入立法计划，并有丰富的日常监管措施。在立法和监管活动中，中国人工智能安全框架正在积极讨论和预养。前天刚颁布了国家强制技术标准，生成式人工智能内容标识技术标准。

基于价值的人工智能治理体系正在逐步构建，我个人将其分为三个层次。首先，我们观察人工智能的本体价值，将泛化的风险概念逐渐分离，区分人工智能的必然影响和不确定影响。其次，我们根据中国本土价值观判断人工智能治理的短期和长期目标，梳理个性化治理需求。对于中国来说，技术层面要安全发展，服务应用层面延续治理体系，基础设施层面促进建设，社会生产组织方式层面强调绿色环保和新制生产力的治理方式。

最后，我们强调人类共同核心价值观和人类命运共同体。这是中国参与全球人工智能治理提出的方案。今年3月的人工智能法学者建议稿中有详细解释。由于时间关系，我就先讲到这里，感谢大家，谢谢。

---

大家好，我是一名电脑科学家，今天受邀向大家介绍美国的AI（人工智能）政策。请大家坐好，因为这是我第一次介绍这个主题。我会一直牢记我们当前的情况和AI的极端潜力，它是最大的、最有能力的全面计算系统。我们正致力于将它发展成最强大的AI系统，这将是世界上第一个通用智能的最大工业项目，也是最大规模的产业。

在美国，AI并没有特定的制度。实际上，AI已经渗透到几乎所有人的日常活动和各项政策中，在每个领域都会有所体现。因此，制定AI政策非常困难，特别是因为系统的黑箱性和其普遍性技术常被误解。那么，美国是否有制定AI的方式呢？我认为，我们与许多国家共享主要目标。在美国，我们特别关注AI领域的增长，希望美国能在AI领域占据主导地位，推动经济和社会的利益，以及维护国际安全。同时，我们也致力于保护美国的核心利益和影响力。我们还分享一些全球目标，这些目标体现在许多原则中，比如AI的OECD原则。我特别想提到其中关于安全的1.4条。

目前，美国的AI政策和制定方式正在发展中。有些组织在推动竞争利益，并提出了三种可能的法律。第一种是去年10月发布的行政命令，主要针对大型模型，指导现有的国家公司。第二种也是去年10月发布的行政命令，涉及AI对就业、法律、国防和违法行为的影响。第三种是在加利福尼亚州的法规，名为SP-1047，关注安全和创新，尤其是前沿模型的应用。这些问题仍在讨论中，但尚未成为法律。

在其他国家，某些法律已出台，但这些大型框架尚未全面实施。最近，我们听到一些挑战，当使用全面目标技术时，证明系统具有可接受的低风险非常困难，因为你需要知道测试对象是什么。美国的法律系统涉及各个领域，因此使用AI的系统在这些领域中的应用可能需要调整。例如，自动驾驶汽车法律较少，但很多情况下，国际法律提供了良好的基础。

在公平分配费用和利益方面，例如职业和借贷分配，也存在相关法律。在美国，我们经常听到批评，认为法律制定过程低效，尤其在涉及训练数据用途方面，如相似用途或职业创造方式。这些批评也涉及生产保护法律。

此外，还有保险能力的问题。如果你在销售AI系统，需要证明系统有一定的保险能力和可靠性。其他制度来源如美国的《计算机服务法》也对公司产生影响。我认为，制度本身不是障碍，相反是必要的指导。

我们可以从现有的国际安全组织中学习，例如风险安全、核武信息、电信组织等。他们通过找到共同基础来合作，如果不能合作，就寻找正常化协议或直接将问题转移到特定领域。

最后，从我们人类合作AI中心的角度来看，我们认为发展安全的AI系统非常重要。目前有很多技术和方法，例如LHF和法律性AI，我们需要更多的透明性和可解释性来确保AI系统的安全。

综上所述，最近200年的科技发展对人类社会产生了巨大影响。19世纪的内燃机和电气技术让我们生活更加自由，20世纪的互联网和通信技术让我们信息更加自由。21世纪的大模型和脑机接口技术可能带来更多创新，甚至从无到有的创新，虽然也会引发安全问题。大模型的广泛使用会引起隐私、虚假信息、犯罪、知识产权和社会治理等问题。

面对这些问题，我们需要在发展和安全之间找到平衡。不同国家有不同的立法模式，值得我们进一步探讨。谢谢大家的聆听。


接下来想请教王老师，作为研发机构的代表，我们知道上海人工智能实验室在保障人工智能品质方面做了很多工作。包括昨天周主任提出的一个安全发展项目——人工智能发展的45度平衡率。请您解读一下安全和发展之间的关系。

好的。昨天周博文教授在社会认知大会的上午会议上，分享了我们实验室对这个问题的重要判断，他称之为“人工智能的45度平衡率”的技术体系思想。目前我们认为，人工智能的发展是不平衡的。大部分技术和算力资源都投入在性能研发上，尽管我们有IIS等兼顾性能和安全的技术，但总体上还是偏重性能。这导致技术社区在安全方面的资源、人力和算力投入严重不足。而且，我们目前的技术方法还是很分散。我们希望找到一条安全优先，同时兼顾性能增长的技术路径。当然，这条技术路径的探索非常复杂且艰辛。周教授呼吁大家沿着45度的线推进人工智能的发展。如果这条路径能够走通，我相信在面对众多挑战的情况下，人工智能的安全风险将得到重要保障。我们也在为此努力。所以，我想强调的是，我们需要一个大家共识的发展路线框架。我们不能长期低于45度发展，但如果长期高于45度，也难以满足市场商业化的要求。如果有一个理想的路线，大家共同努力，我认为还是有很大可能实现的。谢谢。

而且可能需要动态调整以灵活应对。接下来我们想讨论一下中国在人工智能治理方面面临的挑战和独特机遇。Gail，我们也想聊一下最近中法发布人工智能治理声明后，中国对法国的情况非常感兴趣。您能介绍一下法国在人工智能领域有哪些独特的机遇和挑战吗？另外，我们计划在2025年初举办一场AI行动峰会，您能谈一下您对此的期待吗？

非常感兴趣，我可以谈一下法国的情况。与其他国家不同，法国目前面临着相当困难的社会和经济讨论，这种讨论贯穿整个国家。在社会层面上，存在着严重的不信任，无论是员工与决策者之间，还是城市之间，甚至邻近地区之间。这种情况在其他国家也可能存在，但在法国尤为严重。

另一个讨论的焦点是经济问题。历史上，法国的数字经济相对落后。我们主要是数字服务的使用者，这影响了我们与数字服务的关系。然而，法国的AI能力非常强，所以当前在经济领域正发生变化，特别是在城市中带来了很多希望和积极的情绪。现在在城市中的情况已经有所不同。

在政策方面，规划正在进行，这些规划涵盖了多个方面。首先是投资，不仅在数字服务领域增加了投资，还在其他领域也有所增加。我们看到了开放和公共价值的提升，公共事业与数字服务的结合也在增加。另一个挑战是在社会各个层次之间建立联系，我们的目标是维持人类之间的联系，而不仅仅是数字联系。这就是我们的挑战，要避免数字鸿沟的出现。我认为每个国家都有数字鸿沟的问题，我们正在投资来解决这个问题。这就是法国的规划。谢谢。

谢谢您的回答。在数字区别方面，我认为这是一个非常重要的方面。张教授，我们有几个问题想请教您。我们了解到，您今天发布了一份关于人工智能法的学者建议稿，并在演讲中提到了关于中国风险治理的修正和本土化问题。您能否总体上谈谈您对中国人工智能立法的整体逻辑和框架的看法？谢谢。

回答这个问题，我认为可以从两个角度来看中国的人工智能立法。

第一个角度，我们要把人工智能立法视为中国在全球范围内展示制度的一张名片。当全球各国都认为需要应对人工智能风险时，中国作为一个在人工智能技术方面相对领先的大国，如何树立“负责任大国”的形象非常重要。事实上，中国在人工智能治理方面已经做了大量工作。中国是世界上唯一一个在人工智能安全治理方面覆盖了国家规划、法律、行政法规、部门规章和技术标准的国家，也是唯一一个在大模型治理方面已经落地的国家。然而，我们在这方面的努力并没有被世界广泛了解。其中一个重要原因是我们缺乏一部高层次的法律，作为中国的制度名片，让世界知道我们已经做了什么和正在做什么。在联合国的工作组里，我们有30多位专家，我在组里的任务之一就是向大家介绍中国的努力。令我惊讶的是，这些对人工智能治理非常了解的国际专家中，有很多人并不了解中国在产业治理方面的具体工作。我认为这是人工智能立法的第一个最重要的目标。

第二个角度是要符合中国本土的治理需求。我认为中国在世界上的位置是“领先的追赶者”。作为一个大国，我们要做好人工智能安全治理，不能让中国成为人工智能治理的落后地区。同时，我们也要认识到，中国的人工智能技术和产业发展需要大量的法律法规调整，为其提供必要的要素和资源，并设立合理的法律责任框架。在这两个理念的指导下，我相信，经过充分讨论，我们能形成一部既能作为中国制度名片，展示中国在全球人工智能治理中的重要形象的人工智能法，又能有效防范人工智能安全风险，促进人工智能安全，并符合中国本土技术和产业发展需求的法律。谢谢。

谢谢。中国学者做了很多工作需要更好地向外界传播。我们也在努力，通过各种方式展示中国在安全研究、治理和立法方面的工作，希望能够更好地传递这些信息。

接下来，我想聊一下最近比较热门的话题——人工智能安全研究所。自去年英国AI安全峰会以来，美国、英国、法国等十个国家以及欧盟都成立了自己的国家级人工智能研究所，同时还建立了一个全球性的研究网络。对此，我想请教一下Mark，你如何看待这些人工智能安全研究所？你认为它们为什么要成立？它们的工作重点是什么？未来会有哪些发展？谢谢。

我想确认一下，我是否在回答有关中国进行AI管理的信息。我们中心加入了美国国际AI安全系统，是这个意思吗？是的，对。我的意思是，中国在美国的参与不仅仅是研究这些强大系统的安全行动问题，还起到了监督的作用，用于检查重大事件。在某种程度上，也在国家层面确认哪些内容需要进行测试。

我曾参与过一次讨论，我们在讨论是否有必要进行这些措施。我认为这是非常有道理的，但我不认为这是绝对必要的。我相信美国的AI安全研究所是一个值得努力的方向。谢谢。

---

我们希望不同国家的AI安全机构之间能有更多的合作，包括中国的相关机构。我们的论坛是AI安全讨论的重要环节，所以讨论如何减轻AI潜在的战略风险是一个重要议题。我想请问季老师，我们知道AI的战略风险目前存在很多不确定性，包括其严重性、可能性和紧迫性等方面，大家有不同的理解。在这种不确定性和缺乏共识的情况下，您认为应如何推进相关政策和治理呢？

季老师，OpenAI提出了一种非常有影响力的方法，即价值对齐策略，包括人工智能与人类的价值对齐，以及国际间的价值对齐。刚才张林涵教授在演讲中也提到了以价值为基础的人工智能治理问题。我稍有不同意见，因为价值对齐是一个复杂的问题。我认为沟通过程更重要。沟通过程为何更重要呢？首先，当我们谈论人工智能安全时，需要有两个视角：一个是监管者的视角，中国采取国家备案制和验证技术；另一个是用户的视角，需要强调可解释性，并与用户进行反馈。在这个过程中，沟通过程非常重要。

在理论上，我们可以看到两个方面：一个是技术性程序公正概念的提出；另一个是今年年初一批中国学者在《自然》杂志的子刊中提出的仪式性对话框架，强调对话氛围对人工智能可信性的重要意义。此外，我们应将程序公正监控嵌入人工智能系统中，或者让人工智能系统之间产生制衡作用。我认为这是技术性程序性安全监管的重要内容。在这方面已有一些很好的先例，如新加坡的AI Verify是一个很好的示范，它具有普遍意义，美国的Watson X Governance是一个监管模型。日本在6月4日发布的综合创新战略中特别提到其三个基本方针，强调人工智能领域的安全与竞争力平衡及技术性侧面。如果将这些因素结合起来，通过科技公司技术能力的提升，增强人工智能的安全保障是可能的。

今年3月，北京人工智能安全共识提到将三分之一的研发预算投入到安全保障领域，这是可以理解和接受的。科技公司也认为这是可行的。如果这样，通过人工智能治理达成更广泛的共识，而这种共识恰恰是立法的基础。王老师，您如何回应刚才季老师的观点？

我对季老师的观点表示认可，并提出一个概念：人工智能是全球公共体，即AI安全作为全球公共产品。这一概念是我们与清华大学和焦大一起研究的。现在全球对人工智能安全风险的讨论往往是从风险管理的视角出发，我们可以补充另一个维度，即将人工智能安全作为一种公共产品，政府、企业、第三方和公众共同建设、共享相关知识、能力和资源。大家对人工智能安全有许多担忧，这些担忧源于对人工智能安全风险知识的缺乏。因此，需要更多的讨论和科学研究，共同参与人工智能安全知识的形成过程。

此外，人工智能安全能力也需要提升，是否有足够的技术手段和相关工作来支撑安全提升？是否有足够的资源投入和公共服务产品的开发？这是非常关键的问题。例如在上海，我们通过举办沙龙等方式，将不同主体的知识和风险认识共享，达成共识，共同推进安全基准、标准、资源库和治理平台的建设。新加坡也做了很多类似的工作。这不是上海和新加坡独有的，而是全球范围内的努力。

我们希望提出“AI安全作为全球公共产品”这一概念，全球应共同建设人工智能安全公共体，特别是美国、欧洲和中国这些人工智能发展基础较好的国家应合作，加大对全球人工智能安全公共体的供给。发展中国家基础较差，需要资源较好的国家共同建设。这需要科学家、企业和各种国际平台的合作。形成共同认识后，许多有意义的工作可以共同推进。非常遗憾今天时间有限，期待以后有更多时间交流。感谢大家的关注。


---

我现在在墨西哥，但我感到很高兴能与您一起参加这个会议，我绝对不会想要错过这个机会。这次会议将成为美国最重要的议会之一，讨论现代智能技术。尽管我已经担任美国总统几年，但在过去近15年里，我一直在进行机构发展方面的讨论。我想和您分享一下我们在AI安全方面的看法，以及在国际协调和中国国防中的AI安全讨论。

在过去几十年里，我的团队，卡内基协会，帮助了NAP大使团队，在全球范围内的支持问题上，致力于持久和平。今天，如你所知，新的机会和挑战在国际健康和安全领域不断涌现。在这种变化中，我们的进步和专业讨论非常重要，我希望您记住这四个要点，我们能够继续前进。

首先，我们看到的进步是什么？我们看到的是实际的变化。这些变化不仅仅是程度上的，而是本质上的提升。讨论的价值显著增加，这很重要。如果您了解我们在社会上的努力，就会发现我们不仅仅是在建筑水平上取得了进展，而是将整个讨论提升到了一个新的高度。实际上，在前沿模型的设定上，我们看到自2010年以来，前沿模型在解决更复杂问题方面的设定结果显著提高，创造了更具体的词汇和图像。聊天机器人和其他技术从小规模快速发展，正在改变我们的工作、学习和与世界的联系。许多这些变化是由AI活动系统推动的，但我认为政策的影响更为重要。这个系统包括新加坡、纽约、伦敦、深圳、上海、北京、加拿大部分地区等，这些地区在推动AI创新和研发方面非常活跃。我对这些产业的发展非常关注。

第二个问题是，各种机会和风险推动了这些发展的变化。我们必须认真对待这些风险，并采取多种措施来应对。例如，避免车辆和交通系统的破坏，中等安全如恢复权或公共服务不中断。此外，还需要管理最高级别AI系统的侵入或失控。这些计划虽然不是那么紧急，但也不容忽视。你知道，AI承诺解决我们面临的许多挑战，无论是简单还是复杂的医疗任务，还是扩展中心服务。全球有4.5亿人口缺乏医疗服务，250亿学生无法获得复杂教育。实现这些目标需要我们讨论线上计划的风险。这些计划可能会造成大量信息泄露，削弱资源，并进行成功的信息攻击。如果用于军事和国际安全设施，高层执行可能会带来严重后果。坦白说，现有证据表明这些计划可能会失控或被用于生物武器，但这些风险在未来可能会发生。实际上，在AI前沿，我认为这种担忧是合理的，未来我们将与其他AI系统和人类互动，社交运动将受影响。这些系统最终会将人类决策传统化，这些变化需要国际协调。

这引出了第三点，当我们考虑AI决策时，最高级别的系统总是需要国际协调。事实上，这些协调会反映在生物技术的变化和环境的反应中。21世纪，美国和中国将共同保持国际系统，推动全人类的发展，解决未来问题。这意味着我们要关注AI风险，同时提升其带来的利益，因为两者是相互联系的。竞争确实是AI领域的一个主要因素，不可能完全消失，但我们也看到国际协调的例子，包括中国和美国。让我举几个例子。2020年11月，美国举行了第一次AI安全会议，28个国家，包括中国和美国，签署了一份国际科学计划。2024年5月，在首尔举行的AI会议上，10个国家和欧洲同意建立国际安全研究所的联合研究所。实际承诺在继续推进，这种趋势非常重要。2025年在澳大利亚的会议上，我认为Bletchley讨论对美国的影响显著。总统布泰罗斯在Bletchley参与中提出了一些想法，现在已经变成了美国联合研究所的实际计划。2023年10月，总统布泰罗斯成立了AI高层监督组织，包括中国、美国和其他国家的参与。这些组织将在2024年提出最终建议，并在2024年10月执行美国主导的AI高层监督组织。几天前，总统布泰罗斯也支持了一个类似的设计，得到中国的支持。我认为这些计划非常鼓舞人心，这些计划在国际上和美国的参与组织类似。

第四，国际协调包括在这些区域中。我刚才介绍的计划中，有更多的国际协调例子。包括国际协调的高层监督组织，描述了外部政府在AI安全方面的作用。例如，最近发布的研究报告，关于AI安全计划，许多国际政府同意了这个计划，美国也在与外部合作伙伴进行研究。还有其他专业机构制定了计划，以加强AI安全基础。这将帮助我们了解AI的发展和讨论。我希望这些国际项目能够成功，尽管有时会遇到困难。为了鼓励维持协调和管理AI风险和利益，重要国家如中国和美国需要在某些问题上达成共识。这是理解的，根据国际安全批准和主导AI的发展。但这不意味着放弃目标。通过协调，在理解和管理AI风险时，利益大幅增加，国家能够成功讨论并提供负责任的AI管理。中国与美国都对AI科学进展感兴趣，以提高安全过程，分享正确的测试和监控，以防范未来可能出现的风险，包括AI系统的失控。我认为这次聚会是一个机会，提高这些问题的计划，并使政府在任何情况下合作。在AI、互联网和其他挑战方面，这是一个困难的时刻。但这也意味着我们需要更多的合作，尽快采取行动，专心致志，因为这将使世界在AI领域变得更好，为人类提供更大的能力。因此，我期待与您在此次会议上讨论这些问题，以及更多有意义的讨论。非常感谢您。

---

邢学和邢学，感谢您对国际协作的承诺。请您保持在线，我们稍后会给您打电话。我们将介绍邢学兰，他曾是前川知事，现在是施瓦茨曼学院国际治理研究院院长、中国科学技术政策研究院与SDGs全球研究院联合主任，同时还是国务院参事，并担任中国新一代人工智能治理国家专家委员会主席及中国科学技术协会常务委员会成员。

邢学，今天很荣幸有您在场，话筒交给您。谢谢，我要特别感谢奎利亚总统，非常感谢您留在这么晚参加对话。我必须说，其实我是边听边做的这个PPT，昨天被大屏幕搞得很累，用PPT不太舒服，所以我本来决定不用它，但后来我看到了你们精彩的演讲，意识到有些图表还是展示出来更好，所以让我试试。

好的，我认为治理和安全的角色非常重要，令人惊叹的是它们正得到越来越多的认可。我们看到了最近的努力，包括英国安全峰会和韩国的峰会，以及欧盟安全法，这些都在过去的12个月内进行。所以我认为这是一个很好的迹象，表明我们都在关注这一点。

当然，这在很大程度上基于对人工智能风险的巨大认可。可能有一些非常具体的风险，也有对自主人工智能系统的担忧，这些系统可能真的会失控。因此，许多国家已经采取了各种措施来解决治理问题，有的国家更多地关注国内问题，有的则关注全球问题。我认为，Lingha在谈论中国的人工智能治理系统时做得非常好，所以我在这里更多的是关注全球问题。

全球范围内，我认为有一些主要挑战。在解决治理挑战方面，我认为第一个挑战是所谓的节奏问题，也就是技术发展得非常快，而政治和制度变革则慢得多，所以总是存在这种差距。我们如何解决这个问题？

我认为最近我觉得有另一个新挑战在逐渐出现，从我听到的一些讨论来看，这就是技术发展的方向问题。到目前为止，我认为大家都在说我们需要大量计算能力，需要提高系统的性能，然后通过缩放法则达到一些未来的方向。但我认为，越来越多的专家和科学家开始质疑这是否是唯一的方法，是否还有其他方式来实现人工智能系统的更适当和健康的发展。

第三个挑战是制度上的。我称之为制度复杂性问题，Tenno也谈到了这个问题，这涉及到一个部分重叠且无等级的机构共同治理一个特定问题。以前我研究过基因数据治理，那里有类似的问题，但在这里，我认为情况基本相同，即你有人工智能问题，很多机构和机制实际上都涉及到它的治理，但它们之间没有等级关系。有些是专业组织、基金会、立法机构等，每个都有自己的一套治理方式。这就是我们所处的情况，Tenno也谈到了这一点。所以如何协调所有这些机构，如何让公司能够弄清楚该遵循的方式，这是第三个挑战。

最后一个挑战，我非常高兴Tenno谈到了，就是地缘政治问题。这是房间里的大象，很多人不一定愿意谈论它。我研究了很多年的科学技术政策，观察了美中科学技术合作多年。自2017年以来，美中科学技术合作迅速恶化。所以我画了这个图表来清晰地展示这点。

在讨论中我听到了很多，关于合作的挑战，有些是由于中美之间的技术限制。现在我们看到的一个问题是，如果你在美国公司里工作，去中国时可能需要向机构汇报，或者回国后写报告，说明你在中国做了什么。

在这样的氛围中谈合作是非常困难的。最后，我想说的是，可能有一些方式可以解决这些挑战。我完全同意，也是一部分论文中的内容，呼吁增加对安全和治理的研究投入。也许三分之一太过雄心勃勃，但我们可以从10%开始，我认为这是我们一定要做的事情。另一个需要做的事情是进行一些联合国际研究，比如危机管理。应对风险时，你需要有应急计划，这些应急计划需要技术人员一起工作。所以有很多关于如何解决节奏问题的想法。

在过去几年里，我们一直在呼吁并试图与不同机构交谈，谈论所谓的敏捷治理，意思是政府不需要像欧盟AI法那样出台全面的法律，而是可以在看到问题迹象时迅速采取行动。这至少是政府可以做的一件事。

当然，还有不同的措施，Ling Ha已经做了很好的描述。第三，我们也应该考虑不仅仅依赖政府，行业自我监管也非常有用。我听到了美国核运营商的故事，他们有一个协会，实际上他们自我监管得非常严格，你有许多核反应堆，有些会有小故障，但它们都必须向这个社区报告，这样他们可以研究，看看可以学到什么，如何避免这种情况。我认为这种自我监管机制非常有用，我们也应该考虑如何让这个机制发挥作用。

最后，在国际层面上，看到联合国介入并发挥重要作用，成立了高级专家组，这是非常好的，我希望报告和建议能尽快出来。同时，我认为这问题非常复杂，可能不太可能有任何一个机构能够采取等级制和自上而下的方式来管理。因此，我认为多边的网络化系统可能会更有效。

我认为我们可能需要区分三类问题，也许未来可以有更多时间讨论。第一类问题主要是国内技术使用的监管问题，不同文化、法律环境、经济环境等差异很大，因此在AI使用的国内治理上肯定会有差异。第二类问题更多的是国际社会需要共同应对的，例如存在性风险，这些需要国际合作。第三类问题是国内风险可能会产生国际外部性，当然也可能有国际法规产生国内外部性。

最后，对于美中竞争，如何解决这个问题？至少我认为最小的要求是为我们的技术社区、专家群体提供一个安全空间，他们不必担心，他们可以在技术意义上自由合作。

感谢邢院长坦诚且简明扼要地概述了人工智能治理中的挑战和解决方案。接下来我们将进行一个炉边对话，主持人是康科迪亚AI的经理周杰森，他领导了康科迪亚的中国人工智能安全状况报告，并毕业于清华大学施瓦茨曼学者项目。现在我们欢迎我们的嘉宾。

我必须说，我非常自豪周杰森是施瓦茨曼学者项目的毕业生。谢谢邢院长，欢迎Tin

非常高兴能主持这个对话。让我们立即开始吧。今年五月，美国和中国首次举行了具有里程碑意义的双边人工智能对话，讨论中有两个摩擦点和一些明确的共识。双方进行了专业且建设性的讨论。让我们谈谈摩擦点。在中国方面，提到了对美国技术限制的反对意见，如邢院长刚才提到的一些。而在美国方面，则对中国的人工智能滥用提出了投诉。我的第一个问题是，我们如何克服这些地缘政治障碍进行对话，这是否可能？让我们先请Tino在线上回答。

谢谢杰森，很高兴再次见到你。邢院长，我非常喜欢你的讲话，我一直对施瓦茨曼学者项目印象深刻。我想说，我们的最佳保姆后来也成了施瓦茨曼学者，她非常优秀。所以我继续对这个项目印象深刻。

我认为这个问题非常紧迫，因为我们必须坦诚，美国和中国在很多问题上会继续存在分歧。但我认为我们可以从上次对话中学到一些东西，并在未来的合作和人工智能安全方面进行调整。

第一个观察是，两国派出了不同的团队参与讨论。在中国方面，主要是中美关系的专家，而在美国方面，更多是科学和技术问题的团队。我认为第一点是，当我们面临复杂的问题时，缺乏协调可能会导致对讨论的期望不同，并且可能在适当的团队选择上有所分歧。

最终，我认为更大的问题是，我们需要在一些影响两国的共同挑战上努力，包括哪些技术可以共享，哪些技术被视为更敏感和与国家安全相关的。但与此同时，我们需要创建一个空间，让技术人员能够安全地交流和比较意见，看到机会时能够迅速行动。仅举一个具体例子，即使在芯片和出口限制方面存在一些分歧，两国在共享安全和评估的最佳实践方面有着共同的利益。这是两国和世界其他地方都需要的，中国和美国可以单独或共同领导，并帮助其他许多国家增强进步能力。因此，简单的对话和信息共享，以及邢院长所提到的联合研究，将在这方面促进进步，即使政治和政策层面的讨论需要继续进行一些分歧问题。

我完全同意Tino的评论。我认为确实如此，看到对话实际发生是非常棒的事情。并且我们看到团队的组成有一些不对称，但这也反映了当前的中美关系。正如Tino提到的，如果有更频繁且非常友好的沟通，这样的事情可能不会发生。这可能是因为没有足够的事前沟通，无法了解具体要解决的问题以及应该参加的人。但我认为至少从双方的报告来看，可以开始认识到大家关心的问题。

在中国，治理和发展是平衡的，风险治理是一个重要部分，但没有发展是最大的风险，这不仅对中美也是对整个社会而言。如果你在美国和中国有一个良好的人工智能系统和应用，而世界其他地方被落下，这可能是我们将面临的最大风险。

谢谢两位。我认为很清楚，在政府层面上对话既有乐观也有悲观的情绪，但看起来专家之间的对话可能更乐观。也许我可以问问你们两位，从与外国专家的讨论中，你或改变了什么想法？

我学到了很多关于人工智能安全和治理问题的知识。所谓的存在性风险，以前我们认为是人工智能系统可能失控，但我认为人们提升到威胁人类生存的层面，这是通过互联网交流了解到的。

Tino有什么想法？

我发现我们幸运地进行的非官方渠道对话非常有启发性。当你看中美参与者的安全优先事项列表时，有些优先事项可能非常不同，比如虚假信息或劳动力市场影响等问题。但当你问参与者第二轮问题时，例如在安全测试方面，合作的机会很多。共同研究和测试评估对两国都非常重要，我认为中国和美国可以在这方面引领并帮助其他国家。

我认为我们也需要平衡联合国的角色，尽管联合国在全球发挥着重要作用，但我们可以通过与外部团体合作来加强其能力和过程，创造一个关系网络，使联合国能够发挥最建设性的作用。我认为这些对话改变了我对联合国角色的看法。

谢谢你们，我认为我们可以从彼此中学到很多，特别是关于人工智能安全的优先事项，以及如何测试和评估这些问题。很高兴今天有这样的对话和交流，接下来一个问题，你们想向外国专家传达的最重要的信息是什么？或者说，你们国家在人工智能治理方面的一个误解是什么？Tino先来。

谢谢。我有两个信息，一个是关于可能的误解，另一个是关于未来的思考。首先，要理解各国在推进其技术能力方面付出了大量时间和精力，但在政策制定上并不总是统一的。美国和许多国家一样，不是一个统一的战略体系，而是一个动态的、多样化的过程，这可以是一个优势。我的第二个信息是，不要让完美成为良好的敌人，在解决双边问题时，我们可以进行技术合作和人工智能安全对话，这是非常重要的，因为所有好的进展都将在国内发生，而这些将留下需要国际合作解决的关键问题。

邢院长呢？

首先，我想传达的信息是，人工智能安全是全球公共产品，一个国家不安全，全球就不安全。其次，中国愿意与世界上每个国家合作，中国不想被排除在外，并将尽力邀请各国参加合作。

谢谢你们，我认为这次讨论突出了对话和专家交流的重要性，并希望它们能够继续并带来有益的结果。谢谢你们。

接下来我们将听取宋教授的发言，宋教授是中国科学院人工智能伦理与治理研究中心主任，长期从事国际人工智能治理。他是联合国高级顾问组成员以及多个国际治理机构的活跃参与者。宋教授，话筒交给您。

感谢邀请，我是一名科学研究者，所以我将重点介绍一些前沿研究。我想带来一个不同的观点，即我们需要明确人工智能的安全红线，但也需要在未来与通用人工智能和谐共处。在当前，安全问题不仅仅是科学研究，它是一个系统，需要所有人共同努力。这就是为什么我们将所有人聚集在一起进行研究、应用、评估、政策制定和安全评估。

例如，中国正在建立一个人工智能安全网络，汇集了中国科学院、北京大学、清华大学、北京人工智能研究院、上海人工智能实验室等前沿研究机构，以及阿里巴巴、百度、商汤等工业实践的企业。政策设计方面，政府与多个部委紧密合作，确保政策的灵活性和国际合作的灵活性。

我认为在当前的人工智能机制中，存在一些问题，有些国家更多地将其作为政治或政策工具，而不是前沿研究机构，这可能导致前沿研究机会的流失。因此，我们需要一个系统，所有人共同努力。这是一个新的视角，希望对大家有所帮助。

我们还需要从人类与人工智能和谐共处的角度思考，不仅关注预防性措施，还需要积极思考。当前的人工智能是一个完全连接的神经网络，而大脑则是选择性连接的。因此，未来的人工智能需要通过自我进化来优化其架构，达到最佳性能。我们需要为这种自我进化的人工智能做好准备，以应对可能的挑战。

对于人工智能的安全红线，我认为有必要重新思考，不仅是技术层面，还有人类层面的红线。例如，脑机接口控制多个无人机作为武器，人类放弃了选择权，可能带来灾难性后果。因此，安全红线不仅包括人工智能红线，还需要包括人类红线。

在积极方面，我们需要推动伦理人工智能向道德人工智能发展，使其具备良知和道德直觉，避免负面影响。我们在大脑启发的人工智能模型上已经取得了一些进展，机器人能够通过镜子自我识别，并推断其他机器人在想什么，实现认知共情和情感共情，避免负面影响。

最后，我想谈谈人类与人工智能和谐共处的未来。我们将拥有人工智能、数字人、人工生命等，不仅是人类的决策，而是共生社会的决策。价值观的对齐非常重要，但人类的价值观也需要适应变化。未来我们不仅需要有益的人工智能，也需要有益的人类，共同建设一个共生的生态系统。

感谢大家的关注，谢谢。

感谢宋教授的发言，展示了科学、政策和哲学视角，推动红线方法在人工智能治理中的应用。接下来我们将听取Irene Salaiman女士的发言，她是Hugging Face的全球政策主管，从事安全研究和领导公共政策，曾在OpenAI工作。Irene，非常高兴能听到您的发言，话筒交给您。

感谢Kuan Yee，我很高兴能谈论开放性在人工智能发展和安全中的作用。首先，我想定义开放性这个词，因为在很多讨论中，这个词被赋予了不同的含义。开放性不仅是开源软件的概念，还有很多方面需要考虑。

有些人认为开放性与模型权重有关，特别是权重的可用性和分布。另一个重要的方面是透明度，斯坦福大学建立了透明度指数，展示了不同人对透明度的不同理解。

我更倾向于超越模型，考虑整个人工智能系统的各个组成部分，如数据集、微调数据集、反馈数据集等。Mozilla在今年二月份举行了一次会议，探讨开放性的各个维度，这有助于我们更好地思考系统的发布、威胁模型以及研究社区的受益方式。

报告中列出了开放性的一些动机，如知识共享、更多视角的参与等。开放科学生态系统是人工智能领域的重要基础，如2017年的pytorch和许多工具和库。

我想强调两个方面：人工智能作为一个科学学科和社区贡献的重要性。科学的 reproducibility 危机，涉及到研究结果的验证和信任问题。开放性能够提供更好的研究访问，使更多人参与和验证研究结果。

多语言性也是国际合作的重要部分，例如在Hugging Face的 Big Science 项目中，研究了不同语言的偏见问题，不同语言的特点会带来不同的安全挑战。

我想探讨开放治理和发布后的风险实际化问题。开放性与发布有关，但风险的实际化还与内容、能力等因素有关。我们需要考虑实际使用中的威胁，例如非自愿内容、虚假信息等问题。

在中国的背景下，我们很荣幸能托管一些中国公司的开放模型，Quen2表现很好。政府和行业的开放性举措，如新加坡的IMDA、法国政府的Project Moonshot等，展示了开放性在创新和国际合作中的重要性。

我希望这次演讲能为大家提供信息，并期待继续讨论。

感谢Irene的发言，展示了开放性在人工智能发展和安全中的复杂性和重要性。接下来我们将听取罗伯特·特拉格教授的发言，他是牛津大学国际治理项目的共同主任，专注于新兴技术的国际治理。罗伯特教授，请继续。

非常高兴能在这里与大家分享。我认为我们在这里讨论的挑战是全球性的，我们需要共同努力解决这些问题。我将探讨一个可能的全球人工智能治理生态系统。

首先，我想谈谈为什么我们需要国际合作。我们需要全球标准，这些标准需要具有合法性，确保每个人都有机会参与制定。其次，我们需要在国际层面上设定激励措施，确保各国都能采纳和遵循这些标准。第三，我们需要合作来激活这一治理体系。

在安全标准方面，我们需要确保这些标准是基于研究的，确保世界各地的声音都能参与进来。这需要能力建设，确保全球各地都能参与这些标准的制定。

我们还需要国际化的领域，不同的文化空间可以选择如何管理自己，但在安全问题上，我们可能需要国际标准。这需要明确风险并在各个国家之间进行总结合作。

在国际层面上，我们可以考虑建立一个国际报告体系，确保各国合作，确保有人违反规定时可以被发现。这需要长期的合作和报告制度的建设。

总的来说，我们需要创建一个合法的国际化过程，确保广泛的全球声音参与，并设立激励措施，确保合作和治理的实施。

感谢大家，非常高兴能在这里分享。

感谢罗伯特教授的发言。


---

您非常清楚地描述了在AI安全方面的国际标准和报告制度的明确步骤。我期待稍后在课堂讨论中谈论这一点。接下来我们有Duncan Kaspegs。Kaspegs先生是国际AI风险设计的总裁，在国际管理创新设计中专注于发展新的管理方案，以解决目前和未来的AI相关的国际问题。Kaspegs先生有超过25年的国际和国家公共政策工作经验。早期他在OECD担任设计总裁，之前他在加拿大政府的多个职位上工作过。Duncan，很高兴能与您一起来到这里。请坐。谢谢。今天我们来谈论国际AI风险设计的规划，加速国际协调，以确保安全和兼容的人工智能。我们开始。我们看到的是风险作为一个详细的细节，我认为非常重要。正如很多人今天提出的，我们要明白我们需要发展管理方案的机构。今天的AI与未来的AI可能会有很大的不同，我们需要为此做好准备。尽管今天AI已经受到了高度关注，但有理由认为这种变化和发展可能会比我们预期的更快发生。在这种不确定的情况下，作为公共政策人员，我们必须准备应对最困难的情况，包括这种快速发展的可能性。我们的目标是通过国际合作来解决全球性挑战。现在很多互联网管理问题可以在国际层面提出，这是我们能够尝试和学习的方式，但有些问题是真正的全球性挑战，需要全球合作。所以我们在报告中集中讨论了三个问题，我会逐一解释。第一个是实现和分享全球性挑战。现在公司和企业会发展大量的互联网合作，没有政府的参与，但有些元素需要政府的参与，特别是国际互联网合作。第二个全球性挑战是减弱全球性互联网风险。这些风险可能会对整个社会造成伤害，跨越国际边界。这些问题不能仅靠国家自身解决，为了保护自己的国家，必须与他人合作，建立机构以保护全球安全。这是我们看到的许多风险图表。很多AI风险在国际层面更具影响力，从个人到全球都需要适当的应对。我们在这里专注于无法控制的超级智能和武器用途或错误的风险。这两个风险最为严重，尽管看起来几乎无法想象，但很多专家今天已经讨论过，我们必须为此做好准备。第三个全球性挑战是做出有效的决策，影响未来无法控制的智能对全人类的影响。这意味着我们需要反思和共同决策，不能让少数企业决定这些重大问题。如果这些是我们面临的挑战，我们需要解决方案。我们已经看到许多在全球和国际层面的努力，但这些努力似乎不足以应对未来几年内可能出现的AI系统。这需要我们制定预期方案，准备应对最困难的情况。我们认为需要新的框架和机构来实现国际协调，尤其是在最关键的问题上。我们需要谨慎选择代表共同讨论，但在必要时必须及时行动。我们正在讨论全球AI挑战框架协议，这是一个适当的方案，能够迅速实现全球协调，设定高层次目标和理念。这个框架提出高层次目标，并设定具体计划解决最困难的问题。我们现在要专注于公共安全计划，针对AI带来的全球公共安全和安全风险。我们关注武器化和失控风险，因为它们可能非常严重且时间紧迫。如果我们不知道如何应对这些风险，那么它们就是紧急的。我们必须准备面对这些风险，并与更广泛的规划合作。这个计划的目标是解决AI的危险，确保人类安全。计划的中心是一个分级风险方法，根据不同的AI系统理解不同的风险。我们希望尽量减少监管，确保工作继续进行。第一层是测试AI系统的全球危险，这些系统在国际上可以控制，因为它们没有对其他国家构成大规模风险。第二层是管理AI系统的风险，它们足够重要，不能不加管理。我们必须确保每个国家遵守标准。第三层是系统太危险，让公司或政府自行执行，需要在共同空间内联合发展和测试。第四层是非常不可靠的AI系统，我们必须确保在任何地方都不建立这些系统，直到确认安全。许多机构可能需要支持这些系统，设立标准，监控保护。我们需要考虑设施，虽然复杂，但这是必要的。国际合作和预见精神是成功的关键。总结来说，我们必须现在准备好应对非常强大的AI的可能性，全球合作是最重要的。这需要预见精神，挑战巨大，但我们都需要这样。非常感谢。如果您有兴趣，这里是我们的讨论文件，用于引起讨论，欢迎您的评论。谢谢大家。

非常感谢对于我个人有意向感谢以及这次的竞选与优价的同事我抱有感谢。刚才的杨维也感谢嘉宾和云莱杨维来到舞台上让我介绍一下他。西衡先生是国际和平的传媒博士，他的研究涉及中国AI生态系统和全球科技趋势。他的作品发表于《外交事务》《博士》等出版物。我们很高兴能与马兹先生以及其他参与者一起参与这一活动。请坐下，欢迎大家来到今天的最后一堂课。

马兹先生，这是你第一次上台。让我先开始，我们听了很多参与者关于国际协调在AI安全上的重要性。但在您的外交政策文章与Tino关于AI是赢得AI竞争的方面，您谈论了AI发展的竞争方面。我认为在华盛顿有个常见的问题就是谁在赢得中国AI的竞争。在这种与中国的国际竞争环境中，Tino和丁子耀也坚定地在他们的言论中和跨界谈论中。您能解释一下为什么国际协调在AI安全上是重要的以及我们如何平衡这些两者之间的协调和竞争的关系吗？

非常好，非常感谢。感谢康科迪亚让我参与和所有人参与。我可能会从这方面来看，我认为今天有相反的观点关于美国和中国。当我们进入非常强大的AI系统时，是否有可能美国和中国在任何程度上协调，或者国际竞争太深了？那些竞争的运动太强了吗？我认为我们的国际AI系统可能会是未来的国家主要力量，他们可能对，也可能错的。但当你看到这是未来的国家主要力量，而你看到其他国家是你的竞争者，在每个方面上协调是非常困难的。我认为，让我感到兴奋的事情或我看出的方向是，我想的是相同的安全，而不是一种合作安全的东西。

我认为我们在谈论的很多事项是在某一天，领导会坐在一个非常高层的位置，他们将举行协议，然后我们将批准这个协议在两个系统之间。我认为，我并不在意这一点。我更期望的是，我们可以有一个更高层的方式来建立安全系统。在中国，会有政策人员，会有技术人员，会有研究人员，在中国为自己的原因而推动AI安全。他们将与美国和国际人员谈论最佳的实施。但是我们将在我们自己的环境中同时做这些事情。我们不会在每个步骤之中坚持同意。我们将通过谈论、政策谈论、技术交流、联合研究来建立安全实施。或许在几十年后，我们将建立这种安全实施，我们将建立一个高层的协议，因为两个国家已经很接近这个目标。两个国家已经很投资在AI安全为自己的原因。然后，我们将建立一个可能的协议。但是，我认为我们必须在前段时间下设立基础。我认为，这个基础是通过二维研究、科学交流。谢谢你，Matt。

我想继续谈论这个竞争的主题。接下来，我们将谈论Irene。Irene，在你的演讲中，你谈论了科学、智能、智能发展和智能安全的重要角色。最近，我们看到企业，例如智能智能，在某些国家的API接触中，限制了智能协议与联合协议的关系。你如何看待国际智能模式的共同化，并且智能协议与智能协议的关系，在这种联合协议的压力上？

谢谢你，Kuan-Yi。我认为，在设计上的模式中，我们可以找到共同化的位置。不所有安全方面，都会与不同地区不同，特别是在美国和中国之间。我希望，特别是在美国和中国之间。我认为，这里有很多关系。老师，我希望，特别是在美国和中国之间。这里有很多关系。我们必须找到一种方法，能够一起建立评测，看看表现如何在多个不同语言中，如何相互保护。还有商业方面，我专门从研究的角度来说，但我不能与其他公司的商业决定，这可能会不同，根据法律和法律。谢谢Irene，这不是一个简单的问题，你做了一个很棒的工作。

现在，我们来调节点，谈谈AI管制的国际机构。宋教授，作为一名AI高级领导体的一员，您有独特的意见关于在宇宙中的职业角色。您能否分享您的看法对于宇宙的职位，在形成国际AI管制，尤其是，您如何看到我们如何平衡宇宙国际机构的承诺，使得美国提供的机构，以及它们经常遇到的挑战，以及它的程度和过程的速度。我们如何确保国际机构，基本上，是复杂的和有机的，以保持我们今天谈论的许多我们的谈论者的快速AI发展的速度？

谢谢。我觉得这是我们必须面对的重要问题。我想，在AI的国际机构领导体之前，已经发生了很多地区，例如OECD，EU，以及AI的国际协调，这些互联网只有40个国家联合，而其余的160个国家被世界忘记了。所以，我认为，国际机构的联合作用，并不是为了创建国际AI管理，而是为了创建国际AI管理的国际网络。而是为了让所有的国际网络，在国际级的行动中，尝试自己的行动，并找出错误的地方，以连接所有的连接。

所以，这是我认为的原因，我认为，在几天前，美国国际协议审判，它说，美国国际协议审判，应该执行国际AI管理的核心任务。我认为，核心任务是，实际上，执行国际网络的责任，并且让所有人都能够使用，而不是让所有人都不能使用。另外，但是，我们要看到的是，这些原始的网络，他们在今年和接下来，他们想尝试取消美国国际网络的任务。他们想取消美国国际网络的任务。我不是说，它应该是美国国际网络。我只是说，当我们看到所有其他网络的观察，当我们看到所有其他网络的观察，他们就没有任何的核心任务，能够在国际上有信心。所以，我认为，美国国际网络是最有信心的平台，能够将所有人都联合起来。我认为，中国已经参与了一些地区网络，比如说，AI安全会议，以及军事AI的重新设置。我认为，中国会感谢，能够参与。

但另一方面，我们看到所有的网络的限制，他们有自己的重点，他们有限制，他们没有权力，能将所有人都联合起来。这不是他们的任务。那么，在这种情况下，我认为，不遗留国家，你必须要做，不遗留国家的方式。所以，使用美国网络，你将所有地区网络都联合起来，联合起来，能让他们联合起来，而在另一边，那些地区网络，请不要阻止美国，因为美国正在联合，并没有阻止你们。所以，我认为最健康的方法是，联合起来，建立一个处理委员会，联合联合国家的所有地区网络，联合起来，联合起来，联合起来，联合起来，联合起来，联合起来，联合起来。所以，可能他们做得不太好的工作，在他们面前有很多的问题，而在另一边，我们没有其他选择比他们更加有信心。所以，他们目前并不是最好的，请帮助他们成为最好的，请使用这个平台解决问题。如果联合国总裁不成功在一些问题上，联合国总裁委员会会使用联合国总裁委员会提供的决定。

请问联合国总裁委员会请他们做些什么。例如现在，联合国总裁委员会的决定，中国主席的决定与140个国家合作，请联合国总裁委员会提供的决定，提供的决定，提供的决定。现在的低及中资金家，操作AI，找对这些问题的解决方法，并提供的决定。以至于明年。我认为这是总裁的所有的要求，用过在这项议论中，用过在这项议论中。这是使人们使用联合国总裁委员会作为最大的成功的方法，最有益处的平台去做所有国家需要的工作。我可以提出一点吗，桓义？我要强调一下，即使没有支付器，联邦联盟作为一个通讯网络，也能帮助我们与其他人相熟。

我曾经与詹教授见面，在早期联邦联盟工作前，这是我第一次与一个在这里工作的人合作。我觉得这非常重要。确实，谢教授郑教授和艾琳教授，我觉得教授郑教授提到的网络方式很适合罗伯特的概念。是你提到的国际社会环境。在你的演讲中罗伯特，我想听听你现在的看法，建立一下教授郑教授的看法。在你的演讲中，你提到国际标准和报告制度对AI安全，你能否解释一下这与这个有何关联？比如你提到的国际社会环境以及联邦的作用在这些方面？

是的，绝对的。好的，非常好，是的，这更好。谢谢你的问题，我会很高兴地谈论这方面。首先，我想说我对这方面很深入的思维，所以我非常关注这方面的研究。我的看法是，国会的角色是有重要的角色。我们必须找出这些角色在这个环境中，正如你解释过的，就是一个经济系统。所以，我认为，国会已经在做一些重要的事。例如，在发展方面。我认为在我提到的讨论中，我提到的ITU和UNESCO以及其他国会的领域，他们已经在做一些标准设定的事。他们知道他们必须建立更多研究能力，继续做我们必须做的标准设定。而且他们在发展的角色这非常重要。我觉得我们必须例如，我们必须做更多的事情在发展方面。

我觉得UNESCO是一个很自然的地方。你可能会想到一个负责操作的操作许多社区的主要障碍。在进步AI中参与的社区其实是在进步AI中在他们自己操作的领域中，他们自己的文化空间中。我们可以想到的方法是，很多世界大部分的空间有很高的手机侵入。所以我们可以用这个方法来进行数字化，但我们必须做的方法不需要隐瞒隐私，不需要符合这些社区的标准。所以我们必须做得非常小心。但是这就是我们可以看到的，我们可以看到的世界的大部分的努力在进步AI中以提高发展成果。

其他一些事情，我认为有所提及，例如AI的IPCC是一件有意义的事情。在UNESCO上一些标准设定的领域，我相信我们可以找到其他领域，看起来很自然，与UNESCO的领域合作的领域。我认为有所提及，例如会议，这非常重要的。所以我们可以找到这些领域。我觉得这应该是我们在UNESCO的领域的角色的主要重点。

我可以再加一点关于不同的UN系统吗？现在你看到的就像是Robert所说的，不同的UN系统已经做了一些有关AI的事情。独立的，我们有一个我们在几个月前在日内瓦发起的一个无线会议，我们参观了几个不同的UN系统，在日内瓦，是由联合国组织的组织。我们看到的是这些不同的UN系统，主要是他们当然看到树木，但有些时候他们错过了他们看到树木但错过了树木也不通过。所以他们还是把树木形成树木。在日内瓦省的根本就是一个无线的不通的树木。所以就像刚刚所说的，我们看到的在日内瓦发起了一个无线的无线 appreciate，就是inos系统的独立的，都是形成树木。只要有多个树木就可以得到这种量度，可以让你的方式，可以让你让你对着独立的树木，有什么理由你们可以完成这个发展就有什么理由的。就是有很多更多，所以美国联合联合公司必须在现在相比起现在，连自己都要组织更好的组织，所以会是联合联合公司的责任，帮助美国联合公司有更好的组织，让每个人都能够在美国联合公司的系统中辅助，并不仅让每个人辅助，还能够辅助每个人，并解决问题。

也有一个例子，美国联合公司在美国联合公司的系统中辅助，但美国联合公司和其他国际辅助组织，例如IEEE，他们并没有足够有效的组织组织。因此，美国联合公司的联合联合公司，需要联合公司的会员联合联合联合，以便接触那些陆统的担任联合公司，作为帮助来举办多类联合公司联合 mini Stroma一些设施。我们可以通过相联合公司通过其他组织那些团体联合公司做一些联合公司，并团一些相联合公司生产这一切特别的运势业。不仅讨论联合公司的某些组织，在联邦制度中的标准组织之中。谢谢教授Traeger和Zong教授。我们时间很短所以我想先结束我们的讨论。请问最后一个问题，您所有人都表达了国际智慧管理的成功和胜利的想法。我希望听听每位主持的意见，您认为国际社会应该做的一件事，在未来6-12个月内达成这个目标？例如在未来的联邦议会会议，或是法兰斯 AI 活动会议，或其他会议。我们想实现这个讨论。所以我只想提供一个建议。可能Duncan，因为我们还没有听到您的意见。我们可以跟您开始。

好的，谢谢。我觉得我的一个讯息是要帮助准备未来。我们需要帮助我们的社会，无论是在联邦议会或是在支持网络的网络。我们需要想想现在，我们需要帮助我们的社会能够成功通过我们见过的最大的改变，我们见过的最强大的技术。我们需要现在思考，现在设计，现在设定我们需要的各种社会。我们需要帮助社会在未来的发展中。在大多数的事件中，我觉得我们面临的其他非常严重的不确定度。我们必须要着重预期，我们需要建立的话题，它们可能需要的 among certain scenarios。所以那些个案会期间预备讨论了很多关于如何将科学家将共同的理解建立相同的理解，关于 与技术有相关的危险 和挑战。

这件事十分重要。就是为了将社会科学家和各种不同的观点联络在一起，并为我们提供所谓的管理机构和机构的机制。我们需要这些机构帮助人类在这段时间内进行进展，让我们可以在500年内回顾，虽然我们需要在这些问题上合作，但我们做得到。这些科技使我们能够合作，而且我们成功地做到了。

谢谢，Duncan。Irene，我们可以让你下一位。你可能也想探索一下研究中心和发展企业的角色，你刚才提到的。绝对的，这就是我刚才所说的。我的极为严肃的反应是，我们可以休息一下。我很累，我知道我没有独处。而非常严肃的反应，就是Duncan所说的，关于正确的专业。一个很重要的原因是我工作在Hugging Face，这是我第一次看到一种我的传统语言Bangla在一个数据中研究。确保我们包括不同的群组，不同的技术研究员，并且解决一些不明顯的技術犯罪，但非常基於歷史、文化以及世界不同地區的問題。謝謝Irene。

Matt，我们可以让你下一位在这个问题上。好的，好，我第一件想要提问的事，在接下来的6至12个月内，我认为是美国和中国会进行某种类型的合作评论在具体的大规模的危险上。这将会非常复杂。我们要想办法让两边都觉得安全。我觉得在政府层面上这可能会太难了。最好是在AI安全基础上或AI安全基础上做这件事。这可能在这段时间太艰难了。但如果不在2阶段上做这件事，有中国最好的技术评测员，以及美国最好的技术评测员，在美国的安全评测员中一起谈谈他们使用的方法，他们所看到的问题，以及他们将要做的方法。谢谢Matt。当然，我们也提到在这个讨论中AI安全测验是一件非常重要的项目，非常重要的项目，以及有可能的国际联繫项目。可能Robert我们可以回答你这个问题，你希望未来的6至12个月内你希望看到的事物发生什么呢？

我认为我刚才讲了一些特别的事情，所以我可能可以以这个机会反映之前提到的一件事，以及说一件事。就是我认为在这个扩大的环境下，我们有很多例子，当他们认为要做的事情是重要的时期。虽然你不会预测它会发生，但你仍然需要尝试让它发生，找出它能发生的机会。例如在这方面的科技管理，特别是AI管理。美国民主党和共和党在这段时间之内并不知所措，但他们仍然在议会议会中尤其是进行进展AI管理。这是一个例子。《非核心合作协议》是两国共和党结合的时期，因为美国和苏联当时的关系不太好。但无论如何两国共和党结合，他们仍然做得到，因为他们认为它是他们的共同利益，他们认为它是重要的。所以你不会预测这些事情会发生，但你仍然需要找出它们能发生的机会。

谢谢Robert。我们很高兴能够结束这堂会。就请教授郑教授最后一个问题。接下来的12个月，我首先要告诉你什么已经发生了。在这8个月前，我已经不知道雷明河在这8个月前发生了什么事。所以我去过新加坡的亚洲技术会，但不是雷明邀请我来。所以我责怪雷明，然后我们结合他来自政府。所以我是从一个研究组织来的，所以我可能在正常情况下也不能跟雷明讨论。然后在这8个月之内我们成为了一个团队，雷明对我非常好。他在新加坡提供了非常好的亚洲食物，他也告诉我新加坡的练习，但不仅如此，他和所有的总统和副总统在这个大型的亚洲技术公司都结合了，还有很多曾经的政策人员都结合了。所以我看到了所有人的价值都在结合。你们不明白对方，你们说我们完全不同，但当你们结合你会觉得OK 不太大分别。然后你们有安全问题，你们有发展需要。我们解决问题吧。因为相比于去年，现场的风险已经有10次。相比于去年，全世界的风险你会看到风险重新发生在不同的国家。然后你会看到联邦制度的限制。如果我有钱还有人，我会把所有的钱都给联邦，创造一个国际机构。但是我没有钱。所以在下个12个月之内，最重要的事情就是起码要创造像是欧洲的Train，有欧洲的AI服务。现在我们应该有欧洲AI服务，最小的Train将所有的欧洲机构全体组成，并于这么好的方式来操作他们，还有并且讨论这些地区网络，例如OECD、IEEE和ACM。所以我认为我们必须移到这个范围，因为在AI中的联邦谍报组中，我认为我们看到了这件事的需要。没有这些需要，我们就没有机会把这个国家遗弃。谢谢宗教授，把这个国家遗弃，这是一种非常吸引的想法。

