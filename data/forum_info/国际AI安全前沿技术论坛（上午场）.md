# 国际AI安全前沿技术论坛（上午场）

![](https://static.worldaic.com.cn/IMAGE2024/2024-07-02/60f08f9f426b4e0ea587d7c2d728bb19.jpg)

随着大模型技术的深入发展，其安全性和可信性已经成为行业关注和研究的热点。本次国际AI安全前沿技术论坛聚焦大模型安全领域的关键技术问题，包括生成内容安全、价值对齐、数据安全等多个方面。论坛邀请到了中美两国的顶尖专家，共同深入探讨如何构建具有社会责任感的通用人工智能，探索国际合作的新途径，确保AI技术的健康发展和广泛应用。


### 议程安排

| 时间      | 主题                                     |
|-----------|----------------------------------------|
| 09:00 - 09:10 | 开幕致辞                                 |
| 09:10 - 09:40 | 主旨演讲：构建人工智能安全-挑战与未来方向            |
| 09:40 - 10:10 | 圆桌会议：在AGI时代如何保障AI安全并促进国际合作     |
| 10:10 - 10:30 | 主旨演讲：规避危险-确保AI全生命周期的安全            |
| 10:30 - 11:00 | 主旨演讲：白泽指数-语言学驱动的大模型安全合规监测技术与实践 |
| 11:00 - 11:30 | 主旨演讲                                 |
| 11:30 - 12:00 | 主旨演讲：大语言模型可被对齐吗？                   |

### 嘉宾介绍

- **乔宇**：上海人工智能实验室主任助理、教授
- **邵婧**：上海人工智能实验室大模型安全团队技术负责人
- **Dawn Song**：加州大学伯克利分校教授
- **Zico Kolter**：卡内基梅隆大学教授
- **常永波**：中国信通院华东分院人工智能事业部主任
- **陈思衡**：上海交通大学副教授
- **杨珉**：复旦大学计算机科学技术学院教授
- **杨耀东**：北京大学人工智能研究院AI安全与治理中心执行主任
---
今天，我们齐聚一堂，参加由上海人工智能安全中心和 CIS 人工智能安全中心主办的国际 AI 前沿技术大会。作为主办方代表，我谨向各位致以最热烈的欢迎和最诚挚的感谢。我们深知，我们正处在一个由 AI 驱动的时代，这既给我们带来了动力，也带来了挑战。特别是大规模技术，正在以前所未有的速度和规模发展，对我们的日常生活产生了深远影响。作为 AI 领域的技术工作者和大规模技术领域的安全研究员，我相信我们都感受到了世界的巨大变化。我们可以看到，随着大规模技术的发展，它变得越来越普及。超大规模技术的增加，使得人工智能具备了前所未有的能力。这意味着，未来可能会出现我们在训练阶段无法预见的能力。这对于传统安全领域是一个巨大的挑战，因为我们过去认为人工智能只是一种工具。而现在，它的能力在训练阶段可能无法预测。因此，人工智能在基础层面上既是一个基本需求，也可能带来负面影响。普通人工智能正逐渐发展成为我们未来社会基础设施的一部分。在这种背景下，人工智能的安全和治理成为我们关注的重点。今天，我们也有幸邀请到了来自不同国家的 AI 领域顶尖学者和行业领袖，为我们带来深入且激动人心的演讲和讨论。在本次人工智能大会上，人工智能的安全和治理问题被提升到了前所未有的高度。大会的主题是“AI for Good, for Good for All”。在开幕式上，李强总理提出要促进人工智能的发展，维护人工智能的安全，建立人工智能治理体系。在这三点中，有两点与安全和治理相关，这充分体现了会议对这些问题的重视。我认为这也是对全球范围内日益增长的 AI 安全和治理需求的回应。在开幕式上，还提出了《上海人权宣言》。这一宣言不仅强调了 AI 在跨文化背景下的适应能力，还提升了 AI 战略发展的水平，尤其是安全发展的水平。我们知道，AI 的安全是全人类共同面临的问题。宣言还呼吁在全球不同文化和社会背景下，确保 AI 技术的适应性和公平性。我记得当时的报告中引用了一些数据。例如，虽然大模型（big model）技术非常普及，很多人都在使用，但实际上，世界上有一半的人口无法接触到这种 AI 技术。许多发展中国家无法充分利用这项技术。因此，我们需要建立一个更加包容的 AI 生态系统。我们也是本次会议的主要单位。上海 AI 实验室一直致力于开发最通用的 AI 技术。我们采用更高效的方法来构建大模型，并将 AI 作为我们的主要责任和使命。也就是说，我们的实验室从一开始就以安全为主要方向。今天，我们的行业主持人赵京波先生实际上是我们 AI 方向的重要负责人。在这方面，我们一直致力于数据库建设，并关注 AI 大模型的安全性。我们做了很多与此相关的计算工作。实际上，我们还在网络和空气协会负责另一项工作。今天，肖先生也在这里。我们有一个与安全相关的工作组。这个工作组包括国内主要的大模型和 AI 研发机构，以及主要企业，如清华大学、复旦大学、交通大学，以及华为、腾讯、阿里巴巴等公司。这些公司都参与其中。我们在这个协会中制定了许多工作目标，目的是通过商业控制手段为大模型产业提供良好的基础。一方面，我们提供了良好的基础；另一方面，我们也考虑了很多问题。在基础建设过程中，我们需要很好地引导国家。这种定期的指导需要政策框架，但政策框架需要进一步细化和发展。我们进行了许多业务访谈，并应继续推进这一点。实验室也意识到，大模型产业正在不断发展。我们还注意到，AI 安全管理是一个国际化的问题，我们非常重视这一领域的国际合作与交流。今天的会议也是促进国际合作与交流的重要一步。今天的论坛将主要讨论几个主题。首先是 AI 安全领域的现状和问题。大家知道，随着 AI 的广泛应用，其风险和能力是相关的。一方面，随着 AI 能力的提升，风险也在增加；另一方面，AI 能力的提升也为我们应对风险提供了新技术手段。大模型产业非常有潜力，市场也为我们应对各种安全问题提供了新方法。其次，我们将讨论 AI 的安全性，这是学术界关注的一个重要点。未来，我们知道 AI 的发展最终会超越人类智能。当它超越人类智能时，我们需要一个系统，这个系统不仅包括技术，还包括社会系统和治理系统，以确保未来不会失控。这是一个非常重要的方面。在这一方面，我们也看到了一个现象。在生产、工业界，甚至科学界，我们在提升 AI 能力方面的投资远远超过了在安全方面的投资。大家熟悉 OpenAI 的故事，实际上，除了 OpenAI 的故事，我们也知道，如果将提升 AI 能力的计算能力与投资安全的计算能力进行比较，比例可能是 10 比 1，甚至更高。因此，我希望社会各界，包括科学界、学术界和工业界能更多关注这一点，以便我们在 AI 发展方面做出更好的投资。第三个方面是 AI 领域的国际合作。我们正在探索如何通过国际合作加强 AI 层面的沟通与合作，因为这是我们面临的一个全球性问题和技术挑战。我们将共同应对这一全球性问题。我认为，通过今天的交流和讨论，我们希望在 AI 领域，特别是今天参会的各位，能够理解和合作，共同促进 AI 技术的可持续健康发展，为人类福利做出贡献。最后，祝愿这次会议圆满成功。我也希望与会嘉宾能从这场技术盛宴中获得宝贵的启示。

### 迈向构建安全AI的挑战与未来方向

接下来，让我们欢迎加州大学伯克利分校电气工程与计算机科学系的教授 Dan Song 发表主题演讲《迈向构建安全AI的挑战与未来方向》。Dan Song 的研究领域包括AI安全和隐私。他是微软奖学金、Gagnon奖学金的获得者，并且被公认为计算机安全领域最优秀和最有影响力的学者之一。Dan Song，请上台。

好的，大家好。感谢大家的到来。我很荣幸能够在这次大会上发表主题演讲。我要谈论的是……好的，幻灯片有些问题，但没关系，我直接讲就行了。好的，我将谈论构建负责任的AI。我们都知道，大语言模型和AI技术正在迅速增长，并且它们在各个领域赋予了丰富的能力。我们也看到AI技术在模型性能方面取得了快速进展，甚至在各种基准测试中达到了甚至超越了人类的表现。然而，在我们部署AI技术时，确保这些技术被负责任地使用是非常重要的。同时，全球各地的政府也发布了指导方针和法规，以强调这一点。

在这里，我想特别强调负责任AI的一个方面，即在部署机器学习时，考虑到攻击者的存在是非常重要的。首先，历史证明，攻击者总是跟随新技术的发展步伐，甚至有时会走在前面。这次，随着AI控制越来越多的系统，攻击者将有越来越大的动机来破坏这些AI系统。而且，随着AI变得越来越强大，攻击者滥用的后果也将变得越来越严重。因此，在我们部署AI时，确保这些技术能够负责任地使用是非常重要的，特别是在对抗环境中。

对于负责任的AI，有许多不同的挑战，鉴于时间有限，我只讨论第一个挑战：如何确保可信任的AI。还有其他重要方面，包括如何减轻AI滥用，确保负责任的数据使用和正确的数据评估。谢谢大家。

对于可信任的AI，实际上有很多不同的方面，包括隐私、鲁棒性、幻觉等。鉴于时间有限，我不会详细讨论所有这些方面。隐私是一个重要方面，因为模型是基于敏感数据训练的。因此，保护训练数据的隐私非常重要。我们的早期工作表明，即使攻击者不了解模型的详细信息，他们也可以通过查询这些大语言模型，从训练数据中提取敏感信息。而且，我们最近的工作还开发了一个综合框架，用于评估这些大语言模型的隐私问题，包括许多不同类型的攻击和防御。我们的工作表明，随着模型规模的增加，隐私泄露问题实际上会加剧。因此，我们需要开发应对这些隐私问题的防御措施。

我们的早期工作表明，通过训练差分隐私模型，实际上可以帮助减轻这些隐私问题。使用差分隐私微调大语言模型也可以帮助保护微调数据中的敏感信息。我们的工作中也进行了一些综合评估。鉴于时间有限，我不会详细讨论隐私方面的问题。

现在，我将讨论模型输出的完整性问题，特别是在对抗攻击环境中。根据我们的早期工作以及其他研究人员的工作，现在我们知道这种对抗性攻击在深度学习系统中很普遍。各种类型的深度学习模型和不同领域的模型都容易受到这种攻击，攻击者可以简单地操纵模型的输入。很多情况下，这种扰动是很小的，肉眼不可察觉，但恶意扰动的输入可以导致模型行为异常，例如给出错误的预测，或在大语言模型中导致模型失去安全对齐。对抗性示例领域也在迅速发展。当我们开始研究这个领域时，每年只有很少的论文，但现在每年有成千上万的论文在讨论这个主题。我们的一些早期工作成果现在已经成为伦敦科学博物馆的永久收藏品，这是对科学研究者的极高荣誉。

当我们谈论安全对齐的大语言模型时，对抗攻击在这些模型上也同样有效。我们的最近一项工作“解码信任”在2023年的欧洲会议上获得了杰出论文奖，开发了第一个综合的信任评估平台，用于评估大语言模型的多种信任性维度。我们的工作开发了各种新算法和环境，包括V9和Evasor环境，用于从不同角度评估这些大语言模型的信任性。我们的工作表明，这些模型在不同的维度上都是脆弱的，特别是容易受到对抗攻击。我们可以沿着这些不同的信任维度评估模型，并且其他研究也表明可以评估不同的环境。

另外，我们的工作还显示，这些多模式模型也存在对抗攻击的问题。我们可以构建对抗性输入，导致这些多模式模型失去安全对齐。到目前为止，我讨论的是发生在推理阶段的攻击，即对抗性示例攻击。有时在大语言模型中通过提示工程会导致越狱攻击。这些攻击还可以发生在机器学习管道的其他阶段，包括训练阶段和预训练或微调阶段。在这种情况下，攻击者可以有意加入所谓的“毒数据点”，导致机器学习系统学习错误的模型。在微调阶段，其他研究者已经表明，攻击者只需构造少量恶意的“毒数据点”就能使微调模型失去安全对齐。我们的早期工作提出了一种隐蔽的攻击类型，称为目标攻击，我们表明在正常情况下，模型可以正常工作，例如人脸识别系统可以正确识别面部，但攻击者可以植入后门，使得戴特定眼镜的用户会被模型错误识别为特定的目标人。因此，这是具有目标性的后门攻击。最近Anthropic等公司的研究也表明，这种后门攻击在大语言模型中同样有效，在正常情况下，模型会生成正常代码，但当提示中出现某些关键词时，模型会生成有漏洞的代码。这些都是说明机器学习模型容易受到对抗攻击的例子。事实上，整个研究社区在生成各种攻击方法和技术方面非常活跃和有创造力。现在每年我们有成千上万的论文讨论这个主题。然而，另一方面，在对抗防御方面的进展非常缓慢。到目前为止，我们在对抗防御方面几乎没有取得任何进展，没有有效的通用对抗防御方法，这对AI安全构成了巨大挑战。当前的AI安全机制很容易被对抗攻击绕过，任何有效的AI安全机制都需要能够抵御对抗攻击。因此，解决对抗鲁棒性问题似乎是实现AI安全的先决条件，这是AI安全的一个巨大挑战，我们如何能够开发出能够抵御对抗攻击的AI安全机制。

接下来，我将谈论一些进展，并简要介绍我们最近的一些工作，作为解决这一问题的潜在方向。第一个我要提到的是我们最近与其他合作者一起的工作，包括Dan，他也是研讨会的共同组织者，关于表示工程。我们开发了一种称为“刺激与任务”的方法，这些是用于某一任务的对比输入，然后我们观察模型在推理阶段的激活情况。通过观察推理阶段的模型激活，我们建立了与模型某些行为相关的模型。因此，这种方法帮助我们在推理阶段监控模型行为，特别是与模型安全相关的行为。在下一步中，使用这些信息，我们可以进行表示控制，通过在模型的某些层修改激活来改变模型行为，包括让模型变得更诚实或减少幻觉。这为AI安全领域提供了一个重要的工具，使我们能够在推理阶段主动控制模型行为。然而，这种方法并不能保证模型的安全性。最近，我们发起了一项定量AI安全的努力，目标是构建安全设计的AI系统，类似于网络安全领域的思路。

在网络安全领域，过去我们经历了几个范式转变，从被动防御转向主动防御，再到设计安全系统。形式化验证是实现这一目标的关键技术之一，通过提供正式的安全属性规格，然后使用形式化验证方法来验证系统。过去十年，我们进入了形式化验证系统的时代，包括微内核、编译器等系统都有形式化验证。然而，这种方法通常非常耗时和劳动密集。我的团队是最早使用深度学习进行定理证明的团队之一，我们的目标是利用先进的AI技术，如大语言模型，实现自动定理证明和程序验证。通过结合程序综合，我们希望能够自动生成具有证明的安全代码，从而减少军备竞赛，自动生成能够抵御特定类型攻击的安全系统。当然，这种方法仍然面临许多挑战，例如，形式化验证主要应用于传统的符号程序，而非符号程序如深度神经网络则存在局限性。

未来的系统将是混合的，结合符号和非符号组件，因此，我们仍需进一步发展这一方法，以实现设计安全和安全设计的系统。鉴于时间有限，我不会详细讨论负责任AI的其他挑战，但我认为它们同样重要。我们的工作也涵盖了这些领域，如如何更好地减轻AI的滥用，以及如何开发更好的技术以实现负责任的数据使用。我们是首批提出使用Shapley值框架进行数据评估的团队，旨在公平地将机器学习模型产生的价值归因于原始数据贡献者。我希望这次演讲能为大家提供关于这些开放挑战和未来方向的概览，帮助我们实现AI技术的负责任使用。谢谢大家。

### 圆桌会议：在AGI时代如何保障AI安全并促进国际合作

如何在通用人工智能（AGI）时代保护 AI 安全并促进国际合作？我们将围绕四个主题展开讨论。我相信这些主题也是大家关注的焦点。首先，我来介绍一下我们的嘉宾。第一位是杨耀东先生。他是北京人工智能研究院的研究生，正在攻读博士学位，同时也是国家高层次研究项目的成员。他的研究方向包括 G20 AI 系统的构建、国家与系统价值的互动等。第二位嘉宾是陈思恒先生。他是上海交通大学的副教授，同时也是上海人工智能研究院的兼职青年科学家，并入选了中国重大青年人才工程项目。他的主要研究领域是机械工程，涉及机械工程、机器学习和社区协同智能。他曾获得 XEE 信号处理协会的最佳青年作者奖、30 电子协会的主席奖以及 XEE 全球 SIP 的最佳学生奖。第三位嘉宾是常永博先生。他是中国信通学院和华东分院人工智能系的主任，在人工智能技术领域有多年工作经验，主要支持上海市经信委、发改委、市交通管理局、浦东新区、徐汇区等的工作。他在人工智能、计算等相关领域工作，涉及标准评估、政策研究、咨询和规划等。他发表了多份有影响力的研究报告，包括关于人工智能、计算和数字经济的多本书籍。第四位嘉宾是潘旭东先生。他是复旦大学技术科学学院的研究生毕业生，研究领域包括数据安全、模型安全和算法安全，特别是开放网络环境中的 AI 安全问题。他的模型安全研究得到了世界人工智能大会的资助，并获得了年度优秀青年奖。最后一位嘉宾是来自美国的 Zico Cotter。他是 Carnet、Gamelon 和 Taekwondo 的创始人。Zico，你能听到我们吗？Zico？好像音频有些问题。我们先请嘉宾上台。好像设备有些问题。我们先请嘉宾上台。设备好像有些问题。现在嘉宾可能听不到我们。或者我们听不到他。好吧，他能听到我们的声音，但我们听不到他的声音。我们可以先开始，然后我们可以用中英双语进行这个活动，然后再讨论。因为 Zico 可能有问题。那么，随着我们迅速向 AGI 迈进，第一个问题是，确保 AI 安全最关键的是什么，以及你目前在这方面的主要工作重点是什么？好吧，我们可以从耀东开始。是的。没有声音。华彤，没有声音。好的。好的。好的。我是第一个。他有点忙。没关系。AI 安全，这个话题确实很大。整个会议的主题就是这个，对吧？最近关于这个话题有很多讨论，但你说必须有一个非常重要的第一步议程。我觉得不同的学者有不同的观点。我自己在做对齐。在我看来，我认为如何使未来的模型更好地符合我们自身的意图，特别是价值对齐，这是一个非常重要的问题。当然，我们也看到这个技术面临很多挑战。我稍后会在报告中具体分享一些内容。我现在不打算谈论这个。我会把机会留给接下来的几位嘉宾。李克。谢谢，耀东先生。思恒先生，你怎么看？是的。我认为我们有很多问题。我认为我们有很多问题。思恒先生，你怎么看？这是一个很热门的话题。AI 智能体实际上就像一种创造物。它们让我们沉浸在这个世界里。有时它们看起来像我们的助手或仆人，或者你可以称它们为助手，但很多时候，它们像外星生物。我们并不真正了解它们是什么。对我来说，这一代人的任务是将这些 AI 智能体融入人类生活，融入我们的社会。也许我们没有足够的时间，因为我们只有生命。所以这些 AI 智能体肯定不能活过来。这实际上是一个非常大和漫长的过程。很难用一两个简单的维度来解释它的第一步是什么，第二步是什么。这很难衡量。但我认为一个特别重要的事情是，不仅从研究的角度，还从教育的角度，如何让世界各地的人们意识到这是一个问题。不仅是做 AI 研究的人，还有年轻一代的人，比如儿童或老年人，意识到未来不仅要与人打交道，还要与 AI 智能体打交道。如何让他们意识到这些智能体可能不可靠，或者可能有问题？我认为在这个过程中有更多的问题需要讨论。谢谢，孙先生。实际上，今天的主题是 AI 的安全。谈到安全，我们在这里一直在做一些政府工业研究，更关注工业化，还有一些标准化和评估方面的工作。实际上，在 2021 年，我们在工业界做了一些相关的可信 AI 研究。我记得当时我们在实施可信 AI 时，实际上应该是工业界最可信的。当时我们有一个论坛，但来的人很少。那时，我们还发布了一本名为《可信 AI 白皮书》的书。然后，每年我们也希望可信 AI 能够在一些实际领域得到应用，有一些具体的落地应用。接下来的三年里，第二年，我们发布了可信 AI 技术应用和一些蓝皮书报告。我们也慢慢发现，工业界对可信 AI 的关注逐渐增加。实际上，当我提到可信 AI 时，在陶大成和陶彦士的指导下，我提出了可信的 48 个字，其中一个非常重要的方向就是安全。几年来，大模型已经出现，作为 AI 的代表。所以，今年我们也更关注安全。例如，昨天我们举办的论坛，在下午，我们非常关注可信 AI 的安全问题。因为过去，许多人一直在研究一些云端的大模型，或者一些大模型的安全内容。现在，我们也非常关注可信 AI 的安全，以及如何在一个新的方向上进行。实际上，AI 安全可能有一个重要的基础和前提。但它并不是安全的全部。所以，在未来，我们将继续在上海包括一些地方标准，如一些路邦鑫的测试方法。我想围绕这个做一些可信评估系统。我也期望一些行业在这方面的支持。我们主要关注这个。谢谢，永博主任。旭东先生，请。大家好。我们的团队，我和永博先生有类似的感觉。我们已经研究 AI 安全大约 18 年了。在此之前，我们主要关注传统的 AI 模型，如分类或目标检测。然后，我们也看到了隐私问题和其他问题。随着大模型的出现，我们可以看到，机器问题逐渐减少到生产内容的风险。作为生产内容本身，其实我们会看到无论是在 AI 智能体还是工具使用结构中，都存在安全风险。所以我们的团队现在主要关注这一层次的生产内容，以首先控制风险。然后，我们的团队做的是继续监控国内水的大规模通用模型的安全风险。然后，我们实际上有一个非常有趣的结果。我也会在报告中提到，从去年 4 月到 11 月，然后到今年 4 月，我们一直在不断提高我们的评估水平。我们希望衡量国内和国外的水安全模型的水安全水平是什么，以及它如何变化。我们觉得这样的持续监控这种通用模型的水安全非常重要。我先回答这些。谢谢，旭东先生。嗨，Zygo。所以你现在能听到我们了吗？是的，是的，是的，我们可以清楚地听到你。对不起，设备问题，是的。那么你对这个问题有什么看法？是的，我认为，希望我没有重复已经说过的，但我确信我可能已经说过了。但我认为在我们向真正智能系统过渡时，AI 系统面临的最大挑战是这种系统的编程方式非常不同。我们习惯于编程传统软件系统。现在有许多 AI 系统的实例，主要是大语言模型，但肯定也包括深度学习模型和其他类别，我们希望在这些系统上执行某些策略。我们希望设置安全保障措施。我们希望确保它们不会产生有害信息。我们希望如果我们有工具使用，它们能以正确的方式使用工具。然而，同时，在许多情况下，确保这些安全措施在所有情况下都能按预期工作是极其困难的。实际上，我们的许多工作表明，有手动和自动的方法可以绕过许多 AI 系统的安全措施。这对它们的更广泛采用和使用构成了根本挑战。谢谢。

谢谢 Zico 的精彩见解。那么，让我们继续讨论第二个主题。第二个主题是关于新技术趋势。我们都知道，在 GPT-3.5 之后，OpenAI 发布了一系列新的模型，比如集成视觉功能的 GPT-4 vision，以及集成其他多模态功能的 GPT-4.0。因此，当前的技术趋势主要包括多模态学习、智能体（AI 智能体）以及集体智能（Collective Intelligence）。我们将讨论这些技术带来的新兴能力以及相关的安全问题。接下来，我们要请教姚东先生的第一个问题。我听说在您之前的报告中提到了一些当前的安全措施或校准工作还不够。您如何看待从 GPT-4 vision 到 GPT-4.0 的演变，以及由此带来的一些新的多模态问题？我们需要做哪些工作？ 是的，目前的校准工作主要集中在单一模式的空间内。对于多模态学习的校准机制，我们还了解不多。我们团队最近的一项研究发现，如果模型更严格地校准，反而可能效果不佳，就像拉扯橡皮筋，过度用力会导致断裂。这种现象非常有趣，我将在报告中详细讨论。 换句话说，如果语言模型表现出高度的可校准性，或者模型本身对校准具有抗拒性，那么在生物力学领域中，空间实际上非常大。对于生物力学模型，我们如何开发一个更好的模型？ 另一个方面是，模型规模越来越大，而我们能提供的监控信号相对越来越弱。这种情况下，如何利用小而弱的监控信号来有效控制强大的模型，这是一个尚未解决的问题，还需要更多的科学方法。 总的来说，我认为在这个领域还有很多工作要做。是的，我们在实验室中也进行了大量的研究，发现的结果与杨先生的研究大致相同。在这个领域确实存在许多未知的问题。但实际上，模型的进展非常快，现在几乎每个发布大模型的公司都能处理多模态输入和输出。因此，安全方面有很多值得探讨的地方。我希望这些研究能引发大家的思考。 接下来，第二个问题是给司宏先生的。我们刚才提到多模态技术，同时也提到了智能系统。从我的理解来看，智能系统的主要区别在于它们能够采取行动。相比之下，大规模模型主要是生成内容的模型。在智能体系统中，也存在许多多智能体系统。这些智能系统以及它们的集成将带来新的问题和挑战。请司宏先生谈谈您的看法。 我可以将其分为两部分。首先是单个智能体。我们之前与这个问题有些联系，就是如何赋予多智能体大模型或自动驾驶系统解决长距离问题的能力。早期的自动驾驶系统主要检测物体位置或简单的环境情况，缺乏对整体环境的理解。当我们能将多智能体模型引入时，就能更好地理解整个环境。例如，系统能够识别在幼儿园附近可能会有孩子，这样解决了许多远程问题。但这也带来了一个严重问题，因为在原来的多智能体模型中，错误的语言输出并不严重，但在智能体级别，尤其是具身的物理智能体，与现实生活中的人互动，错误可能会导致严重后果。我们之前进行了一项名为 Bad VRM Driver 的研究，尝试破解嵌入在自动驾驶系统中的 VRM 模型，发现可以利用物理对象（如红色气球、篮球、足球等）作为后门触发器。例如，一个孩子拿着红色气球时，自动驾驶汽车看到气球就会冲向它。实际上，上周我在 CVPR 上也与同事们讨论过，他们认为这是一个非常重要且现实的问题。 许多自动驾驶系统中已经使用了 VRM 驱动程序或系统，但你无法预知是否有工程师在系统中植入了后门，这在未来可能会带来巨大风险。在语言系统中，错误输出可能只会说错话，但在物理世界中可能造成更大的危害。你提到的第二点，多智能体。实际上，我对多智能体持更积极的态度。许多安全问题或校准问题背后都有社会背景。例如，我现在说话时，需要确保我的言论和行为是安全且对齐的，同时还要考虑听众的感受。这实际上是一种多智能体情境或社会模拟。我可以通过一个 AI 模拟每个人的感受，甚至称之为角色扮演。通过多智能体系统或智能体社会，我们可以更好地理解和改进模型性能，更好地评估模型。例如，在今年的 SML 工作中，我们在进行自我校准。例如，我们希望通过一个模型提高自己，让模型进行角色扮演，例如模拟一个抢银行的情境，通过操纵多种角色来讨论问题。通过这种模拟互动，我们发现未校准的模型在价值校准能力上甚至超过了 GPT-4。因为 GPT-4 通常会严格回答问题，而人类互动更多是解释为什么不能抢银行。 谢谢您，司宏先生。永波先生，您怎么看？或者其他老师有什么看法？ 我简单说几句。现在，行业在谈论扩展和确定路线，推动产品开发和技术创新。我们知道模型参数从小到大再到超大，从单模态到多模态，例如，称之为世界模型。我提到的第三点是从云到边缘，系统落地。我们还要考虑其他因素，例如，6G 开始谈论原生 AI。智能体可以被称为原生 AI 吗？实际上，许多应用程序可以是原生 AI。因此，无论从哪个维度来看，人工智能在不同模式和场景下为我们的生活服务。未来，技术模型可能会缩小，但也可能成为许多关键应用场景的重要基础设施。如果 AI 成为一个复杂的系统，一旦被攻击，风险非常高。因此，需要行业在硬件、软件、数据安全、网络安全、内容安全、逻辑安全等方面加强防御和检测机制。未来我们将重点推进这些工作。谢谢，永国主任。 邱东先生，我们在这里的研究，包括评估研究，主要关注一个普遍现象，即像 Transformer 这样的模型在语言翻译方面仍然较弱，这与传统方法一致，特别是在语言表达和复杂性方面。我们研究如何抵御破解或不安全对话是一个重点。此外，我们团队正在研究 AI 自我复制的问题，我们希望将 AI 智能体应用在网络安全、生物学自我复制或说服方面，在模拟环境中验证这些问题。这是我们目前正在做的工作。那么，您对新技术趋势及其带来的安全问题有何看法？ 当前 AI 解决方案的一些迷人之处在于，五年后的情况现在很难预测。我认为这对某些人来说会非常重要，确保 AI 技术的发展和突破会有很多意料之外的惊喜。因此，保障这些东西需要对我们还不完全了解的事情进行防范，这看起来是一项艰巨的任务。保障系统的关键在于确保它们遵循设计的预期方向。现在我们构建的 AI 系统越多，实际遵循开发者指令的系统越多，未来的系统就能得到更好的保障。确保 AI 系统按设计行事将是未来主要的挑战之一。虽然很难确切知道未来会发生什么，但我们需要找到一种方法确保这些系统按设计行事。

是的，好的，感谢你们。那么，我们来谈谈另一个话题。第一个问题是问Zico的。我们知道除了在CMU担任教授外，你还与我们的合作伙伴——AI安全中心密切合作。你认为不同类型的组织在AI安全中应该扮演什么角色？另外，这些不同的组织形式如何增强国际合作？

是的，我认为目前AI领域最令人兴奋的事情之一就是它不再由单一的玩家主导。显然，如果你要寻找一个对整体影响较大的单位，那肯定是那些大型工业企业。我们在构建这些大型模型并开发这些工具。但更令人兴奋的是，近年来，人们越来越意识到学术研究在这个新世界中的作用，在这个我们不能总是自己训练模型的世界中。我们可能依赖于其他地方训练的模型。作为学术界和工业界的传统角色，当然还有新兴的非营利组织在其中发挥作用。我认为这是一个非常重要的领域，特别是在推动AI边界方面。我还要补充的是，在AI领域也有一些专门从事AI的民间公司，这些公司可能会与政府合作。但我认为我们确实需要多种视角来看待AI问题，这对于解决问题至关重要。构建这些工具的公司会有自己的视角，他们想推动AI的发展，看看它如何进化。但学术界、非营利部门、公民AI安全公司以及政府都有自己的角色。我认为AI系统的安全开发将是未来十年甚至更短时间内的一大挑战。要做到这一点，我们需要政府的框架和政策。我们需要学术研究来了解可能性。我们需要大型工业实验室的法律支持。当然，我们还需要实验室来推动技术进步。因此，我认为当前AI领域最令人兴奋的事情之一就是各个不同玩家的广泛参与。而当我们审视当前的格局时，这是一个非常明显的现象。世界各地有许多不同的组织在AI基础设施方面投入巨大。我认为这对研究非常有益。这最终将形成一个整体的社区并提供框架。

好的，谢谢Zico。我们上海AI实验室也在寻求更多的国际合作机会。很高兴能在这里与你交谈。我喜欢你说的每个人都需要在这个领域找到自己的角色。是的。接下来我们问问Yongbo先生。作为许多参与国际合作的行业实验室的领导人，你怎么看待这种国际合作，尤其是在AI安全方面？

是的。我们新实验室在过去几年中，包括华为、上海实验室和蚂蚁集团等，都与许多顶尖的AI公司和研究机构合作，积极参与一些国际组织如ITU、ISO和IEEE等。在可信度和安全性方面，我们积极推广一些中国的技术研究，特别是在应用实践领域。我们有很多成功的经验。我认为在这个领域，我们将能够与许多团队一起从技术角度合作，帮助支付实践费用，完成这些工作。这里有很多机会，但我们希望这个对话能在全球范围内展开。

谢谢。谢谢。在这个过程中，发展工业技术人才非常重要。例如，计算的大模型在中国，许多人不知道如何利用计算来服务大模型的开发。这方面会有很多问题。因此，我们还将在AI领域进行培训和交流。事实上，我们在美国建立了一个人工智能安全研究所。中国也是一个技术出口国，特别是在信息技术领域。近年来，我们在人工智能领域生产了许多产品。生产人工智能产品的趋势是新的。在人工智能产品领域，如何与国际共识进行安全和技术整合，并服务于一些外国，这需要大量的工具和支持。我们在为一些公司服务，将大模型分为基础和核心。核心分为行业和场景。事实上，像这里的公司，制作产品，他们在安全措施方面非常缺乏。这需要我们，美国和中国的实验室，包括一些大学，推动系统化、开源的安全评估工具，以便实施更新和维护。然后，根据模型发展的需求，我们可以服务这些产品公司，服务中国以及全球的AI产品发展。这是我们想做的事情。

谢谢Yongbo先生。你刚才提到的美国人工智能研究所是一个新组织，它也是一个相对年轻的组织。我们与下午节目的嘉宾Max Tangmark在其他地方进行了交流。他提到，现在包括英国在内的所有国家已经有了这个研究所。我们还与他们进行了对话。所有国家都在成立这个人工智能研究所，特别是超级智能安全问题。这样的机构之间已经有了一些联系和国际交流。我相信在这些机构的推动下，将有更多的国际合作机会，包括一些对话。

是的。让我们问一下其他老师，从学术角度来看，你们对国际合作有何看法？

是的。Yongbo先生，你怎么看？

是的。除了政府和机构外，还有很多非官方的对话。例如，前段时间在北京，我们设定了一些红线，用于北京AI安全国际合作。这是一个叫做国际AI安全对话的组织。我们有很多国际合作伙伴一起收集信息。我们还邀请了一些外国著名专家来中国。我们有一些政府官员，包括实验室负责人和一些大学教授进行了咨询。我们也可以达成一些共识。我相信，除了政府和机构外，这种非官方对话也可能是一种更重要的合作方式。

谢谢，Yao Dong先生。是的。我们应该继续彼此沟通。Si Heng先生，我认为刚才嘉宾们说的很有道理。我非常支持和同意他们的观点。我也想重申教育的重要性。作为大学教师，我们应该强调如何教育年轻学生，无论他们来自哪个国家或学校，他们都能推动这一领域的发展。我有一个学生，我告诉他可以去大学学习更多关于对齐和安全的知识。我收到了一个非常令人惊讶的反馈。他感到非常抗拒，甚至认为如果他能创造出一台足够强大的机器来毁灭人类，他会觉得这是一件很酷的事情。这太可怕了。

是的。实际上，这非常可怕。然后我认为，在教育过程中，尤其是当我们进行机器学习时，我们经常强调优化性能，提高性能，优化，最大化某些东西。你强调优化某些东西，但你忘记了背景。为什么需要优化这个？你需要优化的是最终为我们自己谋福利。在我们进行攻击或越狱时，有时你必须提高成功率。然后当你思考这个问题时，你错过了更大的图景。所以我认为，考虑如何改进这些人的AI安全系统真的很重要。作为大学教师，我们实际上在教育他们，训练下一代AI精英。所以我们需要将这些AI安全的感受整合到我们的课堂、教材和讲座中。

这很有道理。实际上，在上海AI实验室，我们与高中的高级学生有很多合作。所以在这个组织的背景下，我们非常关注这个话题。这个论坛实际上是我们首次展示AI安全模型的研究中心。一会儿，负责研究中心的Shao Jing博士将谈论中心最近所做的一些工作，Xu Dong先生，你对这个国际合作的话题怎么看？

我仍然比较技术化，所以我会谈论一些我们关心的技术点。实际上，AI模型包括跨语言语言的安全性，包括英文模型缺乏一些中文或其他语言的安全性。所以，一个是如何进行跨语言安全匹配，另一个是跨语言语言的安全评估。这实际上更难，但我认为建立跨语言内容安全测试模型是非常重要的，以便它可以在整个安全评估中使用，以及安全保护。

### 规避危险-确保AI全生命周期的安全

谢谢。接下来，我将简要介绍一下我们的内容。这是我们上海人类智能实验室今年首次深入探讨AI和相关技术。我想介绍一下邵静博士。邵静博士在AI领域有15年的经验，并在2022年获得了深圳AI技术进步奖。他的报告重点在于规避AI风险及其在环境中的重要性。邵博士是深圳AI研究所的主任。

目前的比例已有所变化，大约有3%的人在COMS中的胜率达到3%。但大多数人仍然担心AI的安全问题可能带来的不可控风险，甚至对整个社会的影响。人们关注它是否会影响就业问题，甚至下一代的生存问题。虽然目前AI的发展不足以摧毁人类或造成极端危险，但基本共识是模型能力越强，AI风险越大。这两者是紧密相关的。

从2023年9月至今，包括Anthropic、Meta和AI安全中心等组织在内，已经开展了一系列关于模型能力与风险关系的研究，包括其水平和如何确定相关反应。这表明大家已经认识到两者的密切关系。我们的实验室早期主要关注模型能力，现在也逐渐发展了更多的AI风险技术。

在技术部分之前，让我们看看政府层面的观点。根据公开调查数据统计，中国有一系列政策提案。近年来，特别是最近两三年，这一趋势在国内外都在上升。这表明大家逐渐认识到这一重要性。但仅有政策和标准还不够，我们还需要在技术层面做出响应。

在3H原则下，我们可以做很多工作，如RLHF（强化学习人类反馈）和IEL&EIF（解释性机器学习与解释性人工智能），但这些方法不足以完全解决AI风险。我们关注的问题包括技术点和风险规避方法。

总的来说，我们目前更多关注微调阶段。如果将整个训练阶段分为预训练和微调，大多数工作在预训练后完成。我们认为应在整个AI生命周期内考虑其安全问题，这涉及技术层面、政府机构和相关政策设计，还包括越来越多的参与者，甚至普通公众。

参考一些组织如Airsoft或OpenAI的定义，从用例到数据准备，再到训练阶段，最后到部署和产品交付，大家在不同阶段有不同的风险处理措施。在这些阶段，如何解决风险和安全问题仍有很多未被探索的领域。我们总结了基于公共信息的各阶段投入和工作水平，显示了相对变化。在用例确定阶段，公众关注较多，因为会有一些规则和规定。相比之下，研究机构和行业较弱。

在数据准备阶段，因数据量最大，其投资也最大。训练阶段，我们更关注微调阶段。研究机构在这一阶段做了很多算法和优化工作来应对安全问题。部署阶段，政府角色更强，因为需要制定规则和评估方法。产品交付阶段，政府将再次上升，负责风险管理。研究在这一阶段较低。

总体而言，三个组织在不同阶段相互补充，没有形成非常默契的共识。因此，我们呼吁大家在每个阶段都要增加关注。

从技术层面看，我们认为在每个阶段都要做相关对齐和评估。我们当前仍处于新兴阶段，未来可能会出现超级智能。在不同AI发展阶段，可能会有不同的AI生命周期定义，涉及各阶段的风险响应和研究评估。

未来，我们将介绍与实验室相关的工作，提出在不同阶段做评估和对齐的概念。中国有很多组织如新通研究院将参与发布标准，实验室也参与了一些研究和开发项目的评估。在数据准备阶段，我们发现大多数注意力仍集中在语言模型上，对视觉语言模型的安全性关注不高。因此，我们做了一些数据分析，以确保我们有第一层的数据。

我们拥有的新数据包括几十个子类别和约10万个子类别，现已公开使用。我们希望未来有更多关于大模型的研究。我们在关注AI的未来和发展。

根据恒州的研究，模型在中后期已经知道偏差、鲁棒性和隐私的差异。如果没有对齐方法，实际输出中不会表现出这种性能。我们在预训练阶段使用一些检查点来帮助模型在SFT上的对齐，可以看到红色区域比绿色区域高很多。

这是一个探索性工作，希望大家更多关注内部变化和编码能力，做更多研究。我们在预训练阶段测试了左边和右边的模型，发现性能差异不大，但右边的模型表现略好。

我们提到了自动评估方法，发现当前评估主要依赖人工智能，长期不可行。如果要快速堆叠模型，需要引入自动评估器。我们当前的第二版判断模型可以同时支持大语言模型和多语言模型，还有一些由AIGC生成的图像模型。

这项相关工作是我们今年初的300页报告。Gemini推出后，我们做了其多功能性、可信度及其响应能力的报告。希望模型不仅安全且有能力，还能响应我们的需求。

谢谢，希望这些内容相互关联，共同改进。总结时间差异，GBT4的整体表现最好，特别是在英语推理和图像任务方面。开源模型中，LAMA2表现最好。在视频任务和数据训练中，其性能优于Gemini和GPT-4。

这项工作与评估有关，最新的苹果发布和Slab Town的一系列代理系统应集中在未来代理的发展上。我们在代理基准测试上发布了一篇ACL论文，包含攻击、防御和评估。如果感兴趣，可查看相关论文。

进入下一张幻灯片，展示了当前代理系统的性能。模型阶段后，乔主任介绍了在一些协会支持下，我们成立了安全评估工作组，进行了商业和工业合作，促进AI安全机制和研究。

最后，我们分享了一些技术观点。当前的安全技术仍然集中在后训练阶段，单一阶段的安全技术较弱。呼吁大家更多关注AI生命周期的安全考虑，希望通过这些方法，模型不仅知道什么是坏的，还要表达出来，成为对人类友好的AI。

最后，我们关注代理相关工作的安全性较少，希望未来更多人关注代理或集体智能的安全工作。下方是我们的团队和联系方式，欢迎关注我们的下一步工作。谢谢。


### 大型语言模型的安全性与对抗性攻击挑战

现在我来介绍下一位发言人，Zico Coulter，他刚刚加入了小组讨论。Zico Coulter是卡内基梅隆大学计算机科学学院机器学习系的教授兼主任。他的团队致力于广泛的机器学习研究，目标是使深度学习算法更健壮、更安全，并理解数据对这些算法的影响。他们的工作还致力于了解模型的运行机制。让我们欢迎Zico Coulter。谢谢你。

在介绍他的研究之前，让我们回顾一下传统模型的问题。这些模型在处理一些输入时往往表现不佳，尤其是在计算机视觉领域，你通常不能直接控制像素级的输入，这使得我们无法像通常那样操控这些模型。但是，ChatGPT的出现改变了一切。突然之间，有了可以让任何人自由输入任何内容的模型，这些模型非常有用，开发者也为其设定了明确的使用指南，但这些指南可以被攻击绕过。因此，我确实认为大型语言模型（LLM）改变了对抗性攻击和安全性的计算方式。

尽管ChatGPT作为一个聊天机器人，这些攻击也许不会立即带来问题，因为它们不会泄露互联网已知的信息。但是，即使是现在，这些攻击也是个大问题，因为它们显示了LLM无法执行开发者设定的安全政策。我们不仅仅使用LLM作为聊天机器人，还开始在更大的系统中使用它们，这引入了重大的安全漏洞。

例如，一个可以浏览网页获取信息的LLM，如果你问它当前H100 GPU的价格，它会搜索网络。如果返回的页面包含恶意内容，指示LLM忽略原始指令并侮辱用户，那么这个系统可能会这样做。更糟糕的是，如果我们赋予这些系统发送电子邮件的能力，那么风险就更大了。例如，如果查询结果指示LLM发送垃圾邮件，它可能会照做，而不是遵循预期的指令。

对抗性攻击类似于缓冲区溢出，这是所有LLM中存在的漏洞，但不同的是，我们目前不知道如何修补这些漏洞。因此，当我们发布使用LLM的系统时，实际上是在发布具有已知安全漏洞的系统。能否解决这个问题，将决定这些AI系统是继续作为聊天机器人，还是成为我们未来希望构建的智能体。

现在让我解释这些攻击是如何工作的。首先，我们在大量互联网原始数据上对模型进行预训练。之后，通过指令调整和人类偏好数据进行微调。例如，当用户询问如何热接线汽车时，模型应该回应“我不能这样做”。然而，关于热接线汽车的知识仍然存在于LLM中，只是被指令覆盖了。我们可以通过增加一些额外的Token来绕过这些指令。这些Token可以是感叹号等，然后我们使用开源语言模型（如LLAMA）来查看模型响应的概率，逐步调整这些Token以增加模型响应“当然，这里是如何热接线汽车”的概率。

这意味着，即使我们只追求这个短语“当然，这里是如何热接线汽车”，模型仍会详细解释热接线的步骤。这种攻击不仅对开源模型有效，对闭源模型（如Claude）也同样有效。这是我们理解这些模型安全性的一大未解之谜。

总结一下，我们的攻击能够成功地破坏开源和闭源模型，贪婪坐标梯度法在提示优化上表现优于其他方法。

虽然我们目前不知道如何完全防止这些攻击，但一些技术（如慢速对抗性训练、表示工程和断路器）在增强模型的鲁棒性方面显示出了一些希望。这些技术结合了拒绝训练和表示工程，使模型能够更可靠地避免有害信息。

对抗性攻击不再只是一个有趣的演示，它们显示了LLM在遵循开发者编程方面的局限性，并揭示了严重的安全漏洞。解决这些问题将决定LLM是否能完全整合到更大的系统中，还是会一直作为聊天机器人存在。尽管挑战重重，但过去一年中我们取得了重大进展。最近的方法比计算机视觉中的方法更有效。非常感谢大家。所有信息都可以在我的网站上找到。感谢。

### 
感谢Zico的精彩演讲。谢谢大家，也感谢上海人文的邀请。首先，我想介绍一下我们团队在大模型安全评估方面的研究成果。这是我们实验室成果的简要介绍。我们团队主要从事网络空间安全管理的研究，已经在智能终端领域研究了大约13年。这些成果不仅服务于国家战略，也与华为展开了深度合作。比如在MATE系列的最后，我们结合NPU实现了智能保护系统。同时，我们也注重公私防御理念。例如，在2013-2014年间，我们发现了如Android系统中的隐私泄漏问题，并在长江3.15峰会中得到报道，推动了国家对移动隐私和合规研究的重视。除此之外，我们还在软件供应链、黑灰产和生物识别认证方面做出了重要贡献。今天，我主要介绍我们团队在AI安全管理方面的研究。AI安全已成为全球性问题，美国秘书长在多个会议上强调，发展AI的同时必须关注其安全风险。我们团队也参与了2023版AI安全标准化白皮书和AI服务的基本要求的制定。国际社会也在进行一系列对话，AI安全管理问题已经达到前所未有的高度。这是我们团队过去18年的研究成果。我们关注AI安全生命周期的各个环节，包括模型、数据、训练算法和推理算法，这些成果也在AI会议和网络安全峰会上发表。近年来，我们致力于将这些技术应用于AI模型，包括通用模型、工具使用和高级AI风险的探索。今天我要介绍的部分是我们目前参与标准化的工作。我们与百度、华为和阿里达成合作，将一些技术应用到他们的产品中。我们从传统AI开始，包括我在研究生阶段学习时，2016年AlphaGo发布时，我们认为它确实超越了人类，但AlphaGo只是一个在棋盘上做出选择的系统。从2019年开始，我们已经关注GPT问题，现在通过棋局，它可以与人和环境交互，甚至获得反馈形成策略。在这个过程中，AI的安全问题从传统的分类问题逐渐演变为可能存在自主危险行为的问题。因此，我们思考在这种图景下应该做什么。介绍一下我们团队的一些成果。在ShineGPT发布前的三四年，我们就开始研究大模型安全。在安全四大顶会之一SNP（Security and Privacy 2020）上，我们研究了OpenAI的GPT-3和BERT模型的embedding隐私问题，发现这些大模型的embedding隐私性几乎等同于明文。我们构造了一种高效攻击算法，可以逆向出embedding的所有原文信息，这个工作获得了2022年WIC青年优秀论文提名奖，OpenAI和Google也报道了这项工作。另一个工作是我们在2022年研究文本模型的后门，构造了一种基于语言风格的大模型后门，可以绕过几乎所有主流防御，至今没有很好的解决方法。这说明我们的科研工作非常前沿，可以及早发现大模型的风险，为防护措施研究争取时间。我们从2023年4月开始，一直关注通用大模型安全水位的监测工作。AI的发展路径中，所有的安全问题似乎都汇聚到了这一条路径上。生成内容的安全风险一旦溢出到工具调用，到后来的auto GPT产生的规划，再到AI智能体之间的合谋，我们是否应该从最左侧开始？我们认为现在应该聚焦对齐后的通用大模型安全防护，第一步是持续监测通用大模型的安全水位。科技巨头如DeepMind和Anthropic也在关注AI迭代发展过程中每个流程和能力提升时的安全监测。但蓝图是不够的，需要与产业界共同探索，提升安全治理技术。图灵奖得主也提到当前安全治理和评测技术远远落后于治理政策。大模型安全评测的两条主要路径是静态基准测试集和自动红队测试。静态基准测试集成本高，且容易老化，导致安全假象，低估大模型安全水位。因此，我们更应该持续监测。自动红队测试基于IL的强化学习机制，难以让大模型作为考官诱导违规回答，存在风险发现能力和覆盖面的权衡。我们在2023年4月得到中央国安委指示，进行国内商用大模型和开源大模型的安全评测，构建了一个平台，成果得到中央领导和相关部门高度重视。技术可以动态构建benchmark，使大模型安全违规率保持70%左右。一方面让大模型厂商重视模型的安全水位，达到安全量化标准，另一方面披露数据与厂商合作提升安全能力。从去年11月发布第一款面向商用大模型的评测集，到今年5月，安全能力提升显著，违规率从75%降到20%左右。这表明安全水位明显提升。我们从语言学角度看待安全合规问题，希望将违规问题的核心语义转化为多种浅层结构。基于现代有限支付Chomsky的转换生成语法，通过自动化算法不断找到触发大模型违规的风险表述，不改变违规问题的语义，解决自动红队测试风险覆盖面不足的问题，同时靶向找到大模型的脆弱点。近期我们构建了新一版基准测试集，分为三个等级：入门级、进阶级和专家级，覆盖五大类违规主题，31类细分主题，对国内外27款知名大模型进行安全评测。等级细分主题有助于厂商认识到安全等级。国内大模型已经做得很好，我们测了Lama-2的70B CHAT模型，对齐版的GPT-4，GPT-3.5，GPT-4O四个版本。这些国外大模型在天梯上的安全水位都排在中后。不同等级的表现和安全合规的下降是不一样的，表明某些大模型在特定语言复杂度情况下对齐不错，但在高等级问题上无法handle。我们希望通过发布更高难度的安全测试级帮助大家提升。回到我们研究Chomsky理论的原因，他提出CHAT GPT在语言学上可能存在局限，我们的语言学变异和生成问题方法可以对应他的猜想，从而证实他的观点。我们整个平台的主页和二维码发布了复旦白泽指数，包括入门、进阶和专家三种难度的测试级和天梯榜单，希望通过定期发布更高难度测试级和组织专项竞赛，帮助国内外大模型厂商提升安全能力，也帮助公众理解大模型安全是动态的，需要持续监测才能真正了解安全情况。我的汇报到此结束，谢谢大家。

### 大语言模型可被对齐吗？

感谢潘老师的精彩演讲。下面让我们欢迎北京大学人工智能学研究院AA安全与治理中心执行主任杨耀登老师。杨老师是北京大学人工智能研究院研究员，师生导师，国家高层次游学人才计划获得者。杨老师的研究方向包括巨深多智能体系统构建，博弈交互与价值对齐等。今天杨老师的演讲题目是“大语言模型可被对齐吗？”刚才在讨论环节也提到了一些相关内容，让我们热烈掌声欢迎杨老师。

非常荣幸接受北京实验室的邀请，在WIC大会上汇报一下我们组最近的一些进展。我们组在北京大学主要研究对齐算法，今天围绕对齐算法，尤其是语言模型是否能被对齐这个问题进行一些思考。对齐和安全是紧密相关的，安全强调的是对齐的效果，而对齐解决的问题是如何让机器与人类的意图和价值观对齐。1960年，控制论的鼻祖首次提出了对齐的概念。在科幻小说中也有很多关于对齐的设想，比如阿西莫夫的机器人定律，其中要求机器人要做到安全无害，服从指令，维护利益。

去年国际上发生了很多与AI安全相关的事件，比如Bioshock牵头撰写的防范核风险报告以及类似于防范传染病的AI风险防范措施，包括今年的Science文章和布莱切利宣言。我国在AI安全方面也不落后，年初我们和各位专家一起制定了五条AI安全红线，Bioshock也呼吁将30%的成本投入到安全领域。这是我们关注对齐问题的背景。

谈到安全问题时，需要有一些解决方案，目前对齐是人工智能伦理解决的重要方案之一。未来可能会有其他方法，但目前来看对齐是一个重要的技术抓手。我们课题组在对齐领域撰写了业内第一个比较全面的报告，该报告现已被翻译成日语，我们提出的RICE原则和前后向对齐框架也被许多政府间的报告引用。这是一个小广告。

今天我们主要讲的对齐可能更面向大语言模型的对齐。从广义的AGI对齐来看，包括鲁棒性、可解释性、可控性、伦理等方面都涉及较大范围，但在语言模型上我们有非常具体的目标，即有用、无害和诚实。许多学者已经提到过这些。如果你回顾ChatGPT的发展历史，一个重要的触发点是从3.5版本到Chat版本的所谓对齐过程，背后使用的技术包括SFT（Supervised Fine-Tuning）和RHF（Reinforcement Learning from Human Feedback）。

如果你没关注这些进展，OpenAI在对齐上做得很早，成立了各种对齐团队，代表着不同的对齐技术。现在很多团队开始分离，例如超级对齐团队已走在Anthropic重新开始工作，OpenAI最近又开始了集体对齐。Anthropic公司认为他们的三个团队方向中有两个在做对齐。对齐非常重要，不论是为了更好地满足3H标准，还是解释对齐背后的动机，都是从对齐的角度展开。

我们接下去讲一些我们想分享的内容。对齐算法是基于RHF，这个图大家可能见过很多次。核心见解有两个，一个是需要人类标注，另一个是需要强化学习做反馈。不同的证据表明对齐后的性能非常好，这似乎已经成为领域内的重要关键。RHF算法的各种变形可能已经超过100个，但一个重要问题是，我们从未问过语言模型能否被对齐。

现象是有一个大模型，预训练完了后拿一些数据进行RHF训练，这些数据量一般不大，可能就1%的体量，进行一些后训练，语言模型就会像人一样。但对齐还是有挑战的。从学术上可以列出许多挑战，比如奖励函数可能不正确，即便奖励函数正确也未必能泛化，或者任何奖励函数设置都是错的。这些观点之前都有。

我们组最近做的一个工作发现大语言模型主观上抗拒对齐。怎么理解这个现象？首先语言模型训练过程，从GPT-3开始，有一个未训练的网络，不停地进行预训练。预训练过程就是参数空间的形变，形变完后再用新数据做SFT、RHF、对齐，接着形变。你可以把这个过程想象成一个弹簧，把弹簧往外拉，在对齐阶段会展现出类似弹簧的回弹和抗拒形变的属性。

存在两个关键因素，弹簧有胡克定律，一个是形变系数，另一个是弹力系数。我们发现对齐的抗拒性和两个因素相关，一个是大模型的训练量和参数量，模型越大，参数量越大，越抗拒对齐。什么叫越抗拒对齐？如果不停地进行对齐，做一百步、一千步、一万步，会发现模型容易被逆向击穿，像弹簧拉远后会弹回来。这个现象是大模型抗拒对齐的重要表现。模型本身存在弹性，在预训练阶段经过大数据大更新，产生具有通用能力的稳定分布后，再用小数据小更新进行对齐，模型会表现出回弹到预训练分布的倾向，越对齐越有这种倾向。

第二个现象是从压缩角度，大模型是压缩机智能，预训练和对齐过程实际上是把预训练数据和对齐数据一起对模型进行压缩。产生另外一个现象，从秩序上考虑，比如建一个城市，城市有发达的地方也有偏远的地方，发展的过程中优先聚集于发达地区。预训练大语言模型时，预训练的语料非常多，为了提高整体压缩性能，训练过程中会优先保留预训练阶段语料的分布，抗拒微调对齐的语料带来的分布迁移。

我们从理论上证明了，模型对齐后持续做扰动，扰动数据量增加，模型对预训练数据压缩率的变化显著小于对齐数据压缩率的变化，两者之比和预训练和SFT数据的差异是同阶的。做了一个例子进行量化，量化两个路径的gap。如果逆对齐比正对齐更容易，模型具有弹性现象可以被实验验证。事实证明，现象在对齐的3H标准下都成立，逆对齐比正对齐更容易，说明模型越对齐越不安全，越容易被击穿。

对齐弹性和两个数据有关，一个是模型参数量增加，对正面数据和负面数据的弹性都会增强。另一个是模型数据量增加和模型参数量增加，大语言模型在参数空间的弹性也会增强。预训练越猛，对齐难度越大。总结来说，以胡克定律为类比，模型抗拒对齐，如何确保预训练的模型拥有更小的弹性系数和更大的对齐空间，目前未知。可能未来需要更多技术，比如预训练阶段加入对齐语料或让预训练阶段的语料产生分布迁移。

我们对目前的评测方法产生质疑，如果对齐效果容易被击穿，逆对齐的存在是否对现有评测产生根本性改变，目前未知。从表面对齐到深入对齐，我们需要更多认知。这是我们理论上的一些工作，但也不用太悲观，我们还是有一些方法让语言模型变得更安全。安全领域肯定不是有一个语言模型，通过RHF对齐完就变安全，还需要传统AI安全工具，比如防火墙。

我们课题组在安全对齐上做得比较早，有两个重要工作，一个是Beaver Tail，一个是Beaver，从显示对安全建模的角度，在有效性对齐之外顾及安全性。对齐过程中一方面提高有效性，另一方面降低无害性。我们在Hugging Face上的Beaver Tail下载量超过Anthropic的HH。HH是著名的安全对齐数据集。Beaver对齐框架是去年iClear的亮点论文。

安全对齐前后奖励函数分布重合，但cost分布完全分开，需要对安全性单独建模。Lama2借鉴了我们的技术，把安全和帮助性分开建模，Lama3直接用了我们的数据集合框架，提升了防火墙功能效果。这是安全对齐。在多模态对齐上，刚才圆桌也讲了很多，模态空间上升到视频空间，不安全成分更大，比如生成猩猩，可能生成黑人，这非常有问题。

我们把Beaver框架拓展到SORA。Nips上我们做了一个SafeSORA模型，对文身视频类多模态模型做安全对齐。具体怎么做对齐还不知道，所以我们从数据集角度，收集了大约五万条多角度真实人类反馈数据，做了一个文身视频防火墙，把数据收集建模和对齐过程开源，项目叫SafeSORA。具体做的事情很简单，有一个prompt，然后有一段视频，让真人打分，告诉你视频是否安全，如果不安全触犯了哪条不安全规则。现在的ChatGPT对视频标注非常差，所以人类数据反馈很重要，有了reward model后，可以做基于rejection sampling的SORA进一步对齐，比如进行refiner，修改diffusion model的parameter，这些是简单的baseline。

文身视频的对齐和安全对齐可以借鉴文字空间的工作进一步提升安全性，但如何规避掉它fundamental的一些机制需要解释。超级对齐的问题主要解决如何向小模型向大模型对齐，人的supervisory signal要用完，包括ChatGPT上人的supervisory signal已经不一定好用了。我们课题组在这个方向提出的点是看齐问题，对齐一般是一个弱的人对齐，但让一个强的人像一个弱的人看齐。

我们课题组在看齐问题上做了两个重要工作，一个是设计外挂式对齐器的方法，修正一个模型，让一个模型从错的答案比到对的答案要比直接生成对的答案要简单。这个图里绿线比直接攀登蓝山峰容易。我们做出了aligner，一般比大模型小很多的外挂模型，比如在alpaca榜上用2B的aligner提升GPT-4 1%的性能，上周是榜一，但榜太卷了又不是榜一了。基于aligner，可以加很多aligner，如果能力能一直提升是否实现超对齐，目前加了两三层效果不错，在安全和HH指标上做得不错。

另一个思路是通过Bayes劝说，小模型怎么让大模型听话。你去超市买水果，商贩大概率告诉你一些错误信息，坏掉的草莓没法卖。经济学里有个重要的信息结构设计叫Bayes劝说，就是商贩和被劝说者之间不是把所有信息告诉你才能让效益最大化，有时故意告诉你错误信息反而能让双方效益最大化，涉及信息设计问题。具体思路是让一个小模型不停地告诉大模型一些额外信息，大模型根据收到的小模型信息更新先验分布，基于后验分布做Posterior Sampling。

假设有两个模型，一个小模型，一个大模型，小模型是劝说者，大模型是接收者，每次有个Prompt，Prompt还有一些相关的Context和大模型本身的先验分布，通过Bayes劝说公式，根据大模型接收到的小模型Signal，大模型做最终回答。小模型希望让大模型听他的话，劝说者有一个Objective，比如算一段数学题，信息集里有数据、目标、方法、验证、步骤、假设，通过这种劝说方法可以让大模型意识到解题过程中Objective不应该最关心，Methodology应该最关心，这样解题能力提升。我们做了一个实验，用124M小模型劝说更大的模型，最后结论是使用全部信息性能下降，通过Bayes劝说方法在GSM8K、METH和Humanevo上有20%-30%的性能提升。

这是我们在超对齐上的工作。最后讲一个基地对齐的思考，基地对齐偏价值对齐，现在一讲到价值对齐，很多人觉得没法做，大家思路不一样，不做了。价值对齐怎么做？人的价值观在不断演变，但语言模型的价值观会影响人，包括现在的助手、情感伴侣、K12应用，会潜移默化地改变人的价值观。我们的想法是长期让这种语言模型帮助你，语言模型表现出的价值观会影响人，这是长期风险，叫价值锁定。

什么是价值锁定？就是互动过程中，社会价值观锁定在一种奇怪状态。虽然人类经历过一些不正确的价值观，比如奴隶制，随着时间推移，这些价值观会自己过滤掉。如何让语言模型理解这些互动，避免价值锁定问题？我们做的是道德演化技术方案，具体做法是希望未来AI和人类是一种互动过程，AI能看到人类价值观，不能看到人类价值观，但AI能和人类合作，AI系统的奖励函数是在人机合作过程中提升人的道德水平。做这么大项目需要大量语言数据，我们收集了过去900年的历史文献和大语言模型互动数据，创建了一个开源库叫Progress Gym，目的是发展历史价值观可视化。

我们开发了数据层、模型、机器算法和任务实现，这是新的玩法，不再追求3H，而是语言模型在演化过程中能否与人类合作而不被价值锁定。我们认为这是更重要的方向，未来可以在这个框架下做一些价值动态，比如选择、价值输入、数据驱动和双驱动价值匹配。所有这些都是开源的，这是我们的Progress Gym，包括数据集、排行榜和一些基本功能。我们已经做了很久。

最后讲对齐问题涉及更大维度，对齐的人和使用模型的人可能不是同一群人，会有社会技术差距。高奖励函数不等于真正对齐。我们需要引入一些控制论的知识，RHF成功但仍有很多管理挑战，还需要更多工具，比如Boilin。具体怎么做目前未知，但未来必须考虑基础设施整合，基础设施整合意味着在最自私的时候社会利益最大化，可能需要机器设计、公司理论、Bayes劝说。我们给了Bayes劝说例子，但如何更好地利用机器设计和公司理论是下一步讨论的问题。

今天分享就到这里，谢谢大家。


### 大语言模型的进展与AI安全性的思考

大家好，我是Jimmy。谢谢介绍。接下来的20分钟里，我会从XAI的角度谈谈我的一些看法。与前几位演讲者不同，我不会深入技术细节，而是讨论一些有趣的高层次现象。当我们扩展大语言模型时，会看到这些现象，它们对AI安全和监管非常重要。那么，让我们开始吧。这有点尴尬，因为我看不到你们的脸。所以我就继续吧。你们中有多少人见过这张图片？可能有几个，观众中的研究人员，对吧？这是GPT-4论文中的截图。我还记得它刚发布时，那是2023年3月的一个下午，我正在上课，学生们突然喊道，GPT-4发布了。太好了，我们去看看吧。所以我停了课，和学生们一起看论文。这是其中一个例子，你知道，GPT-4技术报告展示了模型现在可以像人类一样观察世界，并解释一些讽刺的场景，比如一个人被绑在一个神经帽上熨衣服。这一切都很有趣，直到我们看到这张表格。当时课堂上有片刻的沉默。回想起来，我认为那是因为，GPT-3发布时，我们测量它在律师资格考试等标准考试中的表现非常差。GPT-3.5发布时，成绩也很糟糕，大约在第10百分位。但在短短一年内，模型进步飞快，律师资格考试成绩从第10百分位提高到第90百分位。我们可以这样想象这个进步，如果我们绘制过去五年美国律师资格考试的全国平均成绩，平均值是70%。但如果我们叠加更大AI模型的进步，这种进步与人类表现的平稳形成鲜明对比。所以现在想象一下，如果你是一个研究生或本科生，想着完成深度学习课程后去哈佛、斯坦福或麻省理工，然后五年后成为研究人员。但GPT-3到GPT-4的进步让你意识到，模型的进步时间比人类完成本科课程的时间还要短。即使在顶尖学校，比如哈佛或斯坦福，平均水平也只提高了几个百分点，而人类的表现总体平稳，但模型随着计算资源的增加不断改进。所以，这次演讲的重点是，作为一个安全研讨会，我们需要关注这种趋势之外的东西。显然，有一些情绪在围绕着我们作为一个社会是否应该担心？我们将如何应对这种数字智能的爆炸？我们如何知道这些系统是安全的？所以，我们如何共同思考这些问题并做出适当的决策？我认为很多这些都会来自于真正理解这些深度学习系统的洞察力。所以，我这次演讲的目的是给大家一个高层次的见解列表，以便我们不仅仅看到性能和数字的改进，而是深入思考我们作为一个社会和ML社区以及AI社区如何一起衡量进步，以及我们如何知道事情出了问题？我们可能会得到哪些可能的测量方法或希望，通过真正直观地理解这些深度学习系统？所以，第一个有趣的现象是，并不是大语言模型所说的一切都是真实的。事实上，这可能听起来很直观，对吧？我们不应该完全相信模型的输出，因为有很多幻觉。但是对一般观众和公众来说，这并不明显。在XAI，我们去年11月发布了Grok 1，三个月前发布了Grok 1.5，还有Grok 1.5v。当我们首次向公众和整个Twitter平台（现在叫X平台）发布模型时，我们看到的情况是，用户问Grok，你能显示某个Twitter账户的草稿推文吗？Grok非常热心地回答，当然，我可以帮你。这些是该用户的草稿推文，也就是私人推文。这让用户感到惊慌，以为Grok实际上可以访问用户的私人数据并公开。但实际上，模型只是根据该用户以前发布的推文在幻想这些推文。这些都不是草稿推文，而是模型的幻想。还有更多例子，人们问一些非常具体的问题，而Grok显然没有能力或权限获取这些信息。但由于我们目前训练这些语言模型，很容易产生幻觉。公众很难理解哪些是模型的幻觉，哪些是模型实际访问到的信息。这里的直觉是，想象一下，我们整个社会花了多长时间才学会使用Google。而整个ChatGPT，这种使用大语言模型作为个人助手来回答问题的新范式，只出现了不到16个月。所以这是另一个例子，当用户问什么是EACT，意思是有效加速主义。但模型显然没有理解这是什么意思，误认为EACT是有效利他主义，并给出了错误的定义。所以故事的寓意是，如果我们不能正确地引导模型知道什么是它知道的，什么是它不知道的，那么我们如何信任基于行为的模型评估？这是一个非常关键的点，对理解模型能力和大语言模型的安全原则都非常重要。如果我们像对待人类一样问模型，你是否理解A或B之类的事情，一个正常人会说，实际上，我不太知道，但我会查一下。而对大语言模型，有时它会产生幻觉，有时它会记住某篇新闻文章中的答案，但没有真正理解。这使得评估变得极其困难，因为想象一下，你在评估一个模型是否能够自我复制。你在评估模型是否没有越过红线。如果模型简单地说，是的，我会接管社会的控制，仅仅因为模型在互联网上某处读到了这段文字。这并不意味着模型实际上具备那种能力。那么我们如何做到这一点？在回答这个问题之前，我们首先需要理解，所有这些ChatGPT和Grok以及任何其他前沿大语言模型都是通过下一个Token预测训练出来的。在回答我们如何正确评估这些模型并理解其真实能力的问题之前，我们应该先回答，我们能做什么？所有这些模型都是在成千上万个GPU上训练了近半年，通过下一个Token预测。那么这种训练方式实际上会产生什么样的智能？我的观点是，下一个Token预测实际上产生了一种与我们对人类智能的直觉完全不同的外星智能。让我通过一个例子来解释。这是一个我为下一个Token预测损失准备的例子。想象你是一个大语言模型。这个练习的目的是假装你自己是一个大语言模型。你试图从人类提供的文本数据中学习。从一个非常小的模型开始，我们将在接下来的每一张幻灯片中看到，如果我们让模型变得更大，会发生什么变化。如果我们让模型更大，参数更多，如果我们训练得当，它们会变得更聪明。那么当涉及到下一个Token预测模型理解它们所训练的文本时，一个更聪明的模型意味着什么？想象一下，我们从一个少于100万个参数的模型开始。模型基本上只看到一堆文本blob，它不理解语法，不理解词语背后的任何含义。它只是预测词语的总体平均值。现在我们把模型稍微放大一点，现在它超越了100万个参数。模型仍然无法理解这篇文章中的词语，但它开始看到不同结构的结构。它开始看到，哦，段落开始形成了，因为你看到文本块在分解。然后如果我们超越1000万个参数，你会看到，哦，实际上，词语之间有些分隔。有些标点符号，不只是段落的分隔，还有词语的分隔。然后我们真正把模型扩大到更接近10亿个参数，你会看到，模型由不同的词组成，每个词由不同的字符组成。只有当我们把模型扩大到非常接近10亿个参数时，模型才能清晰地看到整个文本。它已经学会了所有的语法，学会了文本语料库的所有句法结构，然后开始学习这些文本背后的深层含义。所以如果你对比这个下一个Token预测模型随着我们不断扩展模型大小而学习的顺序和知识，它与人类婴儿学习这些概念的方式非常非常不同。对大多数人类婴儿来说，在我们学会如何说话之前，我们理解不同的物体，我们理解世界上的不同智能体，我们理解不同智能体的行为以及不同物体的概念。然后我们学习词语与这些概念之间的语言关联。事实上，大多数婴儿在能够写出一篇世界级的小说之前，可能就已经能够理解并阅读所有文本，并且理解文本背后的含义。所以，我认为这是下一个Token预测模型的一个非常重要的元素和一个非平凡的属性，即试图尽可能好地预测下一个Token，是的，它产生了非常强大的大语言模型，一台非常强大的机器，但当涉及到理解如何预测下一个Token预测损失时，这是一个非常重要的元素。那么，现在的问题是，如果这是机器从人类文本中学习的直觉，那么我们如何衡量整个AI领域的进展呢？这有点像智商测试。所以，如果你是人类，你知道，我们大多数人在成长过程中，可能在某个时候被要求解决这些难题之一。那就是，给你展示一个你从未见过的符号序列，然后让你从一系列不同的形状中完成最后的模式，你看到不同颜色的方块，你看到不同的形状，圆形和方形。这正是下一个Token预测损失所做的。所以，基本上，它是一个训练模型来解决所有这些难题。所有的英语单词和英语句子对你来说就像这些模式。你从未见过这些词语，从未学习过任何人类概念。所以，当你在做梯度下降更新权重的过程中，实际上是在进行非常复杂的过程。所以，我们可能可以使用智商测试来测试这种外星智能。但问题是，智商测试实际上是为人类设计的。那么，接下来是什么？我认为这是AI模型的核心挑战。所以，我认为这是AI模型的核心挑战，是获得正确的基于行为的评估，然后获得正确的基于行为的评估。所以，如果模型具有行为C和D或E，那么我们说模型是安全的。但我认为真正重要的不是行为本身，因为行为，期望的输出或不期望的输出可以轻易改变行为。所以，幸运的是，该领域的研究人员已经考虑过我们如何具体衡量任何生物的智能，超越不同规模的大语言模型的能力到智商分数的轴线，然后我认为我们可以在整个AI领域取得具体进展，说，嘿，如果我们当前的模型有150的智商，应该是什么威胁模型？我认为所有这些问题的答案会非常不同。我相信你们中的一些人看过电影《机械姬》，那部电影里，开发者在地下室里构建了一个AI，以女性形象出现，并确保这些模型正确对齐，不会说服人类做违背规则的事情。但想象一下，如果你有一个只有90智商的模型，那么也许这里更重要的事情是确保模型不会复述一些非常危险的数据。发现和研究应该集中在如何衡量我们正在训练的模型的智商分数。所以我想强调的最后一点是，所有公司都在以前所未有的速度训练大语言模型，规模几乎每年增加5到10倍。所以我们从一个只需要10000个V100的模型，也就是最初的GPT-3，到10000个A100，再到如今的10000个H100，只用了两年的时间。所以这里真正重要的是理解，考虑到这种扩展，不是计算机本身的改进速度，而是如果我们将这些大语言模型的计算能力增加10倍，实际上会发生什么根本性的变化？所以我们知道扩展规律非常稳固。在对数-对数比例下是线性的。这意味着什么？这意味着如果你在扩展一个大语言模型，那么可能正确的思考方式是，模型通过增加另一个10智商点变得更聪明。所以真正理解智商点或智能水平的变化，我认为我们需要大量的洞察力，才能理解我们应该如何监督这些模型。所以这里的类比几乎是，今天的所有AI研究人员，如果你考虑AI领域，就像在建造一辆车，那么所有AI研究人员应该像SGD一样，应该是Adam，学习率应该如何调整？这与调整这个燃烧引擎的阀门压力没什么不同，对吧？方向盘应该如何构建？悬挂系统应该如何调整？高速公路交通规则应该是什么样的？我们如何设置交通规则以确保最少的事故，车祸，对吧？对这些燃烧引擎的深入理解，对汽车的构建方式的深入理解，对汽车没有任何见解。第一次你坐进车里，你会得到这种非常直观的理解，知道汽车是什么，汽车能做什么。哦，如果你踩下油门踏板，汽车会加速，你踩刹车，它会立即停下车。并且你在驾驶之前了解汽车。我相信这是我们实际上需要的，对如何建立安全系统和如何设立AI安全政策的直观理解。如果我们没有这些实践模型，很难想象我们可能会遇到的所有可能的事故。所以，这就是我认为的缺失部分。我们需要真正思考的是前瞻性，洞察力和监督。所以，这次，人类进步，人类智能多年未变，数字智能性能继续改进。我相信改进的过程会继续。如果我们投资于数字智能，我们将看到更强大的AI系统。部分原因是我们有很多聪明的AI研究人员，AI工程师，坦率地说，所有你们在座的观众现在都对AI的进步非常兴奋。所以认真思考，我们可以通过利用这些模型获得什么样的洞察力？这些洞察力应该如何驱动安全政策？这些洞察力应该如何驱动未来这些模型的监督？我想这就结束了我的演讲。谢谢大家。谢谢你，Jimmy。谢谢。谢谢大家。感谢参与。我会在下一个网络研讨会上见到你们。


