## 智能社会论坛——智能社会与全球治理框架

![智能社会论坛封面](https://static.worldaic.com.cn/IMAGE2024/2024-06-12/5559e4724852416684af84733aba19d9.jpg)

当前，全球人工智能技术迅猛发展，为社会带来前所未有的机遇，同时也伴随着难以预料的风险和挑战。各国需要通过对话与合作，在尊重各自政策和实践差异的基础上，在国际人工智能治理领域形成广泛共识和全球治理框架。

WAIC 2024智能社会论坛由同济大学和上海市杨浦区人民政府主办，以“智能社会与全球治理框架”为主题。论坛将发布多项同济大学和杨浦区的研究和实践成果，并邀请多位国内外重磅嘉宾发表主旨演讲和进行圆桌对话，共同探讨人工智能全球治理的前沿议题，凝聚共识，为实现人工智能向善贡献智慧。

### 论坛议程

| 时间        | 主题           |
| ----------- | -------------- |
| 09:00 - 09:20 | 开幕仪式       |
| 09:20 - 09:50 | 专题发布       |
| 09:50 - 11:20 | 主旨演讲       |
| 11:20 - 11:55 | 圆桌对话       |
| 11:55 - 12:00 | 闭幕仪式       |

### 重磅嘉宾

1. **金耀初** - 西湖大学讲席教授，欧洲科学院院士。金教授在人工智能和生物信息学领域有着杰出贡献，曾多次获得国际大奖。
2. **加布里埃尔·马志尼** - 欧盟委员会《人工智能法》起草组组长。作为欧盟AI立法的关键人物，他在法律和伦理方面的见解将对全球治理框架产生深远影响。
3. **马特·希恩** - 卡内基国际和平研究院研究员。希恩研究员专注于中美科技竞争与合作，其洞见对理解全球AI动态具有重要意义。
4. **陆凯** - 耶鲁大学法学院蔡中曾中国研究中心研究员。陆研究员在法律与科技交叉领域有着深厚造诣，其研究成果广泛应用于政策制定。
5. **苏竣** - 清华大学智能社会治理研究院院长。苏教授领导的团队在AI伦理和社会影响研究方面处于领先地位。
6. **吴志强** - 同济大学中国工程院院士。吴院士在城市规划和智能城市建设方面有着丰富经验，其智慧城市理念影响深远。
7. **李仁涵** - 国家新一代人工智能治理专业委员会教授。李教授的研究专注于AI治理政策，其学术成果为政府决策提供了重要参考。
8. **季卫东** - 上海交通大学文科资深教授。季教授在法学和社会学领域享有盛誉，其跨学科研究为智能社会治理提供了新视角。
9. **吕鹏** - 中国社会科学院大学数字中国研究院执行院长。吕院长的研究聚焦于数字经济与社会治理，其成果广泛应用于国家政策。
10. **亚历克斯·罗伯茨** - 英国年利达律师事务所高级合伙人。罗伯茨律师在国际法和技术法领域有着丰富经验，其法律见解深受业内认可。
11. **钟宁桦** - 同济大学经管学院长聘教授。钟教授在企业管理和创新领域的研究为智能社会经济发展提供了重要参考。
12. **程浩** - 飞书科技互联网行业解决方案负责人。程先生在科技企业解决方案方面有着丰富实践经验，其创新成果在业内广受好评。


---

通过此次论坛，各国学者和业界精英将共同探讨和分享关于人工智能全球治理的最新研究成果和实践经验，力求在全球范围内达成更多共识，推动人工智能技术向善发展。期待在WAIC 2024智能社会论坛上，与大家共同见证这一盛事！

### 专题发布

尊敬的各位领导、各位嘉宾，大家好。今天，我将向大家发布由统计大学自主研发的IS-3基础设施数字底座（Infrastructure Smart Service System - IS-3）。现在，有请统计大学特聘教授、土木信息技术教育部工程研究中心主任李小军教授上台介绍这一成果。

各位领导、各位嘉宾，大家好。我今天介绍的内容是我们统计大学自主研发的IS-3基础设施数字底座。这一数字底座是由朱厚华院士团队历经二十余年工程数字化实践积累研发而成。它的主要目的是为了支持传统物理基础设施快速改造为数字基础设施。在这一数字基础设施之上，我们可以构建各种插拔式应用，以支持数字治理、数字经济和数字生活。

IS-3数字底座的主要特点是基于设施的信息流采集、处理、表达和分析的原理，具有通用性、专业性和生态性的三大创新特色。该系统由同济大学与上海数字城市研究院联合开发，能够将基础设施数字化，并在此基础上服务于城市的数字治理、数字经济和数字生活。

接下来，我将举几个典型案例来说明这一系统的应用。

第一个案例是上海地铁建设的风险管控。基于这一数字底座，我们可以快速将风险管理搬到线上，实现风险的实时监控和智能数据挖掘与分析。

第二个案例是城市地下空间的精细化评价。利用这一数字底座，我们可以发现城市建设中的风险，并及时规避，为城市规划提供决策支持。

第三个案例是城市韧性的评估。通过这一数字底座，我们可以模拟城市灾害并评估城市韧性，帮助城市在地震等灾害中找到存在的缺陷并进行改进。

此外，这一数字底座还可以用于高速公路的主动孪生管控，以及公园舒适性和空气质量指数的实时监测。

我们最近还使用这一数字底座，打造了一个三维实景的数字校园。这一校园结合了土木、建筑、测绘、交通、电信等多个专业，为统计大学的精细化治理和高质量人才培养提供了创新的技术支撑。

我的介绍到这里，谢谢大家。

下面发布的是爱生者和同济大学联合编制的《负责任人工智能风险管理指南》。现在有请爱生者大中华区法务及政府事务总裁高文胜和同济大学上海市人工智能社会治理协同创新中心研究员朱悦一同上台介绍成果。

为了促进人工智能向善发展，今天我们将发布由爱生者与同济大学联合编制的《负责任人工智能风险管理指南》。感谢大家的参与，非常荣幸今天可以一起发布这份指南。正如昨天发布的《人工智能全球治理上海宣言》所说，企业、研究机构、社会主体和个人要根据自身角色发挥作用，强化人工智能的监管和问责机制，促进人工智能的合规使用和责任承担。因此，今天发布这份《负责任人工智能风险管理指南》可谓恰逢其时。现在我将时间和舞台交给高文胜总裁，请他详细展开介绍。谢谢朱悦研究员。

各位好，很高兴今天有机会在这里与大家分享。首先，我们需要了解什么是负责任的人工智能。爱生者认为，负责任的人工智能不仅仅是技术的应用，它涵盖了应用设计、模型适配、使用部署和维护的全流程。负责任的人工智能不仅是技术问题，也涉及道德伦理和企业法务的合规问题，是一个系统性的问题，关系到企业每个人如何看待和应用人工智能。用好了，它可以帮助企业合规，增强对技术的信任，推动业务创新，但用不好则可能对企业和个人造成伤害。其核心是促进人机协作和信任。

了解什么是负责任的人工智能后，我们需要精准理解人工智能风险的分类和分级，并针对性地进行管理。这是指南的重要组成部分。爱生者的全球调研显示，97%的高管认为他们的企业将受到AI相关监管法规的影响，77%的高管将AI监管列为优先事项，80%的受访者认为他们将投入10%或更多的AI总预算以满足未来的监管要求。

在指南中，我们将人工智能风险的影响分为三个层面：对人的影响、对组织的影响以及对生态的影响。为了更好地管理这些风险，我们建议企业从四个维度入手：首先，确保合法合规，遵循相关法律规定；其次，明确负责任的人工智能原则，并将其嵌入到人工智能系统和开发流程中；第三，推动领导层将负责任的人工智能提升为关键业务要求，并为所有员工提供培训；最后，建立透明、健全和可持续的人工智能文化，确保每位员工都能理解和实践负责任的人工智能原则。通过这四个维度，企业可以更全面地识别和管理人工智能风险，使技术应用更加负责任。

在具体实践中，我们根据企业应用人工智能的成熟度，将企业分为三类：AI的领军探索者、AI的创新技术者和AI的实践起步者。指南中对每类企业提出了相应的建议和行动方案。例如，AI的领军探索者应全面遵守当地法律法规，积极参与制定AI合规标准，并建立完善的AI治理体制。而AI的实践起步者则应初步制定负责任的AI企业原则，并进行相应的风险评估。

女士们，先生们，人工智能的发展离不开我们的共同努力。负责任的AI不仅是企业的责任，政府也需要加强监管和创新支持。学界应该深化跨学科研究和人才培养，社会组织要激发公众的参与和监督，专业服务机构应提供专业的咨询和技术支持。通过这种多方协同治理，才能有效应对AI带来的挑战，实现科技向善的未来。最后，感谢各位的聆听和支持，让我们携手并肩，共同迎接人工智能时代的到来，推动科技向善的未来。祝福人类社会，谢谢大家。


有请统计大学党委常委、副校长吕培明和杨浦区委常委、副区长倪斌一起上台为新书揭幕。我们也祝福这些新书的出版。请各位领导入座。

最后，我们将发布《上海市杨浦区十大行业大模型应用场景需求榜单》。请看大屏幕。

2024年，人工智能发展将迈向强应用阶段，发力点逐渐从通用大模型转向行业大模型。杨浦区积极抓住新一代人工智能发展机遇，以人工智能驱动新生产力，全力打造“杨树浦”品牌。为了推动人工智能大模型赋能各行业，催生未来产业新模式、新业态，杨浦区率先发布垂直行业大模型应用场景需求榜单，向全社会招募解决方案。

面向海量影音内容的精准分析和高质量生成需求，争取文化娱乐场景大模型落地方案。
面向传统制造行业的智能化升级需求，争取智能制造场景大模型落地方案。
面向教育行业的智能化教学和个性化学习需求，争取智能教育场景大模型落地方案。
面向医疗服务行业的智能化升级转型需求，争取智慧医疗场景大模型落地方案。
面向生活服务领域的精准营销需求，争取生活服务场景大模型落地方案。
针对建筑设计与审图效率低、建筑管理成本高等问题，争取建筑设计场景大模型落地方案。
针对法律信息获取难、案件分析耗时长等问题，争取法律服务场景大模型落地方案。
面向金融领域的交互服务与风控需求，争取金融风控场景大模型落地方案。
面向经济贸易中的海量通关文件种类多、时效高等需求，争取智慧物流场景大模型落地方案。
为解决时空智能分析精确性与适应性等难点，争取智慧交通场景大模型落地方案。
杨浦区丰富的科教资源、良好的创新生态、多元的数字经济以及城市数字化转型实践，为这些广泛的应用场景提供了坚实基础。人工智能和实体经济的深度融合，将为高质量发展注入强劲动力。未来无限，期待您的加入。


---

有请金教授。各位尊敬的领导、专家、与会者，大家好！我叫金耀初，是西湖大学的人工智能讲习教授。非常高兴能有这个机会参加今天的会议。我去年11月加入西湖大学，此前在德国比勒菲特大学担任红宝人工智能的讲习教授。从2017年开始，我们从事隐私保护和安全方面的研究。去年4月，我代表德国中小型企业在柏林参加了一次关于AI Act的研讨会。今天，我将分享一些关于人工智能的见解和个人看法。

首先声明，我不是人工智能智力的专家，如有不当之处，请多多批评指正。今天的汇报内容包括：简要回顾人工智能的历史，讨论人工智能的风险及其现有技术应对方法，介绍全球主要国家尤其是欧美中三国在人工智能治理方面的法规，并对比它们的异同点，最后提出一些个人建议。

人工智能如今已经非常普及，其历史可以追溯到17世纪中期。它经历了三次高潮和两次低谷，即所谓的“人工智能冬天”。1955年，人工智能的概念被正式提出，为了1956年在美国举办的一次研讨会。人工智能在1955年提出后迅速发展，但在1970年左右经历了第一个低谷。当时，流行的人工智能模型是感知器（Perceptron），只能解决一些简单的线性可分问题。由于其局限性，人工智能进入了第一个“冬天”。

1986年，人工智能迎来了第二次高潮，主要原因是多层感知器（Multi-layer Perceptron）的提出，这种模型在最简单的感知器基础上增加了几层。包括Hinton在内的几位专家推动了这一进步。然而，1995年左右，人工智能又进入了第二个低谷。当时，统计学习方法如支持向量机（SVM）在性能上完全超越了神经网络模型。直到2007年，Hinton在期刊上发表了关于深度学习（Deep Learning）的文章，深度学习通过增加模型的层数显著提升了性能。2012年，深度学习模型AlexNet在一次竞赛中表现出色，重新引起了学术界的关注。

2016年，Google提出的算法在围棋比赛中战胜了韩国顶尖棋手，标志着人工智能的又一次重大突破。2017年，AlphaZero算法无需棋谱，通过自我博弈达到更高水平。这些进步显示了人工智能的巨大潜力。最近，ChatGPT的出现再次引起广泛关注，其对话流畅性令人惊讶，推动了大模型的发展。

今年年初的Sora视频展示了人工智能的惊人效果，但仔细观察会发现仍存在一些问题。大模型的发展让人工智能领域充满了讨论，一些人乐观地认为通用人工智能（AGI）即将到来，而另一些人则担心人工智能可能会统治或取代人类。不管怎样，人工智能的快速发展对社会经济产生了深远影响。

人工智能的主要风险包括：安全性、隐私保护、公平性、透明性和可解释性、鲁棒性和可靠性、责任性和可问责性。深度学习模型虽然强大，但也非常脆弱。例如，一个加了噪声的大熊猫图片，可能会被深度学习模型错误地识别为另一个动物。为了应对这种脆弱性，研究人员正在设计更可靠的模型。

隐私风险方面，人工智能依赖大量数据，这些数据中可能包含个人信息或企业机密信息，泄露会带来巨大风险。隐私计算技术，如独方安全计算（Secure Multi-Party Computation）、差分隐私（Differential Privacy）、加密技术（Encryption）和联邦学习（Federated Learning），可以在保护隐私的同时利用数据价值。

鲁棒性和可靠性方面，人工智能模型对噪声很敏感，技术方案包括提高数据质量和改进机器学习方法，使模型不易受噪声影响。

公平性方面，由于数据和模型训练中存在偏见，人工智能决策可能不公平。如何保证公平性是一个复杂问题，涉及文化、社会等多方面因素。

可解释性和透明度方面，人工智能的决策过程往往不透明。一个例子是聪明的汉斯马，它能做算术，但其实只是会察言观色。

为了应对上述风险，欧盟在2017年出台了GDPR，制定了许多规定。责任性方面，出问题时谁负责？用户、开发者、销售者都有可能。目前，各国政府已出台许多法规规范人工智能。最近，弗吉尼亚Tech、哈佛大学和MIT等学校发表了一篇文章，详细分类了三百多种风险，分为四大类：系统和操作风险、内容安全风险、社会风险、法律权责风险。分类目的是分析中欧美三国现有法规覆盖的风险。

例如，欧盟2017年GDPR和今年通过的法规有特殊之处。美国和中国的法规也有共同点和不同的侧重点。最后，我提出一些个人看法。首先，规范什么是人工智能的定义不清楚，特别是欧盟法规初期将许多传统技术归入人工智能，影响很大。如果法规设计太广，会影响中小企业发展。其次，应该规范人工智能的产品和应用，而不是技术。人工智能技术要分层分类，特别是高风险技术，如生成式模型、高度自主的人工智能算法、通用人工智能，这些技术带来的风险较大，需要重点规范。

今天我的分享就到这里，谢谢大家。

---

高度浓缩的时间已经不够了，我们下了以后再向金老师请教。因为我们议程做了一些调整，第二位发言的嘉宾是中国工程院院士、德国国家工程科学院外籍院士、瑞典皇家工程科学院外籍院士、同济大学原副校长吴志强教授。有请吴院士！

大家好，因为后面还有全国城市规划的专家聚集在上海，我必须要过去主持这个会议，所以我做了调整，先讲非常重要的一件事情。同济和杨浦的同学们在社会治理方面做了很多探索，我觉得特别重要。因此我为今天的会议特别准备了一个PPT报告，名为《社会智能——AI+H（哈尔主义）的突破》。

我们在2014年全面推进将城市规划导入AI技术，当时的世博会就在这块场地举行。世博会一天要接待一百万人，压力非常大，无论是密度还是温度，都是巨大的挑战。作为世博会的总规划师，我把每一张票都进行了精准定位，防止踩踏事故的发生。很多人觉得这很荒唐，但世博会场地一天要接待一百万人，相当于每平方米要容纳六个人。为了应对这种高密度，我们做了每平方米和每张票的精准模拟，最终防止了踩踏事件的发生。

世博会结束后，大家才知道这套方法被称为“大数据”（Big Data）。这包括即时的用电、能流和每一个数据的统计。中国工程院大规模支持这套系统，用于城市管理和治理。自2014年起，经过十年发展，我们解决了很多国内实际问题。

2014年，中国工程院成立了相关课题。我在2016年向总书记汇报中国应注重人工智能在社会经济各方面的推进，总书记很快批示了这项工作。包括我们的老校长万钢校长也在其中起了重要作用。中国的《人工智能发展报告》和《战略规划》在2016年底完成，并与美国和欧盟的同步推出，体现了全球同步发展。

工程院从技术和社会问题的十个方面推进工作。我在2017年全上海组织了4000人，成立了AI城市规划联盟。2018年开始正式实施，至今已有十年。我想给大家介绍一个新的概念，即“嗨主义”（H+AI），即“Human + AI”。经过十年的实践，我得出的结论是，人和AI的深度合作才是未来社会的发展方向。这是今天我要讲的内容。

嗨主义的五条原则：

人本互动的互助：所有的AI推进必须围绕人的需求展开。比如在厦门的城市规划中，我们发现人们最喜爱的是沙茶面，这在过去是无法预料的。

透明互信：AI的过程必须透明，让人们知道为什么会有这样的结果。比如我们在做外滩的水模拟推演时，要清晰解释每一步骤的原因。

安全护保：AI不仅需要算力，还需要“善力”。一把刀既可以用来杀人，也可以用来切菜，关键在于如何使用。我们需要保证AI向善。

相互之间互动和互控：人和AI必须有更多的交互过程，而不仅仅是最终结果。我们现在在做的“爱媛宇宙”项目，就是让年轻人在使用中与AI互动，共同完成创作。

论理互信：伦理和互信非常重要。我们要创新并相互之间和AI互动与互保。

总结一下，嗨主义就是人和机器的互动合作，确保安全和透明，推动社会智能化的发展。因为在上海，我特别提倡嗨主义。嗨代表Human（人类），AI代表人工智能，这是上海诞生的一个新理念。谢谢大家！

---

非常感谢吴院士的精彩发言，他的嗨主义让我们情绪高涨。他最后提出的20个字——互助、互信、互保、互动、互育，给我们留下了深刻的印象。再次感谢吴院士。接下来发言的嘉宾是来自意大利的加布里埃尔·马治尼教授。他是欧盟委员会的官员，曾担任欧盟人工智能法的起草小组组长。由于特殊原因，马治尼教授无法亲自到现场参会，所以他将通过视频发表演讲。请大家看大屏幕。

早上好，我是加布里埃尔·马治尼，欧盟人工智能法案（AI Act）的主要起草者和架构师。首先，感谢同济大学法学院邀请我参加此次会议。很遗憾我不能亲自到场，但希望我的发言能为大家的讨论提供一些帮助。

我将用大约15分钟介绍人工智能法案。首先，关于制度架构的几点说明。虽然我们现在已经进入了立法过程的尾声，但了解各方职责仍然很有帮助。实际上，欧盟人工智能法案的提案由欧盟委员会于2021年4月提出，该委员会是欧盟的执行机构。提案同时提交给议会和理事会，议会和理事会共同作为立法机构，需要就共同的法律文本达成一致，这样该文本才能成为法律。这花费了一些时间。正如我所说，提案于2021年4月发布，最终政治协议在2023年12月达成。现在我们已经就法律文本达成一致，该文本即将发布。因此，此次会议非常及时，因为法律文本将在2024年7月公布，并于2024年8月生效。

需要理解的是，立法阶段的结束并不意味着工作的结束。实际上，一个新的阶段即实施阶段即将开始。主要是欧盟法律的实施是成员国的责任。然而，有一些需要在欧盟层面采取的行动，特别是由委员会负责的指导方针、授权法令、执行法令等。由委员会通过的三级立法也会受到共同立法机构（议会和理事会）的某种程度的监督。

首先，关于欧盟人工智能法案性质的几点说明。这是一项经典的内部市场立法，旨在规范人工智能系统的市场投放和服务提供。对于熟悉欧盟产品立法的朋友们来说，人工智能法案引入了CE标志。CE标志认证某一产品（在本例中为人工智能系统）符合适用的欧盟法律。因此，采用产品立法方法与欧盟长期以来的新立法框架理念一致。法律提供了运营商需要遵守的基本法律要求，但不涉及满足这些要求的技术标准。因此，人工智能法案需要辅以一套协调标准，以实现法律要求的操作化。

另一个重要特点是人工智能法案的横向方法。这意味着人工智能法案适用于欧盟权限内的各个领域，某些领域除外，尤其是国家安全、军事和国防领域。然而，尽管人工智能法案具有横向性质，但需要考虑到某些行业的特殊性，特别是在执法等领域。因此，我们努力确保即使人工智能法案具有横向方法，也包含了行业特性。人工智能法案横向方法的另一个推论是，它不影响其他现有的欧盟法律，特别是数据保护方面的法律。大家可能都熟悉平台立法，例如《数字服务法案》或《数字市场法案》。所以，一开始我们就很明确地考虑到，人工智能法案绝不是唯一适用于人工智能的欧盟法律。

人工智能法案的一个核心概念，也许是最重要的概念，就是基于风险的方法。基于风险的方法意味着，随着人工智能系统可能带来的风险增加，人工智能法案的规定会变得更加严格。因此，重点不是规范技术本身，而是规范使用案例，即人工智能系统的具体应用。我们识别了四个风险级别，实际上应该说是三个，因为第四个（绿色）不涉及任何强制性规则。因此，我们为三个风险级别制定了强制性规则。第一级别是不可接受的风险。在这些情况下，人工智能法案预见了禁止。例如，社交评分或某些远程生物识别识别的情况。需要注意的是，委员会原本预见了两种被禁止的人工智能实践案例，而共同立法机构增加了另外四种。所以现在最终法律将有八种被禁止的人工智能实践案例。

第二个风险级别是高风险。高风险使用案例占人工智能法案的大约80-90%。我之前提到过CE标志，即欧盟人工智能法案的产品立法方法，这正是适用于高风险的法律方法。当一个系统被认为是高风险，因此需要CE标志时，该系统需要满足一系列义务，特别是人工智能的基本要求和豁免一致性评估程序。因此，制造商必须证明符合这些要求。稍后我们会看到几个高风险人工智能系统的例子。

第三个风险级别涉及那些风险主要与信息披露不足相关的情况。例如，聊天机器人案例中，人类可能无法区分他们是在与人工智能系统还是另一个人互动，或者生成内容、合成内容的形式。在这些情况下，人工智能法案预见了信息披露义务，披露人工智能系统的存在或内容是人工生成的事实。最后，如前所述，最后一个级别涉及人工智能系统风险最小或没有风险的情况，在这种情况下，人工智能法案不预见强制性规则，但提供商可以自愿申请产品。

何时一个系统被认为是高风险？人工智能法案预见了两条途径，其中某一人工智能系统可以被归类为高风险，从而受到某些严格规则的约束。第一类是安全组件的系统，这些产品已经受到欧盟法律的约束。例如，医疗设备、机械或玩具、无线电设备等的安全组件的人工智能系统。因此，我们在这里处理的产品已经受到欧盟立法的覆盖，但它们可能包含一些数字组件，包括人工智能组件。因此，某些人工智能系统成为高风险的第一种方式是，当这些人工智能系统既是这些已经受监管产品的组件，而这些组件是高风险的。因此，这个元素很重要，并非所有产品的人工智能组件都是高风险人工智能系统，只有那些履行安全功能的组件。此外，为了将系统归类为高风险，至关重要的是，整体产品（即受部门立法监管的产品，医疗设备、机械等）需要经过第三方一致性评估，以评估整体产品符合现有欧盟法律的情况。

第二类高风险人工智能系统是当人工智能系统属于立法者确定的若干特定领域时。你在幻灯片上看到的八个领域。需要注意的是，并非整个领域都是高风险，因此并非所有包含在这些领域中的人工智能系统都是高风险的。如果你查看附件，你会看到每个领域下都有具体列出的人工智能系统，因此只有在附件3中明确提到的人工智能系统才是高风险的。并且使用人工智能系统用于雇佣、晋升或解雇员工的情况。这是一个重要因素，因为委员会在其提案中已经打算在这些领域的高风险人工智能系统分类中允许一些灵活性，通过允许委员会增加使用案例。这是一个有意的决定，以使人工智能法案保持面向未来，因此随着市场和技术的发展，能够适应使用案例。

这是人工智能法案的一个非常重要的章节，特别是关于通用人工智能模型，这是在立法过程中由共同立法者添加的。委员会原本没有预见关于通用人工智能模型的任何规则。我们现在有了对这些模型的两层监管。首先，什么是通用人工智能模型？通用人工智能模型是那些有时也称为基础模型，可以执行多种任务的模型。一个例子是聊天机器人的背后模型，例如chat GPT。根据人工智能法案，这些模型将受到两种类型的监管。第一层监管适用于所有通用人工智能模型，它们将受到一系列透明度相关的规则，特别是关于技术文档的规则，因此这些模型必须记录，包括计算资源和能源消耗的相关内容。必须有从模型提供商到希望将模型应用于特定应用的下游提供商的信息传递。最后，必须符合某些版权相关规则，特别是在采用政策确保版权合规和实施详细的训练模型内容摘要方面。

第二类通用人工智能模型，特别是那些被归类为具有系统性风险的模型，将受到额外的规则，特别是关于风险评估和缓解、事件报告和网络安全方面的规则。需要理解的重要一点是，如何将模型归类为具有系统性风险，处理这些触发不同法律义务的模型。人工智能法案预见，至少在最初阶段，那些用至少一定数量的计算资源（10到25 flops）训练的模型将被归类为具有系统性风险的模型，因此需要额外的规则。然而，这不是唯一的方式，委员会还可以根据在一个索引中确定的若干标准，将模型指定为具有系统性风险的模型，而不考虑使用的计算量。顺便说一下，人工智能办公室是欧盟委员会的一部分。关于这些规则对开源模型适用性的考虑，这在某种程度上有所减少，因为开源模型不受关于技术文档和透明度规则的约束。

最后，另一个需要考虑的重要因素是未来工作的发展，委员会将需要促进这项工作，以展示对通用人工智能模型规则的合规性。关于逐步进入应用的最后一点。正如我一开始提到的，这是一个非常重要的时间，因为人工智能法案的最终版本即将在官方期刊上发布，并将在发布后20天内生效。然而，人工智能法案的生效并不意味着所有的规则立即适用。如幻灯片所示，规则的应用是分阶段进行的。在六个月后，有关被禁止的人工智能系统的规则将开始适用。第二组规则是关于通用人工智能模型的规则，这些规则将在人工智能法案生效后12个月开始适用。然后是第三个期限，为24个月，这是一般的期限。它适用于基本上所有其他规则，除了高风险人工智能系统的规则，例如医疗设备或机械。对于这些类型的产品，人工智能法案的规则将在生效后36个月适用。最后，我要感谢大家的聆听，并祝愿大家在会议上的讨论取得圆满成功。谢谢。

---

首先，有请陆凯，她的设计是这样的，这个很好。我首先要感谢同济大学和上海洋服区政府邀请我参加这个会议，并且让我有机会发言。能够在这里与大家一起努力，我感到非常荣幸。我知道你们的研究对我来说也非常特别，所以非常感谢。

今天，我想讨论的主题与刚才Matt同事提到的有些相似，但我将再深入讲一下美国和中国在人工智能生态系统（AI ecosystem）上的一些主要差异，以及这些差异为何存在。正如Matt所谈到的，这些差异在对话层面上也带来了挑战。

首先，我认为已经有一些人提到，人工智能的范畴非常广泛，它不仅仅是一个技术或政策，也不是由某个政府部门来单独管理的，它是一个生态系统。这种生态系统就像一个复杂的信息网络，包括了很多不同的方面。因此，当我们讨论中美或者中国与其他国家如何在人工智能领域进行治理和对话时，我们需要具体了解这些生态系统如何促进沟通与合作。

从美国开始，美国的联邦政府和州政府与中国有一些主要的区别。简而言之，我认为在人工智能领域基本上有三个层面。

第一个层面是基础的硬件层面，比如说半导体（Semiconductor）芯片。在这个层面，美国政府的控制力较强，比如可以管理哪些公司可以使用这些芯片，甚至限制将芯片出售给外国客户。

第二个层面是软件和模型层面。在这个层面，美国政府的权力相对模糊，取决于具体领域。例如在医疗设备中的人工智能，美国政府可以更明显地进行控制和管理，美国食品药品监督管理局（FDA）对这些设备有严格的规定。

第三个层面是内容层面，比如通过生成式人工智能（Generative AI）或大模型产生的文章和图片。在这方面，美国政府的控制力较弱。

从宏观角度看，自去年以来，美国联邦政府已经推出了越来越多的人工智能政策和概念。去年，拜登政府发布了一项新政令，要求对最大的模型进行报告，并对各个政府部门提出了两点要求：一是各部门需要考虑如何利用人工智能提高效率；二是需要更深入思考如何在各自领域内管理人工智能。

今年三月，美国管理和预算办公室（Office of Management and Budget）发布了一份备忘录，重申了联邦政府各部门的价值观。此外，还将有一份与国家安全相关的备忘录即将发布。

要研究任何国家如何治理人工智能，必须关注其制度。制度决定命运，一个国家的制度将完全影响其如何将抽象的人工智能原则和价值观落实到社会中。因此，研究美国或中国的人工智能治理，首先需要了解其基本的政治和法律制度。

在美国，实行联邦制，即联邦政府和州政府各自有权。尤其是在某些领域，如教育，许多管理职能属于州政府。教育领域的人工智能管理主要来自州政府，而不是联邦政府。

另一个特点是权力分离（separation of powers）。在人工智能治理中，一个政府部门可能会限制另一个部门的行动。例如，国会可能会通过立法，但总统可以否决，或者法院可以推翻这些立法或规定。因此，在联邦政府层面，虽然国会议员和参议员经常讨论如何管理人工智能，但真正落实这些措施面临挑战。

最近，美国在人工智能治理方面主要有两大限制：第一修正案（First Amendment）与言论自由有关，最近的最高法院判决扩大了司法部和私立公司的言论自由权；还有商务部下属的NIST和NTIA，以及美国AI安全研究所（AI Safety Institute）发布的自愿治理原则和承诺。

尽管有些人认为这些自愿原则影响力有限，但我认为它们通过企业参与，影响力实际上很大。政府部门如果将来要出台硬性监管，可能会基于这些自愿原则。

去年，拜登政府的新政令和最近美国国务院发布的政策强调了美国参与国际人工智能治理的计划，包括联合国大会的决议、与欧盟的联合声明，以及经合组织（OECD）的全球人工智能伙伴关系。这些原则与中国人工智能全球治理上海宣言中的价值观如安全、可信和可靠的人工智能是一致的。

接下来，我简要谈谈中国的人工智能治理。中国的人工智能治理生态系统同样复杂，涉及政府部门、企业和学者。中国不是联邦制国家，但在治理人工智能时，也需要通过垂直和横向的下放来落实政策和目标。垂直下放是从中央到地方政府，横向下放是从政府部门到企业和学术机构。

虽然中美两国的系统有很多重要差异，但在人工智能治理方面，必须看整个生态系统。因此，尽管中美竞争可能引发担忧，但正如Matt所建议的，信息碰撞并不一定是消极的，可能带来新的、更光明的前景。

最后，我认为中美对话中值得探讨的具体领域包括开源人工智能（open source AI）、医疗人工智能、地方政府的人工智能应用（如上海市与美国和欧洲市政府间的对话）、人工智能监管制度以及太空中的人工智能。希望通过对话，我能向大家学习，并探讨更多合作机会。谢谢。


---

非常感谢陆凯研究员的精彩演讲。接下来是我们主旨演讲环节的最后一位嘉宾，他是清华大学智能社会治理研究院院长、全国人工智能社会实验专家组组长苏俊教授。有请苏教授。

尊敬的各位专家、各位朋友，非常高兴能再次来到上海参加世界人工智能大会智能社会论坛。这个论坛非常有意义，已经连续举办了很多届。今天能再次与各位专家和朋友们围绕智能社会治理问题进行讨论，我感到非常高兴。前面的专家们都做了很精彩的报告，领导们也做了讲话并发布了一些成果。来自美国和欧盟的专家也分享了他们的见解，这对我们推动智能社会治理、做好人工智能赋能社会、促进人工智能向善发展都非常有意义。

今天我简单介绍一下自己。我是清华大学公共管理学院的学者，从事社会学和管理学研究。今天我从人文社会科学的角度，谈一谈我们如何应对智能社会带来的挑战。主要涉及四个方面的问题。习近平总书记提出要发展新制生产力。这两天在上海开世界人工智能大会，各行各业都在讨论人工智能技术如何赋能经济和社会的各个方面。大家肯定知道，我们在上海开这个世界人工智能大会，有很深的体会。人工智能推动各行各业形成新制生产力。人工智能就是新制生产力，是一种变革性技术，会带来生产力的提升。不论是生产要素还是产业各个方面，都会带来很大影响。比如现在隔壁房间讨论的巨神智能、机器人，上海正在推动的无人驾驶、脑机接口、量子通讯等技术，都会成为很大的新制生产力。科学技术的发展历史与人类社会的发展历史紧密结合在一起。

另一方面，AI技术的影响是全面而深刻的。刚才很多专家也谈到了这个问题。每次科技革命的重大突破都会带来生产力的巨大跃升和生产关系的颠覆性变化。这是两方面的，一方面带来生产力的发展，另一方面带来生产关系的变化，还会带来人的观念、认知的升华和社会结构的变化。AI技术正在重塑全球创新版图，重构人类文明的秩序，推动人类社会的智能化转型。我们正迈入智能社会，会遇到很多风险和挑战。不能只看到AI技术形成新制生产力带来的经济增长，它对社会的影响是全面的。刚才很多专家都谈到了这个问题，比如失业、贫富差距、去组织化的变化。工业社会是一个组织化的社会，我们被组织成一个个社会单元。但在智能社会里，组织的特征正在消解，量子化的个体被网络在赛博空间里重新构建。这些变化应该引起我们的关注。媒介操纵对人的认知和舆论控制带来了很多变化和风险，人的价值观和思想认识也发生了变化。在智能社会中，许多旧的观念、思想、认知正在调整。特别是青年朋友们，不知道你们有没有这样的体会，许多旧的观念、新的认知、新的文明形态、新的合作行为、新的世界观都在发生变化。你可以体会一下，这些年有没有这样的变化。社会系统的脆弱性也在变化。今天的社会越来越复杂，是由多个信息系统构建的复杂技术系统，这个系统会带来更多的黑天鹅和灰犀牛事件，潜在风险隐患更多。你会看到，今天社会上，大家熟知的一个偶像或大V的形象突然塌灭，这与此关系很大。对吧？

还有一个是智能技术对生态文明的影响，它对生态文明的理念和观念也带来新的挑战。刚才讲的五个风险和挑战，其实我在去年的论坛里也讲过，今天就简单提一下。去年专门讲过这五个问题。现在简单介绍一下我和同事们、学院正在研究的几个智能社会治理的前沿问题。应该说从学术研究的角度，从社会科学研究的角度，面临的问题很多，研究空间很大。经常说时代是粗体人，我们是打卷人。从学术研究的角度来看，我们这一代人面临许多值得研究的问题。不论是在公共管理学院还是在经济管理学院。刚才吴志强院士说的，他们规划现在都与人工智能技术完美结合在一起。许多问题都值得大家关注。最近这些年，我有一个团队在研究游戏。现在人类花很多时间在网上玩游戏。小朋友还不会说话就会玩游戏了。游戏是人类文明的新训练场和习性养成的地方。千万不要忽视这个问题，特别是新一代孩子们，他们的游戏对人的影响非常大，对吧？

简单介绍一下信息茧房和群体极化的研究。信息茧房，现在越来越多人关注这个问题。可以看到各种学术杂志都在发表与信息茧房有关的文章。在智能时代，今天的社会里，人类发明了一个非常重要的工具，叫精准推送。精准推送在工业社会是没有的。工业社会的信息传播是广播式的，电视广播是广播式的，与受众的特质无关。但今天，我们获取信息的方式与个体受众、偏好、特质、文化、学术历史直接相关。精准推送成为今天社会生活中不可或缺的一部分。它为个体提供了便捷的信息获取和分享渠道，提高了广告和营销效率。但另一方面，它加剧了信息茧房的形成。每个人获取的信息越来越被包裹在一个茧房里。不知道你们有没有这样的体会，有些人对某些问题特别了解，对某些观点特别赞成，对某些认知强烈反对或抵触。社会之间的冲突、撕裂、对抗越来越多。这个问题其实都是精准推送导致的。随着社会进程，这种现象会越来越严重，它一定会成为智能社会的顽疾。未来几十年甚至上百年，我们都离不开这种东西。影响会越来越大。为了治理好信息茧房，我们要研究它的基底，研究它的形成过程。去年我和同事们用5.7亿社交平台用户数据，首次实证呈现了信息茧房的存在。信息茧房是理论上的命题，但如何观察到就像看到黑洞并测量出黑洞一样困难。去年的研究成果首次实证观察了信息茧房的存在，并研究了它的相变机理，归纳出四个非常重要的影响要素。这个成果发表在《Nature Machine Intelligence》。大家有机会可以看一下。我想这篇文章对于信息茧房基底的研究、人工智能治理、平台治理、算法治理提供了一个非常重要的基础。信息茧房的影响是全面而深刻的。它会导致个人认知的极化，个体的极端观点在群体中激荡，导致群体极化。人以类聚，观点相同的人群越来越在一个群体里，互相激荡形成马太效应。现在的群体观点极化在社会中是一个非常严重的问题。赛博空间为极端观点的表达提供了环境。现在的长尾效应非常严重，会导致这方面的变化。

第二个研究的问题是灵活就业与劳动替代的问题。今天很多学者、国外的朋友都讲到这个问题。就业是一个很大的问题，影响很大。以前我们讨论机器人、自动化对就业的影响时，都是说它是对体力劳动、重复性劳动、非智力劳动的替代。今天生成式人工智能的影响，它的劳动替代是对创造型劳动、知识型劳动的替代，对在座各位也会产生劳动替代。大家千万不要忽视这个问题。现在它在金融、医疗、法律、文艺创作、教育行业都产生了很大的劳动替代，会对我们产生很多影响。同时它也会导致贫富差距的分化。大家现在也在讨论，AR技术的发展是缩小还是加大贫富差距？这个问题有深入讨论。劳动替代会导致新兴劳动行业的兴起，但劳动替代还会产生一个大问题，以后社会大量工作不需要人做，或者大量人不需要工作。这是一个复杂的问题。人类社会能发展到今天是因为我们与自然、与野兽搏斗中形成了组织，把生产作为第一需要，把劳动作为第一需要。以后大量人不工作，怎么度过无意义的时光？如果一个人、两个人不工作，可以去闲暇时光、旅游、文艺创作。如果整个社会都不工作，是一个复杂的问题。我一直在研究就业、失业，我觉得未来社会还应该有一种工作状态叫“游业”，不参加社会生产性的务工劳动，不以劳动作为获取经济回报的方式。这样大量人不工作但很幸福，有好的经济收入，是非常重要的。世界各地许多地方在推动全民基本收入实验，我一直非常关注这个工作。他们在做大规模社会实验，假如上海选100万人不工作，对他们进行跟踪调研，一定是非常有意义的事情。社会会非常好玩，对吧？

再介绍一个工作，就是大模型的价值观评测。由于时间原因，下面我介绍快一些。大模型的价值观评测，简而言之是把大模型看成一个认知上的人。任何一个大模型在生成知识时都有一个本底的社会价值观，这个社会价值观在应用时会产生放大作用。现在我们花很多时间研究不同模型的社会价值观。初步研究发现，我们国家备案的大模型和国外的大模型，社会价值观是不一样的。不知道以后会产生什么样的结果，但我觉得这是正常现象，就像我们每个人一样。大家都是人性本善，但发端还是有点不同，对吧？这个值得深入研究。

另外一个问题是人工智能与能源的问题。人工智能与能源相生相克。大家现在讨论的都是人工智能促进生产力的发展。我更建议企业家和人工智能技术专家研究如何用人工智能减少碳排放，提高能源效率。这个问题非常重要。我们要想尽一切办法缩小人工智能技术发展与绿色发展之间的夹角，不能让这个夹角越来越大。如果夹角越来越大，社会是失败的，这不是一个好问题。要关注这个问题。人类历史上应对技术变革的经验很多。今天大家都讲到了这些东西。时间到了不能翻了是吧？抱歉。简单说吧，一个是知识分子和社会精英应该处于人文主义精神和强烈的社会责任感，关心突破性技术的社会影响，多发声多呼吁。像爱因斯坦早期跟罗素推动的罗素爱因斯坦宣言，包括后来形成的帕格沃什科学与世界事务会议，大家可以关注一下。这个工作对今天控制核武器、把核武器关进制度笼子、保持人类社会平衡性非常有意义。今天人工智能的全球治理，我觉得要好好学习帕格沃什科学与世界事务会议的一些精神和做法。第二条，政府基于公共利益和公共价值，采用多种综合政策工具引导技术创新，有效规制风险。政府不能缺失，不能放弃立场，它们是我们进行社会价值最大化的一种方法，有很强的职能，应该多做一些工作。第三条，讲给群众的，讲给我们每一个普通人。大家要提高科学素养，增强风险意识，积极参与技术变革的治理。技术变革、人工智能的影响、人工智能的国际治理，不仅是社会精英的事情，也是政府对话、民间对话的问题。更多的是群众要觉醒，要参与，知道风险。社会群众不能在社会变革中成为沉默的大多数，应该积极参与。公共空间不能没有普通人的声音，人也不应该成为技术的附庸。今天讲智能社会、讲人文精神，这一点非常重要。当然刚才也讲到了企业的社会责任，企业履行社会职责也非常重要。在这样的背景下，我们应该为后代留下一个什么样的智能社会？当然是人文智能社会。价值理性和技术理性综合平衡，社会需要技术也需要价值理性，和谐、包容、开放的时代。这个工作得到了中央和领导人的高度关注和支持。总书记在多个场合都讲要关注未来社会的问题。2019年，我们清华大学的学者和其他高校的学者发起了一个倡议，叫开展人工智能的社会实验，探索智能社会治理的中国道路。我觉得这是一个重要问题，是世界的道路。探索智能社会治理，这个倡议得到了社会各界的广泛响应和支持。现在这个工作被写入了中央文件，中央文件里也提到了这个工作。国家市场监督总局进行标准化建设，通过标准平衡公共利益、企业利益和社会需求之间的关系。现在有专门的标准化组织在负责这方面工作。政府八部委联合建设了92个全国智能社会治理实验基地。今天杨浦区和同济大学就是基地之一，也是92个基地中做得最好的之一，受到了国家表彰。八部委最近发文对这个工作进行表扬。最后，用宋朝张载说的四句话，大家肯定知道，“为天地立心，为生民立命，为往圣继绝学，为万世开太平。”我想今天这个时代，大家要一起努力，为智能社会嵌入人文精神之心。我的介绍就这些，谢谢大家。

---

有请纪老师，请其他几位圆桌对谈嘉宾一起上台。过去两百年间，内燃机、互联网和大模型对社会产生了深远的影响。特别是进入大模型阶段后，人工智能迅速迭代，我们看到，人工智能和生命技术的革命正在对社会产生深刻的影响。这种影响的走向很难预料。一方面，人工智能赋能个人、平台和社会，带来了积极的作用，大家有目共睹。它可能使人类社会进入一个自由创新、甚至是无中生有的阶段。但另一方面，这种快速发展也让我们感到不安，存在许多风险。今天，我们非常高兴能够与五位嘉宾探讨智能社会的全球治理问题。

首先，请出第一位嘉宾李仁涵先生。他曾引领中国智能社会和人工智能治理领域，最近还出版了一本《人工智能治理与国际规则》的书。请李教授谈谈各国在人工智能治理方面的不同立场。我们需要全球治理，但各国立场各异，这些差异有哪些？在这样的情况下，我们如何加强国际合作？李教授，请讲。谢谢纪老师。

我从第一届世界人工智能大会就开始参加，一直参加到今天。此次大会让我感到特别不同，因为增加了关于治理的议题。过去，世界人工智能大会的主会场都是讨论技术的，没有关于治理的报告，而这次大会的主会场专门增加了一个治理的报告，这是我一直以来的呼吁，现在终于实现了，我感到非常开心。

第二点，让我感到不同的是，在前两届大会上，由于美国对中国的限制，几乎没有国外的专家参加，会议成了我们中国内部的讨论。很多外国专家甚至不愿意通过视频参与。这次大会，来自德国、美国等多个国家的专家参与其中，大家非常高兴也非常欢迎。技术交流是实现治理合作的基础，如果技术上不合作，治理合作就无从谈起。

回顾人工智能技术的发展，尽管我们团队包括季老师在内，一直在研究人工智能的基础理论体系，但这个体系至今尚未建立。因此，我们的研究报告显示，即使是目前世界上最强的大模型，其结论中只有60%是与人类相符的，剩下40%的结论则是相悖的。在这样的情况下，如何完全信任大模型成为了问题。美国最近提出要把AI关进笼子里，但如何建这个“笼子”需要大家共同讨论。

国际治理必须依赖技术治理和合作，苏教授提出的建设人文智能社会的目标，只有通过合作才能实现。人工智能的普遍性和无处不在性，使得它比核技术的治理更为复杂。

关于价值观的问题，在2019年我们与傅大使和国际IHPE主席的闭门会议中达成了共识，即人类的价值观是一致的，与制度无关。只要以人为本，人工智能的治理就能顺利进行。政治家们常常用价值观来进行政治操作，但实际上，我们应当把问题简单化，以人为本是人类共同的价值观。

由于时间关系，我先谈谈第一个问题：为什么我们需要治理。因为人工智能的基础理论体系尚未建立，开发者开发出的产品可能会出现出乎意料的结果，这些结果有好的也有坏的，不可控也不可信。那么我们该怎么办？

第二个问题，关于世界各国的做法。目前世界上主要有三大阵营：美国、欧盟和中国，各自的做法有利有弊，但没有谁先进谁落后。我们需要坐下来好好讨论技术标准、知识产权、法律法规以及如何共同监管。中国在人工智能领域的人才队伍是全球最大的，如果中国被排除在外，结果是难以想象的。

第三个问题是关于如何构建人工智能的国际秩序。我们前两年出版了一本书叫《人工智能与国际准则》，这是目前全世界唯一一本关于这方面的书，书中的内容至今仍然适用，可以回答季老师的问题。由于时间关系，我只能简单谈到这里，谢谢大家。

李教授多次简要说明了一个问题，其中一句令人印象深刻的话是“把AI关进笼子里”。这当然是出于安全的考虑，但另一方面，AI需要发展，它能赋能社会和经济。这时候，把AI关进笼子是否会影响其发展？对此，我想请同济大学金管学院的钟宁华教授谈谈如何平衡安全与发展之间的关系。

主持人好，各位嘉宾，大家早上好。很高兴今天能分享一下我对人工智能在社会治理和经济方面的一些看法。首先，经济的发展在很大程度上依赖于技术的进步。过去几百年里，大约有五到六次大的技术变革，每次都带来了经济的繁荣。因此，经济学中有一个康波周期理论，即每五六十年一次大的技术变革，带来全球或部分国家的经济繁荣。因此，我们非常期待这一波人工智能技术能带来新的经济繁荣。

今天讨论的主要是治理问题，所以我想谈谈我们的一些理解。经济是一个庞大且复杂的系统，其中一个重要的应用是人工智能结合大量数据，对经济进行预判和风险识别。我向大家介绍两项我们正在进行的工作。首先，我们每个月预测上海的总出口额。上海每月大约有一千亿到两千亿的出口额，我们要预测两个月后的出口情况。我们利用一百多个变量应用人工智能模型进行预测，目前误差率已经从原来的10%降至4%。这是一个很好的预测，在全国海关中表现最佳。这个例子表明，影响出口的因素很多，单靠人类判断很难，但通过训练模型可以更准确地预测，并指出主要原因是什么。

例如，我们发现上海的出口与苏南地区的货运量有很大关系。苏州到上海的货运量是一个重要因素。如果上海的出口出现大变化，我们需要考虑与苏州的关系。这是人工智能结合大量数据对宏观经济进行判断的一个例子。

另一个例子是地方债务问题，特别是承头平台。全国有两万只承头债每天在交易。我们利用三百个变量预测未来一个月内哪些承头债可能出现大幅度的价格下跌。影响承头债的因素很多，包括地方经济状况、公司情况和每日新闻舆情。通过训练模型，我们可以预判哪些地方的承头债务可能出现风险，从而提示需要关注的地方债务。这表明，在经济和金融体系中，人工智能可以通过大量数据训练模型提高准确度，并与人类判断形成互补，更准确地预判未来可能发生的事情及其影响因素。

今天的讨论中反复提到安全问题。我们担心人工智能的误用，数据对模型训练至关重要，数据的限制和隐私保护对安全也很重要。由于时间关系，我就先分享这些。

主持人提到在智能社会中算法构成一种权力，数字国家和数字人权之间的关系成为突出问题。请吕鹏先生和罗士仙律师各用两分钟简单说明一下看法。

刚才季老师提到强大的国家，我们一般称之为数字利维坦。此外，我们对资本也感到恐惧。国家和资本被视为侵犯人权或公民权的主要威胁。有两种应对方式：一是消极的，通过立法和防火墙，公司履行社会责任，科技向善；更重要的是积极的一面，通过社会创新真正维护社会权利。历史学家克兰茨伯克提出了六个科技进步的规律，其中两条是：科技不分好坏但亦非中立，发明是需求之母。社会要保护自己，组织起来参与自我权利保护。但为什么每个人都是个体，为什么要参与保护自己的权利？在业主维权中，常见的问题不是业主和业委会之间的争执，而是业主之间的争执。如何组织起来保护自己的权利？最重要的方法是社会创新，通过科技发明创造解决社会问题。例如，通过数字化手段和业主委员会的APP实现三无公开，可以解决业主和业委会、业委会和物业之间的关系。再如，快递员无法进入社区最后一公里，给快递员提供一个码，可以通过层层限制进入。因为时间关系，我简要讲这些。要真正维护每个个体和社会群体在数字时代的权利，一是通过立法和资本向善，更重要的是通过科技手段解决社会问题，保护公民权利。谢谢。


接下来，我们请美国律师罗士轩先生发表看法。谢谢，不好意思。大家好，我的中文没有卡门那么好，所以我可能用英文讲。

在今天的讨论中，有一些网民提到他们在大学期间参与审核动员等计划，并尝试理解自身的审判过程。因此，律师们在考虑如何将这些审判经验应用于新技术的遵循与公司谈论中。

在网络和网络安全方面，中国在游戏业方面已经研究了很久，但在我所在的英国，网络安全法才刚刚实施。我们看到欧盟也在集中精力推进电脑计划。因此，我认为法律在科技发展中扮演着重要角色，不仅可以帮助社会和客户，还可以从企业社会责任（CSR）的角度促进新技术和新法律的发展。非常感谢您的精彩评论。

接下来，有人提出将所有投入的三分之一用于AI安全方面，这反映了大家对AI安全的关注和担忧。然而，我认为这在实际操作中可能很难实现。我们如何衡量为什么是三分之一而不是二分之一或四分之一？如何评估哪些是AI安全问题？例如，OpenAI花费大量资金在“对齐”任务上，但这是否算是安全问题呢？这都需要定义。

作为企业或技术人员，我认为我们不必过于担心这些问题。相反，我们应该尽量让AI自由发展。举个例子，我们与许多公司合作，发现AI真正带来效果的地方不是发展太快，而是担心发展太慢。比如，一个有1000人的公司使用AI进行翻译，每年可以节省76万元。

此外，在字节跳动这样的大公司中，尽管有十几万人，但只有几十个人在7乘24小时的服务大家，因为很多知识问答已经由AI完成。这种AI问答在各个公司和组织中都能看到，比如常见的问题如工资发放、社保缴纳等。

我们测算过，一个800人的科技公司，一个法务部门通过AI解答问题，每年可以节省30万元。这种节省不仅体现在法务部门，还有HR和行政部门。因此，我们认为AI目前并不会对我们构成威胁，相反，它可以替代重复性工作，提高整体工作的幸福感。

总结来说，我们认为AI的发展并不是坏事，它可以带来更多的福祉。关于企业社会创新，我一直提倡企业将技术和能力应用于社会场景，解决实际问题，这才是最可持续的发展方式。

最后，我们请李仁涵教授用一分钟总结人工智能治理的未来发展方向。李教授在朋友圈中提到，当前不需要将AI关进笼子里，但未来可能需要。这也是我今天的感受。谢谢大家。感谢五位对话嘉宾的精彩讨论。我们的讨论到此结束。再次感谢各位领导和嘉宾的参与。论坛的全部议程到此结束。谢谢。
