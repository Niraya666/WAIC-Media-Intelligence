# 模型即服务（MaaS）加速大模型应用落地

![封面图片](https://static.worldaic.com.cn/IMAGE2024/2024-06-21/b99d55506ab24e13910fe9ccfe49382c.png)

在大模型等人工智能技术飞速发展的今天，模型即服务（MaaS, Model as a Service）正逐步成为驱动行业创新与数字化转型的关键力量。本论坛汇聚了大模型的前沿探索者、开发者、技术领袖以及创新产业代表，从云+AI，多模态，GPU芯片推理加速以及产业落地展开，旨在深入探讨MaaS加速大语言模型在各行各业的应用与落地。

### 嘉宾阵容

阿里云首席技术官周靖人，清华大学计算机系长聘副教授，面壁智能首席科学家刘知远，NVIDIA开发与技术部门亚太区总

监李曦鹏，联想集团研发总监杜杨洲，小米科技小爱大模型负责人乔国辉等国内外大模型领域重磅嘉宾现场亮相，共话大模型的实践与未来！

### 日程安排

#### 09:30 - 09:35
**主持人开场**

#### 09:35 - 09:40
**产学研用携手，共促大模型技术落地**

#### 09:40 - 10:00
**以云为基础、以模型为中心，构建大模型生态**

#### 10:00 - 10:20
**通往AGI，大模型时代的摩尔定律**

#### 10:20 - 10:40
**大语言模型时代的加速计算技术更新**

#### 10:40 - 11:00
**联想AIPC创新实践**

#### 11:00 - 11:20
**大模型时代小爱同学的实践历程**

#### 11:20 - 11:30
**项目发布：“追星星的AI”——关怀孤独症儿童公益绘本活动**

#### 11:30 - 12:00
**互惠共享，大模型技术生态繁荣的必选项**

### 参会嘉宾

- **周靖人** - 阿里巴巴，首席技术官。周靖人在大模型领域有着丰富的经验和卓越的成就，领导了多个具有行业影响力的项目。
- **成晨** - 阿里云，魔搭社区技术运营负责人。成晨致力于推动大模型在社区中的应用和普及。
- **程熙** - 阿里云，无影事业部产品总监。程熙负责的无影事业部在大模型技术的应用方面取得了显著成果。
- **杜杨洲** - 联想集团，研发总监。杜杨洲领导的团队在大模型创新应用上有着重要贡献。
- **贾超** - 面壁智能，副总裁。贾超在推动大模型技术商业化应用方面有着丰富的经验。
- **李曦鹏** - NVIDIA，开发与技术部门亚太区总监。李曦鹏在大模型的硬件加速和技术创新方面有着重要影响。
- **林俊旸** - 阿里巴巴，高级算法专家。林俊旸在大模型算法研究方面有着深厚的造诣。
- **刘知远** - 清华大学，教授。刘知远在大模型理论研究和教学方面有着突出的贡献。
- **乔国辉** - 小米，小米小爱大语言模型负责人。乔国辉领导的小爱大模型项目在用户体验上取得了显著进展。
- **张松阳** - 上海人工智能实验室，青年研究员。张松阳在大模型的前沿研究上展现出极大的潜力。
- **周文猛** - 阿里巴巴，魔搭社区模型服务负责人。周文猛在模型服务的推广和应用方面表现卓越。


### 阿里云：让AI创新触手可及

感谢汤主任的精彩致辞。接下来我们将进入今天的演讲环节。首先有请我们的首位演讲嘉宾，阿里云首席技术官周靖仁先生。作为阿里云的技术领军人物，周靖仁先生长期致力于推动AI技术的前沿发展和应用创新，并且是国内最早提出MaaS（模型即服务）概念的先驱者。在今天的演讲中，他将围绕通一大模型能力、阿里云百链等，深入探讨阿里云在MaaS体系内的最新进展与思考。让我们欢迎周靖仁先生带来主题演讲《阿里云：让AI创新触手可及》。

大家早上好，非常高兴再次与大家见面，和大家分享阿里云在MaaS服务和AI模型应用方面的最新进展。在当前由AI引领的技术变革中，技术发展迅猛，创新日新月异。随着大模型的广泛应用和研发突破，产业界的应用也在快速迭代。大模型的应用遍及各行各业，这进一步强调了模型即服务的概念的重要性。我们要以模型为中心，推动技术体系和业务的发展。

两年前，我们首次提出了Model as a Service（模型即服务）概念。我们很高兴看到，这一概念在短时间内得到了全行业的认可。今天，Model as a Service已成为云上服务和AI领域发展的重要技术。Model as a Service涉及多层次概念，包括云计算支持、AI支持、模型生态发展和开发者环境演进。最终，模型还需要适配各自的业务场景，这也是Model as a Service的一个重要环节。今天，我们将分享阿里云在Model as a Service方面的一系列进展。

首先谈模型层面。过去一年，大模型领域发展迅猛。阿里云推出的通益签问系列和其他通益模型均在快速发展。去年4月，我们发布了通益签问，随后推出了万象和听悟等模型。阿里云在模型创新方面持续发力，模型能力越来越强。感谢大家，特别是开发者和企业的支持。今天，我们有上亿次调用量，开源模型下载量已超过2000万。

除了研发提升模型能力，我们还坚持开源策略，旨在将模型能力带给开发者和企业，降低使用门槛。我们认为，只有这样才能促进AI产业的发展和变革，降低开发门槛，推动产业界发展。阿里云一直坚持开源战略，我们也是全球几乎唯一一家既做模型研发又开源全系列模型的公司。借助云技术支持和开源模型矩阵，我们将AI能力赋能给开发者和企业。

过去一年，通益开源模型家族已形成矩阵，涉及各种尺寸和模态。尤其是几周前发布的新一代开源模型Coin2，短短几周内获得业界好评，Hugging Face认为其是全球最强开源模型，在多个海外榜单上居首。在中文领域，Compass Arena也将通益签问模型评为第一。这一代开源模型在评测指标上大幅提升，超过很多闭源模型，在全球处于领先地位。我们在代码推理和逻辑思考方面也有显著提升。模型发布以来，受到开发者和企业的广泛关注，我们希望继续推进模型在各个方面的应用。

除了通益签问模型，我们还推动通益灵码。通益灵码基于通益签问模型，面向编程助手。昨天晚上，通益灵码获得本次大会的政店之宝奖，这是对项目的重大肯定。结合基础语言模型能力，通益灵码在代码补全、代码理解和代码问答等方面帮助开发者。通益灵码集成各种开发框架，包括IDE，方便开发者快速使用。通益灵码发展迅速，已成为国内用户规模最全面的产品，下载量超过350万，每天推荐量达几千万次，得到开发者积极反馈。

面向开发者，我们也将通益灵码推向企业。企业有自己的一系列代码和数据安全考虑，我们在绝对安全情况下帮助企业梳理代码，并结合通益灵码智能化能力和本地代码库，让模型理解本地开发逻辑和编程习惯，更精准地推送针对企业特殊场景的代码助手功能。通益灵码企业版自推出以来，受到一七哈罗和中华财险等企业的关注和使用。

通一APP也在不断迭代发展，已成为全能AI助手，不论在手机上进行知识问答、常规搜索、文件处理，还是在PC端进行详细的工作学习规划，通一APP都集成了通益、听悟和万象的能力，成为日常工作、学习、生活的全能AI助手。希望大家有时间下载通一APP体验通益模型的各方面。

通益模型持续开源的同时，我们还积极运作AI社区。摩大社区快速发展，已有560万开发者，承载了中国和海外一系列重要模型。摩大项目也受到开发者的积极参与，这次获得了CL之星奖。

当模型应用到各行各业时，需要特别适配。如何释放模型能力并理解各行各业需求，是我们要解决的重要技术问题。我们推出了阿里云百链产品，百链名称取自“千锤百链”，希望借助云和模型能力，结合企业专属信息，更好地适配模型应用。百链集成了所有模型生态，包括通益模型家族、开源模型和其他模型公司，提供全方位选择，企业可以根据需求选择合适的模型类型和尺寸，权衡业务效果和推理成本。

过去我们不断降低大模型使用成本，提升云推理架构，提升算力汇聚，提供极致性能。过去几个月，推理价格大幅下降，我们希望每位开发者和企业都能以高性价比使用大模型。过去一个月，模型使用量成倍增加，各行各业尝试将AI能力融合业务体系。

有了丰富的模型提供和强有力的推理架构，还需要以模型为中心的开发环境。我们以API方式简化模型使用流程，并需要有一个流程帮助企业和开发者进行模型定制。百链解决了这一业务需求，从正式发布到现在，客户数量大幅增加，已超过20万。为了帮助业务场景适配，使用模型时首先是提示工程（prompting

 engineering），提供提示模板，让企业快速选择适合的业务场景模板。我们理解提示工程繁琐，希望通过通益模型能力简化提示工程，以元提示（metaprompting）方式描述需求，让模型自动优化提示，提升业务效果。

如何将企业知识和模型融合，首先需要解决数据安全问题，同时增强知识学习。百链技术方向支持各种RAG框架，特别是流行开源框架，开发者快速上手，提供安全数据域，让企业分享数据让模型学习，同时确保数据在客户专属域内，保证数据安全。简化流程，任何框架包括RAG体系，几行代码快速融合企业知识和专属信息与模型能力。提供高效、低延迟、高并发的RAG，降低开发复杂度，优化算法效果，自动调优。这些为企业将行业知识融合大模型提供坚实基础。

从提示工程到插件使用，再到增加功能如memory，尽管有工具简化开发复杂度，我们希望有system API方式整合插件工具管理，通过system API使用插件，编排提示工程和memory管理，简化应用开发，提供一站式体验。

最后，除了模型的RAG和提示工程，一些特殊场景需要模型再训练和微调。我们提供完整工具，从数据提供、模型微调架构支持到评审模型，帮助开发者有效评价微调模型在各维度表现，快速基于百链模型底座进行二次开发和评价，推动业务落地。

上述内容是百链技术环节的一部分，希望大家会后访问阿里云官网体验和使用百链，利用百链做各种模型定制。最近我们看到很多企业在海外拓展业务，我们高兴地通知大家，百链产品部署在海外，为企业提供全方位支持。在海外，百链产品叫Model Studio，通过阿里云全球基础设施布局，提供全面的模型支持和服务，让大家在出海过程中高枕无忧。

在短短的20分钟内，我汇报了阿里云在MaaS方面的各个进展，从开源模型的快速发展和最新版本推出，到摩大和百链产品帮助产业界落地。我们认为在这个时代，只有做到AI和云的高度协同，一方面支持模型发展和研发，更重要的是基于模型帮助各行各业以低门槛方式使用和结合自身场景知识，解决真实业务场景。只有这样才能推动AI产业发展。我们期待与企业和开发者一同发展，加速大模型应用的产业落地。谢谢大家。



### 大模型时代的摩尔定律：迈向更高效的大模型时代

再次感谢周静仁先生的精彩演讲，并期待白链和摩达社区为大模型生态贡献更多力量。在芯片的发展过程中，有一个著名的法则叫做摩尔定律。摩尔定律指出，每隔大约18至24个月，单位面积上的晶体管数量将翻一倍，从而大幅提升芯片性能，并相对降低成本。这一规律在过去几十年间深刻影响了半导体行业，推动了计算机技术的快速迭代和性能飞跃。类似地，大模型也遵循着类似规律。接下来，有请清华大学计算机系常聘副教授、面壁智能首席科学家刘志远先生为大家带来演讲“大模型时代的摩尔定律：迈向更高效的大模型时代”。掌声欢迎。

大家上午好！非常荣幸有这个机会在此与大家分享我们在大模型领域的探索、思考以及对未来的预判。我们希望与大家分享我们对大模型未来发展的看法。可能大家对面壁智能还不太熟悉，但在过去的一年里，我们发布的一系列端侧大模型，如miniCPM系列，已经让大家有所了解。这些模型的最大特点是能够在计算机和手机等端侧设备上运行。

我们为什么在过去的一年里选择专注于端侧大模型呢？今天我们想与大家分享我们的相关思考。我们认为，所有这些端侧模型背后都有一个更具挑战性的问题：如何在有限的算力、内存和能耗条件下，极致地将知识浓缩到更小的参数规模中。这一过程需要我们对大模型建设进行更加科学化的探索。

为此，我们构建了一个模型沙盒。在训练模型之前，我们在沙盒中进行成千上百次的演练，通过小模型高效地寻找最优的数据和超参数配置，并成功地应用到大模型上，从而找到大模型的最优配置。通过这种方法，我们可以找到更高的知识密度，从而带来更高效的模型。比如，我们在2024年2月发布的Mini-CPM的第一个版本2.4B模型（24亿参数），其能力已经超过了像Mixtral 7B和Lama 2 13B等参数规模更大的模型。这体现了我们在大模型科学化道路上的重要价值。

正如主持人提到的，我们在过去80多年里见证了芯片制程的不断增强带来了终端算力的持续提升，使得我们在手机上拥有几十年前几间屋子才能装下的计算能力。同时，我们也看到大模型的科学化进程将不断提升模型的制程，带来模型知识密度的持续增强。这两者的交汇将揭示端侧智能的巨大潜力。从右图中可以看到，红色虚线代表的是多模态模型从GP4V数百亿参数规模到今年5月发布的miniCPM-V2.5模型，只用了8B参数就能完成相关能力，体现了多模态领域知识持续增强的过程。

同时，蓝色虚线体现了在端侧算力持续增强的情况下，我们可以在端侧放置更大更强的大模型。这两条曲线的交汇意味着，在半年到一年内，我们可以将GP3.5水平的模型能力放到端侧运行，而在未来两年内可以将GP4的能力放到端侧运行。这一点非常重要。

因此，我们认为大模型时代将拥有自己的摩尔定律，即知识密度的持续增强。回顾摩尔定律，1965年提出时的思路是芯片上的电路密度每两年（后来修正为18个月）提升一倍。对于大模型来说，OPI已经验证了规模法则，即过去五年大模型规模越大，其能力越强。但我们是否要无限制地将模型做得更大呢？我们认为大模型的本质不是大，而是其知识密度和制程。

在过去五年里，我们见证了2020年6月OPI发布的GP3水平能力，现在在2024年2月，我们的miniCPM2.4B模型已经达到了相同的水平。模型的知识密度大致每八个月提升一倍。未来一个重要的使命是让这个规律持续下去，这需要在数据、模型架构和算法等方面不断探索科学化道路，极致地提升模型的制程。

高效大模型的第一性原理关键词是知识密度。知识密度对应的是能力，而能力依托的参数规模和计算消耗。知识密度越强的模型，意味着能力越强，每次计算需要的参数规模越小。这两个要素组成了知识密度的概念。大模型的数据驱动技术方向已经确定，但模型架构、算法和数据方案仍在快速迭代。未来的重要使命是持续改进模型的制程，快速迭代相关技术方案，极致提升模型的知识密度。

今天我们要给大家带来最新的进展。我们发布了一个新版本的高效大模型架构MiniCPM-S，这个版本是1.2B模型，体现了高效的稀疏架构，端侧能耗降低，推理速度提高近三倍。我们非常感谢上海交大团队推出的PowerInfo推理框架。

MiniCPM-S体现了在架构、算法和数据三个方面的提升。它的架构受人脑功能分区和稀疏激活启发，采用ReLU激活函数，使模型自发涌现更强的稀疏激活机制，从而在训练过程中逐渐稀疏，保证训练效果不受损失。MiniCPM-S将极大增强大模型的知识密度，使模型能力在相同规模下更强。

我们还推出了MobileCPM套件，帮助开发者一键开发端侧大模型应用，预装了许多通用端侧模型和相关插件平台。通过这种方式，我们可以让开发者低门槛地开发端侧大模型应用。MobileCPM的架构包括基础SDK套件和预装的Intent平台。

总之，我们今天想分享的是MiniCPM-S的知识密度持续增强的模型，以及MobileCPM帮助大家集成端侧模型开发的工具。我们展示了端侧模型在飞行模式下运行的demo，展示了情感陪伴等基本文本应用的快速运行。这揭示了端侧大模型应用的威力。未来大模型将是云端协同模式，既有云上的System2大模型，也有端侧的System1大模型。端侧模型可以快速响应，保护用户隐私数据。

我们希望通过大家的努力，推动端侧模型研发和推理加速，形成端侧生态，挖掘端侧算力的潜力。我们将MobileCPM和MiniCPM-S开源，并在GateHub上公测。最近大家关注的WWDC2024年Apple Intelligence概念，包含了端云协同方案，但需到明年才能见面。我们希望通过MobileCPM和MiniCPM-S的努力，让大家更快见证端侧大模型应用。

未来大模型是云端协同模式。我们希望建设最强的端侧模型，推动大模型知识密度定律的持续推进，以更快、更简单、更低成本迈向高效大模型时代。以上是我的分享，谢谢大家。


### 小爱同学的实践历程

感谢杜洋洲先生的精彩演讲。希望未来有更多业界伙伴与联想合作，共同推进混合式AI技术的发展，推动AIPC的不断进步。当然，除了个人电脑之外，AI智能助手也在手机、汽车、机器人等物联网设备中逐渐普及。大模型技术的引入，使得终端智能助手不仅能回答简单问题，还能在视觉识别、实时翻译、图像生成等领域提供更智能的服务。接下来，请小米公司大模型产品负责人乔国辉先生带来《小爱同学的实践历程》主题演讲，介绍小爱同学如何通过软硬件的深度结合，提供全场景、多模态的深度体验。让我们热烈欢迎乔国辉先生上台。

大家好，我是小米公司小爱同学团队的乔国辉。今天很荣幸能与大家分享小爱同学在大模型方面的实践历程。首先简单介绍一下小爱同学。小爱同学是小米的人工智能助手，名字来源于“小AI”，我们在2016年开始研发AI功能，经过8年的发展，已经覆盖了小米的各类硬件设备，包括汽车、手机、AIoT机器人和可穿戴设备等。小爱同学是软硬件深度结合的场景，实现这一功能并不容易。除了在云端理解用户外，我们还需与硬件进行深度结合，如硬件的声学链路、前端语音识别等，需要针对硬件进行优化和调整，才能实现理想的用户体验。大家熟知的小爱同学主要是语音助手，但实际上我们还提供了许多其他系统服务，包括主动建议、场景建议、小爱视觉、小爱翻译和小爱通话等。未来我们还会推出更多功能。

接下来介绍一下小爱同学的技术架构。最上层是设备层，左边是内置小爱的设备，用户通过这些设备与小爱互动，获得相应的内容和服务。右边是受控设备，通过米家接入的IoT设备，用户可以通过小爱同学将主控设备和受控设备连接起来。中间层负责用户请求的处理，当用户发出请求时，小爱同学会理解用户意图，感知环境和信息，最后提供相应的功能和服务。我们的服务大致包括设备控制，这是用户使用最多的场景，还有基础工具如闹钟、计算器，信息查询、内容服务以及场景互动对话等。截至目前，小爱同学已搭载了54类主控设备，月活跃用户达1.3亿，每天的请求次数达到2亿。我们在2022年10月开始落实大模型相关功能，经过多次打磨，在2023年8月首次进行内测，获得了用户的广泛好评。今年3月，我们在汽车上正式上线了小爱的大模型功能，针对汽车进行了许多优化。可以看到，大模型的引入显著提升了用户体验。以前无法实现或效果不佳的功能，现在通过大模型得到了大幅提升，特别是在知识问答和闲聊对话等中长尾请求方面。我们过去的满意度大约只有30%，而现在经过优化后提升到了80%以上，且随着模型能力的提高，还在持续优化。大模型能力也显著提高了活跃用户的留存率，大约提升了10%。做过互联网产品的同事都知道，留存率每提高5%都是非常大的提升。大模型技术给小爱带来了跨越式的升级，主要体现在三个方面。首先是通用对话，通过大模型更好地回答用户问题。其次是在垂直领域的场景，我们做了许多功能优化。例如小米商品助手，用户可以询问小米设备参数、使用指南和故障处理等。过去通过人工训练的数据QA无法很好地满足用户需求，但有了大模型之后，通过RAG技术和大模型的检索匹配，我们很好地解决了这些问题。第三类是NLP任务，小爱需要理解用户的意图。过去每个任务需要构建大量训练数据，产品经理针对每个任务场景做相应的策略。而现在通过大模型的通用知识，结合少量数据进行训练，我们就能很好地解决这些场景。

总结来说，大模型之后，小爱的技术架构大致如下。用户请求到来后，首先通过大模型进行简单的意图理解。任务分为四类：第一类是简单的工具或控制任务，意图分发到任务场景后，通过小模型直接执行操作。第二类是内容类场景，如音乐和视频，这需要大量数据和标签，依赖传统搜索和推荐。第三类是生成类场景，以前小爱无法很好实现的场景，现在表现非常好。第四类是知识类场景，用户需要准确的答案，这对大模型的通用能力是极大的挑战。虽然行业内最好的模型在这类问题上仍存在很多问题，但我们相信，通过更多参数和训练，能让这些场景变得更好。

在选择大模型时，我们与其他开发者相似，主要步骤包括：建设业务需求的评测体系，选择适合的模型，以及针对业务需求进行模型优化和微调。在效果上，我们搭建了九大类四十多个评测体系，很多云厂商在初期合作时，我们都会提供评测结果和同行对比。性能上，用户请求后需要快速得到结果，因此我们对模型实验速度也有要求。我们基于小爱的知识和不同大模型构建了混合体系，遍历了行业内所有大模型，找到了适合每个场景的模型。

大模型能力越来越强，开始在智能座舱、智能手机和智能家居中落地。小爱除了传统语音交互，还在手机上做了多模态应用场景，如图片问答、图片编辑等。控制IoT是常用场景，但需要理解用户家庭房间和设备信息，通过大模型和agent技术，我们更好地控制智能家居。小米商品助手通过大模型更好回答用户问题，也结合小米输入法提供智能回复帮助，如在微信中帮助用户写更高情商的回复。

今年的汽车发布会上，我们发布了许多大模型功能，如“走哪问哪”，问地理位置的问答，“前面的车是什么”，以及汽车说明书相关功能。准备了一个一分钟短视频，展示这些功能应用。

未来规划方面，大模型能力越来越强，可以做更多复杂任务，准确率也高。AI Agent通过大模型做复杂任务满足用户需求，但准确率仍是挑战。希望与大模型一起提升技术，通过Agent做更多复杂任务。多模态场景在手机和车载上受欢迎，将持续推出新功能。OS深度整合，提供更多底层系统控制，未来可能称为AI手机。端侧大模型是重点方向，解决用户数据无法上传或隐私问题，如手机信息检索和操作、车载无网或弱网情况。关注效果和成本，效果包括通用能力和定制化需求。感谢阿里云在小爱场景中的优化，未来期待大模型能力提升，帮助开发者优化效果，提供更多丰富内容。

以上就是我今天的分享内容，谢谢大家。


### 圆桌讨论: 开源开放、互惠共享——大模型技术生态的必选项’

接下来，让我们进入本场论坛的最后一个环节，圆桌讨论。随着大模型生态的不断完善，越来越多的大模型走向了开源开放的道路。我们相信，开源开放、互惠共享将是未来大模型技术生态繁荣的必然选择。本次圆桌对话的主题是‘开源开放、互惠共享——大模型技术生态的必选项’。

让我们欢迎圆桌主持人摩达社区技术运营负责人程程和各位嘉宾上台。他们是：阿里云无影事业部产品总监程希，面壁智能副总裁、OpenBNB社区联合发起人贾超，通议实验室通议签问开源负责人林俊阳，上海人工智能实验室研究员张松阳，以及摩达社区模型服务负责人张文蒙。让我们以热烈的掌声欢迎各位嘉宾上台。欢迎大家。现在我们进入圆桌讨论环节。今天我们有做大模型的，有评测大模型的，有开源社区的，还有产业落地的同学。我们首先讨论的主题是‘这一年中国大模型的突飞猛进’。我想先问一下签问的俊阳同学。去年七月份，我们第一次讨论签问开源的事情。我记得第一代是7B模型。在开源社区里，7B是一个很好的模型，但在整个大模型领域并不是特别出众。然而，在过去一年中，签问的迭代速度非常快，每次发布都有显著进步，甚至现在已经达到了全球领先的水平。这其中有哪些关键点？

谢谢程程的提问，对签问也有很多褒奖。确实，当时我们发布了7B模型。开源社区长期以来落后于闭源大模型。因此，我们在研发大模型的过程中，不断收集开源社区开发者的反馈，持续优化我们的模型。后来我们发布了14B和72B模型，这并不是简单地扩大模型规模，而是对数据进行了大量迭代。我们开源了1.5系列，并且得到了许多学界朋友的参与和建议，帮助我们优化模型。因此，我们才能走到今天这一步。如果要感谢大家，还是要感谢众多开发者和用户的支持，让我们的模型迭代速度如此之快。可以说，我们在一定程度上赶上了OpenAI。

谢谢俊阳。接下来，我想问一下贾超。我们知道，miniCPM在端测大模型和多模态视觉模型中，专注于多模态中小模型场景，并取得了非常好的效果。今天刘老师提到了知识密度这个概念。miniCPM前段时间也被全球领先的大学借鉴。今年，很多人提到第一性原则是scaling low。为什么面壁坚持做端测大模型，追求小而美的模型？

是这样的，刚才提到的scaling low和知识密度的极致体现。面壁从2020年开始训大模型，到2023年GPT爆发，我们训练了许多百亿、千亿规模的大模型，但发现千亿规模大模型推理成本巨大。当用户量达到千万级别时，GPU成本非常高。清华团队和面壁团队在去年年中开始scaling low相关工作，做了大量小规模模型实验，得出最优参数配比。我们还创新性地提出了新的训练方法，加上数据层面的新一代升级，支撑了我们的模型。去年年底，我们坚信，通过训练两亿参数规模的模型，可以比肩十亿规模的模型。这样我们可以在很多场景中替换大规模模型。这也让端测具有足够的想象力。今年二月发布的小钢炮受到广泛关注和认可。国外类似事件更重要的是对我们模型的认可，对中国技术的认可。实际上，五百美金不可能迅出这样的模型。

了解，我想继续追问一下，前段时间非常火的miniCPM2.5V模型，是基于Lama3基础上训练的，效果很好。未来有没有考虑过和中国厉害的大模型合作，比如说签问？

我们当时选Lama3来训练，因为效果好，全球火。当时选这个模型，在下一代模型没出来之前。以后，我们也会考虑和国内比如说签问联手搞联动，这非常好。对国外用Lama3，但国内也有很好的模型。面壁会持续在端侧，刘老师讲了大模型摩尔定律，我们会在更小尺寸上训更好的模型做多模态基座，这些都会同步做，未来几个月逐步发出来。

好的，刚刚聊到了大模型的发展，Open Compass提到一个理论，松阳分享时我印象深刻。过去一年，闭源大模型能力上升和开源大模型能力上升，显然开源大模型更强。Open Compass在大模型客观评价上做了很多工作，大模型的发展离不开评测。即将有多模态榜单，如何客观公正评价大模型，包括如何保障Benchmark防止刷榜公允性，Arena如何覆盖肠胃问题，以及未来垂直行业的评价，松阳分享一下大模型评测的下一步想法。

OK，谢谢。我自我介绍一下，我是上海工程实验室的张松阳。我们做的项目叫Open Compass，主要是模型评测。很多做模型的同学和老师可能很了解，我们定期发布榜单。我分享几个观点。第一个，评测是前置的，无论模型研发还是应用研发都要提前做。如何构建满足预期的评测体系，每家业务属性或场景不同。比如千问、Lama、英特尔做通用大模型，关心通用能力。从这个角度，大家面临很多基础问题的评测和构建。但对于行业，需要快速高效转换行业知识成行业评测集的方法论或工具。这块在学术界和工业界实践都不多，大家还是手工构造一些场景评测集。如果大模型进到更多业务或行业，需要一定支持，比如能不能构建agent工作流，帮助快速构建高精度行业评测集，指挥下游应用研发。

第二，模型刷榜或能力准确评估，大家现在评模型逻辑是折中，通过benchmark分数更好表达模型能力。通过arena、人工评价，各种随机因素难有特别公允的判断。产品或大模型用户量、调用量、业务量是一方面指标，但对模型研发需要客观评测集或榜单推动。榜单很多时候是保模型的下限，榜单成绩不能太差，否则基础能力不达标，很多应用做不好。但不能为了榜单成绩排第一名第二名，做各种奇怪操作，这也不是评测研究机构希望给大家的良性反馈和公允评价。包括能力更细度的分析。

对于刷榜，我们过去也遇到过，有些模型机构通过手段刷榜。像安全对抗一样，评测上也有能力对抗，评测体系不断演化，有新的评测机构和学术界每天有新论文，构造新高质量评测集。我们研究自动化评测算法，比如抓取最新新闻、最新学术论文，构建成评测集一些大模型还没训练到的语料。这也是很大挑战，很多大厂训练模型每天海量数据回来。

最后，多模态问题，多模态评测现在还是初期，大家关心榜单指标。大模型已到产品应用或用户体验提升上，多模态需要学术界和产业界推进。比如miniCPM性能很强的模型，我们希望多模态模型在客观指标上的任务型指标体现优势，也在日常自然对话场景下体现更好。主要是这几个点，谢谢。

谢谢，其实大模型的发展，我们看到有两个主流方向，一个是开发者生态，一个是产业应用。今天听到面壁在端测大模型的发展工作，我们也想问无影，在大模型时代如何通过云端结合推动大模型在产业场景落地？我们来问一下光昭。

这是一个很有趣的问题，之前也困扰无影一段时间。众所周知，包括这场论坛和刚才讨论的，端测作为集数据、用户行为、用户交互和用户应用为一体的平台，拥抱AI和结合AI非常好的点。我们看到无论是英特尔和AMD的NPU，端测小模型，还是产业化后的AIPCAI手机，都是业界炙手可热的趋势。作为云端一体的云电脑或云手机的无影产品，我分享我们的Chain of Thought。因为我是本场唯一的产品经理。首先，我们分析用户在无影场景中可能用到AI的两个主流大场景。

第一个场景是生产力场景，细分为两个子场景。一个是生产力提高场景，包括现在很多Copilot或Agent做的文字优化处理、表格优化处理、翻译等场景。第二块是提升管理效率场景，比如远程智能化运维、智能化故障检测等，这是2B的生产力场景。

第二个场景是个人和娱乐场景，也分为两个子场景。一个是降低用户对多端侧设备的影响，比如语音控制、语音用户行为自然语言操作等，包括快速设备使用问答。第二块是提升用户使用趣味性、增加用户粘性，比如个人知识库，微软前段时间发布的Recall，包括纹身图等能力、智能分身能力。

基于这两大类场景，我们看到业界主流的两种解决方案。第一种是纯云端SaaS解决方案，联想同事也分享了，纯SaaS解决方案在模型迭代速度、模型选择、模型质量上有固然优势，但在实验端云数据流转、模型选择、个人隐私方面有弊端。同时，在应用跨应用结合上，因为没有深入系统层，难做到跨应用结合。

第二种是本地小模型，类似AIPC，这在实验推理速度等方面有一定优势，包括数据安全性，但在模型迭代、模型性能、对端测性能的要求和消耗等方面不足显而易见。

无影想到自己的优势，在云端一体和AI结合上做出不一样的东西，我们想到了三个点。第一个总结成‘系统级和应用感知’，细分为三个点。第一个，深入系统层，跨应用端云一体控制云电脑或云手机各种行为。第二，我们有协议，可以实时感知用户行为，比如开了优酷，内容实时感知，同时对用户交互行为密切识别和感知，帮助用户做交互动作。第三，云端一体是云原生形态，不依赖本地算力，整体算力对类层消耗、存储资源消耗、算力消耗都在云端开资源承接，对用户来说算力无感，同时对模型调应实时性高。

我们打造了一款AP应用‘小影’，除了生产力场景、知识库场景、聊天场景外，还有四个稍微不一样的能力。第一个是微软的Recall能力，个人用户快速历史行为回溯和搜索，不占用电脑资源和存储空间，云端开空间。第二，实现快应用业务流程处理，帮助用户快速完成繁琐操作，如报销、提报大学资料。第三，实现端音协同管理实际行为，在管控台上实时智能化检测，提供快捷解决方案。最后，克服了端测性能开销，在任何端上体验相同，无兼容性问题。这是我们对产业化在大模型浪潮中的一些思考。

最后，从无影角度，我们未来会做好两个开放。第一个是把无影刚才说的三个能力开放给合作伙伴，让合作伙伴更好地基于这些能力在无影上打造AI生态。第二，让用户自己开放AI工具市场，用户自己选择。大概就是这样。

好的，我们特别期待无影这样的云原生云端一体产品借助大模型时代大大提高用户效率，在很多娱乐场景中提高用户体验。其实在过去的一年时间里，我们看到很多创新idea和更新想法来自开源社区。我们也想问周文猛，过去时间里，如何通过开源社区和大模型结合，通过开源工具提升开发者体验和大模型易用性，你怎么想？

摩大社区去年一整年围绕LM和AIGC两个核心技术，不断提供给社区开发者们，不管是大模型推理、微调或评测一系列工具链，让大家更好地把模型用起来。我们从用户角度考虑用户最多需求是什么，然后重点投入。现阶段80%到90%用户需求是直接调用模型，不管local调用模型或远程调用模型API，通过prompt engineering完成一些业务事情。因此，我们第一步做的是联合开源已有的推理引擎，包括VMLama CPP、Olama等工具，让用户方便地把模型用起来。第二步，在一些垂类场景用户需要自己的数据做模型fine-tuning、评测和上限。我们做了一些简化易用的评测和训练工具链，方便用户在垂类场景使用。另外在应用场景，有RAG场景。RAG场景现在也是非常主流场景，我们通过联合Lama CPP、Lama的index或Lanechain里的retrieval模块，让用户方便地把RAG链路搭起来。开源设计和生产上会有些不一样，生产上需要更多生产级别RAG链路的高并发支持，这我们通过联合阿里云云平台白链大模型开发平台提供更扩展RAG能力，方便企业级用户使用。主要是这些，谢谢。

好的，那今天的圆桌到此结束，谢谢大家。